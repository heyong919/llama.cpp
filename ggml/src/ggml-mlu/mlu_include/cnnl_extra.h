/*************************************************************************
 * Copyright (C) [2019-2024] by Cambricon, Inc.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
 * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
 * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
 * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 *************************************************************************/
#ifndef CNNL_EXTRA_H_
#define CNNL_EXTRA_H_

/******************************************************************************
 * CNNL_EXTRA: CNNL Extra Library
 ******************************************************************************/

#define CNNL_EXTRA_MAJOR 1
#define CNNL_EXTRA_MINOR 12
#define CNNL_EXTRA_PATCHLEVEL 3

#include "cnnl.h"

#ifndef CNNL_WIN_API
#ifdef _WIN32
#define CNNL_WIN_API __stdcall
#else
#define CNNL_WIN_API
#endif
#endif

// Disable warning messages for deprecated API-s by default
#ifndef CNNL_WARN_DEPRECATED
#define CNNL_WARN_DEPRECATED 0
#endif

#ifndef CNNL_DEPRECATED_FOR

#if CNNL_WARN_DEPRECATED

#define CNNL_PP_EVAL_SECOND_(a1, a2, ...) a2
#define CNNL_PP_EVAL_SECOND(...) CNNL_PP_EVAL_SECOND_(__VA_ARGS__)
#define CNNL_DEPRECATED_MSG0_() deprecated("Dropped support")
#define CNNL_DEPRECATED_MSG1_(f...) deprecated("Use '" #f "' instead")
#define CNNL_DEFER_MSG0_() _, CNNL_DEPRECATED_MSG0_
#define CNNL_DEPRECATED_MSG(...) \
    CNNL_PP_EVAL_SECOND(CNNL_DEFER_MSG0_ __VA_ARGS__(), CNNL_DEPRECATED_MSG1_, _)(__VA_ARGS__)

#if defined(__cplusplus) && __cplusplus >= 201309L
// C++14 feature [[deprecated(...)]]
#define CNNL_DEPRECATED_FOR(...) [[CNNL_DEPRECATED_MSG(__VA_ARGS__)]]
#define CNNL_DEPRECATED_ENUM_FOR CNNL_DEPRECATED_FOR
#else
#if defined(__has_extension)
#if __has_extension(attribute_deprecated_with_message)
#define CNNL_DEPRECATED_SUPPORT_MSG 1
#endif
#endif  // defined(__has_extension)

#if defined(__GNUC__) && (__GNUC__ > 4 || (__GNUC__ == 4 && __GNUC_MINOR__ >= 5))
#define CNNL_DEPRECATED_SUPPORT_MSG 1
#endif

#if CNNL_DEPRECATED_SUPPORT_MSG
#define CNNL_DEPRECATED_FOR(...) __attribute__((CNNL_DEPRECATED_MSG(__VA_ARGS__)))
#undef CNNL_DEPRECATED_SUPPORT_MSG
#elif defined(_MSC_VER)
#define CNNL_DEPRECATED_FOR(...) __declspec(CNNL_DEPRECATED_MSG(__VA_ARGS__))
#else
#define CNNL_DEPRECATED_FOR(f) __attribute__((deprecated))
#endif  // CNNL_DEPRECATED_SUPPORT_MSG

#if !defined(__clang__) && (defined(__GNUC__) && __GNUC__ < 6)
#define CNNL_DEPRECATED_ENUM_FOR(...)
#else
#define CNNL_DEPRECATED_ENUM_FOR CNNL_DEPRECATED_FOR
#endif

#endif  // defined(__cplusplus) && __cplusplus >= 201309L

#else   // CNNL_WARN_DEPRECATED == 0
#define CNNL_DEPRECATED_FOR(...)
#define CNNL_DEPRECATED_ENUM_FOR(...)
#endif  // CNNL_WARN_DEPRECATED

#endif  // CNNL_DEPRECATED_FOR

#if defined(__cplusplus)
extern "C" {
#endif

/******************************************************************************
 * Cambricon CNNL Data Structure: Detection Output
 ******************************************************************************/
/*!
 * @brief Enumeration variables describing the detection output algorithms that are used in the
 * implementation of the detection output operation.
 *
 */
typedef enum {
  CNNL_DETECTION_OUTPUT_SSD        = 0, /*!< The SSD kernel is implemented.*/
  CNNL_DETECTION_OUTPUT_YOLOV2     = 1, /*!< The YOLOv2 kernel is implemented.*/
  CNNL_DETECTION_OUTPUT_YOLOV3     = 2, /*!< The YOLOv3 kernel is implemented.*/
  CNNL_DETECTION_OUTPUT_FASTERRCNN = 3, /*!< The Faster R-CNN kernel is implemented.*/
  CNNL_DETECTION_OUTPUT_YOLOV4 = 4, /*!< The YOLOv4 kernel is implemented.*/
  CNNL_DETECTION_OUTPUT_YOLOV5 = 5, /*!< The YOLOv5 kernel is implemented.*/
  CNNL_DETECTION_OUTPUT_REFINEDET = 6, /*!< The REFINEDET kernel is implemented.*/
  CNNL_DETECTION_OUTPUT_RETINANET = 7, /*!< The RetinaNet kernel is implemented.*/
  CNNL_DETECTION_OUTPUT_PPYOLO = 8, /*!< The PPYOLO kernel is implemented.*/
  CNNL_DETECTION_OUTPUT_YOLOX = 9, /*!< The YOLOX kernel is implemented.*/
  CNNL_DETECTION_OUTPUT_YOLOV8 = 10, /*!< The YOLOV8 kernel is implemented.*/
} cnnlDetectionOutputAlgo_t;

#ifndef CNNL_QUANTIZE_LAYOUT_ENUM_DECLARED

/******************************************************************************
 * Cambricon CNNL quantization layout
 ******************************************************************************/
/*!
 * @brief Enumeration variables describing the layout of quantization.
 *
 * It is deprecated and will be removed in future release.
 * Use cnnlQuantizeScheme_t instead.
 */
typedef enum {
  CNNL_QUANTIZE_NONE = 0,  /*!< No quantization is applied.*/
  CNNL_QUANTIZE_PER_TENSOR = 1,  /*!< Quantization is applied by tensor.*/
  CNNL_QUANTIZE_PER_CHANNEL = 2,  /*!< Quantization is applied by channel.*/
  CNNL_QUANTIZE_PER_TOKEN = 3,  /*!< Quantization is applied by token.*/
  CNNL_QUANTIZE_GROUP_WISE = 4, /*!< Quantization is applied by group.*/
} cnnlQuantizeLayout_t;

#endif

/******************************************************************************
 * Cambricon CNNL quantization algorithm
 ******************************************************************************/
 /*!
 * @brief Enumeration variables describing the algorithm of quantization.
 */
typedef enum {
  CNNL_NO_QUANT = 0,
  /*!< No quantization is applied.*/
  CNNL_WEIGHT_ONLY = 1,
  /*!< Quantization is applied to weight only.*/
  CNNL_SMOOTH_QUANT = 2,
  /*!< Quantization is applied to weight and input in smoothQuant algorithm.*/
} cnnlLLMQuantAlgo_t;

/******************************************************************************
 * Cambricon CNNL Transformer Layernorm position
 ******************************************************************************/
/*!
 * @brief Enumeration variables describing the position of layernorm for transformer.
 */
typedef enum {
  CNNL_TRANSFORMER_NO_LAYERNORM = 0,  /*!< No layernorm is applied.*/
  CNNL_TRANSFORMER_PRE_LAYERNORM,  /*!< Layernorm is applied before the transformer operation.*/
  CNNL_TRANSFORMER_POST_LAYERNORM,  /*!< Layernorm is applied after the transformer operation.*/
} cnnlTransformerLayernormMode_t;

/******************************************************************************
 * Cambricon CNNL Transformer Normalization Type
 ******************************************************************************/
/*!
 * @brief Enumeration variables describing the types of normalization for transformer.
 */
typedef enum {
  CNNL_TRANSFORMER_LAYERNORM = 0,
  /*!< Layer normalization is applied.
   * The calculation formula of Layernorm is
   *     output = (input - mean(input)) / sqrt(var(input) + eps) * ln_scale + ln_bias.
   * Where, \p ln_scale and \p ln_bias is necessary.
   */
  CNNL_TRANSFORMER_RMSNORM = 1,
  /*!< RMS normalization is applied.
   * The calculation formula of RMS normalization is
   *     output = input / sqrt(mean(input ^ 2) + eps) * ln_scale + ln_bias.
   * Where, \p ln_scale or \p ln_bias is optional.
   */
  CNNL_TRANSFORMER_SCALENORM = 2,
  /*!< Scale normalization is applied.
   * The calculation formula of Scale normalization is
   *     output = input / sqrt(sum(input ^ 2) + eps) * ln_scale.
   * Where, the element number of \p ln_scale should be 1.
   */
} cnnlTransformerNormType_t;

/******************************************************************************
 * Cambricon CNNL TransformerAttention Position Embedding type
 ******************************************************************************/
/*!
 * @brief Enumeration variables describing the type of position embedding for transformer attention.
 */
typedef enum {
  CNNL_ATTN_NO_POSITION_EMBEDDING = 0,
  /*!< Performs the operation without position embedding.*/
  CNNL_ATTN_CROSS_ROTARY_EMBEDDING = 1,
  /*!< Performs the operation with cross rotary embedding type (for GPT-J).*/
  CNNL_ATTN_FOLD_ROTARY_EMBEDDING = 2,
  /*!< Performs the operation with folded rotary embedding type (for GPT-NEOX).*/
  CNNL_ATTN_RELATIVE_POSITION_EMBEDDING = 3,
  /*!< Performs the operation with relative position embedding type.*/
  CNNL_ATTN_ALIBI_POSITION_EMBEDDING = 4,
  /*!< Performs the operation with ALiBi embedding type.*/
  CNNL_ATTN_CROSS_ROTARY_EMBEDDING_2D = 5,
  /*!< Performs the operation with 2D cross rotary embedding type.*/
  CNNL_ATTN_FOLD_ROTARY_EMBEDDING_2D = 6,
  /*!< Performs the operation with 2D folded rotary embedding type.*/
} cnnlTransformerAttentionPositionEmbeddingType_t;

/******************************************************************************
 * Cambricon CNNL Transformer Layernorm And Residual Structure
 ******************************************************************************/
/*! @brief Enumeration variables describing the implementation types of layernorm and residual
 *  for the building block of transformer network.
 */
typedef enum {
  CNNL_TRANSFORMER_NO_LAYERNORM_NO_RESIDUAL = 0,
  /*!< Performs the operation without layernorm and residual.
       Structure is as follows:
       @verbatim
        [----(op)----]
       @endverbatim */
  CNNL_TRANSFORMER_NO_LAYERNORM_WITH_RESIDUAL = 1,
  /*!< Performs the operation without layernorm and with residual.
       Structure is as follows:
       @verbatim
                        -------------
                       |             |
                  [---------(op)----(+)----]
       @endverbatim */
  CNNL_TRANSFORMER_PRE_LAYERNORM_NO_RESIDUAL = 2,
  /*!< Performs layernorm before the operation and without residual.
       Structure is as follows:
       @verbatim
        [----(ln)----(op)----]
       @endverbatim */
  CNNL_TRANSFORMER_POST_LAYERNORM_NO_RESIDUAL = 3,
  /*!< Performs layernorm after the operation and without residual.
       Structure is as follows:
       @verbatim
        [----(op)----(ln)----]
       @endverbatim */
  CNNL_TRANSFORMER_PRE_LAYERNORM_INSIDE_RESIDUAL = 4,
  /*!< Performs layernorm before the operation with layernorm inside residual structure.
       Structure is as follows:
       @verbatim
                        ---------------------
                       |                     |
                  [---------(ln)----(op)----(+)----]
       @endverbatim */
  CNNL_TRANSFORMER_PRE_LAYERNORM_OUTSIDE_RESIDUAL = 5,
  /*!< Performs layernorm before the operation with layernorm outside residual structure.
       Structure is as follows:
       @verbatim
                                -------------
                               |             |
                  [----(ln)---------(op)----(+)----]
       @endverbatim */
  CNNL_TRANSFORMER_POST_LAYERNORM_INSIDE_RESIDUAL = 6,
  /*!< Performs layernorm after the operation with layernorm inside residual structure.
       Structure is as follows:
       @verbatim
                        ---------------------
                       |                     |
                  [---------(op)----(ln)----(+)----]
       @endverbatim */
  CNNL_TRANSFORMER_POST_LAYERNORM_OUTSIDE_RESIDUAL = 7,
  /*!< Performs layernorm after the operation with layernorm outside residual structure.
       Structure is as follows:
       @verbatim
                        -------------
                       |             |
                  [---------(op)----(+)----(ln)----]
       @endverbatim */
} cnnlTransformerLayernormResidualStructure_t;

/******************************************************************************
 * Cambricon CNNL Transformer data order mode for layout
 ******************************************************************************/
/*! @brief Enumeration variables describing the data order of the filter matrix.
 */
typedef enum {
  CNNL_TRANSFORMER_MATRIX_ORDER_ROW = 0, /*!< Row is the first in the matrix.*/
  CNNL_TRANSFORMER_MATRIX_ORDER_COL,  /*!< Column is the first in the matrix.*/
} cnnlTransformerMatrixOrder_t;

/******************************************************************************
 * Cambricon CNNL ROI code type
 ******************************************************************************/
/*! @brief Enumeration variables describing the types of ROI (region of interest). */
typedef enum {
  CNNL_ROI_CORNER = 0,
  /*!< The origin of ROI is in the corner of the image.
   *   The order of ROI elements is [roi_x_start, roi_y_start, roi_x_end, roi_y_end].*/
  CNNL_ROI_CORNER_BATCHID = 1,
  /*!< The origin of ROI is in the corner of the image.
   *   The order of ROI elements is [roi_x_start, roi_y_start, roi_x_end, roi_y_end, batch_id].*/
  CNNL_ROI_BATCHID_CORNER = 2,
  /*!< The origin of ROI is in the corner of the image.
   *   The order of ROI elements is [batch_id, roi_x_start, roi_y_start, roi_x_end, roi_y_end].*/
  CNNL_ROI_CENTER = 3,
  /*!< The origin of ROI is in the center of the image.
   *   The order of ROI elements is [roi_x_start, roi_y_start, roi_x_end, roi_y_end].*/
  CNNL_ROI_CENTER_BATCHID = 4,
  /*!< The origin of ROI is in the center of the image.
   *   The order of ROI elements is [roi_x_start, roi_y_start, roi_x_end, roi_y_end, batch_id].*/
  CNNL_ROI_BATCHID_CENTER = 5,
  /*!< The origin of ROI is in the center of the image.
   *   The order of ROI elements is [batch_id, roi_x_start, roi_y_start, roi_x_end, roi_y_end].*/
  CNNL_ROI_CORNER_SCORE = 6,
  /*!< The origin of ROI is in the corner of the image.
   *   The order of ROI elements is [roi_x_start, roi_y_start, roi_x_end, roi_y_end, score].*/
  CNNL_ROI_SCORE_CORNER = 7,
  /*!< The origin of ROI is in the corner of the image.
   *   The order of ROI elements is [score, roi_x_start, roi_y_start, roi_x_end, roi_y_end].*/
  CNNL_ROI_CENTER_SCORE = 8,
  /*!< The origin of ROI is in the center of the image.
   *   The order of ROI elements is [roi_x_start, roi_y_start, roi_x_end, roi_y_end, score].*/
  CNNL_ROI_SCORE_CENTER = 9,
  /*!< The origin of ROI is in the center of the image.
   *   The order of ROI elements is [score, roi_x_start, roi_y_start, roi_x_end, roi_y_end].*/
} cnnlRoiLayoutType_t;

/******************************************************************************
 * Cambricon CNNL NMS type
 ******************************************************************************/
/*! @brief Enumeration variables describing the NMS (Non-Max Suppression) modes.
 */
typedef enum {
  CNNL_DEFAULT_NMS = 0,
  /*!< The multi-label traditional NMS mode is applied.
   *   The Intersection over Union (IoU) is used to compare the thresh.*/
  CNNL_GREEDY_NMS  = 1,
  /*!< The multi-label greedy NMS mode is applied.
   *   The calculation formula is: IoU - pow(d/c, beta),
   *   where \p d represents the square of the distance between two center points of box,
   *   and \p c represents the square of the minimum diagonal length of circumscribed
   *   rectangle of two boxes.
   *   The beta equals to 0.6.*/
  CNNL_DIOU_NMS  = 2,
  /*!< The multi-label Distance-IoU (DIoU) NMS mode is applied.
   *   The calculation formula is: IoU - pow(d/c, beta),
   *   where \p d represents the square of the distance between two center points of box,
   *   and \p c represents the square of the minimum diagonal length of circumscribed
   *   rectangle of two box.
   *   The beta is index part of the ratio of d to c.*/
  CNNL_DEFAULT_SINGLE_LABEL_NMS = 3,
  /*!< The single label traditional NMS type is applied. The loss function Intersection
   *   to Union (IoU) is used to compare the thresh.*/
} cnnlNmsType_t;

/******************************************************************************
 * Cambricon CNNL Masked Softmax Type
 ******************************************************************************/
/*! @brief Enumeration variables describing the modes of ::cnnlMaskedSoftmax
 */
typedef enum {
  CNNL_MASKED_SOFTMAX_NO_MASK = 0,
  /*!< No mask is applied.
   *   The calculation formula is: y = softmax(x * scale).
   *   Multiplies input by the scalar \p scale and computes a softmax on the result. */
  CNNL_MASKED_SOFTMAX_ADD_MASK = 1,
  /*!< Add mask mode is applied.
   *   The calculation formula is: y = softmax(x * scale + mask).
   *   Multiplies input by the scalar \p scale and computes a softmax with mask on the result.
   *   The mask is calculated by adding the intermediate result \p x * \p scale. */
  CNNL_MASKED_SOFTMAX_UPPER_TRIANGLE_MASK = 2,
  /*!< Upper triangle mask mode is applied.
   *   The calculation formula is: y = softmax(maskUpperTriangle(x * scale)).
   *   maskUpperTriangle means that it returns the lower triangular part of the matrices \p x
   *   and the other elements of the result tensor are set to -10000. */
  CNNL_MASKED_SOFTMAX_MASKED_FILL = 3,
  /*!< Masked fill mode is applied.
   *   The calculation formula is: y = softmax(masked_fill(x * scale, mask, -10000.)).
   *   masked_fill means that it fills the elements of \p x with \p mask where \p mask is true.
   */
  CNNL_MASKED_SOFTMAX_UPPER_TRIANGLE_MASK_NEG_INF = 4,
  /*!< Upper triangle mask neg inf mode is applied.
   *   The calculation formula is: y = softmax(maskUpperTriangle(x * scale)).
   *   maskUpperTriangle means that it returns the lower triangular part of the matrices \p x
   *   and the other elements of the result tensor are set to -inf. */
  CNNL_MASKED_SOFTMAX_MASKED_FILL_NEG_INF = 5,
  /*!< Masked fill mode is applied.
   *   The calculation formula is: y = softmax(masked_fill(x * scale, mask, -inf)).
   *   masked_fill means that it fills the elements of \p x with \p mask where \p mask is true.
   */
} cnnlMaskedSoftmaxOp_t;

/******************************************************************************
 * Cambricon CNNL TreeEnsemble post mode type
 ******************************************************************************/
/*!
 * @brief
 * Enumeration variables describing the post modes applied to the output in the ::cnnlTreeEnsemble function.
 *
 */
typedef enum {
  CNNL_TE_NONE = 0,
  /*!< No post progress is applied in the operation.*/
  CNNL_TE_SOFTMAX  = 1,
  /*!< The softmax post progress is applied in the operation.*/
  CNNL_TE_LOGISTIC = 2,
  /*!< The logistic post progress is applied in the operation.*/
  CNNL_TE_SOFTMAX_ZERO  = 3,
  /*!< The softmax_zero post progress is applied in the operation.*/
  CNNL_TE_PROBIT  = 4,
  /*!< The probit post progress is applied in the operation.*/
  CNNL_TE_AVERAGE  = 5,
  /*!< The average-calculating of output is applied in the operation.*/
  CNNL_TE_SUM  = 6,
  /*!< The sum-calculating of output is applied in the operation.*/
  CNNL_TE_MIN  = 7,
  /*!< The min-calculating of output is applied in the operation.*/
  CNNL_TE_MAX  = 8,
  /*!< The max-calculating of output is applied in the operation.*/
} cnnlTreeEnsemblePostMode_t;

/******************************************************************************
 * Cambricon CNNL TreeEnsemble comparison mode type
 ******************************************************************************/
/*!
 * @brief
 * Enumeration variables describing the comparison modes applied to nodes in the ::cnnlTreeEnsemble function.
 *
 */
typedef enum {
  CNNL_TE_BRANCH_DIFF = 0,
  /*!< All nodes share the same comparison mode in the operation.*/
  CNNL_TE_BRANCH_LEQ = 1,
  /*!< Whether feature value is less than or equal to node value.*/
  CNNL_TE_BRANCH_LT = 2,
  /*!< Whether feature value is less than node value.*/
  CNNL_TE_BRANCH_GTE = 3,
  /*!< Whether feature value is greater than or equal to node value.*/
  CNNL_TE_BRANCH_GT = 4,
  /*!< Whether feature value is greater than node value.*/
  CNNL_TE_BRANCH_EQ = 5,
  /*!< Whether feature value is equal to node value.*/
  CNNL_TE_BRANCH_NEQ = 6,
  /*!< Whether feature value is not equal to node value.*/
  CNNL_TE_BRANCH_MEMBER  = 7,
  /*!< Whether comparison mode is a member of the set.*/
} cnnlTreeEnsembleNodeMode_t;

// Group: Version Management
/*!
 * @brief Retrieves the version of Cambricon CNNL_EXTRA library. The version of Cambricon CNNL_EXTRA
 * is composed of \p major, \p minor and \p patch. For instance, major = 1, minor = 2, patch = 3,
 * the version of Cambricon CNNL_EXTRA library is 1.2.3.
 *
 * @param[in] major
 * Input. A pointer to host memory that stores the major version of Cambricon CNNL_EXTRA library.
 * @param[in] minor
 * Input. A pointer to host memory that stores the minor version of Cambricon CNNL_EXTRA library.
 * @param[in] patch
 * Input. A pointer to host memory that stores the patch version of Cambricon CNNL_EXTRA library.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
void cnnlExtraGetLibVersion(int* major,
                            int* minor,
                            int* patch);

/******************************************************************************
 * Cambricon CNNL-Extra Debugging
 ******************************************************************************/
// Group:Debugging
/*!
 * @brief Sets the mode of the Cambricon CNNL-Extra debugging tool that can generate operator
 * information files for all the operators that are called. The generated file
 * contains the operator information including inputs shapes, outputs shapes,
 * parameters and inputs real data based on the settings of \p mode.
 *
 * @param[in] mode
 * Input. The mode of the debugging tool.
 * - When \p mode is set to 0, the tool will be disabled and no
 *   operator information files will be generated.
 * - When \p mode is set to 1, the tool will generate operator
 *   information files for all the operators that are called, without the real
 *   data of the inputs of the operators included in the files.
 * - When \p mode is set to 2, the tool will generate operator
 *   information files for all the operators that are called. Only part of the inputs real
 *   data of the operators is included in the files. If the environment variable
 *   CNNL_GEN_CASE_DUMP_DATA is set to 1, all of the inputs real data of the operators will be
 *   included in the files. For more information about setting environment variable,
 *   see "Cambricon CNNL User Guide".
 * - When \p mode is set to 3, the tool will print operator
 *   information on the screen without the inputs real data for all the operators
 *   that are called instead generating information files.
 *
 * @note
 * - When \p mode is out of range [0, 3], the tool will be disabled,
 *   and no operator information files are generated.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
void cnnlExtraSetGenCaseMode(int mode);

/******************************************************************************
 * Cambricon CNNL Data Structure: Descriptor
 * The struct represent the quantization information of input and filter
 ******************************************************************************/
/*!
 *  @brief The descriptor that holds quantization information including
 *  \p mode, \p layout, \p position, \p scale, \p offset and \p quant_bit_size.
 *  You need to call the ::cnnlCreateQuantizeDescriptor function to create a descriptor,
 *  and call the ::cnnlSetQuantizeDescriptor function to set the information of the descriptor.
 *  Also, you need to destroy the Cambricon CNNL context at the end with the ::cnnlDestroyQuantizeDescriptor
 *  function.
 *
 *  It is deprecated and will be removed in future release.
 *  Use cnnlQuantizeExStruct instead.
 */
typedef struct cnnlQuantizeStruct* cnnlQuantizeDescriptor_t;

// Group:Quantization
/*!
 * @brief Sets the quantization descriptor.
 *
 * @deprecated
 *   This function is deprecated and will be removed in future release.
 *   Use cnnlSetQuantizeExDescriptor instead.
 *
 * @param[in,out] desc
 *   Input/output. The quantization descriptor of the tensor. For detailed information,
 *   see ::cnnlQuantizeDescriptor_t.
 * @param[in] mode
 *   Input. The quantization mode.
 *   For details, see cnnlQuantizeMode_t in Cambricon CNNL Developer Guide.
 * @param[in] layout
 *   Input. The quantization layout.
 * @param[in] position
 *   Input. A scalar of fixed position factor that is used for quantization.
 * @param[in] scale
 *   Input. A scalar of scale factor that is used for quantization.
 * @param[in] offset
 *   Input. A scalar of offset factor that is used for quantization.
 * @param[in] dev_position
 *   Input. A scalar array of fixed position factor that is used for quantization.
 * @param[in] dev_scale
 *   Input. A scalar array of scale factor that is used for quantization.
 * @param[in] dev_offset
 *   Input. A scalar array of offset factor that is used for quantization.
 * @param[in] quant_bit_size
 *   Input. A scalar of bit size that is used for quantization.
 * @param[in] fake_quant
 *   Input. A Boolean value indicating whether QAT (Quantization-Aware Training) mode is used for quantization.
 * @retval CNNL_STATUS_SUCCESS
 *   The function ends normally.
 * @retval CNNL_STATUS_BAD_PARAM
 *   \p desc is nullptr.
 */
CNNL_DEPRECATED_FOR(cnnlSetQuantizeExDescriptor)
cnnlStatus_t CNNL_WIN_API cnnlSetQuantizeDescriptor(cnnlQuantizeDescriptor_t desc,
                                                    cnnlQuantizeMode_t mode,
                                                    cnnlQuantizeLayout_t layout,
                                                    int position,
                                                    float scale,
                                                    int offset,
                                                    const void* dev_position,
                                                    const void* dev_scale,
                                                    const void* dev_offset,
                                                    int quant_bit_size,
                                                    bool fake_quant);

// Group:Quantization
/*!
 * @brief Sets the quantization descriptor.
 *
 * @deprecated
 *   This function is deprecated and will be removed in future release.
 *   Use cnnlSetQuantizeExDescriptor instead.
 *
 * @param[in,out] desc
 *   Input/output. The quantization descriptor of the tensor. For detailed information,
 *   see ::cnnlQuantizeDescriptor_t.
 * @param[in] mode
 *   Input. The quantization mode.
 *   For details, see cnnlQuantizeMode_t in Cambricon CNNL Developer Guide.
 * @param[in] layout
 *   Input. The quantization layout.
 * @param[in] position
 *   Input. A scalar of fixed position factor that is used for quantization.
 * @param[in] scale
 *   Input. A scalar of scale factor that is used for quantization.
 * @param[in] offset
 *   Input. A scalar of offset factor that is used for quantization.
 * @param[in] position_desc
 *   Input. The descriptor of position tensor.
 * @param[in] dev_position
 *   Input. A scalar array of fixed position factor that is used for quantization.
 * @param[in] scale_desc
 *   Input. The descriptor of scale tensor.
 * @param[in] dev_scale
 *   Input. A scalar array of scale factor that is used for quantization.
 * @param[in] offset_desc
 *   Input. The descriptor of offset tensor.
 *   Asymmetric quantization is not supported currently. This parameter must be nullptr.
 * @param[in] dev_offset
 *   Input. A scalar array of offset factor that is used for quantization.
 *   Asymmetric quantization is not supported currently. This parameter must be nullptr.
 * @param[in] quant_bit_size
 *   Input. A scalar of bit size that is used for quantization.
 * @param[in] exponent_bit_size
 *   Input. A scalar used to represent the exponential bit size. Reserved for future use.
 * @param[in] fake_quant
 *   Input. A Boolean value indicating whether QAT (Quantization-Aware Training) mode is used for quantization.
 * @retval CNNL_STATUS_SUCCESS
 *   The function ends normally.
 * @retval CNNL_STATUS_BAD_PARAM
 *   \p desc is nullptr.
 */
CNNL_DEPRECATED_FOR(cnnlSetQuantizeExDescriptor)
cnnlStatus_t CNNL_WIN_API
cnnlSetQuantizeDescriptor_v2(cnnlQuantizeDescriptor_t desc,
                             cnnlQuantizeMode_t mode,
                             cnnlQuantizeLayout_t layout,
                             int position,
                             float scale,
                             int offset,
                             const cnnlTensorDescriptor_t position_desc,
                             const void *dev_position,
                             const cnnlTensorDescriptor_t scale_desc,
                             const void *dev_scale,
                             const cnnlTensorDescriptor_t offset_desc,
                             const void *dev_offset,
                             int quant_bit_size,
                             int exponent_bit_size,
                             bool fake_quant);

// Group:Quantization
/*!
 * @brief Sets the quantization descriptor.
 *
 * @deprecated
 *   This function is deprecated and will be removed in future release.
 *   Use cnnlSetQuantizeExDescriptorModeAndDtype instead.
 *
 * @param[in,out] quant_desc
 *   Input/output. The quantization descriptor of the tensor. For detailed information,
 *   see ::cnnlQuantizeDescriptor_t.
 * @param[in] layout
 *   Input. The quantization layout.
 * @param[in] quant_bit_size
 *   Input. A scalar of bit size that is used for quantization.
 * @param[in] exponent_bit_size
 *   Input. A scalar used to represent the exponential bit size. Reserved for future use.
 * @param[in] fake_quant
 *   Input. A Boolean value indicating whether QAT (Quantization-Aware Training) mode is used for quantization.
 * @retval CNNL_STATUS_SUCCESS
 *   The function ends normally.
 * @retval CNNL_STATUS_BAD_PARAM
 *   \p quant_desc is nullptr.
 */
CNNL_DEPRECATED_FOR(cnnlSetQuantizeExDescriptorModeAndDtype)
cnnlStatus_t CNNL_WIN_API
cnnlSetQuantizeDescriptorBehaviour(cnnlQuantizeDescriptor_t quant_desc,
                                   cnnlQuantizeLayout_t layout,
                                   int quant_bit_size,
                                   int exponent_bit_size,
                                   bool fake_quant);

// Group:Quantization
/*!
 *  @brief Creates a descriptor pointed by \p desc
 *  for the quantization operation.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use cnnlCreateQuantizeExDescriptor instead.
 *
 *  @param[out] desc
 *   Output. A pointer to the quantization descriptor that
 *   holds information about the quantization operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_ALLOC_FAILED
 *    \p desc is NULL.
 */
CNNL_DEPRECATED_FOR(cnnlCreateQuantizeExDescriptor)
cnnlStatus_t CNNL_WIN_API cnnlCreateQuantizeDescriptor(cnnlQuantizeDescriptor_t *desc);

// Group:Quantization
/*!
 *  @brief Destroys a quantization descriptor.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use cnnlDestroyQuantizeExDescriptor instead.
 *
 *  @param[in] desc
 *    Input.  A pointer to the struct of the quantization descriptor.
 *  @retval CNNL_STATUS_SUCCESS
 *    - The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    - \p desc is NULL.
 */
CNNL_DEPRECATED_FOR(cnnlDestroyQuantizeExDescriptor)
cnnlStatus_t CNNL_WIN_API cnnlDestroyQuantizeDescriptor(cnnlQuantizeDescriptor_t desc);

/******************************************************************************
 * Cambricon CNNL OP: detection output
 ******************************************************************************/
/*! The descriptor of the ::cnnlDetectionOutput_v2 operation.
 *
 */
typedef struct cnnlDetectionOutputStruct *cnnlDetectionOutputDescriptor_t;

// Group:Detection Output
/*!
 *  @brief Creates a descriptor pointed by \p detection_output_desc
 *  for the detection output operation.
 *
 *  @param[out] detection_output_desc
 *   Output. A pointer to the detection_output descriptor that holds
 *   information about the detection output operation.
 *
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *      - \p detection_output_desc is NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCreateDetectionOutputDescriptor(cnnlDetectionOutputDescriptor_t *detection_output_desc);

// Group:Detection Output
/*!
 *  @brief Sets Yolov3 detection output descriptor with parameters.
 *
 *  @param[in,out] desc
 *    Input/output.  A pointer to the struct of detection output descriptor.
 *  @param[in] mode
 *    Input.  The algorithm of the detection output operation. Must be \p CNNL_DETECTION_OUTPUT_YOLOV3.
 *  @param[in] num_batch
 *    Input.  The number of batches for each feature map. Can be any positive integer as long as the input
 *            tensor size is less than 2GB.
 *  @param[in] num_input
 *    Input.  The number of input feature maps. Must belong to [1, 7].
 *  @param[in] num_anchor
 *    Input.  The number of anchors, which are assumed same for all input feature maps.
 *            Must belong to [1, 8].
 *  @param[in] num_class
 *    Input.  The number of possible classes of each detected object.
 *            Must belong to [1, 1000].
 *  @param[in] num_entry
 *    Input.  The number of entries for every predicted point in a feature
 *            map. Must be 5, where \p 5 represents x, y, w, h and confidence respectively.
 *  @param[in] num_keep
 *    Input.  The largest possible number of bounding boxes. Must belong to [1, 2048].
 *  @param[in] height_image
 *    Input.  The height of detection network input image. Must be greater than 0.
 *  @param[in] width_image
 *    Input.  The width of detection network input image. Must be greater than 0.
 *  @param[in] clip
 *    Input.  An integer indicating whether to enable clipping when decoding.
 *    \p 0 means disable and \p 1 means enable.
 *  @param[in] scale_x_y
 *    Input.  The scale factor for scaling boxes center when decoding. Must be greater than 0.
 *  @param[in] unnormalize_scale_h
 *    Input.  The scale factor for scaling boxes height. Must be greater than or equal to 1.
 *  @param[in] unnormalize_scale_w
 *    Input.  The scale factor for scaling boxes width. Must be greater than or equal to 1.
 *  @param[in] thresh_confidence
 *    Input.  The minimal threshold for marking a box as an object. Must belong to [0, 1].
 *  @param[in] thresh_nms
 *    Input.  The minimal threshold for marking and eliminating a duplicate box.
 *            Must belong to [0, 1].
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - \p desc is NULL.
 *    - \p mode is not \p CNNL_DETECTION_OUTPUT_YOLOV3.
 *    - Parameters do not meet requirements.
 *
 *  @par API Dependency
 *  - You need to call ::cnnlCreateDetectionOutputDescriptor() to create
 *    a descriptor before using this function.
 *  - You need to call ::cnnlDestroyDetectionOutputDescriptor() to destroy
 *    the descriptor when cnnlDetectionOutputDescriptor_t is no longer needed.
 */

cnnlStatus_t CNNL_WIN_API
cnnlSetYolov3DetectionOutputDescriptor_v2(cnnlDetectionOutputDescriptor_t desc,
                                          const cnnlDetectionOutputAlgo_t mode,
                                          const int num_batch,
                                          const int num_input,
                                          const int num_anchor,
                                          const int num_class,
                                          const int num_entry,
                                          const int num_keep,
                                          const int height_image,
                                          const int width_image,
                                          const int clip,
                                          const float scale_x_y,
                                          const float unnormalize_scale_h,
                                          const float unnormalize_scale_w,
                                          const float thresh_confidence,
                                          const float thresh_nms);

// Group:Detection Output
/*!
 *  @brief Sets Yolov4 detection output descriptor with parameters.
 *
 *  @param[in,out] desc
 *    Input/output.  A pointer to the object of detection_output descriptor.
 *  @param[in] mode
 *    Input.  The algorithm of the detection_output operation. Must be \p CNNL_DETECTION_OUTPUT_YOLOV4.
 *  @param[in] num_batch
 *    Input.  The number of batches for each feature map. Can be any positive integer as long as the input
 *            tensor size is less than 2GB.
 *  @param[in] num_input
 *    Input.  The number of input feature maps. Must belong to [1, 7].
 *  @param[in] num_anchor
 *    Input.  The number of anchors, which is assumed same for all input feature maps.
 *            Must belong to [1, 8].
 *  @param[in] num_class
 *    Input.  The number of possible classes of each detected object.
 *            Must belong to [1, 1000].
 *  @param[in] num_entry
 *    Input.  The number of entries for every predicted point in a feature
 *            map. Must be 5, where \p 5 represents x, y, w, h and confidence respectively.
 *  @param[in] num_keep
 *    Input.  The largest possible number of bounding boxes. Must belong to [1, 2048].
 *  @param[in] height_image
 *    Input.  The height of detection network input image. Must be greater than 0.
 *  @param[in] width_image
 *    Input.  The width of detection network input image. Must be greater than 0.
 *  @param[in] thresh_confidence
 *    Input.  The minimal threshold for marking a box as an object. Must belong to [0, 1].
 *  @param[in] thresh_nms
 *    Input.  The minimal threshold for marking and eliminating a duplicate box.
 *            Must belong to [0, 1].
 *  @param[in] scale_x_y_ptr
 *    Input.  The pointer of the scaling factor of each input data. The size equals to \p num_input.
 *  @param[in] beta
 *    Input.  The index part of the pow(d/c, beta) in the \p CNNL_GREEDY_NMS and
 *            \p CNNL_DIOU_NMS mode, where \p d denotes the square of the distance
 *            between two center points of a box, and \p c denotes the square of
 *            the minimum diagonal length of circumscribed rectangle of two boxes.
 *            In the \p CNNL_GREEDY_NMS mode, \p beta is always equal to 0.6.
 *  @param[in] nms_type
 *    Input.  The NMS mode. See ::cnnlNmsType_t for details.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - \p desc is NULL.
 *    - \p mode is not CNNL_DETECTION_OUTPUT_YOLOV4.
 *    - Parameters do not meet requirements.
 *
 *  @par API Dependency
 *  - You need to call ::cnnlCreateDetectionOutputDescriptor() to create a descriptor
 *    before using this function.
 *  - You need to call ::cnnlDestroyDetectionOutputDescriptor() to destroy the descriptor
 *    when the descriptor is no longer needed.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetYolov4DetectionOutputDescriptor(cnnlDetectionOutputDescriptor_t desc,
                                       cnnlDetectionOutputAlgo_t mode,
                                       int num_batch,
                                       int num_input,
                                       int num_anchor,
                                       int num_class,
                                       int num_entry,
                                       int num_keep,
                                       int height_image,
                                       int width_image,
                                       float thresh_confidence,
                                       float thresh_nms,
                                       const float *scale_x_y_ptr,
                                       float beta,
                                       cnnlNmsType_t nms_type);

// Group:Detection Output
/*!
 *  @brief Sets the YOLOv5 detection output descriptor with parameters.
 *
 *  @param[in,out] desc
 *    Input/output.  A pointer to the object of detection_output descriptor.
 *  @param[in] mode
 *    Input.  The algorithm of the detection_output operation. Must be \p CNNL_DETECTION_OUTPUT_YOLOV5.
 *  @param[in] num_batch
 *    Input.  The number of batches for each feature map. Can be any positive integer as long as the input
 *            tensor size is less than 2GB.
 *  @param[in] num_input
 *    Input.  The number of input feature maps. Must belong to [1, 7].
 *  @param[in] num_anchor
 *    Input.  The number of anchors, which is assumed same for all input feature maps.
 *            Must belong to [1, 8].
 *  @param[in] num_class
 *    Input.  The number of possible classes of each detected object.
 *            Must belong to [1, 1000].
 *  @param[in] num_entry
 *    Input.  The number of entries for every predicted point in a feature
 *            map. Must be 5, where \p 5 represents x, y, w, h and confidence respectively.
 *  @param[in] num_keep
 *    Input.  The largest possible number of bounding boxes. Must belong to [1, 2048].
 *  @param[in] height_image
 *    Input.  The height of detection network input image. Must be greater than 0.
 *  @param[in] width_image
 *    Input.  The width of detection network input image. Must be greater than 0.
 *  @param[in] thresh_confidence
 *    Input.  The minimal threshold for marking a box as an object. Must belong to [0, 1].
 *  @param[in] thresh_nms
 *    Input.  The minimal threshold for marking and eliminating a duplicate box.
 *            Must belong to [0, 1].
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - \p desc is NULL.
 *    - \p mode is not CNNL_DETECTION_OUTPUT_YOLOV5.
 *    - Parameters do not meet requirements.
 *
 *  @par API Dependency
 *  - You need to call ::cnnlCreateDetectionOutputDescriptor to create a descriptor
 *    before using this function.
 *  - You need to call ::cnnlDestroyDetectionOutputDescriptor to destroy the descriptor
 *    when the descriptor is no longer needed.
 */

cnnlStatus_t CNNL_WIN_API
cnnlSetYolov5DetectionOutputDescriptor(cnnlDetectionOutputDescriptor_t desc,
                                       cnnlDetectionOutputAlgo_t mode,
                                       int num_batch,
                                       int num_input,
                                       int num_anchor,
                                       int num_class,
                                       int num_entry,
                                       int num_keep,
                                       int height_image,
                                       int width_image,
                                       float thresh_confidence,
                                       float thresh_nms);

// Group:Detection Output
/*!
 *  @brief Sets the YOLOX detection output descriptor with parameters.
 *
 *  @param[in,out] desc
 *    Input/output.  A pointer to the object of detection_output descriptor.
 *  @param[in] mode
 *    Input.  The algorithm of the detection_output operation. Must be \p CNNL_DETECTION_OUTPUT_YOLOX.
 *  @param[in] num_batch
 *    Input.  The number of batches for each feature map. Can be any positive integer as long as the
 *            input tensor size is less than 2GB.
 *  @param[in] num_input
 *    Input.  The number of input feature maps. Must be 3.
 *  @param[in] num_class
 *    Input.  The number of possible classes of each detected object.
 *            Must belong to [1, 1000].
 *  @param[in] num_entry
 *    Input.  The number of entries for every predicted point in a feature
 *            map. Must be 5, where \p 5 represents x, y, w, h and confidence respectively.
 *  @param[in] num_keep
 *    Input.  The largest possible number of bounding boxes. Must be num_batch * 4096.
 *  @param[in] height_image
 *    Input.  The height of detection network input image. Must be greater than 0.
 *  @param[in] width_image
 *    Input.  The width of detection network input image. Must be greater than 0.
 *  @param[in] unnormalize_scale_w
 *    Input.  The scale ratio between the width of network and origin image. Must be greater than 0.
 *  @param[in] unnormalize_scale_h
 *    Input.  The scale ratio between the height of network and origin image. Must be greater than 0.
 *  @param[in] thresh_confidence
 *    Input.  The minimal threshold for marking a box as an object. Must belong to [0, 1].
 *  @param[in] thresh_nms
 *    Input.  The minimal threshold for marking and eliminating a duplicate box.
 *            Must belong to [0, 1].
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - \p desc is NULL.
 *    - \p mode is not CNNL_DETECTION_OUTPUT_YOLOX.
 *    - Parameters do not meet requirements.
 *
 *  @par API Dependency
 *  - You need to call ::cnnlCreateDetectionOutputDescriptor to create a descriptor
 *    before using this function.
 *  - You need to call ::cnnlDestroyDetectionOutputDescriptor to destroy the descriptor
 *    when the descriptor is no longer needed.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetYoloxDetectionOutputDescriptor(cnnlDetectionOutputDescriptor_t desc,
                                      cnnlDetectionOutputAlgo_t mode,
                                      int num_batch,
                                      int num_input,
                                      int num_class,
                                      int num_entry,
                                      int num_keep,
                                      int height_image,
                                      int width_image,
                                      float unnormalize_scale_w,
                                      float unnormalize_scale_h,
                                      float thresh_confidence,
                                      float thresh_nms);
// Group:Detection Output
/*!
 *  @brief Sets the YOLOv2 detection output descriptor with parameters.
 *
 *  @param[in]  detection_output_desc
 *    Input.  A pointer to the object of detection_output descriptor.
 *  @param[in]  mode
 *    Input.  The algorithm of the detection_output operation. Must be \p CNNL_DETECTION_OUTPUT_YOLOV2.
 *  @param[in] num_class
 *    Input.  The number of possible classes of each detected object.
 *            Must belong to [1, 1000].
 *  @param[in] num_anchor
 *    Input.  The number of anchors, assuming same for all input feature maps.
 *            Must belong to [1, 8].
 *  @param[in] num_coords
 *    Input. The coordination numbers of a bounding box. Must be 4.
 *  @param[in] num_batch
 *    Input.  The number of batches for each feature map. Can be any positive integer as long as the input
 *            tensor size is less than 2GB.
 *  @param[in] confidence_thresh
 *    Input.  The minimal threshold for marking a box as an object. Must belong to [0, 1].
 *  @param[in] nms_thresh
 *    Input.  The minimal threshold for marking and eliminating a duplicate box.
 *            Must belong to [0, 1].
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - \p detection_output_desc is NULL.
 *    - \p mode is not \p CNNL_DETECTION_OUTPUT_YOLOV2.
 *    - Parameters do not meet requirements.
 *
 *  @par API Dependency
 *  - You need to call ::cnnlCreateDetectionOutputDescriptor to create a descriptor before using this function.
 *  - You need to call ::cnnlDestroyDetectionOutputDescriptor to destroy the descriptor when
 *  ::cnnlDetectionOutputDescriptor_t is no longer needed.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetYolov2DetectionOutputDescriptor(cnnlDetectionOutputDescriptor_t detection_output_desc,
                                       cnnlDetectionOutputAlgo_t mode,
                                       int num_class,
                                       int num_anchor,
                                       int num_coords,
                                       int num_batch,
                                       float confidence_thresh,
                                       float nms_thresh);

// Group:Detection Output
/*!
 *  @brief Assigns ssd detection output descriptor with parameters.
 *
 *  @param[in,out]  detection_output_desc
 *    Input/output.  A pointer to the object of detection_output descriptor.
 *  @param[in]  mode
 *    Input.  The algorithm of the detection output operation. Must be \p CNNL_DETECTION_OUTPUT_SSD.
 *  @param[in]  num_input
 *    Input.  The number of input tensor. Must be 2.
 *  @param[in]  num_batch
 *    Input.  The number of batches for each feature map. Can be any positive integer as long as the input
 *            tensor size is less than 2GB.
 *  @param[in]  num_class
 *    Input.  The number of possible classes of each detected object.
 *            Can be any positive integer as long as the input tensor size is less than 2GB.
 *  @param[in]  num_box
 *    Input.  The number of candidate bounding boxes. Can be any positive integer
 *            as long as the input tensor size is less than 2GB.
 *  @param[in]  share_location
 *    Input.  An integer indicating whether to share the boxes coordinates among different classes.
 *            \p 0 means not to share while \p 1 means to share.
 *  @param[in]  background_label_id
 *    Input.  The label ID of background. Must belong to [0, num_class - 1].
 *  @param[in]  code_type
 *    Input.  The type for decoding.
 *    - 0: \p CodeType_CORNER.
 *    - 1: \p CodeType_CENTER_SIZE.
 *    - 2: \p CodeType_CORNER_SIZE.
 *  @param[in]  variance_encoded_in_target
 *    Input.  An integer indicating whether to use variance when decoding.
 *    \p 0 means not to use while \p 1 means to use.
 *  @param[in]  clip
 *    Input.  Whether to clip when decoding, 0: disable, 1: enable.
 *  @param[in]  num_topk
 *    Input.  The value of topK. Determine the number of box kept before NMS step.
 *  @param[in]  num_keep
 *    Input.  The largest possible number of bounding boxes. Must belong to [1, 2048].
 *  @param[in]  thresh_confidence
 *    Input.  The minimal threshold for marking a box as an object. Must belong to [0, 1].
 *  @param[in]  thresh_nms
 *    Input.  The minimal threshold for marking and eliminating a duplicate box.
 *            Must belong to [0, 1].
 *  @param[in]  eta
 *    Input.  The decaying factor used to adaptively update thresh_nms during nms.
 *            Must belong to (0, 1].
 *  @param[in]  min_thresh_nms
 *    Input.  The minimum value for \p thresh_nms. When \p thresh_nms updated by eta is smaller than
 *            or equal to \p min_thresh_nms, \p thresh_nms will no longer be updated.
 *            Must belong to [0, 1].
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - \p desc is NULL.
 *    - \p mode is not CNNL_DETECTION_OUTPUT_SSD.
 *    - Parameters do not meet requirements.
 *
 *  @par API Dependency
 *  - You need to call ::cnnlCreateDetectionOutputDescriptor to create a descriptor
 *    before using this function.
 *  - You need to call ::cnnlDestroyDetectionOutputDescriptor to destroy the descriptor
 *    when the descriptor is no longer needed.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetSsdDetectionOutputDescriptor_v2(cnnlDetectionOutputDescriptor_t detection_output_desc,
                                       cnnlDetectionOutputAlgo_t mode,
                                       const int num_input,
                                       const int num_batch,
                                       const int num_class,
                                       const int num_box,
                                       const int share_location,
                                       const int background_label_id,
                                       const int code_type,
                                       const int variance_encoded_in_target,
                                       const int clip,
                                       const int num_topk,
                                       const int num_keep,
                                       const float thresh_confidence,
                                       const float thresh_nms,
                                       const float eta,
                                       const float min_thresh_nms);
// Group:Detection Output
/*!
 *  @brief Assigns RetinaNet detection output descriptor with parameters.
 *
 *  @param[in,out]  detection_output_desc
 *    Input/output.  A pointer to the object of detection_output descriptor.
 *  @param[in]  mode
 *    Input.  The algorithm of the detection_output operation. Must be \p CNNL_DETECTION_OUTPUT_RETINANET.
 *  @param[in]  num_input
 *    Input.  The number of input tensor. Must be 10.
 *  @param[in]  num_class
 *    Input.  The number of possible classes of each detected object. Must be 80.
 *  @param[in]  num_anchor
 *    Input.  The number of possible anchors of each detected object. Must be 9.
 *  @param[in]  num_topk
 *    Input.  The value of topK, which determines the number of boxes kept before NMS step.
 *            Must belong to (0, 1000].
 *  @param[in]  thresh_confidence
 *    Input.  The minimal threshold for marking a box as an object. Must belong to (0, 1).
 *  @param[in]  thresh_nms
 *    Input.  The minimal threshold for marking and eliminating a duplicate box.
 *            Must belong to (0, 1).
 *  @param[in]  num_detection
 *    Input.  The largest possible number of bounding boxes. Must belong to (0, num_topk].
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - \p desc is NULL.
 *    - \p mode is not CNNL_DETECTION_OUTPUT_RETINANET.
 *    - Parameters do not meet requirements.
 *
 *  @par API Dependency
 *  - You need to call ::cnnlCreateDetectionOutputDescriptor create a descriptor before using this function.
 *  - You need to call ::cnnlDestroyDetectionOutputDescriptor destroy the descriptor when
 *  ::cnnlDetectionOutputDescriptor_t is no longer needed.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetRetinaDetectionOutputDescriptor(cnnlDetectionOutputDescriptor_t detection_output_desc,
                                       cnnlDetectionOutputAlgo_t mode,
                                       const int num_input,
                                       const int num_class,
                                       const int num_anchor,
                                       const int num_topk,
                                       const float thresh_confidence,
                                       const float thresh_nms,
                                       const int num_detection);

// Group:Detection Output
/*!
 *  @brief Assigns refinedet detection output descriptor with parameters.
 *
 *  @param[in,out]  detection_output_desc
 *    Input/output.  A pointer to the object of detection_output descriptor.
 *  @param[in]  mode
 *    Input.  The algorithm of the detection output operation. Must be \p CNNL_DETECTION_OUTPUT_REFINEDET.
 *  @param[in]  num_input
 *    Input.  The number of input tensor. Must be 4.
 *  @param[in]  num_batch
 *    Input.  The number of batches for each feature map. Can be any positive integer as long as the input
 *            tensor size is less than 2GB.
 *  @param[in]  num_class
 *    Input.  The number of possible classes of each detected object.
 *            Can be any positive integer as long as the input tensor size is less than 2GB.
 *  @param[in]  num_box
 *    Input.  The number of candidate bounding boxes. Can be any positive integer
 *            as long as the input tensor size is less than 2GB.
 *  @param[in]  share_location
 *    Input.  An integer indicating whether to share the boxes coordinates among different classes.
 *            \p 0 means not to share while \p 1 means to share.
 *  @param[in]  background_label_id
 *    Input.  The label ID of background. Must belong to [0, num_class - 1].
 *  @param[in]  code_type
 *    Input.  The type for decoding.
 *    - 0: \p CodeType_CORNER.
 *    - 1: \p CodeType_CENTER_SIZE.
 *    - 2: \p CodeType_CORNER_SIZE.
 *  @param[in]  variance_encoded_in_target
 *    Input.  An integer indicating whether to use variance when decoding.
 *    \p 0 means not to use while \p 1 means to use.
 *  @param[in]  clip
 *    Input.  An integer indicating whether to enable clipping when decoding. \p 0 means disable and \p 1 means enable.
 *  @param[in]  num_topk
 *    Input.  The value of topK, which determines the number of boxes kept before NMS step.
 *  @param[in]  num_keep
 *    Input.  The largest possible number of bounding boxes. Must belong to [1, 2048].
 *  @param[in]  thresh_confidence
 *    Input.  The minimal threshold for marking a box as an object. Must belong to [0, 1].
 *  @param[in]  thresh_nms
 *    Input.  The minimal threshold for marking and eliminating a duplicate box.
 *            Must belong to [0, 1].
 *  @param[in]  eta
 *    Input.  The decaying factor used to adaptively update thresh_nms during nms.
 *            Must belong to (0, 1].
 *  @param[in]  min_thresh_nms
 *    Input.  The minimum value of \p thresh_nms. When \p thresh_nms updated by eta is smaller than or
 *            equal to \p min_thresh_nms, \p thresh_nms will no longer be updated.
 *            Must belong to [0, 1].
 *  @param[in]  object_score
 *    Input.  The minimal threshold for background label and object label. Must belong to [0, 1].
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - \p desc is NULL.
 *    - \p mode is not CNNL_DETECTION_OUTPUT_REFINEDET.
 *    - Parameters do not meet requirements.
 *
 *  @par API Dependency
 *  - You need to call ::cnnlCreateDetectionOutputDescriptor to create a descriptor before using this function.
 *  - You need to Call ::cnnlDestroyDetectionOutputDescriptor to destroy the descriptor when
 *  ::cnnlDetectionOutputDescriptor_t is no longer needed.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetRefinedetDetectionOutputDescriptor_v2(cnnlDetectionOutputDescriptor_t detection_output_desc,
                                             cnnlDetectionOutputAlgo_t mode,
                                             const int num_input,
                                             const int num_batch,
                                             const int num_class,
                                             const int num_box,
                                             const int share_location,
                                             const int background_label_id,
                                             const int code_type,
                                             const int variance_encoded_in_target,
                                             const int clip,
                                             const int num_topk,
                                             const int num_keep,
                                             const float thresh_confidence,
                                             const float thresh_nms,
                                             const float eta,
                                             const float min_thresh_nms,
                                             const float object_score);

// Group:Detection Output
/*!
 *  @brief Sets Faster R-CNN detection output descriptor with parameters.
 *
 *  @param[in,out] detection_output_desc
 *    Input/output.  A pointer to the object of detection_output descriptor.
 *  @param[in] algo
 *    Input.  The algorithm of the detection_output operation. Must be \p CNNL_DETECTION_OUTPUT_FASTERRCNN.
 *  @param[in] num_input
 *    Input.  The number of input tensor. Must be 3.
 *  @param[in] num_batch
 *    Input.  The number of batches for each feature map. Can be any positive integer as long as the input
 *            tensor size is less than 2GB.
 *  @param[in] num_box
 *    Input.  The number of possible classes of each detected object. Can be any positive integer
 *            as long as the input tensor size is less than 2GB.
 *  @param[in] num_class
 *    Input.  The number of possible classes of each detected object.
 *            Can be any positive integer as long as the input tensor size is less than 2GB.
 *  @param[in] height_image
 *    Input.  The height of detection network input image. Must be greater than 0.
 *  @param[in] width_image
 *    Input.  The width of detection network input image. Must be greater than 0.
 *  @param[in] scale
 *    Input.  The scale of ROIs. Must be greater than 0.
 *  @param[in] thresh_nms
 *    Input.  The minimal threshold for marking and eliminating a duplicate box.
 *            Must belong to [0, 1].
 *  @param[in] thresh_confidence
 *    Input.  The minimal threshold for marking a box as an object. Must belong to [0, 1].
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - \p detection_output_desc is NULL.
 *    - \p mode is not CNNL_DETECTION_OUTPUT_FASTERRCNN.
 *    - Parameters do not meet requirements.
 *
 *  @par API Dependency
 *  - You need to call ::cnnlCreateDetectionOutputDescriptor to create a descriptor before using this function.
 *  - You need to call ::cnnlDestroyDetectionOutputDescriptor to destroy the descriptor when
 *  ::cnnlDetectionOutputDescriptor_t is no longer needed.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetFasterRCNNDetectionOutputDescriptor(cnnlDetectionOutputDescriptor_t detection_output_desc,
                                           cnnlDetectionOutputAlgo_t algo,
                                           int num_input,
                                           int num_batch,
                                           int num_box,
                                           int num_class,
                                           int height_image,
                                           int width_image,
                                           float scale,
                                           float thresh_nms,
                                           float thresh_confidence);

// Group:Detection Output
/*!
 *  @brief Sets Ppyolo detection output descriptor with parameters.
 *
 *  @param[in,out] desc
 *    Input/output.  A pointer to the struct of detection output descriptor.
 *  @param[in] mode
 *    Input.  The algorithm of the detection output operation. Must be \p CNNL_DETECTION_OUTPUT_PPYOLO.
 *  @param[in] num_batch
 *    Input.  The number of batches for each feature map. Can be any positive integer as long as the input
 *            tensor size is less than 2GB.
 *  @param[in] num_input
 *    Input.  The number of input feature maps. Must belong to [1, 7].
 *  @param[in] num_anchor
 *    Input.  The number of anchors, which are assumed same for all input feature maps.
 *            Must belong to [1, 8].
 *  @param[in] num_class
 *    Input.  The number of possible classes of each detected object. Must belong to [1, 1000].
 *  @param[in] num_entry
 *    Input.  The number of entries for every predicted point in a feature
 *            map. Must be 5, where \p 5 represents x, y, w, h and confidence respectively.
 *  @param[in] num_keep
 *    Input.  The largest possible number of bounding boxes. Must belong to [1, 2048].
 *  @param[in] height_image
 *    Input.  The height of detection network input image. Must be greater than 0.
 *  @param[in] width_image
 *    Input.  The width of detection network input image. Must be greater than 0.
 *  @param[in] clip
 *    Input.  An integer indicating whether to enable clipping when decoding.
 *    \p 0 means disable and \p 1 means enable.
 *  @param[in] scale_x_y
 *    Input.  The scale factor for scaling boxes center when decoding. Must be greater than 0.
 *  @param[in] unnormalize_scale_h
 *    Input.  The scale factor for scaling boxes height. Must be greater than or equal to 1.
 *  @param[in] unnormalize_scale_w
 *    Input.  The scale factor for scaling boxes width. Must be greater than or equal to 1.
 *  @param[in] thresh_confidence
 *    Input.  The minimal threshold for marking a box as an object. Must belong to [0, 1].
 *  @param[in] thresh_nms
 *    Input.  The minimal threshold for marking and eliminating a duplicate box.
 *            Must belong to [0, 1].
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - \p desc is NULL.
 *    - \p mode is not \p CNNL_DETECTION_OUTPUT_PPYOLO.
 *    - Parameters do not meet requirements.
 *
 *  @par API Dependency
 *  - You need to call ::cnnlCreateDetectionOutputDescriptor() to create
 *    a descriptor before using this function.
 *  - You need to call ::cnnlDestroyDetectionOutputDescriptor() to destroy
 *    the descriptor when cnnlDetectionOutputDescriptor_t is no longer needed.
 */

cnnlStatus_t CNNL_WIN_API
cnnlSetPpyoloDetectionOutputDescriptor(cnnlDetectionOutputDescriptor_t desc,
                                       const cnnlDetectionOutputAlgo_t mode,
                                       const int num_batch,
                                       const int num_input,
                                       const int num_anchor,
                                       const int num_class,
                                       const int num_entry,
                                       const int num_keep,
                                       const int height_image,
                                       const int width_image,
                                       const int clip,
                                       const float scale_x_y,
                                       const float unnormalize_scale_h,
                                       const float unnormalize_scale_w,
                                       const float thresh_confidence,
                                       const float thresh_nms);

// Group:Detection Output
/*!
 *  @brief Sets the YOLOv8 detection output descriptor with parameters.
 *
 *  @param[in,out] desc
 *    Input/output.  A pointer to the object of detection_output descriptor.
 *  @param[in] mode
 *    Input.  The algorithm of the detection_output operation. Must be \p CNNL_DETECTION_OUTPUT_YOLOV8.
 *  @param[in] num_input
 *    Input.  The number of input tensor. Currently must be 1.
 *  @param[in] num_batch
 *    Input.  The number of batches for each feature map. Can be any positive integer as long as the input
 *            tensor size is less than 2GB.
 *  @param[in] num_class
 *    Input.  The number of possible classes of each detected object.
 *            currently only support 1 class.
 *  @param[in] num_entry
 *    Input.  The number of entries for every predicted point in a feature
 *            map. Must be 4, where \p 4 represents x, y, w, h.
 *  @param[in] num_keep
 *    Input.  The largest possible number of bounding boxes. Must belong to [1, 2048].
 *  @param[in] max_wh
 *    Input. The maximum box width and height in pixels.
 *  @param[in] num_box_limit
 *    Input.  The largest possible number of bounding boxes. Must belong to [1, 12000].
 *  @param[in] thresh_confidence
 *    Input.  The minimal threshold for marking a box as an object. Must belong to [0, 1].
 *  @param[in] thresh_nms
 *    Input.  The minimal threshold for marking and eliminating a duplicate box.
 *            Must belong to [0, 1].
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - \p desc is NULL.
 *    - \p mode is not CNNL_DETECTION_OUTPUT_YOLOV8.
 *    - Parameters do not meet requirements.
 *
 *  @par API Dependency
 *  - You need to call ::cnnlCreateDetectionOutputDescriptor to create a descriptor
 *    before using this function.
 *  - You need to call ::cnnlDestroyDetectionOutputDescriptor to destroy the descriptor
 *    when the descriptor is no longer needed.
 */

cnnlStatus_t CNNL_WIN_API
cnnlSetYolov8DetectionOutputDescriptor(cnnlDetectionOutputDescriptor_t desc,
                                       cnnlDetectionOutputAlgo_t mode,
                                       int num_input,
                                       int num_batch,
                                       int num_class,
                                       int num_entry,
                                       int num_keep,
                                       int max_wh,
                                       int num_box_limit,
                                       float thresh_confidence,
                                       float thresh_nms);

// Group:Detection Output
/*!
 *  @brief Sets the computation mode of the detection output operation.
 *
 *  @param[in] desc
 *   Input. A pointer to the struct of detection output descriptor.
 *   The algorithm of the detection output operation must be set with the
 *   cnnlSetXXXDetectionOutputDescriptor function. Currently the following
 *   algorithm modes are supported in this function:
 *   - \p CNNL_DETECTION_OUTPUT_YOLOV3
 *   - \p CNNL_DETECTION_OUTPUT_YOLOV4
 *  @param[in] compute_mode
 *  Input. The computation mode of preferring precision or performance.
 *  Currently the \p CNNL_COMPUTATION_FAST and \p CNNL_COMPUTATION_HIGH_PRECISION
 *  computation modes are supported. For details, see cnnlComputationPreference_t
 *  in Cambricon CNNL Developer Guide.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - \p desc is NULL.
 *    - \p compute_mode is invalid.
 *
 *  @par API Dependency
 *  - You need to call ::cnnlCreateDetectionOutputDescriptor() to create
 *    a descriptor before using this function.
 *  - You need to call cnnlSetXXXDetectionOutputDescriptor() to set the algorithm mode
 *    of the detection output operation before using this function.
 *  @note
 *  - If the algorithm of the detection output operation is ::CNNL_DETECTION_OUTPUT_YOLOV2, this function is only supported on MLU300 series and MLU500 series.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetDetectionOutputComputePreference(cnnlDetectionOutputDescriptor_t desc,
                                        const cnnlComputationPreference_t compute_mode);

// Group:Detection Output
/*!
 *  @brief Destroys the detection output descriptor.
 *  It should be called at the end of the context to avoid memory leak.
 *
 *  @param[in] detection_output_desc
 *    Input.  A pointer to the struct of detection output descriptor.
 *
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_EXECUTION_FAILED
 *    One of the following conditions are met:
 *     - \p detection_output_desc is NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlDestroyDetectionOutputDescriptor(cnnlDetectionOutputDescriptor_t detection_output_desc);

// Group:Detection Output
/*!
 *  @brief Creates the detection output operation. The detection output
 *  follows the object detection backbone network, and the pipeline
 *  includes decode, filter, nms and topk (if topk bbx with score is needed). Users can choose
 *  different modes to apply different detection output algorithms. Currently the detection output mode
 *  supports SSD, REFINEDET, RetinaNet, Yolov2, Yolov3 and Faster R-CNN. More detection output modes
 *  will be supported in the future.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *           detection_output operation.
 *  @param[in,out] detection_output_desc
 *    Input/output. The descriptor of the detection output operation.
 *  @param[in] input_desc
 *    Input. Pointer to the host memory that stores an array of cnnlTensorDescriptors which holds
 *    dimension, data type and layout of input tensors. The meaning of tensor descriptors
 *    varies based on different ::cnnlDetectionOutputAlgo_t:
 *    - SSD: \p input_desc contains 2 elements.
 *      - The first element means bbox prediction. When share_location = 1,
 *        the shape of the first element must be [num_batch, 4, num_box]; when
 *        share_location = 0, the shape can be [num_batch, num_class * 4, num_box] or
 *        [num_batch, num_class, 4, num_box].
 *      - The second element means bbox class confidence. The
 *        shape of the element must be [num_batch, num_class, num_box].
 *        See ::cnnlSetSsdDetectionOutputDescriptor_v2 for detailed parameter limitation.
 *    - YOLOV2: \p input_desc contains 1 element. The element means bbox coordinates and class
 *      prediction. The shape of the element must be [num_batch, num_channel, 1, num_anchor*H*W], and the data layout must be [num_batch, num_channel, num_anchor, H, W], where
 *      num_channel = (num_classes + 5) and H must be equal to W.
 *      See ::cnnlSetYolov2DetectionOutputDescriptor for detailed parameter limitation.
 *    - YOLOV3/4/5: input_desc contains \p input_num elements. All elements share the same
 *      physical meaning, that is, bbox coordinates and class prediction. The shape of elements
 *      follows the pattern [num_batch, H, W, num_channel]. All elements must have the same
 *      \p num_batch and \p num_channel, where num_channel = num_anchor * (num_classes + 5).
 *      This operation assumes all input tensors have same \p num_anchor.
 *      See ::cnnlSetYolov3DetectionOutputDescriptor_v2, ::cnnlSetYolov4DetectionOutputDescriptor,
 *      or ::cnnlSetYolov5DetectionOutputDescriptor for detailed parameter limitation.
 *    - FASTERRCNN: \p input_desc contains 3 elements.
 *      - The first element means bbox prediction. The shape of the element must be
 *        [num_batch, num_class, 4, num_box].
 *      - The second element means class prediction. The shape of the element must be
 *        [num_batch, num_class, 1, num_box].
 *      - The third element means region of interest (RoI). The shape of element must be
 *        [num_batch, 1, 5, num_box], where \p 5 represents x1, y1, x2, y2 and placeholder.
 *        (x1, y1) and (x2, y2) represent the top-left and bottom-right points of a bbox.
          The fifth value occupied by a placeholder is currently not used.
 *        See ::cnnlSetFasterRCNNDetectionOutputDescriptor for detailed parameter limitation.
 *    - REFINEDET: \p input_desc contains 4 elements.
 *      - The first two elements are the same as those of \p input_desc in SSD mode.
 *      - The third element means anchor refine module (arm) locationprediction.
 *        When share_location = 1, the shape of element must be [num_batch, 4, num_box];
 *        when share_location = 0, the shape of element can be [num_batch, num_class, 4, num_box]
 *        or [num_batch, num_class * 4, num_box], and the number of dimensions must be equal to that of the first element.
 *      - The fourth element means arm confidence prediction. The shape of element must be
 *        [num_batch, 2, num_box]. See ::cnnlSetRefinedetDetectionOutputDescriptor_v2 for detailed
 *        parameter limitation.
 *    - RETINANET: \p input_desc contains 3 elements.
 *      - The first element is the class prediction. The shape of this element must be
 *       [num_batch, num_anchor * num_class, H, W]. H * W must be smaller than 17920.
 *      - The second element is the bbox coordinates. The shape of this element must be
 *       [num_batch, num_anchor * (x1, y1, x2, y2), H, W], where (x1, y1, x2, y2) represents
 *       the top-left and bottom-right coordinates of a bbox.
 *      - The third element is the anchor, which is used as the bias_desc.
 *    - YOLOV8: \p input_desc contains one elements. The shape of the elements should be
 *       [num_batch, class_num + 4, prediction_num], prediction means the num of box prediction,
 *       4 means the box info [x, y, w, h], (x, y) means the center of box, w and h means
 *       the width and height of the box, class_num is the total class number.
 *       See ::cnnlSetRetinaDetectionOutputDescriptor for detailed parameter limitation.
 *  @param[in] input
 *    Input. Pointer to the host memory that stores an array of pointers to the device memory. The
 *           order of elements in \p input pointer must be consistent with the order of
 *           cnnlTensorDescriptors in the \p input_desc pointer.
 *  @param[in] bias_desc
 *    Input. The descriptor of bias tensor, including anchor height and width. The shape of
 *    bias tensor varies based on different ::cnnlDetectionOutputAlgo_t:
 *    - SSD: bias tensor represents prior box, which is constant for different batch.
 *    The shape of tensor must be [2, 4, num_box].
 *    - YOLOV2: bias tensor represents chimeric anchor and coordinates data. The shape of
 *    tensor must be [4, num_anchor, H, W], where \p 4 represents x-coordinate,
 *    y-coordinate, anchor width and anchor height. The data is generated as follows:
 *    assume anchor_data.shape() = [num_anchor, 2], where \p 2 represents width and
 *    height. Then:
      @verbatim
       for (aidx: 0 ~ num_anchor - 1)
         for (hidx: 0 ~ H - 1)
           for (widx: 0 ~ W - 1)
             bias_data[0][aidx][hidx][widx] = widx
             bias_data[1][aidx][hidx][widx] = hidx
             bias_data[2][aidx][hidx][widx] = anchor_data[aidx][0]
             bias_data[3][aidx][hidx][widx] = anchor_data[aidx][1]
      @endverbatim
 *    - YOLOV3/4/5: bias tensor represents anchor data, which is constant for
 *    different batch. The shape of tensor can be [num_input, num_anchor, 2] or
 *    [num_input * num_anchor * 2], where \p 2 represents width and height (width and height should be greater than 0).
 *    - FASTERRCNN: bias tensor not used. Pass NULL to this parameter.
 *    - REFINEDET: Same with SSD.
 *    - RETINANET: bias tensor represents anchor box. The shape is
 *    [5, anchor_num, (x1, y1, x2, y2)], where (x1, y1, x2, y2) represents
 *    the top-left and bottom-right coordinates of a bbox.
 *    - YOLOV8: bias tensor not used. Pass NULL to this parameter.
 *  @param[in] bias
 *    Input. A pointer to the MLU memory that stores anchor data. Pass NULL to this parameter when \p mode is
 *           \p CNNL_DETECTION_OUTPUT_FASTERRCNN.
 *  @param[in] workspace
 *    Input. A pointer to workspace buffer.
 *  @param[in] workspace_size
 *    Input. The size of workspace.
 *  @param[in] output_desc
 *    Input. Descriptor of output tensor, including dimension, data type and layout. The shape of
 *    tensor are as follows:
 *    - YOLOv2: [num_batch, num_element * max_possible_num_box] or
 *      [num_batch, num_element, max_possible_num_box]
 *    - YOLOX: [num_batch * num_element * max_possible_num_box]
 *    - Other modes: [num_batch, max_possible_num_box, num_element] or
 *      [num_batch, max_possible_num_box * num_element](others), where \p num_element equals 6 for
 *      \p CNNL_DETECTION_OUTPUT_FASTERRCNN mode, (6+num_classes) for YOLOX mode, and 7 for other modes.
 *      The \p max_possible_num_box equals:
 *      - SSD: The smaller one between \p num_keep and \p num_box.
 *      - YOLOV2: \p 256.
 *      - YOLOX:  \p 4096.
 *      - YOLOV2/3/4: The smaller one between \p num_keep and
 *       sum(h[i] * w[i] * num_anchor * num_class), where \p hi and \p wi represent the height and width
 *       of input tensor.
 *      - FASTERRCNN: \p num_box * \p num_class.
 *      - REFINEDET:  The smaller one between \p num_keep and \p num_box.
 *      - RETINANET: \p num_detection.
 *        Valid bbox will be stored at the beginning of corresponding device memory of every
 *        batch. If number of valid bbox is less than max_possible_num_box, the remaining device
 *        memory maintains uninitialized. User can use valid bbox number stored in output_size
 *        pointer to gather data of valid bbox. 7 entries represent
 *        [batch_id, class_id, score, x1, y1, x2, y2], where (x1, y1) and (x2, y2) represent the
 *        top-left and bottom-right point of a bbox. Result in \p CNNL_DETECTION_OUTPUT_FASTERRCNN
 *        mode does not have \p batch_id.
 *      - YOLOV8: [num_batch, max_num_detection, 6], max_num_detection is the user set max output detection num,
 *        6 respresent (x1, y1, x2, y2, score, class id), where (x1, y1) and (x2, y2) represent the
 *        top-left and bottom-right point of a bbox, and the score and category id of the class.
 *  @param[out] output
 *    Output. A pointer to the MLU memory that stores the output data.
 *  @param[out] output_size_desc
 *    Output. Descriptor of \p output_size tensor, including dimension, data type and layout. The shape
 *    of the tensor can be [num_batch] or [1](only for SSD). The data type must be INT32. When \p num_batch
 *    is greater than 1 and the shape of output_size is [1], all results of different batches will be
 *    merged and the value stored in output_size will be the sum of the result number of all
 *    batches.
 *  @param[out] output_size
 *    Output. A pointer to the MLU memory that stores the output size. It is 1D and stores [batch] number of
 *           UINT32-type values, representing the actual number of bounding boxes after detection
 *           output operation for every batch of input.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions must be met:
 *    - \p Handle is NULL.
 *    - \p input is NULL.
 *    - \p output is NULL.
 *    - parameters exceed limitation.
 *
 *  @par Data Type
 *  - input: float, half.
 *  - output: float, half.
 *  - bias: float, half.
 *
 *  @par API Dependency
 *  - ::cnnlCreateDetectionOutputDescriptor needs to be called before this function.
 *  - ::cnnlSetYolov3DetectionOutputDescriptor_v2 (based on the used algorithm)
 *    needs to be called before this function.
 *  - ::cnnlDestroyDetectionOutputDescriptor needs to be called after this function.
 *
 *  @par Scale Limitation
 *  - Large tensor is not supported in YOLOV8.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlDetectionOutput_v2(cnnlHandle_t handle,
                       const cnnlDetectionOutputDescriptor_t detection_output_desc,
                       const cnnlTensorDescriptor_t input_desc[],
                       void **input,
                       const cnnlTensorDescriptor_t bias_desc,
                       void *bias,
                       void *workspace,
                       size_t workspace_size,
                       const cnnlTensorDescriptor_t output_desc,
                       void *output,
                       const cnnlTensorDescriptor_t output_size_desc,
                       void *output_size);

// Group:Detection Output
/*!
 *  @brief Gets extra space size required in the
 *   detection output operation.
 *
 *  @param[in]  handle
 *    Input.  Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *            detection_output operation.
 *  @param[in]  desc
 *    Input.  The descriptor of the detection output operation. For detailed
 *    information, see ::cnnlDetectionOutputDescriptor_t.
 *  @param[in]  input_desc
 *    Input. The descriptor of the input tensor.
 *  @param[in]  mode
 *    Input.  The algorithm of the detection output operation.
 *  @param[out] size
 *    Output. The size of workspace.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - \p handle is NULL.
 *    - \p desc is NULL.
 *    - \p mode does not belong to any of cnnlDetectionOutputAlgo_t definition.
 *    - \p input_desc is NULL.
 *    - \p input_desc valid elements is NULL.
 *    - \p size is NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetDetectionOutputWorkspaceSize(cnnlHandle_t handle,
                                    const cnnlDetectionOutputDescriptor_t desc,
                                    const cnnlTensorDescriptor_t *input_desc,
                                    cnnlDetectionOutputAlgo_t mode,
                                    size_t *size);

/******************************************************************************
 * Cambricon CNNL OP: Proposal
 ******************************************************************************/
/*! The descriptor of the ::cnnlProposal_v2 operation. */
typedef struct cnnlProposalStruct *cnnlProposalDescriptor_t;

// Group:Proposal
/*!
 * @brief generate proposal boxes from scores and bounding box
 * prediction from region proposal network.
 *
 * @par Data Type
 *  - input: float, half.
 *  - output: float, half.
 *  - Data type of input and output must be the same.
 *
 * @par Scale Limitation
 *  - \p batch > 0, \p height > 0, \p width > 0 and \p anchor_num > 0.
 *  - \p im_h > 0, \p im_w > 0, \p im_min_h >= 0, \p im_min_w >= 0 and \p top_thresh > 0.
 *  - \p nms_num is limited in (0, 2000], and \p top_thresh >= \p nms_num.
 *  - \p anchor_num * \p width * \p height must be smaller than 10^6.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *           proposal operation.
 *  @param[in]  proposal_desc
 *    Input. The descriptor of the proposal operation.
 *  @param[in] bbox_pred_desc
 *    Input. The descriptor of the \p bbox_pred tensor.
 *  @param[in] bbox_pred
 *    Input. A pointer to the \p bbox_pred tensor that stores the bounding box regression
      for each anchor. The shape of the tensor is [batch_size, 4, 1, anchor_num * width * height].
 *  @param[in] scores_desc
 *    Input. The descriptor of the \p scores tensor.
 *  @param[in] scores
 *    Input. A pointer to the \p scores tensor that stores the score of being foreground and background
 *    for each anchor. The shape of the tensor is [batch_size, 2, 1, anchor_num * width * height].
 *  @param[in] anchors_desc
 *    Input. The descriptor of the \p anchors tensor.
 *  @param[in] anchors
 *    Input. A pointer to the \p anchors tensor that stores the anchor boxes.
 *    The shape of the tensor is [1, 4, 1, anchor_num * width * height].
 *  @param[in] workspace
 *    Input.  A pointer to an extra workspace required in the operation.
 *  @param[in] workspace_size
 *    Input. The size of the workspace.
 *  @param[in] new_box_desc
 *    Input. The descriptor of the \p new_box tensor, including the dimension information of
 *    the tensor.
 *  @param[out] new_box
 *    Output. A pointer to the \p new_box tensor that stores the generated proposal box.
      The shape of the tensor is [batch_size, 5, 1, nms_num].
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions must be met:
 *    - \p handle is NULL.
 *    - \p new_box is NULL.
 *    - \p bbox_pred is NULL.
 *    - \p scores is NULL.
 *    - \p anchors is NULL.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlProposal_v2(cnnlHandle_t handle,
                                          const cnnlProposalDescriptor_t proposal_desc,
                                          const cnnlTensorDescriptor_t bbox_pred_desc,
                                          const void *bbox_pred,
                                          const cnnlTensorDescriptor_t scores_desc,
                                          const void *scores,
                                          const cnnlTensorDescriptor_t anchors_desc,
                                          const void *anchors,
                                          const void *workspace,
                                          const size_t workspace_size,
                                          const cnnlTensorDescriptor_t new_box_desc,
                                          void *new_box);

// Group:Proposal
/*!
 *  @brief Creates a descriptor pointed by \p desc for the
 *  proposal operation.
 *
 *  @param[out] desc
 *   Output. A pointer to the proposal descriptor that
 *   holds information about the proposal operation.
 *  @retval CNNL_STATUS_SUCCESS
 *   This function ends normally.
 *  @retval CNNL_STATUS_ALLOC_FAILED
 *   The function failed to allocate memory.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateProposalDescriptor(cnnlProposalDescriptor_t *desc);

// Group:Proposal
/*!
 *  @brief Destroys the proposal descriptor that was previously
 *  created by ::cnnlCreateProposalDescriptor.
 *
 *  @param[in] proposal_desc
 *    Input. The proposal descriptor created by ::cnnlCreateProposalDescriptor.
 *  @retval CNNL_STATUS_SUCCESS
 *   The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    - The value of \p proposal_desc was NULL.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyProposalDescriptor(cnnlProposalDescriptor_t proposal_desc);

// Group:Proposal
/*!
 *  @brief Assigns the proposal descriptor with parameters.
 *
 *  @param[in,out] proposal_desc
 *    Input/output. The proposal descriptor create by ::cnnlCreateProposalDescriptor.
 *  @param[in] batch_size
 *    Input. Number of input batches.
 *  @param[in] height
 *    Input. Height of feature map.
 *  @param[in] width
 *    Input. Width of the feature map.
 *  @param[in] anchor_num
 *    Input. Number of anchors of every point in the feature map.
 *  @param[in] nms_num
 *    Input. Number of boxes to be selected in NMS process.
 *  @param[in] top_thresh
 *    Input. Number of boxes to be selected in TopK process.
 *  @param[in] im_min_h
 *    Input. The minimum size of height for boxes to be selected.
 *  @param[in] im_min_w
 *    Input. The minimum size of width for boxes to be selected.
 *  @param[in] nms_scale
 *    Input. The scaling rate of boxes when computing areas of box in NMS process.
 *    It is reserved for future use.
 *  @param[in] stride
 *    Input. Stride in computing anchor. It is reserved for future use.
 *  @param[in] nms_thresh
 *    Input. Threshold of IOU in NMS process.
 *  @param[in] im_h
 *    Input. Height of input images for network.
 *  @param[in] im_w
 *    Input. Width of input images for network.
 *  @param[in] scale
 *    Input. The scaling rate of the size of input images. It is reserved for future use.
 *  @param[in] fix8
 *    Input. Type of input. It is reserved for future use.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    The value of \p proposal_desc is NULL.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetProposalDescriptor(cnnlProposalDescriptor_t proposal_desc,
                                                    const int batch_size,
                                                    const int height,
                                                    const int width,
                                                    const int anchor_num,
                                                    const int nms_num,
                                                    const int top_thresh,
                                                    const float im_min_h,
                                                    const float im_min_w,
                                                    const float nms_scale,
                                                    const float stride,
                                                    const float nms_thresh,
                                                    const float im_h,
                                                    const float im_w,
                                                    const float scale,
                                                    const int fix8);

// Group:Proposal
/*!
 *  @brief Gets the extra space size required in the proposal operation.
 *
 *  @param[in]  proposal_desc
 *    Input. The descriptor of the proposal operation.
 *  @param[out] size
 *    Output. The size of workspace.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - \p proposal_desc is NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetProposalWorkspaceSize(const cnnlProposalDescriptor_t proposal_desc,
                             size_t *size);

/******************************************************************************
 * Cambricon CNNL OP: ProposalFpn
 ******************************************************************************/
/*! The descriptor of the ::cnnlProposalFpn operation.
 *
 *  You need to call the ::cnnlCreateProposalFpnDescriptor function
 *  to create a descriptor, and call the ::cnnlSetProposalFpnDescriptor
 *  function to set the tensor information to the descriptor.
 *  Also, you need to destroy the descriptor at the end with the
 *  ::cnnlDestroyProposalFpnDescriptor function*/
typedef struct cnnlProposalFpnStruct *cnnlProposalFpnDescriptor_t;

// Group:Proposal
/*!
 * @brief Generates proposal boxes from scores and bounding box
 * predictions in Feature Pyramid Network. Now it supports only MLU300 series.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *           proposalFpn operation.
 *  @param[in] proposal_fpn_desc
 *    Input. The descriptor of the proposalFpn operation.
 *  @param[in] new_box_desc
 *    Input. The descriptor of the \p new_box tensor, including the dimension
 *    data type of the tensor.
 *  @param[out] new_box
 *    Output. Pointer to the MLU memory that stores the new_box tensor that
 *    stores the generated proposal boxes. The shape of the tensor is [batch_size, topk_num, 4].
 *  @param[in] new_score_desc
 *    Input. The descriptor of the \p new_score tensor, including the dimension
 *    and data type information.
 *  @param[out] new_score
 *    Output. Pointer to the MLU memory that stores the \p new_score tensor.
 *   It stores the scores of proposal boxes. The shape of the tensor is [batch_size, topk_num, 1].
 *  @param[in] bbox_pred0_desc
 *    Input. The descriptor of the \p bbox_pred tensor at level 0.
 *  @param[in] bbox_pred_0
 *    Input. Pointer to the MLU memory that stores the \p bbox_pred_0 tensor.
 *    It is the bounding box prediction at level 0. The shape of the tensor is
 *    [batch_size, Hi, Wi, anchor_num, 4].
 *  @param[in] bbox_pred1_desc
 *    Input. The descriptor of the \p bbox_pred tensor at level 1.
 *  @param[in] bbox_pred_1
 *    Input. Pointer to the MLU memory that stores the \p bbox_pred_1 tensor.
 *    It is the bounding box prediction at level 1. The shape of the tensor is
 *    [batch_size, Hi, Wi, anchor_num, 4].
 *  @param[in] bbox_pred2_desc
 *    Input. The descriptor of the \p bbox_pred tensor at level 2.
 *  @param[in] bbox_pred_2
 *    Input. Pointer to the MLU memory that stores the \p bbox_pred_2 tensor.
 *    It is the bounding box prediction at level 2. The shape of the tensor is
 *    [batch_size, Hi, Wi, anchor_num, 4].
 *  @param[in] bbox_pred3_desc
 *    Input. The descriptor of the \p bbox_pred tensor at level 3.
 *  @param[in] bbox_pred_3
 *    Input. Pointer to the MLU memory that stores the \p bbox_pred_3 tensor.
 *    It is the bounding box prediction at level 3. The shape of the tensor is
 *    [batch_size, Hi, Wi, anchor_num, 4].
 *  @param[in] bbox_pred4_desc
 *    Input. The descriptor of the \p bbox_pred tensor at level 4.
 *  @param[in] bbox_pred_4
 *    Input. Pointer to the MLU memory that stores the \p bbox_pred_4 tensor.
 *    It is the bounding box prediction at level 4. The shape of the tensor is
 *    [batch_size, Hi, Wi, anchor_num, 4].
 *  @param[in] scores0_desc
 *    Input. The descriptor of the \p scores_0 tensor.
 *  @param[in] scores_0
 *    Input. Pointer to the MLU memory that stores the \p scores_0 tensor.
 *    It is the score of being foreground and background for each anchor at level 0.
 *    The shape of the tensor is [batch_size, Hi, Wi, anchor_num, 1].
 *  @param[in] scores1_desc
 *    Input. The descriptor of the \p scores_1 tensor.
 *  @param[in] scores_1
 *    Input. Pointer to the MLU memory that stores the \p scores_1 tensor.
 *    It is the score of being foreground and background for each anchor at level 1.
 *    The shape of the tensor is [batch_size, Hi, Wi, anchor_num, 1].
 *  @param[in] scores2_desc
 *    Input. The descriptor of the \p scores_2 tensor.
 *  @param[in] scores_2
 *    Input. Pointer to the MLU memory that stores the \p scores_2 tensor.
 *    It is the score of being foreground and background for each anchor at level 2.
 *    The shape of the tensor is [batch_size, Hi, Wi, anchor_num, 1].
 *  @param[in] scores3_desc
 *    Input. The descriptor of the \p scores_3 tensor.
 *  @param[in] scores_3
 *    Input. Pointer to the MLU memory that stores the \p scores_3 tensor.
 *    It is the score of being foreground and background for each anchor at level 3.
 *    The shape of the tensor is [batch_size, Hi, Wi, anchor_num, 1].
 *  @param[in] scores4_desc
 *    Input. The descriptor of the \p scores_4 tensor.
 *  @param[in] scores_4
 *    Input. Pointer to the MLU memory that stores the \p scores_4 tensor.
 *    It is the score of being foreground and background for each anchor at level 4.
 *    The shape of the tensor is [batch_size, Hi, Wi, anchor_num, 1].
 *  @param[in] anchors_desc
 *    Input. The descriptor of the \p anchors tensor.
 *  @param[in] anchors
 *    Input. Pointer to the MLU memory that stores the \p anchors tensor, which is the anchor boxes.
 *    The shape of tensor is [X, 4], where \p X is the accumulation of
 *    height * width * anchor_num of each level.
 *  @param[in] workspace
 *    Input.  Pointer to the MLU memory that stores the extra workspace.
 *  @param[in] workspace_size
 *    Input. The size of workspace.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions must be met:
 *    - \p handle is NULL.
 *    - \p new_box is NULL.
 *    - \p bbox_pred is NULL.
 *    - \p scores is NULL.
 *    - \p anchors is NULL.
 *
 * @par Data Type
 *  - input: float, half.
 *  - output: float, half.
 *  - Data type of input and output must be the same.
 *
 * @par Scale Limitation
 *  - \p batch > 0, \p height > 0, \p width > 0, \p anchor_num > 0, \p nms_num > 0,
 *    \p im_h > 0, \p im_w > 0, \p im_min_h > 0, \p im_min_w > 0, \p level > 1.
 *  - The value of \p nms_num should be in range of [50, 2000].
 *  - The value of \p topk_num should be in range of [0, nms_num].
 *  - The width and height should be in range of [64, 2560].
 *  - The value of \p nms_thresh should be in range of (0, 1).
 *
 * @par API Dependency
 *  - You need to call the ::cnnlCreateProposalFpnDescriptor() and ::cnnlSetProposalFpnDescriptor()
 *    and ::cnnlGetProposalFpnWorkspaceSize_v2() functions before calling this function.
 *  - You need to call the ::cnnlDestroyProposalFpnDescriptor() function after calling this function.
 */
cnnlStatus_t CNNL_WIN_API cnnlProposalFpn(cnnlHandle_t handle,
                                          const cnnlProposalFpnDescriptor_t proposal_fpn_desc,
                                          const cnnlTensorDescriptor_t new_box_desc,
                                          const void *new_box,
                                          const cnnlTensorDescriptor_t new_score_desc,
                                          const void *new_score,
                                          const cnnlTensorDescriptor_t bbox_pred0_desc,
                                          const void *bbox_pred_0,
                                          const cnnlTensorDescriptor_t bbox_pred1_desc,
                                          const void *bbox_pred_1,
                                          const cnnlTensorDescriptor_t bbox_pred2_desc,
                                          const void *bbox_pred_2,
                                          const cnnlTensorDescriptor_t bbox_pred3_desc,
                                          const void *bbox_pred_3,
                                          const cnnlTensorDescriptor_t bbox_pred4_desc,
                                          const void *bbox_pred_4,
                                          const cnnlTensorDescriptor_t scores0_desc,
                                          const void *scores_0,
                                          const cnnlTensorDescriptor_t scores1_desc,
                                          const void *scores_1,
                                          const cnnlTensorDescriptor_t scores2_desc,
                                          const void *scores_2,
                                          const cnnlTensorDescriptor_t scores3_desc,
                                          const void *scores_3,
                                          const cnnlTensorDescriptor_t scores4_desc,
                                          const void *scores_4,
                                          const cnnlTensorDescriptor_t anchors_desc,
                                          const void *anchors,
                                          const void *workspace,
                                          const size_t workspace_size);

// Group:Proposal
/*!
 *  @brief Creates a descriptor pointed by
 *  \p proposal_fpn_desc for the proposalFpn operation.
 *
 *
 *  @param[in] proposal_fpn_desc
 *   Input. A pointer to the proposalFpn descriptor that holds
 *   the information about the proposalFpn operation
 *  @retval CNNL_STATUS_SUCCESS
 *   This function ends normally.
 *  @retval CNNL_STATUS_ALLOC_FAILED
 *   The function failed to allocate memory.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCreateProposalFpnDescriptor(cnnlProposalFpnDescriptor_t *proposal_fpn_desc);

// Group:Proposal
/*!
 *  @brief Destroys the proposalFpn descriptor that was previously
 *  created by ::cnnlCreateProposalFpnDescriptor.
 *
 *  @param[in] proposal_fpn_desc
 *    Input. The proposalFpn descriptor created by ::cnnlCreateProposalFpnDescriptor.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    The value of \p proposal_fpn_desc is NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlDestroyProposalFpnDescriptor(cnnlProposalFpnDescriptor_t proposal_fpn_desc);

// Group:Proposal
/*!
 *  @brief Assigns proposalFpn descriptor with parameters.
 *
 *  @param[in,out] proposal_fpn_desc
 *    Input/output. The descriptor of the proposalFpn operation.
 *  @param[in] nms_num
 *    Input. Number of boxes to be selected in NMS process.
 *  @param[in] topk_num
 *    Input. Number of boxes to be selected in TopK process.
 *  @param[in] im_min_h
 *    Input. The minimum size of height for boxes to be selected.
 *  @param[in] im_min_w
 *    Input. The minimum size of width for boxes to be selected.
 *  @param[in] nms_scale
 *    Input. The scaling rate of boxes when computing areas of the boxes in NMS process.
 *           This parameter is reserved for future use and now it must be 1.0.
 *  @param[in] nms_thresh
 *    Input. Threshold of IOU in NMS process.
 *  @param[in] to_remove
 *    Input. An integer parameter added when calculating height and width of an anchor box.
 *    It is either 0 or 1. The formula is:
 *    @verbatim
      width = x1 - x2 + to_remove
      height = y1 - y2 + to_remove.
      @endverbatim
 *  @param[in] im_h
 *    Input. Height of input images for network.
 *  @param[in] im_w
 *    Input. Width of input images for network.
 *  @param[in] level
 *    Input. The levels of feature maps. It must be 5.
 *  @param[in] nms_across_level
 *    Input. A Boolean value indicating that the function performs NMS level by level or
 *           all levels together.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    The value of \p proposal_fpn_desc is NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetProposalFpnDescriptor(cnnlProposalFpnDescriptor_t proposal_fpn_desc,
                             const int nms_num,
                             const int topk_num,
                             const float im_min_h,
                             const float im_min_w,
                             const float nms_scale,
                             const float nms_thresh,
                             const float to_remove,
                             const int im_h,
                             const int im_w,
                             const int level,
                             const bool nms_across_level);

// Group:Proposal
/*!
 *  @brief Gets the extra space size required in proposalFpn operation.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlGetProposalFpnWorkspaceSize_v2 instead.
 *
 *  @param[in]  proposal_fpn_desc
 *    Input.  The descriptor of the proposalFpn operation.
 *  @param[out] size
 *    Output. The size of workspace.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - \p proposal_fpn_desc is NULL.
 */
CNNL_DEPRECATED_FOR(cnnlGetProposalFpnWorkspaceSize_v2)
cnnlStatus_t CNNL_WIN_API
cnnlGetProposalFpnWorkspaceSize(const cnnlProposalFpnDescriptor_t proposal_fpn_desc, size_t *size);

// Group:Proposal
/*!
 *  @brief Gets the size of the extra workspace required in proposalFpn operation.
 *
 *  @param[in] handle
 *    Input.  Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *            proposalFpn operation.
 *  @param[in] proposal_fpn_desc
 *    Input.  The descriptor of the proposalFpn operation.
 *  @param[in] mlvl_score_desc
 *    Input.  An array of the descriptors of all score tensors. The minimum length of the array is
 *            the number of levels that is set in \p proposal_fpn_desc.
 *  @param[out] size
 *    Output. The size of workspace.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - \p proposal_fpn_desc is NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetProposalFpnWorkspaceSize_v2(cnnlHandle_t handle,
                                   const cnnlProposalFpnDescriptor_t proposal_fpn_desc,
                                   const cnnlTensorDescriptor_t mlvl_score_desc[],
                                   size_t *size);

/******************************************************************************
 * Cambricon CNNL OP: Roipool
 ******************************************************************************/
/*!
 *  @brief The descriptor of the ::cnnlRoipool operation.
 *  You need to call the ::cnnlCreateRoipoolDescriptor funtion to create a
 *  descriptor, and call the
 *  ::cnnlSetRoipoolDescriptor() or ::cnnlSetPsRoipoolDescriptor() according to the different algorithm
 *  to set the information of the roipool operation to the descriptor. Also, you need
 *  to destroy the descriptor at the end with the ::cnnlDestroyRoipoolDescriptor
 *  function.
 *
 *  It is deprecated and will be removed in future release.
 */
typedef struct cnnlRoipoolStruct *cnnlRoipoolDescriptor_t;

/*!
 * @brief Enumeration variables describing the implementation modes of
 * RoIPooling.
 *
 * It is deprecated and will be removed in future release.
 */
typedef enum {
  CNNL_ROIPOOL   = 0, /*!< The RoIPooling kernel is implemented.*/
  CNNL_PSROIPOOL = 1, /*!< The PSRoIPooling kernel is implemented.*/
} cnnlRoipoolAlgo_t;

// Group:Roipool
/*!
 *  @brief Performs pooling over each ROI (Region of Interest)
 *  to generate the fixed size of feature map.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[in] handle
 *    Input.  Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *            roipool operation.
 *  @param[in] roipool_desc
 *    Input.  The descriptor of the roipool operation.
 *  @param[in] input_data_desc
 *    Input.  Pointer to the host memory that stores an array of cnnlTensorDescriptors that holds dimension, data type and layout
 *            of \p input_data tensors.
 *  @param[in] input_data
 *    Input.  Pointer to the host memory that stores an array of pointers to the device memory of \p input_data. The element
 *            order of this input_data array must be consistent with the element order of the cnnlTensorDescriptors array.
 *  @param[in] input_rois_desc
 *    Input.  Pointer to the host memory that stores an array of cnnlTensorDescriptors that holds dimension, data type and layout
 *            of \p input_rois tensor. The meaning of tensor descriptor varies based on different ::cnnlRoipoolAlgo_t:
 *            - CNNL_ROIPOOL: \p input_rois_desc contains multiple elements with the element number equal to the batch size.
 *              The \p input_rois supports \p CNNL_LAYOUT_NCHW and \p CNNL_LAYOUT_NHWC. In \p CNNL_LAYOUT_NCHW, the shape of the
 *              tensor is [rois_num, rois_offset]), while in \p CNNL_LAYOUT_NHWC, the shape is [rois_offset, rois_num].
 *            - CNNL_PSROIPOOL: \p input_rois_desc contains 1 element. The \p input_rois supports CNNL_LAYOUT_NCHW
  *             In \p CNNL_LAYOUT_NCHW, the shape of the tensor is [rois_num, rois_offset])
 *              while in \p CNNL_LAYOUT_NHWC, the shape is [rois_offset, rois_num].
 *  @param[in] input_rois
 *    Input.  Pointer to the host memory that stores an array of pointers to the device memory of \p input_rois. The element order
 *            of this \p input_rois array must be consistent with the element order of the cnnlTensorDescriptors array.
 *  @param[in] workspace
 *    Input.  A pointer to the MLU memory that is used as an extra workspace.
 *  @param[in] workspace_size
 *    Input. The size of workspace.
 *  @param[in] output_data_desc
 *    Input.  The descriptor of the output tensor, containing dimension
 *            and data type. The shape of the output tensor is [total_rois_num,height,width,channel].
 *  @param[out] output_data
 *    Output.  A pointer to the output data.
 *
 *  @par Data Type
 *   - input_data: half, float
 *   - input_rois: half, float
 *   - output_data: half, float
 *   - Data type of \p input_data, \p input_rois and \p output_data must be the same.
 *
 *  @note
 *   - The \p input_data tensor of roipool and ps_roipool only supports CNNL_LAYOUT_NHWC.
 *   - The shape of the \p input_data tensor of ps_roipool must be
 *     [batch_size, height, width,[output_dim,group_size,group_size]].
 *   - The roi number of each batch of the roipool and ps_roipool must be the same.
 *   - The \p input_rois tensor only has two dimensions. In CNNL_LAYOUT_NCHW, the shape of
 *     the tensor is [rois_num, rois_offset]; in CNNL_LAYOUT_NHWC, the shape of the tensor is
 *     [rois_offset, rois_num].
 *
 *  @par API Dependency
 *  - ::cnnlCreateRoipoolDescriptor() needs to be called before this function.
 *  - ::cnnlSetRoipoolDescriptor() or cnnlSetPsRoipoolDescriptor() needs to
 *    be called before this function.
 *  - ::cnnlDestroyRoipoolDescriptor needs to be called after this function.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - For CNNL_ROIPOOL:
 *      - \p handle is NULL.
 *      - \p input_data_desc is NULL.
 *      - \p input_data is NULL.
 *      - \p input_rois_desc is NULL.
 *      - \p input_rois is NULL.
 *      - \p roipool_desc is NULL.
 *      - \p output_data_desc is NULL.
 *      - \p output_data is NULL.
 *      - \p rois_offset is not equal to 5.
 *      - Thelayout type of input data is not equal to CNNL_LAYOUT_NHWC.
 *      - The layout type of input rois in not equal to CNNL_LAYOUT_NHWC or CNNL_LAYOUT_NCHW.
 *      - The input data type is not CNNL_DTYPE_FLOAT or CNNL_DTYPE_HALF.
 *      - The input rois type is not CNNL_DTYPE_FLOAT or CNNL_DTYPE_HALF.
 *      - The output data type is not CNNL_DTYPE_FLOAT or CNNL_DTYPE_HALF.
 *      - The input data type is not equal to input rois type or input data type is not equal to
 *        output data type.
 *    - For CNNL_PSROIPOOL:
 *      - \p handle is NULL.
 *      - \p input_data_desc is NULL.
 *      - \p input_data is NULL.
 *      - \p input_rois_desc is NULL.
 *      - \p input_rois is NULL.
 *      - \p roipool_desc is NULL.
 *      - \p output_data_desc is NULL.
 *      - \p output_data is NULL.
 *      - The layout type of input data is not CNNL_LAYOUT_NHWC.
 *      - The layout type of input rois in not equal to CNNL_LAYOUT_NHWC or CNNL_LAYOUT_NCHW.
 *      - \p group_size is not equal to pooled_height.
 *      - \p pooled_height is not equal to pooled_width.
 *      - \p group_size is smaller than 1.
 *      - \p output_dim is smaller than 1.
 *      - \p rois_offset is not equal to 5.
 *      - The input data type is not CNNL_DTYPE_FLOAT or CNNL_DTYPE_HALF.
 *      - The input rois type is not CNNL_DTYPE_FLOAT or CNNL_DTYPE_HALF.
 *      - The output data type is not CNNL_DTYPE_FLOAT or CNNL_DTYPE_HALF.
 *      - The input data type is not equal to input rois type or input data type is not equal to
 *        output data type.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlRoipool(cnnlHandle_t handle,
                                      cnnlRoipoolDescriptor_t roipool_desc,
                                      const cnnlTensorDescriptor_t input_data_desc[],
                                      const void *const input_data[],
                                      const cnnlTensorDescriptor_t input_rois_desc[],
                                      const void *const input_rois[],
                                      void *workspace,
                                      size_t workspace_size,
                                      const cnnlTensorDescriptor_t output_data_desc,
                                      void *output_data);

// Group:Roipool
/*!
 *  @brief Creates a descriptor pointed by \p desc
 *  for the roipool operation.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[out] desc
 *   Output. A pointer to the roipool descriptor that
 *   holds information about the roipool operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_ALLOC_FAILED
 *    \p desc is NULL.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlCreateRoipoolDescriptor(cnnlRoipoolDescriptor_t *desc);

// Group:Roipool
/*!
 *  @brief Destroys the roipool descriptor.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[in] desc
 *    Input.  A pointer to the struct of the roipool descriptor.
 *  @retval CNNL_STATUS_SUCCESS
 *    - The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    - \p desc is NULL.
 *  @note
 *  - This function must be called after calling roipool operation to prevent a host memory leak.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlDestroyRoipoolDescriptor(cnnlRoipoolDescriptor_t desc);

// Group:Roipool
/*!
 *  @brief Sets the descriptor of the roipool operation.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[in,out] desc
 *    Input/output.  The descriptor of the roipool operation.
 *  @param[in] algo
 *    Input.  The algorithm of the roipool operation.
 *  @param[in] spatial_scale
 *    Input.  The spatial scale of each regions of interest.
 *  @param[in] rois_type
 *    Input.  The mode of regions of interest. The \p rois_type only supports
 *            \p CNNL_ROI_BATCHID_CONRER and \p CNNL_ROI_CORNER_BATCHID.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One of the following conditions is met:
 *      - The value of \p desc is NULL.
 *      - The \p algo is not equal to CNNL_ROIPOOL.
 *      - The \p spatial_scale is smaller or equal than 0.
 *      - The \p rois_type is not equal to \p CNNL_ROI_BATCHID_CONRER or \p CNNL_ROI_CORNER_BATCHID.
 *
 *  @par API Dependency
 *  - ::cnnlCreateRoipoolDescriptor() needs to be called before this function.
 *  - ::cnnlDestroyRoipoolDescriptor() needs to be called after this function.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlSetRoipoolDescriptor(cnnlRoipoolDescriptor_t desc,
                                                   const cnnlRoipoolAlgo_t algo,
                                                   const float spatial_scale,
                                                   const cnnlRoiLayoutType_t rois_type);

// Group:Roipool
/*!
 *  @brief Sets the descriptor of the ps_roipool operation.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[in,out] desc
 *    Input/output.  A pointer to the struct of roipool descriptor.
 *  @param[in] algo
 *    Input.  The algorithm of the roipool operation.
 *  @param[in] output_dim
 *    Input.  The channel dimension of the output tensor.
 *  @param[in] group_size
 *    Input.  The pooled height and width of the output tensor. This operation assumes that
 *            pooled height = pooled width.
 *  @param[in] spatial_scale
 *    Input.  The scaling factor that maps input coordinates into the box coordinates.
 *  @param[in] rois_type
 *    Input.  The mode of regions of interest. The \p rois_type must be
 *            \p CNNL_ROI_CORNER_BATCHID or \p CNNL_ROI_CORNER_BATCHID.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One of the following conditions is met:
 *      - The value of \p desc is NULL.
 *      - The \p algo is not equal to CNNL_PSROIPOOL.
 *      - The \p output_dim is smaller than 1.
 *      - The \p group_size is smaller than 1.
 *      - The \p spatial_scale is smaller or equal to 0.
 *      - The \p rois_type is not equal to CNNL_ROI_CORNER_BATCHID or CNNL_ROI_BATCHID_CORNER.
 *  @par API Dependency
 *  - ::cnnlCreateRoipoolDescriptor() needs to be called before this function.
 *  - ::cnnlDestroyRoipoolDescriptor() needs to be called after this function.
 *
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlSetPsRoipoolDescriptor(cnnlRoipoolDescriptor_t desc,
                                                     const cnnlRoipoolAlgo_t algo,
                                                     const int output_dim,
                                                     const int group_size,
                                                     const float spatial_scale,
                                                     const cnnlRoiLayoutType_t rois_type);

// Group:Roipool
/*!
 *  @brief Gets extra space size required in roipool operation.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[in]  handle
 *    Input.  Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *            roipool operation.
 *  @param[in]  desc
 *    Input.  The descriptor of the roipool operation.
 *  @param[in]  input_desc
 *    Input.  The descriptor of the input tensor.
 *  @param[in]  algo
 *    Input.  The algorithm of the roipool operation. See ::cnnlRoipoolAlgo_t.
 *    for details.
 *  @param[out] size
 *    Output. The size of workspace.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - \p handle is NULL.
 *    - \p input_desc is NULL.
 *    - \p algo is not supported.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlGetRoipoolWorkspaceSize(cnnlHandle_t handle,
                            const cnnlRoipoolDescriptor_t desc,
                            const cnnlTensorDescriptor_t input_desc[],
                            cnnlRoipoolAlgo_t algo,
                            size_t *size);

/******************************************************************************
 * Cambricon CNNL OP: BertPre
 ******************************************************************************/
/*! The descriptor of the ::cnnlBertPre_v2 operation.
 *
 *  You need to call the ::cnnlCreateBertPreDescriptor function to create a descriptor,
 *  and call the ::cnnlSetBertPreDescriptor function to set the information
 *  to the descriptor. Also, you need to destroy the descriptor at the end
 *  with the ::cnnlDestroyBertPreDescriptor function.
 */
typedef struct cnnlBertPreStruct *cnnlBertPreDescriptor_t;

// Group:BertPre
/*!
 *  @brief Creates a descriptor pointed by \p desc for
 *  the bertpre operation and allocate memory for it.
 *
 *  @param[out]  desc
 *    Output. A pointer to the bertpre descriptor that holds information about
 *   the bertpre operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_ALLOC_FAILED
 *    The function failed to allocate memory space.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCreateBertPreDescriptor(cnnlBertPreDescriptor_t *desc);

// Group:BertPre
/*!
 *  @brief Destroys the descriptor of the bertpre
 *  operation and free memory for it.
 *
 *  @param[in]  desc
 *    Input. The descriptor of the bertpre operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    The value of \p desc is NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlDestroyBertPreDescriptor(cnnlBertPreDescriptor_t desc);

// Group:BertPre
/*!
 *  @brief Assigns the bertpre descriptor with parameters.
 *
 *  @par API Dependency
 *  - Before using this function, you need to call ::cnnlCreateBertPreDescriptor()
 *  to create a descriptor.
 *
 *  @param[in,out]  desc
 *    Input/output. The descriptor of the bertpre operation.
 *  @param[in]  addition_compute_dtype
 *    Input. The data type of computing accumulation for embedding.
 *  @param[in]  layernorm_compute_dtype
 *    Input. The data type of computing layernorm.
 *  @param[in]  layernorm_eps
 *    Input. The epsilon value of layernorm.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    The value of \p desc is NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetBertPreDescriptor(cnnlBertPreDescriptor_t desc,
                         const cnnlDataType_t addition_compute_dtype,
                         const cnnlDataType_t layernorm_compute_dtype,
                         const float layernorm_eps);

// Group:BertPre
/*!
 *  @brief Performs the preprocessing operation of the BERT network, which is used to complete
 *  the input embedding of network. For example, you can use this function to
 *  finish token embedding, segment embedding and position embedding, and add them
 *  together to get the embedding output, and you can also choose whether to use layernorm
 *  for output.
 *
 *  This operation performs with the following steps:
 *
 *  1. table1 embedding
 *
 *    table1_embedding = embedding (table1, index1, batch_num, seq_len)
 *
 *  2. [optional] table2 embedding
 *
 *    table2_embedding = embedding (table2, index2, batch_num, seq_len)
 *
 *  3. [optional] table3 embedding
 *
 *    table3_embedding = embedding (table3, index3, batch_num, seq_len)
 *
 *  4. addition
 *
 *    embedding_output = table1_embedding + table2_embedding + table3_embedding
 *
 *  5. [optional] layernorm
 *
 *    output = layernorm (embedding_output)
 *
 *  @param[in]  handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *           bertpre operation.
 *  @param[in]  desc
 *    Input. The descriptor of the bertpre operation.
 *  @param[in]  index1_desc
 *    Input. The descriptor of the \p index1 tensor, containing dimension and data type and layout of \p index1.
 *  @param[in]  index1
 *    Input. Pointer to the MLU tensor that stores the \p index1 tensor, which is the index of table1.
 *  @param[in]  table1_desc
 *    Input. The descriptor of the \p table1 tensor, containing dimension, data type and layout of \p table1.
 *  @param[in]  table1
 *    Input. Pointer to the MLU tensor that stores the \p table1 tensor, which can be any one of the token embedding
 *    table, segment embedding table and position embedding table.
 *  @param[in]  index2_desc
 *    Input. The descriptor of the \p index2 tensor, containing dimension and data type and layout of \p index2.
 *  @param[in]  index2
 *    Input. Pointer to the MLU tensor that stores the \p index2 tensor, which is the index of table2.
 *  @param[in]  table2_desc
 *    Input. The descriptor of the \p table2 tensor, containing dimension, data type and layout of \p table2.
 *  @param[in]  table2
 *    Input. Pointer to the MLU tensor that stores the \p table2 tensor, which can be any one of the token embedding
 *    table, segment embedding table and position embedding table.
 *  @param[in]  index3_desc
 *    Input. The descriptor of the \p index3 tensor, containing dimension and data type and layout of \p index3.
 *  @param[in]  index3
 *    Input. Pointer to the MLU tensor that stores the \p index3 tensor, which is the index of table3.
 *  @param[in]  table3_desc
 *    Input. The descriptor of the \p table3 tensor, containing dimension, data type and layout of \p table3.
 *  @param[in]  table3
 *    Input. Pointer to the MLU tensor that stores the \p table3 tensor, which can be any one of the token embedding
 *    table, segment embedding table and position embedding table.
 *  @param[in]  layernorm_scale_desc
 *    Input. The descriptor of the \p layernorm_scale tensor, containing dimension, data type and layout
 *           information.
 *  @param[in]  layernorm_scale
 *    Input. Pointer to the MLU tensor that stores the \p layernorm_scale tensor, which is the scaling
 *    factor of the layernorm.
 *  @param[in]  layernorm_bias_desc
 *    Input. The descriptor of the \p layernorm_bias tensor, containing dimension, data type and layout
 *           information of \p layernorm_bias.
 *  @param[in]  layernorm_bias
 *    Input. Pointer to the MLU tensor that stores the \p layernorm_bias tensor, which is the bias parameter
 *   of the layernorm.
 *  @param[out]  output_desc
 *    Input. The descriptor of the output tensor, containing dimension, data type and layout information.
 *  @param[out]  output
 *    Input. Pointer to the MLU tensor that stores the \p output tensor
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are encountered:
 *    - \p handle is NULL.
 *    - pointer is NULL.
 *    - parameters do not meet scale limitation.
 *  @retval CNNL_STATUS_ARCH_MISMATCH
 *    One or more of the following conditions are encountered:
 *    - This function is run on the hardware platform that is not supported.
 *  @par Data Type
 *  - table1: float, half
 *  - index1: int32
 *  - table2: float, half
 *  - index2: int32
 *  - table3: float, half*
 *  - index3: int32
 *  - layernorm scale: float, half
 *  - layernorm bias: float, half
 *  - output: float, half
 *  - The data types of table1, table2, table3, layernorm scale, layernorm bias and output
 *    must be same.
 *
 *  @par API Dependency
 *  - You need to call the ::cnnlCreateBertPreDescriptor() and ::cnnlSetBertPreDescriptor()
 *  functions before calling this function.
 *  - You need to call the cnnlDestroyBertPreDescriptor() function after calling this function.
 *
 *  @note
 *  - The cnnlBertPre_v2 is used to look up table1, table2 and table3 embedding
 *    and add them in turn.
 *  - The layouts of index1, index2 and index3 must be same, which is two-dimensional.
 *  - The layouts of table1, table2 and table3 must be same, which is two-dimensional.
 *  - The layouts of layernorm scale and layernorm bias must be same, which is one-dimensional.
 *  - The shape of output is three-dimensional.
 *  - The \p hidden_size must divisible by 64.
 *  - The \p layernorm_scale_desc, layernorm_scale, \p layernorm_bias_desc and \p layernorm_bias
 *    must be all NULL or not, which means whether to use layernorm.
 *  - The index1, index2 and index3 can be NULL. When it is NULL, the function uses the ID of
 *    the current token to loop up table, otherwise uses the value of index to look up table.
 *  - The table2 and table3 can be NULL or not respectively. When it is not NULL, the function will not
 *    look up this table and embedding.
 *  - The \p addition_compute_dtype and \p addition_compute_dtype must be float or half. When data
 *    type of table is float, \p addition_compute_dtype and \p addition_compute_dtype is invalid.
 *
 *  @par Performance Optimization
 *  - When the hidden_size is padded to 64, the performance is better.
 *
 *  @par Example
 *  @verbatim
    Dimension of index1: [batch_num, seq_len]

    Dimension of table1: [table1, hidden_size]

    Dimension of index2: [batch_num, seq_len]

    Dimension of table2: [table2, hidden_size]

    Dimension of index3: [batch_num, seq_len]

    Dimension of table3: [table3, hidden_size]

    Dimension of layernorm_scale: [hidden_size]

    Dimension of layernorm_bias: [hidden_size]

    Then we will get the output:

    Dimension of output: [batch_num, seq_len, hidden_size]
    @endverbatim
 */
cnnlStatus_t CNNL_WIN_API
cnnlBertPre_v2(cnnlHandle_t handle,
               const cnnlBertPreDescriptor_t desc,
               const cnnlTensorDescriptor_t index1_desc,
               const void *index1,
               const cnnlTensorDescriptor_t table1_desc,
               const void *table1,
               const cnnlTensorDescriptor_t index2_desc,
               const void *index2,
               const cnnlTensorDescriptor_t table2_desc,
               const void *table2,
               const cnnlTensorDescriptor_t index3_desc,
               const void *index3,
               const cnnlTensorDescriptor_t table3_desc,
               const void *table3,
               const cnnlTensorDescriptor_t layernorm_scale_desc,
               const void *layernorm_scale,
               const cnnlTensorDescriptor_t layernorm_bias_desc,
               const void *layernorm_bias,
               cnnlTensorDescriptor_t output_desc,
               void *output);

/******************************************************************************
 * Cambricon CNNL OP: BertPost
 ******************************************************************************/
// Group:BertPost
/*!
 *  @brief Gets extra space size required in the bert_post
 *  operation.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[in]  input_desc
 *    Input. The descriptor of the input tensor, containing the dimension and the layout information.
 *  @param[out] size
 *    Output. Size of workspace.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One of the following conditions is met:
 *    - The input_desc is not is not initialized.
 *    - The size is NULL.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlGetBertPostWorkspaceSize(const cnnlTensorDescriptor_t input_desc,
                             size_t *size);

// Group:BertPost
/*!
 *  @brief Implements the post processing in the BERT network.
 *
 *  This operation performs with the following steps:
 *
 *  1. X = input[:, 0, :].reshape((batch_size, hidden_size)).
 *
 *  2. T1 = tanh(matmul(X, inner_filter) + inner_bias).
 *
 *  3. T2 = matmul(T1, outer_filter) + outer_bias
 *
 *  4. if is_softmax = true: output = softmax(T2, axis=1);
 *     else: output = T2
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[in]  handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the bert_post
 *           operation.
 *  @param[in]  input_desc
 *    Input. The descriptor of the input tensor.
 *  @param[in]  input
 *    Input. Pointer to the MLU memory that stores the input data.
 *  @param[in]  inner_filter_desc
 *    Input. The descriptor of the \p inner_filter tensor.
 *  @param[in]  inner_filter
 *    Input. Pointer to the MLU memory that stores the inner filter data.
 *  @param[in]  outer_filter_desc
 *    Input. The descriptor of the \p inner_filter tensor.
 *  @param[in]  outer_filter
 *    Input. Pointer to the MLU memory that stores the outer filter data.
 *  @param[in]  inner_bias_desc
 *    Input. The descriptor of the \p inner_bias tensor.
 *  @param[in]  inner_bias
 *    Input. Pointer to the MLU memory that stores the inner bias data.
 *  @param[in]  outer_bias_desc
 *    Input. The descriptor of the \p outer_bias tensor.
 *  @param[in]  outer_bias
 *    Input. Pointer to the MLU memory that stores the outer bias data.
 *  @param[in]  is_softmax
 *    Input. A Boolean value indicating whether to apply softmax on the output.
 *  @param[in]  workspace
 *    Input. Pointer to the MLU memory that is used as an extra workspace.
 *  @param[in]  workspace_size
 *    Input. The size of workspace.
 *  @param[out]  output_desc
 *    Output. The descriptor of output tensor.
 *  @param[out]  output
 *    Output. Pointer to the MLU memory that stores the output data.
 *  @retval  CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval  CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are satisfied:
 *    - \p handle is NULL.
 *    - \p input is NULL.
 *    - \p output is NULL.
 *
 *  @par Data Type
 *   - input: float, half.
 *   - filter: int16, int8, half.
 *   - bias: float, half.
 *   - output: float, half.
 *
 *  @note
 *  - The type of filter is recommended to be int16.
 *    If the type of filter is int8, the precision of output
 *    may not be guaranteed.
 *
 *  @par Scale Limitation
 *   - \p batch_size and \p seq_len must be greater than 0.
 *   - \p hidden_size only supports 512, 768, 1024.
 *   - \p inner_size only supports 512, 768, 1024.
 *   - The \p hidden_size and \p inner_size could be freely combined,
 *     such as (512, 768), (768, 512).
 *   - \p outer_size: 2 <= \p outer_size <= 64.
 *
 *  @par Performance Optimization
 *   - When the dimension of input meets batch_size*seq_len = 512*n(n=1,2,3..), the performance is best.
 *
 *  @par API Dependency
 *   - You need to call the ::cnnlGetBertPostWorkspaceSize() function before calling this
 *   function.
 *
 *  @par Example
 *  @verbatim
    Dimension of input: [batch_size, seq_len, hidden_size]

    Dimension of inner_filter: [inner_size, hidden_size]

    Dimension of outer_filter: [outer_size, inner_size]

    Dimension of inner_bias: [inner_size]

    Dimension of outer_bias: [outer_size]

    Dimension of output: [batch_size, outer_size]
    @endverbatim
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlBertPost(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t input_desc,
                                       const void *input,
                                       const cnnlTensorDescriptor_t inner_filter_desc,
                                       const void *inner_filter,
                                       const cnnlTensorDescriptor_t outer_filter_desc,
                                       const void *outer_filter,
                                       const cnnlTensorDescriptor_t inner_bias_desc,
                                       const void *inner_bias,
                                       const cnnlTensorDescriptor_t outer_bias_desc,
                                       const void *outer_bias,
                                       const bool is_softmax,
                                       void *workspace,
                                       size_t workspace_size,
                                       const cnnlTensorDescriptor_t output_desc,
                                       void *output);

// Group:BertPost
/*!
 *  @brief Implements the post processing in the BERT base-cased network.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @par Formula
 *  - See "cnnlBertBaseCasedPost" in "Cambricon CNNL Extra User Guide" for details.
 *
 *  @param[in]  handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the bert_post
 *    operation.
 *  @param[in]  input_desc
 *    Input. The descriptor of the input tensor.
 *  @param[in]  input
 *    Input.  Pointer to the MLU memory that stores the input data.
 *  @param[in]  filter_desc
 *    Input. The descriptor of the filter tensor.
 *  @param[in]  filter
 *    Input. Pointer to the MLU memory that stores the filter data used in the Fully Connected operation.
 *  @param[in]  bias_desc
 *    Input. The descriptor of bias tensor. A nullptr means no bias.
 *  @param[in]  bias
 *    Input. Pointer to the MLU memory that stores the bias data.
 *    The bias is not used in the operation if this parameter is set to nullptr.
 *  @param[in]  layernorm_mode
 *    Input. The LayerNorm mode. Only supports \p CNNL_TRANSFORMER_PRE_LAYERNORM.
 *  @param[in]  layernorm_scale_desc
 *    Input. The descriptor of the \p layernorm_scale tensor, containing the dimension and layout information.
 *  @param[in]  layernorm_scale
 *    Input. Pointer to the MLU memory that stores the scale data used in the LayerNorm operation.
 *  @param[in]  layernorm_bias_desc
 *    Input. The descriptor of the \p layernorm_bias tensor, containing dimension and layout information.
 *  @param[in]  layernorm_bias
 *    Input. Pointer to the MLU memory that stores the bias data used in the LayerNorm operation.
 *  @param[in]  output_desc
 *    Input. The descriptor of the output tensor.
 *  @param[out]  output
 *    Output. Pointer to the MLU memory that stores an array of output data.
 *  @param[in]  layernorm_output_desc
 *    Input. The descriptor of \p layernorm_output tensor.
 *  @param[out]  layernorm_output
 *    Output. Pointer to the MLU memory that stores an array of LayerNorm output data.
 *  @retval  CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval  CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are satisfied:
 *    - The parameter is NULL.
 *
 *  @par Data Type
 *   - input: float, half.
 *   - filter: half.
 *   - bias: float, half.
 *   - layernorm_scale: float, half.
 *   - layernorm_bias: float, half.
 *   - output: float, half.
 *   - layernorm_output: float, half.
 *   - The data type of input, bias, layernorm_scale, layernorm_bias, output and
 *     layernorm_output must be the same.
 *
 *  @note
 *  - The \p layernorm_mode only supports \p CNNL_TRANSFORMER_PRE_LAYERNORM.
 *
 *  @par Scale Limitation
 *  - \p batch_size and \p seq_len must be greater than 0.
 *  - \p hidden_size only supports 512, 768, 1024.
 *  - \p outer_size only supports 512, 768, 1024 (Any combinations of \p hidden_size
 *    and \p outer_size is supported).
 *  @par Performance Optimization
 *  - When the dimension of input meets \p batch_size * \p seq_len = 512*n(n=1,2,3..), the performance is best.
 *  @par Example
    @verbatim
     Dimension of input: [batch_size, seq_len, hidden_size]

     Dimension of filter: [outer_size, hidden_size]

     Dimension of bias: [outer_size]

     Dimension of output: [batch_size, outer_size]

     Dimension of layernorm_output: [batch_size, seq_len, hidden_size]
    @endverbatim
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlBertBaseCasedPost(cnnlHandle_t handle,
                                                const cnnlTensorDescriptor_t input_desc,
                                                const void *input,
                                                const cnnlTensorDescriptor_t filter_desc,
                                                const void *filter,
                                                const cnnlTensorDescriptor_t bias_desc,
                                                const void *bias,
                                                const cnnlTransformerLayernormMode_t layernorm_mode,
                                                const cnnlTensorDescriptor_t layernorm_scale_desc,
                                                const void *layernorm_scale,
                                                const cnnlTensorDescriptor_t layernorm_bias_desc,
                                                const void *layernorm_bias,
                                                const cnnlTensorDescriptor_t output_desc,
                                                void *output,
                                                const cnnlTensorDescriptor_t layernorm_output_desc,
                                                void *layernorm_output);

/******************************************************************************
 * Cambricon CNNL OP: transformer_enc_dec_attn
 ******************************************************************************/
/*!
 * @brief The descriptor of the ::cnnlTransformerEncDecAttn operation that holds transformer-network parameters.
 *
 * It is deprecated and will be removed in future release.
 */
typedef struct cnnlTransformerEncDecAttnStruct *cnnlTransformerEncDecAttnDescriptor_t;

// Group:Transformer Enc Dec Attn
/*!
 *  @brief Executes the encoder-decoder attention in
 *  TransformerDecoderLayer.
 *
 *  This operation performs with the following three steps:
 *
 *  1. Generate Q
 *
 *    Compute query from input tensor.
 *
 *    query = mlp(input_tensor, attr_kernel_Q)
 *
 *  2. Q x K batch_dot, softmax and QK x V batch_dot
 *
 *    Compute QxK batch_dot for all heads.
 *
 *    for head_id in (0, head_num)
 *
 *      QK = batch_dot(query_in_head, key_in_head)
 *
 *      QK = softmax(QK)
 *
 *      QKV = batch_dot(QK, value_in_head)
 *
 *  3. Generate output
 *
 *    Compute output from QKV.
 *
 *    output = mlp(QKV, attout_kernel)
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *           transformer_enc_dec_attn operation.
 *  @param[in] transformer_enc_dec_attn_desc
 *    Input. The descriptor of the transformer_enc_dec_attn operation.
 *  @param[in] input_tensor_desc
 *    Input. The descriptor of the input tensor, which contains dimension
 *           [batch_size, dec_seq_len, hidden_size] of the tensor.
 *  @param[in] input_tensor
 *    Input. Pointer to the MLU memory that stores the input tensor.
 *  @param[in] attr_kernel_query_desc
 *    Input. The descriptor of the \p attr_kernel_query tensor, which contains dimension
 *           [hidden_size, hidden_size] of the tensor.
 *  @param[in] attr_kernel_query
 *    Input. Pointer to the MLU memory that stores the \p attr_kernel_query tensor, which
 *    is used to do matmul with the input tensor to get the query.
 *  @param[in] attr_bias_query_desc
 *    Input. The descriptor of the \p attr_bias_query tensor, which contains dimension
 *           [hidden_size] of the tensor.
 *  @param[in] attr_bias_query
 *    Input. Pointer to the MLU memory that stores the \p attr_bias_query tensor,
 *    which is the bias value added to the query.
 *  @param[in] enc_output_key_desc
 *    Input. The descriptor of the \p enc_output_key tensor, which contains dimension
 *           [batch_size, head_num, max_seq_len, head_size] of the tensor.
 *  @param[in] enc_output_key
 *    Input. Pointer to the MLU memory that stores the \p enc_output_key tensor.
 *  @param[in] enc_output_value_desc
 *    Input. The descriptor of the \p enc_output_value tensor, which contains dimension
 *           [batch_size, head_num, max_seq_len, head_size] of the tensor.
 *  @param[in] enc_output_value
 *    Input. Pointer to the MLU memory that stores the \p enc_output_value tensor.
 *  @param[in] enc_output_mask_desc
 *    Input. The descriptor of the \p enc_output_mask tensor, which contains dimension
 *           [batch_size, 1, ..., max_seq_len] (can have arbitrary number of 1 between the first
 *            dim "batch_size" and the last dim "max_seq_len", including 0).
 *  @param[in] enc_output_mask
 *    Input. Pointer to the MLU memory that stores the \p enc_output_mask tensor.
 *  @param[in] attout_kernel_desc
 *    Input. The descriptor of the \p attout_kernel tensor, which contains dimension
 *           [hidden_size, hidden_size].
 *  @param[in] attout_kernel
 *    Input. Pointer to the MLU memory that stores the \p attout_kernel tensor, which is
 *    to do matmul with qkv to get the output.
 *  @param[in] attout_bias_desc
 *    Input. The descriptor of the \p attout_bias tensor, which contains dimension of
 *           [hidden_size].
 *  @param[in] attout_bias
 *    Input. Pointer to the MLU memory that stores the \p attout_bias tensor, which is
 *    the bias value added to the qkv output.
 *  @param[in] layernorm_scale_desc
 *    Input. The descriptor of layernorm_scale tensor, which contains dimension of
 *           [hidden_size].
 *  @param[in] layernorm_scale
 *    Input. Pointer to the MLU memory that stores the \p layernorm_scale tensor
 *  @param[in] layernorm_bias_desc
 *    Input. The descriptor of the \p layernorm_bias tensor, which contains dimension of
 *           [hidden_size].
 *  @param[in] layernorm_bias
 *    Input. Pointer to the MLU memory that stores the layernorm_bias tensor.
 *  @param[in]  workspace
 *    Input. Pointer to data address of workspace.
 *  @param[in]  workspace_size
 *    Input. The size of workspace.
 *  @param[in] enc_dec_attn_out_desc
 *    Input. The descriptor of the \p enc_dec_attn_out tensor, which contains dimension of
 *           [batch_size, dec_seq_len, hidden_size].
 *  @param[out] enc_dec_attn_out
 *    Output. Pointer to the MLU memory that stores the \p enc_dec_attn_out tensor.
 *
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of following conditions are encountered:
 *    - \p handle is NULL.
 *    - pointer is NULL.
 *    - parameters do not meet scale limitation.
 *  @retval CNNL_STATUS_ARCH_MISMATCH
 *    One or more of following conditions are encountered:
 *    - This function is run on the hardware platform that is not supported.
 *  @par Data Type
 *    - Data types of \p enc_dec_attn_out, \p input_tensor, \p attr_bias_query, \p enc_output_key,
 *    \p enc_output_value, \p enc_output_mask, \p attout_bias, \p layernorm_scale
 *    and \p layernorm_bias must be the same, either float or half.
 *  @par API Dependency
 *  - You need to call the ::cnnlCreateTransformerEncDecAttnDescriptor(),
 *   ::cnnlSetTransformerEncDecAttnDescriptor_v3() and
 *   ::cnnlGetTransformerEncDecAttnWorkspaceSize() functions before calling this
 *   function.
 *  - You need to call the ::cnnlDestroyTransformerEncDecAttnDescriptor() function
 *   after calling this function.
 *
 * @par Scale Limitation
 *  - When device type is one of MLU500 series:
 *    - \p max_seq_len must be less than or equal to 1024.
 *    - \p head_size_qk and \p head_size_v must be equal, and must be in range of [16, 128].
 *    - \p head_num must be in range of [1, 128].
 *    - All inputs and output must have the same data type, either float or half.
 *    - Sizeof(float) * batch_size * max_seq_len_pad * hidden_size and
 *      sizeof(float) * batch_size * dec_seq_len_pad * hidden_size is limited in the
 *      value range of int type.
 *  - When device type is MLU300 series:
 *    - \p batch_size should be equal to or less than 128.
 *    - \p max_seq_len should be equal to or less than 512 on MLU270, and equal to or
 *      less than 512 on MLU300 series. It should be equal to or less than 128 if \p head_num is 4 and
 *      \p head_size is in [65, 128] on MLU300 series. \p max_seq_len should not be padded.
 *    - \p head_num supports 8, 12 and 16 on MLU270, and supports 4, 8, 12 and 16 on MLU300 series.
 *    - \p head_size_qk and \p head_size_v are in [1, 64] if \p head_num is 4, 8, 12 and 16, and
 *      \p head_size_qk and \p head_size_v should be equal to or less than 128 if \p head_num is 4 and
 *      \p filter type is float or half. If \p filter type is int8 or int16, \p head_size_qk and
 *      \p head_size_v can only be set to 64.
 *    - \p dec_seq_len would be treated as \p beam_size and should be equal to or less than
 *      12 on MLU270; it would be treated as \p beam_size and should be equal to or less
 *      than 32 on MLU300 series.
 *    - \p layernorm_res_mode can be set to \p CNNL_TRANSFORMER_NO_LAYERNORM_WITH_RESIDUAL,
 *      \p CNNL_TRANSFORMER_PRE_LAYERNORM_INSIDE_RESIDUAL and
 *      \p CNNL_TRANSFORMER_POST_LAYERNORM_OUTSIDE_RESIDUAL on MLU270.
 *    - \p layernorm_res_mode can be set to \p CNNL_TRANSFORMER_NO_LAYERNORM_WITH_RESIDUAL,
 *      \p CNNL_TRANSFORMER_NO_LAYERNORM_NO_RESIDUAL and
 *      \p CNNL_TRANSFORMER_PRE_LAYERNORM_INSIDE_RESIDUAL on MLU300 series.
 *
 *  @par Performance Optimization
 *  - When \p head_num is 12, \p batch_size is a multiple of 8, data_type and \p filters_type are half,
 *  \p dec_seq_len and \p max_seq_len are smaller than 128, the performance is best on MLU300 series.
 *
 *  @par Reference
 *  - https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
 *
 * @par Example
 * @verbatim
     Dimension of enc_dec_attn_out: [batch_size, dec_seq_len, hidden_size]

     Dimension of input_tensor: [batch_size, dec_seq_len, hidden_size]

     Dimension of attr_kernel_query: [hidden_size, hidden_size]

     Dimension of attr_bias_query: [hidden_size]

     Dimension of enc_output_key: [batch_size, head_num, max_seq_len, head_size]

     Dimension of enc_output_value: [batch_size, head_num, max_seq_len, head_size]

     Dimension of enc_output_mask: [batch_size, max_seq_len]

     Dimension of attout_kernel: [hidden_size, hidden_size]

     Dimension of attout_bias: [hidden_size]

     Dimension of layernorm_scale: [hidden_size]

     Dimension of layernorm_bias: [hidden_size]
   @endverbatim
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlTransformerEncDecAttn(
    cnnlHandle_t handle,
    const cnnlTransformerEncDecAttnDescriptor_t transformer_enc_dec_attn_desc,
    const cnnlTensorDescriptor_t input_tensor_desc,
    const void* input_tensor,
    const cnnlTensorDescriptor_t attr_kernel_query_desc,
    const void* attr_kernel_query,
    const cnnlTensorDescriptor_t attr_bias_query_desc,
    const void* attr_bias_query,
    const cnnlTensorDescriptor_t enc_output_key_desc,
    const void* enc_output_key,
    const cnnlTensorDescriptor_t enc_output_value_desc,
    const void* enc_output_value,
    const cnnlTensorDescriptor_t enc_output_mask_desc,
    const void* enc_output_mask,
    const cnnlTensorDescriptor_t attout_kernel_desc,
    const void* attout_kernel,
    const cnnlTensorDescriptor_t attout_bias_desc,
    const void* attout_bias,
    const cnnlTensorDescriptor_t layernorm_scale_desc,
    const void* layernorm_scale,
    const cnnlTensorDescriptor_t layernorm_bias_desc,
    const void* layernorm_bias,
    void *workspace,
    size_t workspace_size,
    const cnnlTensorDescriptor_t enc_dec_attn_out_desc,
    const void* enc_dec_attn_out);

// Group:Transformer Enc Dec Attn
/*!
 *  @brief Creates a descriptor pointed by
 *  \p transformer_enc_dec_attn_desc for the transformer_enc_dec_attn operation.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[out] transformer_enc_dec_attn_desc
 *    Output. A pointer to the transformer_enc_dec_attn descriptor that holds
 *   information about the transformer_enc_dec_attn operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_ALLOC_FAILED
 *    The function failed to allocate memory.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlCreateTransformerEncDecAttnDescriptor(
    cnnlTransformerEncDecAttnDescriptor_t* transformer_enc_dec_attn_desc);

// Group:Transformer Enc Dec Attn
/*!
 *  @brief Destroys the transformer_enc_dec_attn descriptor.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[in] transformer_enc_dec_attn_desc
 *    Input. The descriptor of the transformer_enc_dec_attn operation to free memory.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    The value of \p transformer_enc_dec_attn_desc is NULL.
 *
 *  @par API Dependency
 *  - ::cnnlCreateTransformerEncDecAttnDescriptor() need to be called before this function.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlDestroyTransformerEncDecAttnDescriptor(
    cnnlTransformerEncDecAttnDescriptor_t transformer_enc_dec_attn_desc);

// Group:Transformer Enc Dec Attn
/*!
 *  @brief Assigns the transformer_enc_dec_attn descriptor with parameters.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[in,out] transformer_enc_dec_attn_desc
 *    Input/output. The descriptor of the transformer_enc_dec_attn operation.
 *  @param[in]  head_num
 *    Input. The number of heads in self attention.
 *  @param[in]  head_size_qk
 *    Input. The size of one head in self attention for query and key.
 *  @param[in]  head_size_v
 *    Input. The size of one head in self attention for value.
 *  @param[in]  max_seq_len
 *    Input. Sequence length in transformer encoder.
 *  @param[in]  query_factor
 *    Input. The scaling factor to scale the query.
 *  @param[in]  ln_res_mode
 *    Input. The layernorm and residual structure. See ::cnnlTransformerLayernormResidualStructure_t
 *    for details.
 *  @param[in]  use_bias
 *    Input. A Boolean value indicating whether to use bias when computing query and attention output.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One of the following conditions are met:
 *    - The value of \p transformer_enc_dec_attn_desc is NULL.
 *
 *  @par API Dependency
 *  - Before calling this function, you need to call ::cnnlCreateTransformerEncDecAttnDescriptor()
 *    to create a descriptor.
 *
 *  @par Scale Limitation
 *  - \p head_size_qk and \p head_size_v only support 64.
 *  - \p head_num supports 4, 8, 12 and 16.
 *  - \p max_seq_len must be less than or equal to 1024 when \p filter is int8 or int16,
 *    and less than or equal to 512 when \p filter is half or float.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlSetTransformerEncDecAttnDescriptor_v2(
    cnnlTransformerEncDecAttnDescriptor_t transformer_enc_dec_attn_desc,
    const int head_num,
    const int head_size_qk,
    const int head_size_v,
    const int max_seq_len,
    const float query_factor,
    const cnnlTransformerLayernormResidualStructure_t ln_res_mode,
    const bool use_bias);

// Group:Transformer Enc Dec Attn
/*!
 *  @brief Assigns the transformer_enc_dec_attn descriptor with parameters.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[in,out] transformer_enc_dec_attn_desc
 *    Input/output. The descriptor of the transformer_enc_dec_attn operation.
 *  @param[in]  head_num
 *    Input. The number of heads in self attention.
 *  @param[in]  head_size_qk
 *    Input. The size of one head in self attention for query and key.
 *  @param[in]  head_size_v
 *    Input. The size of one head in self attention for value.
 *  @param[in]  max_seq_len
 *    Input. Sequence length in transformer encoder.
 *  @param[in]  query_factor
 *    Input. The scaling factor to scale the query.
 *  @param[in]  ln_res_mode
 *    Input. The layernorm and residual structure. See ::cnnlTransformerLayernormResidualStructure_t
 *    for details.
 *  @param[in]  use_bias
 *    Input. A Boolean value indicating whether to use bias when computing query and attention output.
 *  @param[in] attr_kernel_query_quant_desc
 *    Input. A quantization descriptor that holds quantization information of attr_kernel_query tensor.
 *           For detailed information, see cnnlQuantizeExDescriptor_t.
 *  @param[in] attout_kernel_quant_desc
 *    Input. A quantization descriptor that holds quantization information of attout_kernel tensor.
 *           For detailed information, see cnnlQuantizeExDescriptor_t.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One of the following conditions are met:
 *    - The value of \p transformer_enc_dec_attn_desc is NULL.
 *
 *  @par API Dependency
 *  - Before calling this function, you need to call ::cnnlCreateTransformerEncDecAttnDescriptor()
 *    to create a descriptor.
 *
 *  @par Scale Limitation
 *  - \p head_size_qk and \p head_size_v only support 64.
 *  - \p head_num supports 4, 8, 12 and 16.
 *  - \p max_seq_len must be less than or equal to 1024 when \p filter is int8 or int16,
 *    and less than or equal to 512 when \p filter is half or float.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlSetTransformerEncDecAttnDescriptor_v3(
    cnnlTransformerEncDecAttnDescriptor_t transformer_enc_dec_attn_desc,
    const int head_num,
    const int head_size_qk,
    const int head_size_v,
    const int max_seq_len,
    const float query_factor,
    const cnnlTransformerLayernormResidualStructure_t ln_res_mode,
    const bool use_bias,
    cnnlQuantizeExDescriptor_t attr_kernel_query_quant_desc,
    cnnlQuantizeExDescriptor_t attout_kernel_quant_desc);

// Group:Transformer Enc Dec Attn
/*!
 *  @brief Gets the extra space size required in
 *  the transformer_enc_dec_attn operation.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[in] transformer_enc_dec_attn_desc
 *    Input. The descriptor of the transformer_enc_dec_attn operation.
 *  @param[in] input_tensor_desc
 *    Input. The descriptor of the input tensor, which contains the dimension
 *           [batch_size, dec_seq_len, hidden_size] of the tensor.
 *  @param[in] enc_output_key_desc
 *    Input. The descriptor of the \p enc_output_key tensor, which contains the dimension
 *           [batch_size, head_num, max_seq_len_pad, head_size] of the tensor.
 *  @param[out] size
 *    Output. The size of workspace.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    The value of \p transformer_enc_dec_attn_desc is NULL.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlGetTransformerEncDecAttnWorkspaceSize(
    const cnnlTransformerEncDecAttnDescriptor_t transformer_enc_dec_attn_desc,
    const cnnlTensorDescriptor_t input_tensor_desc,
    const cnnlTensorDescriptor_t enc_output_key_desc,
    size_t *size);

/******************************************************************************
 * Cambricon CNNL OP: TransformerFFN
 ******************************************************************************/
/*! The descriptor of the ::cnnlTransformerFFN operation.
 *
 *  You need to call the ::cnnlCreateTransformerFFNDescriptor function
 *  to create a descriptor, and call the ::cnnlSetTransformerFFNDescriptor_v2 function
 *  to set the information of the ffn operation to the descriptor. Also, you need to
 *  destroy the descriptor at the end with the ::cnnlDestroyTransformerFFNDescriptor
 *  function.
 */
typedef struct cnnlTransformerFFNStruct *cnnlTransformerFFNDescriptor_t;

// Group:Transformer FFN
/*!
 *  @brief Performs the FFN layer in transformer network.
 *
 *  This operation performs with the following steps:
 *
 *  1. fc
 *
 *    intermediate = fc(input, inner_filters, inner_bias)
 *
 *  2. activation
 *
 *    intermediate = active(intermediate)
 *
 *  3. fc
 *
 *    output = fc(intermediate, outer_filters, outer_bias)
 *
 *  4. layernorm
 *
 *    no layernorm: output = ffn(input).
 *
 *    pre layernorm: output = ffn(layernorm(input)) + input.
 *
 *    post layernorm: output = layernorm(ffn(input) + input).

 *  @param[in]  handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *           transformerffn operation.
 *  @param[in] desc
 *    Input. The descriptor of the transformerffn operation.
 *  @param[in]  input_desc
 *    Input. The descriptor of the \p input tensor, containing the dimension and layout information.
 *  @param[in]  input
 *    Input. Pointer to the MLU memory that stores the input tensor.
 *  @param[in]  inner_filters_desc
 *    Input. The descriptor of \p inner_filters, containing dimension and the layout of
 *           \p inner_filters.
 *  @param[in]  inner_filters
 *    Input. Pointer to the MLU memory that stores the \p inner_filters tensor.
 *  @param[in]  inner_bias_desc
 *    Input. The descriptor of \p inner_bias, containing dimension and the layout of
 *           \p inner_bias.
 *  @param[in]  inner_bias
 *    Input. Pointer to the MLU memory that stores the \p inner_bias tensor.
 *  @param[in]  outer_filters_desc
 *    Input. The descriptor of \p outer_filters, containing the dimension and layout of
 *           \p outer_filters.
 *  @param[in]  outer_filters
 *    Input. Pointer to the MLU memory that stores the \p outer_filters tensor.
 *  @param[in]  outer_bias_desc
 *    Input. The descriptor of \p outer_bias, containing the dimension and layout of
 *           \p outer_bias.
 *  @param[in]  outer_bias
 *    Input. Pointer to the MLU memory that stores the \p outer_bias tensor.
 *  @param[in]  layernorm_scale_desc
 *    Input. The descriptor of \p layernorm_scale, containing the dimension and layout
 *           of \p layernorm_scale.
 *  @param[in]  layernorm_scale
 *    Input. Pointer to the MLU memory that stores the \p layernorm_scale tensor.
 *  @param[in]  layernorm_bias_desc
 *    Input. The descriptor of \p layernorm_bias, containing the dimension and layout
 *           of \p layernorm_bias.
 *  @param[in]  layernorm_bias
 *    Input. Pointer to the MLU memory that stores the \p layernorm_bias tensor.
 *  @param[in]  workspace
 *    Input. Pointer to the workspace.
 *  @param[in]  workspace_size
 *    Input. The value of workspace size.
 *  @param[out]  output_desc
 *    Input. The descriptor of the output tensor, containing the dimension and layout of \p output.
 *  @param[out]  output
 *    Input. Pointer to the MLU memory that stores the \p output tensor.
 *  @retval CNNL_STATUS_SUCCESS = 0
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are encountered:
 *    - \p handle is NULL.
 *    - pointer is NULL.
 *    - parameters do not meet scale limitation.
 *  @retval CNNL_STATUS_ARCH_MISMATCH
 *    One or more of the following conditions are encountered:
 *    - This function is run on the hardware platform that is not supported.
 *
 *  @par Data Type
 *  - input: float, half.
 *  - inner filters: int16, int8, half, float.
 *  - inner bias: float, half.
 *  - outer filters: int16, int8, half, float.
 *  - outer bias: float, half.
 *  - layernorm scale: float, half.
 *  - layernorm bias: float, half.
 *  - output: float, half.
 *
 *  @note
 *   - The data type of inner_filters must be same with outer_filters.
 *   - The data type of input, output, inner_bias, outer_bias, layernorm_scale
 *     and layernorm_bias must be the same.
 *   - The data type of inner_filters and outer_filters must be same on MLU300 series.
 *   - Reformatting filters on device is not supported, and set \p enable_reshape_filters to false.
 *   - Layernorm and Residual Structure does not support \p CNNL_TRANSFORMER_POST_LAYERNORM_INSIDE_RESIDUAL.
 *   - When the data type of inner_filters and outer_filters is int8, the following conditions
 *     must be meet:
 *     - FFN is used for BERT base and BERT large.
 *     - The qat function is not supported for BERT large.
 *     - The data type of input must be half for BERT large.
 *     - enable_online_quantify == false.
 *     - layernorm_structure == CNNL_TRANSFORMER_PRE_LAYERNORM_OUTSIDE_RESIDUAL.
 *     - post_scale == 1.0.
 *     - active_mode != CNNL_ACTIVATION_SWISH and device must be MLU300 series.
 *  - This operation supports MLU300 and MLU500 series.
 *
 *  @par Scale Limitation
 *   - The number of dimensions of all tensors must match.
 *   - \p batch_num and \p seq_len must be greater than 0.
 *   - \p hidden_size should be in range of [16, 14336], and \p filter_size should be greater than
 *        or equal to 16. Ensure the size of the tensor is less than 2G bytes. In addition, when
 *        \p hidden_size is greater than or equal to 2048, it must be a multiple of 512.
 *   - If \p enable_inner_layernormal of \p desc is set to true, following requirements must be satisfied:
 *     - PAD_UP(hidden_size, 256) * PAD_UP(filter_size, 256) * sizeof(data type) <= 4MB
 *     - Data types of input, filters and output must be the same, either float or half.
 *     - Dimension of layernorm_scale and layernorm_bias must be [hidden_size + filter_size], where [0:hidden_size]
 *      contains scale and bias for pre or post layernormal, and [hidden_size:-1] contains scale and bias for
 *      inner layernormal. If pre/post layernormal is disabled, data in [0:hidden_size] is ignored.

 *  @par API Dependency
 *  - You need to call the ::cnnlCreateTransformerFFNDescriptor(), and
 *   ::cnnlGetTransformerFFNWorkspaceSize() functions before calling this function.
 *  - You need to call the ::cnnlDestroyTransformerFFNDescriptor() function after
 *   calling this function.

 *
 *  @par Example
 *  @verbatim
     Dimension of input: [batch_num, seq_len, hidden_size]

     Dimension of inner_filters: [hidden_size, filter_size]

     Dimension of inner_bias: [filter_size]

     Dimension of outer_filters: [filter_size, hidden_size]

     Dimension of outer_bias: [hidden_size]

     Dimension of layernorm_scale: [hidden_size] or [hidden_size + filter_size] when enable_inner_layernormal is set to true.

     Dimension of layernorm_bias: [hidden_size] or [hidden_size + filter_size] when enable_inner_layernormal is set to true.

     Then we will get the output:

     Dimension of output: [batch_num, seq_len, hidden_size]
    @endverbatim
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlTransformerFFN(cnnlHandle_t handle,
                   const cnnlTransformerFFNDescriptor_t desc,
                   const cnnlTensorDescriptor_t input_desc,
                   const void *input,
                   const cnnlTensorDescriptor_t inner_filters_desc,
                   const void *inner_filters,
                   const cnnlTensorDescriptor_t inner_bias_desc,
                   const void *inner_bias,
                   const cnnlTensorDescriptor_t outer_filters_desc,
                   const void *outer_filters,
                   const cnnlTensorDescriptor_t outer_bias_desc,
                   const void *outer_bias,
                   const cnnlTensorDescriptor_t layernorm_scale_desc,
                   const void *layernorm_scale,
                   const cnnlTensorDescriptor_t layernorm_bias_desc,
                   const void *layernorm_bias,
                   void *workspace,
                   size_t workspace_size,
                   cnnlTensorDescriptor_t output_desc,
                   void *output);

// Group:Transformer FFN
/*!
 *  @brief Creates a descriptor for the transformerfnn
 *  operation and allocate memory for it.
 *
 *  @param[out]  desc
 *    Output. A pointer to the proposal descriptor that holds the information
 *    about the transformerfnn operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_ALLOC_FAILED
 *    The function failed to malloc memory.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCreateTransformerFFNDescriptor(cnnlTransformerFFNDescriptor_t *desc);

// Group:Transformer FFN
/*!
 *  @brief Destroys the descriptor of transformerfnn and free
 * memory for it.
 *
 *  @param[in]  desc
 *    Input. The descriptor of the transformerfnn operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One of the following conditions are met:
 *    - The value of \p desc is NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlDestroyTransformerFFNDescriptor(cnnlTransformerFFNDescriptor_t desc);

// Group:Transformer FFN
/*!
 *  @brief Assigns the transformerffn descriptor with parameters.
 *
 *  @par API Dependency
 *  - Before using this function, you need to invoke ::cnnlCreateTransformerFFNDescriptor()
 *  to create the descriptor.
 *
 *  @param[in,out]  desc
 *    Input/output. Descriptor of transformerffn operation.
 *  @param[in]  active_mode
 *    Input. Activation mode of ffn layer. It supports relu, gelu and swish.
 *  @param[in]  layernorm_structure
 *    Input. The layernorm and residual structure.
 *  @param[in]  enable_online_quantify
 *    Input. A Boolean value indicating whether to enable online quantization.
 *    When \p enable_online_quantify is true, input and intermediate result quantized
 *    online.
 *  @param[in]  use_quantify_scale
 *    Input. A Boolean value indicating whether to use quantization scale.
 *  @param[in]  enable_reshape_filters
 *    Input. A Boolean value indicating whether to reformat filters. When \p enable_reshape_filters = true,
 *     inner filters and outer filters will be reshaped on device. This parameter is reserved.
 *  @param[in]  inter_quantify_pos
 *    Input. The intermediate result quantization position after fc1 in the offline quantization mode.
 *    It is valid when \p enable_online_quantify is false.
 *  @param[in]  inter_quantify_scale
 *    Input. The intermediate result quantization scale after fc1 in the offline quantization mode.
 *    It is valid when \p enable_online_quantify = false.
 *  @param[in]  hidden_size
 *    Input. The size of embedding.
 *  @param[in]  filter_size
 *    Input. The size of filter.
 *  @param[in]  post_scale
 *    Input. The parameter used to multiply after the second fully connection layer.
 *    Default is 1.0.
 *  @param[in]  active_coef
 *    Input. The coefficient used in the swish activation. Default is 1.0.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    Two of the following conditions are met:
 *    - The value of \p desc is NULL.
 *    - The value of \p post_scale is 0.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetTransformerFFNDescriptor_v2(
    cnnlTransformerFFNDescriptor_t desc,
    const cnnlActivationMode_t active_mode,
    const cnnlTransformerLayernormResidualStructure_t layernorm_structure,
    const bool enable_online_quantify,
    const bool use_quantify_scale,
    const bool enable_reshape_filters,
    const int16_t inter_quantify_pos,
    const float inter_quantify_scale,
    const int hidden_size,
    const int filter_size,
    const float post_scale,
    const float active_coef);

// Group:Transformer FFN
/*!
 *  @brief Sets whether to enable inner layernormal of transformerffn.
 *  Inner layernormal is disabled by default.
 *
 *  @param[in]  desc
 *    Input. The Descriptor of the transformerfnf operation.
 *  @param[in]  enable_inner_layernormal
 *    Input. A Boolean value indicating whether to add another layer normalization layer
 *    between 2 fully connect layers.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One of the following conditions are met:
 *    - The value of \p desc is NULL.
 *
 *  @par API Dependency
 *  Before this function, you need to use ::cnnlCreateTransformerFFNDescriptor()
 *  to create a descriptor and use ::cnnlSetTransformerFFNDescriptor_v2() to set parameters
 *  for the descriptor.
 *  @note
 *  - If \p enable_inner_layernormal is enabled:
 *    - hidden_size = 256
 *    - filter_size = 1024
 *    - Data types of input, filters and output must be HALF.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetTransformerFFNDescriptorInnerLayernormalMode(
    cnnlTransformerFFNDescriptor_t desc,
    bool enable_inner_layernormal
);

// Group:Transformer FFN
/*!
 *  @brief Sets the precision mode of transformerffn. If this function is
 *  never called, compute with the data type of \p input by default.
 *
 *  @param[in]  desc
 *    Input. Descriptor of transformer operation.
 *  @param[in]  use_hp_active
 *    Input. A Boolean value indicating whether to use high precision activation function.
 *  @param[in]  compute_dtype
 *    Input. The data type used in the computing.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One of the following conditions are met:
 *    - The value of \p desc is NULL.
 *
 *  @note
 *  - This function only supports transformer base, BERT base and BERT large.
 *  - The data type of filters must be half or float.
 *  - When the data type of filters is float, the data type used in the computing must be float.
 *  - When the data type of filters and the data type used in the computing are both half,
 *    post layernorm is not allowed.
 * */
cnnlStatus_t CNNL_WIN_API
cnnlSetTransformerFFNPrecisionMode(
    cnnlTransformerFFNDescriptor_t desc,
    const bool use_hp_active,
    const cnnlDataType_t compute_dtype
);

// Group:Transformer FFN
 /*!
 *  @brief Sets TensorFloat-32 mode for transformer ffn with parameter.
 *
 *  @param[in] desc
 *    Input. The transformer ffn descriptor to be set.
 *  @param[in] allow_tf32
 *    Input. The scalar value is used as a flag for whether to open TensorFloat-32 mode.
 *           It can only be 0 or 1. 0 means not using TensorFloat-32 mode, while 1 means using
 *           TensorFloat-32 mode.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One of the following conditions are met:
 *      - \p desc is NULL.
 *      - One or more parameters are unsupported.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetTransformerFFNDescriptorAllowTF32(
    cnnlTransformerFFNDescriptor_t desc,
    int allow_tf32);

// Group:Transformer FFN
/*!
 *  @brief Sets normalization type for TransformerFFN operation
 *  with parameter.
 *
 *  @param[in] desc
 *    Input. The descriptor of the TransformerFFN operation.
 *  @param[in] norm_type
 *    Input. The normalization type of the TransformerFFN operation.
 *           See ::cnnlTransformerNormType_t for details.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One of the following conditions is met:
 *      - \p desc is NULL.
 *      - One or more parameters are unsupported.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetTransformerFFNDescriptorNormType(cnnlTransformerFFNDescriptor_t desc,
                                        cnnlTransformerNormType_t norm_type);

// Group:Transformer FFN
/*!
 *  @brief Gets extra space size needed in the
 *  transformerffn operation.
 *
 *  @param[in]  handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *           transformerffn operation.
 *  @param[in]  desc
 *    Input. The descriptor of the transformerffn operation.
 *  @param[in]  input_desc
 *    Input. The descriptor of the input tensor, containing dimension and the layout of input.
 *  @param[in]  output_desc
 *    Input. The descriptor of the output tensor, containing dimension and the layout of output.
 *  @param[in]  inner_filters_desc
 *    Input. The descriptor of \p inner_filters, containing dimension and the layout of
 *           \p inner_filters.
 *  @param[in]  outer_filters_desc
 *    Input. The descriptor of \p outer_filters, containing dimension and the layout of
 *           \p outer_filters.
 *  @param[out] size
 *    Output. The size of workspace.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    The value of \p desc is NULL.
 *
 *  @par API Dependency
 *  - You need to call the ::cnnlCreateTransformerFFNDescriptor(), and
 *  ::cnnlSetTransformerFFNDescriptor_v2() functions before calling this function.
 *  - You need to call the ::cnnlDestroyTransformerFFNDescriptor() function after
 *  calling this function.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetTransformerFFNWorkspaceSize(cnnlHandle_t handle,
                                   const cnnlTransformerFFNDescriptor_t desc,
                                   const cnnlTensorDescriptor_t input_desc,
                                   const cnnlTensorDescriptor_t output_desc,
                                   const cnnlTensorDescriptor_t inner_filters_desc,
                                   const cnnlTensorDescriptor_t outer_filters_desc,
                                   size_t *size);

// Group:Transformer FFN
/*!
 *  @brief Reshapes filters for lt.
 *
 *  @param[in]  device_type
 *    Input. The MLU device information used for the
 *           host filters reformatting.
 *  @param[in]  inner_filters_desc
 *    Input. The descriptor of \p inner_filters, containing dimension and the
 *           layout of \p inner_filters.
 *  @param[in]  inner_filters_order
 *    Input. The order type of \p inner_filters. See ::cnnlTransformerMatrixOrder_t for details.
 *  @param[in]  inner_filters_input_host
 *    Input. Pointer to the host memory that stores the inner filters.
 *  @param[in]  outer_filters_desc
 *    Input. The descriptor of \p outer_filters, containing dimension and the
 *           layout of \p outer_filters.
 *  @param[in]  outer_filters_order
 *    Input. The order type of \p outer_filters. See ::cnnlTransformerMatrixOrder_t for details.
 *  @param[in]  outer_filters_input_host
 *    Input. Pointer to the host memory that stores the outer filters.
 *  @param[out]  inner_filters_output_host
 *    Output. Pointer to the host memory that stores the reformatted inner filters.
 *  @param[out]  outer_filters_output_host
 *    Output. Pointer to the host memory that stores the reformatted outer filters.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One of the following conditions are met:
 *    - The value of \p desc is NULL.
 *  @par API Dependency
 *  - You need to call the ::cnnlGetTransformerFFNFilterDescriptor() to allocate host space
 *  for inner and outer filters after this function.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlReformatTransformerFFNFilter(cnnlDeviceType_t device_type,
                                 const cnnlTensorDescriptor_t inner_filters_desc,
                                 const cnnlTransformerMatrixOrder_t inner_filters_order,
                                 const void *inner_filters_input_host,
                                 const cnnlTensorDescriptor_t outer_filters_desc,
                                 const cnnlTransformerMatrixOrder_t outer_filters_order,
                                 const void *outer_filters_input_host,
                                 void *inner_filters_output_host,
                                 void *outer_filters_output_host);

// Group:Transformer FFN
/*!
 *  @brief Gets the inner and outer filter descriptor
 *  required in transformerffn operation after reformatting ffn filters with
 *  ::cnnlReformatTransformerFFNFilter.
 *
 *  @param[in]  device_type
 *    Input. The MLU device information used for host filters reformatting.
 *  @param[in]  inner_filters_desc
 *    Input. The descriptor of \p inner_filters, containing dimension and the layout of
 *           \p inner_filters.
 *  @param[in]  inner_filters_order
 *    Input. The order type of \p inner_filters. See ::cnnlTransformerMatrixOrder_t for details.
 *  @param[in]  outer_filters_desc
 *    Input. The descriptor of \p outer_filters, containing dimension and the layout of
 *           \p outer_filters.
 *  @param[in]  outer_filters_order
 *    Input. The order type of \p outer_filters. See ::cnnlTransformerMatrixOrder_t for details.
 *  @param[out]  inner_filters_output_desc
 *    Input. The descriptor of \p inner_filters_output, containing dimension and the
 *           layout of \p inner_filters_output.
 *  @param[out]  outer_filters_output_desc
 *    Input. The descriptor of \p outer_filters_output, containing dimension and the
 *           layout of outer_filters.
 *
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One of the following conditions are met:
 *    - The value of \p desc is NULL.
 *  @par API Dependency
 *  - You need to call the ::cnnlCreateTransformerFFNDescriptor(), and
 *    cnnlSetTransformerFFNDescriptor_v2() functions before calling this function.
 *  - You need to call the cnnlDestroyTransformerFFNDescriptor() function after
 *    calling this function.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetTransformerFFNFilterDescriptor(cnnlDeviceType_t device_type,
                                      const cnnlTensorDescriptor_t inner_filters_desc,
                                      const cnnlTransformerMatrixOrder_t inner_filters_order,
                                      const cnnlTensorDescriptor_t outer_filters_desc,
                                      const cnnlTransformerMatrixOrder_t outer_filters_order,
                                      cnnlTensorDescriptor_t inner_filters_output_desc,
                                      cnnlTensorDescriptor_t outer_filters_output_desc);

// Group:Transformer FFN
/*!
 *  @brief Assigns the transformerffn descriptor with parameters in
 *  the quantization scenario.
 *
 *  @param[in,out]  desc
 *    Input/output. Descriptor of transformerffn operation.
 *  @param[in]  active_mode
 *    Input. Activation mode of ffn layer, which supports relu and gelu.
 *  @param[in]  layernorm_structure
 *    Input. The layernorm and residual structure.
 *  @param[in]  enable_online_quantify
 *    Input. A Boolean value indicates whether to enable online quantization.
 *    When enable_online_quantify = true, input and intermediate result are quantized
 *    online.
 *  @param[in]  enable_reshape_filters
 *    Input. A Boolean value indicates whether to reshape filters on device. This parameter is reserved.
 *  @param[in]  use_quantify_scale
 *    Input. A Boolean value indicates whether to use the quantization scaling factor.
 *  @param[in]  use_quantify_offset
 *    Input. A Boolean value indicates whether to use the quantization offset.
 *  @param[in]  use_hp_active
 *    Input. A Boolean value indicates whether to use high precision activation function.
 *  @param[in]  residual_input_quant
 *    Input. A Boolean value indicates whether to quantize the residual input.
 *  @param[in]  local_input_quant
 *    Input. A Boolean value indicates whether to quantize the local input.
 *  @param[in]  use_fc1_aftergemm_quant_params
 *    Input. A Boolean value indicates whether to use fc1_aftergemm_position and
 *    fc1_aftergemm_scale when dequantizing fc1.
 *  @param[in]  use_fc2_aftergemm_quant_params
 *    Input. A Boolean value indicates whether to use fc2_aftergemm_position and
 *    fc2_aftergemm_scale when dequantizing fc2.
 *  @param[in]  quant_per_tensor
 *    Input. A Boolean value indicates whether to quantize the filters of ffn by tensor.
 *    If this parameter is true, quantization is done by tensor, otherwise by channel.
 *  @param[in]  layernorm_eps
 *    Input. The epsilon value of layernorm.
 *  @param[in]  post_scale
 *    Input. The parameter used to multiply after the second fully connection layer.
 *    Default is 1.0.
 *  @param[in]  active_coef
 *    Input. The coefficient used in the swish activation. Default is 1.0.
 *  @param[in]  hidden_size
 *    Input. The size of embedding.
 *  @param[in]  filter_size
 *    Input. The size of filter.
 *  @param[in]  compute_dtype
 *    Input. The data type used in the computing.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One of the following conditions are met:
 *    - The value of \p desc is NULL.
 *
 *  @par API Dependency
 *  - Before calling this function, you need to use ::cnnlCreateTransformerFFNDescriptor()
 *  to create the descriptor.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetTransformerFFNQATDescriptor(
    cnnlTransformerFFNDescriptor_t desc,
    const cnnlActivationMode_t active_mode,
    const cnnlTransformerLayernormResidualStructure_t layernorm_structure,
    const bool enable_online_quantify,
    const bool enable_reshape_filters,
    const bool use_quantify_scale,
    const bool use_quantify_offset,
    const bool use_hp_active,
    const bool residual_input_quant,
    const bool local_input_quant,
    const bool use_fc1_aftergemm_quant_params,
    const bool use_fc2_aftergemm_quant_params,
    const bool quant_per_tensor,
    const float layernorm_eps,
    const float post_scale,
    const float active_coef,
    const int hidden_size,
    const int filter_size,
    const cnnlDataType_t compute_dtype);

// Group:Transformer FFN
/*!
 *  @brief Assigns the transformerffn descriptor with
 *  quantization parameters in the quantization scenario.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlSetTransformerFFNQATQuantifyParams_v2 instead.
 *
 *  @param[in,out]  desc
 *    Input/output. Descriptor of transformerffn operation.
 *  @param[in]  fc1_output_position
 *    Input. The quantization position of the fc1 output.
 *  @param[in]  fc1_output_scale
 *    Input. The quantization scaling factor of the fc1 output.
 *  @param[in]  fc1_output_offset
 *    Input. The quantization offset of the fc1 output.
 *  @param[in]  local_input_position
 *    Input. The quantization position of the local input.
 *  @param[in]  local_input_scale
 *    Input. The quantization scaling factor of the local input.
 *  @param[in]  local_input_offset
 *    Input. The quantization offset of the local input.
 *  @param[in]  fc1_aftergemm_position
 *    Input. The quantization position of the fc1 aftergemm.
 *  @param[in]  fc1_aftergemm_scale
 *    Input. The quantization scaling factor of the fc1 aftergemm.
 *  @param[in]  fc1_aftergemm_offset
 *    Input. The quantization offset of the fc1 aftergemm.
 *  @param[in]  fc2_aftergemm_position
 *    Input. The quantization position of the fc2 aftergemm.
 *  @param[in]  fc2_aftergemm_scale
 *    Input. The quantization scaling factor of the fc2 aftergemm.
 *  @param[in]  fc2_aftergemm_offset
 *    Input. The quantization offset of the fc2 aftergemm.
 *  @param[in]  inner_filters_positions
 *    Input. Pointer to the MLU memory that stores the quantization positions of the inner filters.
 *  @param[in]  inner_filters_scales
 *    Input. Pointer to the MLU memory that stores the quantization scaling factors of the inner filters.
 *  @param[in]  inner_filters_offsets
 *    Input. Pointer to the MLU memory that stores the quantization offsets of the inner filters.
 *  @param[in]  outer_filters_positions
 *    Input. Pointer to the MLU memory that stores the quantization positions of the outer filters.
 *  @param[in]  outer_filters_scales
 *    Input. Pointer to the MLU memory that stores the quantization scaling factors of the outer filters.
 *  @param[in]  outer_filters_offsets
 *    Input. Pointer to the MLU memory that stores the quantization offsets of the outer filters.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    The value of \p desc is NULL.
*  @par API Dependency
 *  - Before calling this function, you need to use ::cnnlCreateTransformerFFNDescriptor()
 *  to create the descriptor.
 *
 */
CNNL_DEPRECATED_FOR(cnnlSetTransformerFFNQATQuantifyParams_v2)
cnnlStatus_t CNNL_WIN_API
cnnlSetTransformerFFNQATQuantifyParams(
    cnnlTransformerFFNDescriptor_t desc,
    const int fc1_output_position,
    const float fc1_output_scale,
    const int fc1_output_offset,
    const int local_input_position,
    const float local_input_scale,
    const int local_input_offset,
    const int fc1_aftergemm_position,
    const float fc1_aftergemm_scale,
    const int fc1_aftergemm_offset,
    const int fc2_aftergemm_position,
    const float fc2_aftergemm_scale,
    const int fc2_aftergemm_offset,
    const void *inner_filters_positions,
    const void *inner_filters_scales,
    const void *inner_filters_offsets,
    const void *outer_filters_positions,
    const void *outer_filters_scales,
    const void *outer_filters_offsets);

// Group:Transformer FFN
/*!
 *  @brief Assigns the transformerffn descriptor with quantization parameters in the quantization scenario.
 *
 *  Compared with ::cnnlSetTransformerFFNQATQuantifyParams, this API enables you to set more quantization parameters.
 *
 *  @param[in,out]  desc
 *    Input/output. Descriptor of transformerffn operation.
 *  @param[in]  fc1_output_position
 *    Input. The quantization position of the fc1 output.
 *  @param[in]  fc1_output_scale
 *    Input. The quantization scaling factor of the fc1 output.
 *  @param[in]  fc1_output_offset
 *    Input. The quantization offset of the fc1 output.
 *  @param[in]  local_input_position
 *    Input. The quantization position of the local input.
 *  @param[in]  local_input_scale
 *    Input. The quantization scaling factor of the local input.
 *  @param[in]  local_input_offset
 *    Input. The quantization offset of the local input.
 *  @param[in]  fc1_aftergemm_position
 *    Input. The quantization position of the fc1 aftergemm.
 *  @param[in]  fc1_aftergemm_scale
 *    Input. The quantization scaling factor of the fc1 aftergemm.
 *  @param[in]  fc1_aftergemm_offset
 *    Input. The quantization offset of the fc1 aftergemm.
 *  @param[in]  fc2_aftergemm_position
 *    Input. The quantization position of the fc2 aftergemm.
 *  @param[in]  fc2_aftergemm_scale
 *    Input. The quantization scaling factor of the fc2 aftergemm.
 *  @param[in]  fc2_aftergemm_offset
 *    Input. The quantization offset of the fc2 aftergemm.
 *  @param[in]  input_position
 *    Input. The quantization position of the input.
 *  @param[in]  input_scale
 *    Input. The quantization scaling factor of the input.
 *  @param[in]  input_offset
 *    Input. The quantization offset of the input.
 *  @param[in]  inner_weights_position
 *    Input. The quantization position of the inner_weights.
 *  @param[in]  inner_weights_scale
 *    Input. The quantization scaling factor of the inner_weights.
 *  @param[in]  inner_weights_offset
 *    Input. The quantization offset of the inner_weights.
 *  @param[in]  outer_weights_position
 *    Input. The quantization position of the outer_weights.
 *  @param[in]  outer_weights_scale
 *    Input. The quantization scaling factor of the outer_weights.
 *  @param[in]  outer_weights_offset
 *    Input. The quantization offset of the outer_weights.
 *  @param[in]  output_position
 *    Input. The quantization position of the output.
 *  @param[in]  output_scale
 *    Input. The quantization scaling factor of the output.
 *  @param[in]  output_offset
 *    Input. The quantization offset of the output.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    The value of \p desc is NULL.
*  @par API Dependency
 *  - Before calling this function, you need to use ::cnnlCreateTransformerFFNDescriptor()
 *  to create the descriptor.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetTransformerFFNQATQuantifyParams_v2(cnnlTransformerFFNDescriptor_t desc,
                                          const int fc1_output_position,
                                          const float fc1_output_scale,
                                          const int fc1_output_offset,
                                          const int local_input_position,
                                          const float local_input_scale,
                                          const int local_input_offset,
                                          const int fc1_aftergemm_position,
                                          const float fc1_aftergemm_scale,
                                          const int fc1_aftergemm_offset,
                                          const int fc2_aftergemm_position,
                                          const float fc2_aftergemm_scale,
                                          const int fc2_aftergemm_offset,
                                          const int input_position,
                                          const float input_scale,
                                          const int input_offset,
                                          const int inner_weights_position,
                                          const float inner_weights_scale,
                                          const int inner_weights_offset,
                                          const int outer_weights_position,
                                          const float outer_weights_scale,
                                          const int outer_weights_offset,
                                          const int output_position,
                                          const float output_scale,
                                          const int output_offset);

/******************************************************************************
 * Cambricon CNNL Transformer self attention mask mode
 ******************************************************************************/
/*!
 * @brief Enumeration variables describing the modes of mask of the transformer self attention operation.
 */
typedef enum {
  CNNL_TRANSFORMER_SELF_ATTN_NO_MASK = 0,
  /*!< No masking is performed after QK MATMUL.*/

  CNNL_TRANSFORMER_SELF_ATTN_DECODER_MASK = 1,
  /*!< Performs masking in decoder for softmax after QK MATMUL.
   * The dimension of mask is [seq_len, seq_len].
   */

  CNNL_TRANSFORMER_SELF_ATTN_ENCODER_MASK = 2,
  /*!< Performs masking in encoder for softmax after QK MATMUL.
   * The dimension of mask is [batch_size, seq_len].
   */

  CNNL_TRANSFORMER_SELF_ATTN_MIX_MASK = 3,
  /*!< Performs masking for softmax in mixed mode after QK MATMUL.
   * The dimension of mask is [batch_size, seq_len, seq_len].
   */

  CNNL_TRANSFORMER_SELF_ATTN_SELF_GEN_ENCODER_MASK = 4,
  /*!< Performs masking in self-gen encoder for softmax after QK MATMUL.
   * The dimension of mask is [batch_size].
   */

  CNNL_TRANSFORMER_SELF_ATTN_SELF_PACK_MODE = 5,
  /*!< Performs masking for softmax in pack mode after QK MATMUL.
   * In this mode, mask is self-generated and no need to pass in externally.
   * The dimension of valid_token is [batch_size].
   */
} cnnlTransformerSelfAttnMaskMode_t;

 /******************************************************************************
 * Cambricon CNNL OP: TransformerSelfAttn
 ******************************************************************************/
/*! The descriptor of the ::cnnlTransformerSelfAttn operation.
 *
 *  You need to call the ::cnnlCreateTransformerSelfAttnDescriptor function
 *  to create a descriptor, and call the ::cnnlSetTransformerSelfAttnDescriptor_v2
 *  function to set the tensor information to the descriptor.
 *  Also, you need to destroy the descriptor at the end with the
 *  ::cnnlDestroyTransformerSelfAttnDescriptor function.
 */
typedef struct cnnlTransformerSelfAttnStruct *cnnlTransformerSelfAttnDescriptor_t;

// Group:Transformer Self Attn
/* transformer self-attention start */
/*!
 *  @brief Executes the encoder attention in TransformerEncoderLayer and
 *  executes the cached decoder attention in TransformerDecoderLayer.
 *
 *  This operation performs with the following steps:
 *
 *  1. Generate QKV
 *
 *    Compute query key and value from input tensor.
 *
 *    query = mlp(input_tensor, attr_kernel_Q)
 *
 *    key = mlp(input_tensor, attr_kernel_K)
 *
 *    value = mlp(input_tensor, attr_kernel_V)
 *
 *  2. Q x K batch_dot, softmax and QK x V batch_dot
 *
 *    Compute QxK batch_dot for all heads.
 *
 *    for head_id in (0, head_num)
 *
 *      QK = batch_dot(query_in_head, key_in_head)
 *
 *      QK = softmax(QK)
 *
 *      QKV = batch_dot(QK, value_in_head)
 *
 *  3. Generate output
 *
 *    Compute output from QKV.
 *
 *    output = mlp(QKV, attout_kernel)
 *
 *  @param[in]  handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *           transformerselfattn operation.
 *  @param[in] self_attn_desc
 *    Input. The descriptor of the transformerselfattn operation.
 *  @param[in]  desc_input
 *    Input. Descriptor of input, containing dimension of [batch_size, seq_len, in_channel].
 *  @param[in]  input
 *    Input. Pointer to the MLU memory that stores the input tensor.
 *  @param[in]  desc_qfilter
 *    Input. Descriptor of query filter, containing dimension of [hidden_size, in_channel].
 *  @param[in]  qfilter
 *    Input. Pointer to the MLU memory that stores the query filter. The combined filter is
 *           arranged in the order of [query_filters].
 *  @param[in]  desc_kfilter
 *    Input. Descriptor of key filter, containing dimension of [hidden_size, in_channel].
 *  @param[in]  kfilter
 *    Input. Pointer to the MLU memory that stores the key filter. The combined filter is
 *           arranged in the order of [key_filters].
 *  @param[in]  desc_vfilter
 *    Input. Descriptor of value filter, containing dimension of [hidden_size, in_channel].
 *  @param[in]  vfilter
 *    Input. Pointer to the MLU memory that stores the value filters. The combined filter is
 *           arranged in the order of [value_filters].
 *  @param[in]  desc_qbias
 *    Input. Descriptor of query bias, containing dimension of [hidden_size].
 *           This descriptor is optional.
 *  @param[in]  qbias
 *    Input. Pointer to the MLU memory that stores the query bias. The combined bias is
 *           arranged in the order of [query_bias].
 *           This parameter is optional.
 *  @param[in]  desc_kbias
 *    Input. Descriptor of key bias, containing dimension of [hidden_size].
 *           This descriptor is optional.
 *  @param[in]  kbias
 *    Input. Pointer to the MLU memory that stores the key bias. The combined bias is
 *           arranged in the order of [key_bias].
 *           This parameter is optional.
 *  @param[in]  desc_vbias
 *    Input. Descriptor of value bias, containing dimension of [hidden_size].
 *           This descriptor is optional.
 *  @param[in]  vbias
 *    Input. Pointer to the MLU memory that stores the value bias. The combined bias is
 *           arranged in the order of [value_bias].
 *           This parameter is optional.
 *  @param[in]  desc_outfilter
 *    Input. Descriptor of filter of the fully connected layer when computing the output,
 *           containing dimension of [hidden_size, hidden_size].
 *  @param[in]  outfilter
 *    Input. Pointer to the MLU memory that stores the outfilter tensor, which is the filter of
 *    the fully connected layer when computing output.
 *  @param[in]  desc_outbias
 *    Input. Descriptor of bias of the fully connected layer when computing the output,
 *           containing dimension of [hidden_size].
 *           This descriptor is optional.
 *  @param[in]  outbias
 *    Input. Pointer to the MLU memory that stores the outbias tensor, which is the bias of
 *    the fully connected layer when computing output.
 *    This parameter is optional.
 *  @param[in]  desc_mask
 *    Input. Descriptor of mask vector, containing data type, dimensions and layout.
 *           This descriptor is optional.
 *  @param[in]  mask_vec
 *    Input. Pointer to the MLU memory that stores the encoder mask.
 *           This parameter is optional.
 *  @param[in]  desc_valid_token
 *    Input. Descriptor of valid_token. Its data type must be INT32 and shape must be [batch_size].
 *           This descriptor is optional.
 *  @param[in]  valid_token
 *    Input. Pointer to the MLU memory that stores the valid token. It is the valid token number
 *           of every batch size for self gen encoder mask or pack mode. This parameter is optional.
 *  @param[in]  desc_norm_scale
 *    Input. Descriptor of norm_scale vector, containing dimension of [in_channel].
 *           This descriptor is optional.
 *  @param[in]  norm_scale
 *    Input. Pointer to the MLU memory that stores the norm_scale tensor, which is the scale factor
 *           of layernorm. Its dimension depends on the output dimensions. This parameter is optional.
 *  @param[in]  desc_norm_bias
 *    Input. Descriptor of norm_bias vector, containing dimension of [in_channel].
 *           This descriptor is optional.
 *  @param[in]  norm_bias
 *    Input. Pointer to the MLU memory that stores the norm_bias tensor, which is the bias
 *           of layernorm. Its dimension depends on the output dimensions. This parameter is optional.
 *  @param[in] desc_curr_idx
 *    Input. Descriptor of the loop index tensor, containing data type, dimensions and
 *           layout. Its data type must be INT32 and shape must be [1].
 *           This descriptor is optional.
 *  @param[in,out] curr_idx
 *    Input and Output. Pointer to the MLU memory that stores the loop index tensor.
 *          It indicates the number of frames that decoder has already processed.
 *           This parameter is optional.
 *  @param[in] desc_key_cache
 *    Input. Descriptor of \p key_cache tensor, containing data type, dimensions and
 *           layout. Shape must be [batch_size, beam_size, head_num, max_decode_length, head_size]
 *           or [batch_size, beam_size, max_decode_length, hidden_size].
 *           This descriptor is optional.
 *  @param[in,out] key_cache
 *    Input and Output. Pointer to the MLU memory that stores \p key_cache tensor.
 *           The value stored in cache will be updated during the operation.
 *           This parameter is optional.
 *  @param[in] desc_value_cache
 *    Input. Descriptor of value_cache tensor, containing data type, dimensions and layout.
 *    This parameter is optional.
 *    - Decoder: shape must be [batch_size, beam_size, head_num, max_decode_length, head_size]
 *      or [batch_size, beam_size, max_decode_length, hidden_size].
 *    - Encoder: shape must be [batch_size, seq_len, hidden_size].
 *  @param[in,out] value_cache
 *    Input and Output. Pointer to the MLU memory that stores value_cache tensor.
 *           The value stored in cache will be updated during the operation.
 *           This parameter is optional.
 *  @param[in] desc_best_beams_cache
 *    Input. Descriptor of best_beams_cache tensor, containing data type, dimensions and
 *           layout. Shape must be [batch_size, ngroup, beam_size].
 *           This descriptor is optional.
 *  @param[in,out] best_beams_cache
 *    Input and Output. Pointer to the MLU memory that stores best_beams_cache tensor. It is a state
 *           inferred from beam search result, to restore historical K/V cache correctly.
 *           If this is the first frame of decoder, this cache must be initialized with zero
 *           before calling this operation. The value stored in cache will be updated during
 *           the operation. This parameter is optional.
 *  @param[in]  workspace
 *    Input. Pointer to the workspace.
 *  @param[in]  workspace_size
 *    Input. The value of workspace size.
 *  @param[in]  desc_output
 *    Input. Descriptor of output, containing data type, dimensions and layout.
 *    Shape must be [batch_size, seq_len, hidden_size].
 *  @param[out]  output
 *    Input. Pointer to the MLU memory that stores the output tensor.
 *  @retval CNNL_STATUS_SUCCESS = 0
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are encountered:
 *    - \p handle is NULL.
 *    - pointer is NULL.
 *    - parameters are invalid.
 *  @retval CNNL_STATUS_ARCH_MISMATCH
 *    One or more of the following conditions are encountered:
 *    - This function is run on the hardware platform that is not supported.
 *
 *  @par Data Type
 *  - When using encoder attention:
 *    - For MLU270:
 *      - input: float, half
 *      - filter: int16, int8
 *      - bias: float, half
 *      - output: float, half
 *    - For MLU300 series:
 *      - input: float, half
 *      - filter: float, half, int16, int8
 *      - bias: float, half
 *      - output: float, half
 *  - When using cached decoder attention:
 *    - For MLU270:
 *      - input: float, half
 *      - filter: int16, int8
 *      - bias: float, half
 *      - output: float, half
 *    - For MLU300 series:
 *      - input: float, half
 *      - filter: float, half, int16, int8
 *      - bias: float, half
 *      - output: float, half
 *
 *  @note
 *  - If \p curr_idx is NULL, the operation computes encoder attention;
 *    if it is not NULL, the operation computes cached decoder attention.
 *  - When using encoder attention:
 *    - \p input and \p output cannot be homologous operand.
 *    - The content of \p input is not modified by \p self_attn layer.
 *    - Data type of \p input, \p output, \p qbias, \p kbias, \p vbias, \p outbias, \p norm_scale,
 *      \p norm_bias and \p mask_vec must be the same and data type must be half or float.
 *    - Data type of \p qfilter, \p kfilter, \p vfilter and \p outfilter must be same: On MLU270,
 *      data type must be int16 or int8; On MLU300 series, data type must be int16, int8, half or float.
 *    - Data type of \p valid_token must be int32.
 *    - If filter type is int8, precision of output is not guaranteed.
 *  - When using cached decoder attention:
 *    - \p Input and \p output cannot be homologous operand.
 *    - The content of \p input is not modified by \p self_attn layer.
 *    - Data type of \p input, \p output, \p qbias, \p kbias, \p vbias, \p outbias, \p norm_scale
 *      and \p norm_bias must be the same and data type must be half or float.
 *    - Data type of \p key_cache and \p value_cache must be the same and data type must be half or float.
 *    - Data type of \p qfilter, \p kfilter, \p vfilter and \p outfilter must be same:
 *      On MLU270, data type must be int16 or int8; On MLU300 series, data type must be int16, int8, half or float.
 *    - Data type of \p best_beams_cache must be int8.
 *    - Data type of \p curr_idx must be int32.
 *    - Cached decoder attention has two modes: MLU270 mode and MLU300 mode.
 *      - On MLU270 mode, the shape of \p key_cache and \p value_cache must be
 *        [batch_size, beam_size, head_num, max_decode_length, head_size].
 *      - On MLU300 mode, the shape of \p key_cache and \p value_cache must be
 *        [batch_size, beam_size, max_decode_length, hidden_size].
 *    - For cached decoder attention, tensors \p best_beams_cache, \p curr_idx,
 *      \p key_cache and \p value_cache are required and tensor \p mask_vec is not used.
 *    - Cached MLU300 mode requires 8 clusters and does not support post layernormal computations.
 *    - If filter type is int8, precision of output is not guaranteed.
 *
 *  @par Scale Limitation
 *  - On MLU270 or filter type is int8 or int16:
 *    - When using encoder attention:
 *      - When CNNL_TRANSFORMER_SELF_ATTN_PACK_MODE is used, \p batch_size only supports 1.
 *      - \p Head_num supports 8, 12, or 16.
 *      - \p Head_size_qk and \p head_size_v only supports 64.
 *      - \p Hidden_size = head_num * head_size_v.
 *      - \p In_channel = Hidden_size.
 *      - When \p head_num is 8, \p seq_len must be in range of [1, 512].
 *      - When \p head_num is 12 and \p CNNL_TRANSFORMER_SELF_ATTN_PACK_MODE is used, \p max_seq_len
 *        must be in range of [1, 384]. Otherwise, \p seq_len must be in range of [1, 384].
 *      - When \p head_num is 16, \p seq_len must be in range of [1, 128].
 *      - \p Layernorm_res_mode must be:
 *        - CNNL_TRANSFORMER_NO_LAYERNORM_NO_RESIDUAL.
 *        - CNNL_TRANSFORMER_NO_LAYERNORM_WITH_RESIDUAL.
 *        - CNNL_TRANSFORMER_PRE_LAYERNORM_NO_RESIDUAL.
 *        - CNNL_TRANSFORMER_POST_LAYERNORM_NO_RESIDUAL.
 *        - CNNL_TRANSFORMER_PRE_LAYERNORM_INSIDE_RESIDUAL.
 *        - CNNL_TRANSFORMER_POST_LAYERNORM_OUTSIDE_RESIDUAL.
 *      - \p head_num is 12, filter type is int8, clusters are 8 and
 *        \p mask_mode is \p CNNL_TRANSFORMER_SELF_ATTN_ENCODER_MASK
 *        or \p CNNL_TRANSFORMER_SELF_ATTN_PACK_MODE,
 *        \p CNNL_TRANSFORMER_PRE_LAYERNORM_OUTSIDE_RESIDUAL is supported.
 *      - \p mask_mode supports:
 *        - \p CNNL_TRANSFORMER_SELF_ATTN_NO_MASK.
 *        - \p CNNL_TRANSFORMER_SELF_ATTN_ENCODER_MASK.
 *      - When \p head_num is 8, \p CNNL_TRANSFORMER_SELF_ATTN_DECODER_MASK is supported.
 *      - When \p head_num is 12, \p CNNL_TRANSFORMER_SELF_ATTN_MIX_MASK is supported.
 *    - When using cached decoder attention:
 *      - \p batch_size must be in range of [1, 64].
 *      - \p beam_size must be in range of [1, 12].
 *      - \p max_decode_length must be in range of [1, 256].
 *      - \p head_num only supports 8.
 *      - \p head_size_qk and \p head_size_v only supports 64.
 *      - \p hidden_size = head_num * head_size_v.
 *      - \p in_channel = \p hidden_size.
 *      - \p Ngroup must be in range of [1, max_decode_length] and less than 6.
 *      - \p Curr_idx must be in range of [0, max_decode_length).
 *      - \p Layernorm_res_mode must be:
 *         - \p CNNL_TRANSFORMER_NO_LAYERNORM_WITH_RESIDUAL.
 *         - \p CNNL_TRANSFORMER_PRE_LAYERNORM_INSIDE_RESIDUAL.
 *         - \p CNNL_TRANSFORMER_POST_LAYERNORM_OUTSIDE_RESIDUAL.
 *  - On MLU300 series and filter type is fp16 or fp32:
 *    - When using encoder attention:
 *      - \p seq_len must be in range of [1, 512].
 *      - \p head_num must be in range of [1, 16].
 *      - \p hidden_size = head_num * head_size_v.
 *      - \p in_channel = \p hidden_size.
 *      - \p layernorm_res_mode support:
 *        - CNNL_TRANSFORMER_NO_LAYERNORM_NO_RESIDUAL.
 *        - CNNL_TRANSFORMER_NO_LAYERNORM_WITH_RESIDUAL.
 *        - CNNL_TRANSFORMER_PRE_LAYERNORM_NO_RESIDUAL.
 *        - CNNL_TRANSFORMER_POST_LAYERNORM_NO_RESIDUAL.
 *        - CNNL_TRANSFORMER_PRE_LAYERNORM_INSIDE_RESIDUAL.
 *        - CNNL_TRANSFORMER_PRE_LAYERNORM_OUTSIDE_RESIDUAL.
 *        - CNNL_TRANSFORMER_POST_LAYERNORM_OUTSIDE_RESIDUAL.
 *      - \p mask_mode supports:
 *        - CNNL_TRANSFORMER_SELF_ATTN_NO_MASK.
 *        - CNNL_TRANSFORMER_SELF_ATTN_ENCODER_MASK.
 *        - CNNL_TRANSFORMER_SELF_ATTN_SELF_GEN_ENCODER_MASK.
 *      - When \p head_num is 4 and filter type is fp16, CNNL_TRANSFORMER_SELF_ATTN_MIX_MASK is supported.
 *      - \p head_size_qk and \p head_size_v are generally in range of [16, 64]. If \p head_size_qk and
 *      \p head_size_v are in range of [65, 129), the following rules should be met:
 *         - \p seq_len must be in range of [1, 512].
 *         - \p head_num must be in range of [1, 16].
 *         - \p hidden_size = head_num * head_size_v.
 *         - \p in_channel = hidden_size.
 *         - \p compute_type must be CNNL_DTYPE_HALF.
 *         - \p Layernorm_res_mode supports:
 *           - CNNL_TRANSFORMER_NO_LAYERNORM_NO_RESIDUAL.
 *           - CNNL_TRANSFORMER_NO_LAYERNORM_WITH_RESIDUAL.
 *           - CNNL_TRANSFORMER_PRE_LAYERNORM_NO_RESIDUAL.
 *           - CNNL_TRANSFORMER_POST_LAYERNORM_NO_RESIDUAL.
 *           - CNNL_TRANSFORMER_PRE_LAYERNORM_INSIDE_RESIDUAL.
 *           - CNNL_TRANSFORMER_PRE_LAYERNORM_OUTSIDE_RESIDUAL.
 *           - CNNL_TRANSFORMER_POST_LAYERNORM_OUTSIDE_RESIDUAL.
 *         - \p mask_mode supports:
 *          - CNNL_TRANSFORMER_SELF_ATTN_NO_MASK.
 *          - CNNL_TRANSFORMER_SELF_ATTN_ENCODER_MASK.
 *          - CNNL_TRANSFORMER_SELF_ATTN_SELF_GEN_ENCODER_MASK.
 *      - If \p head_size_qk and \p head_size_v = 80 is deployed, the following rules should be met:
 *        - \p seq_len must be in range of [1, 128].
 *        - \p head_num must be 4.
 *        - \p hidden_size = head_num * head_size_v.
 *        - \p in_channel supports 320, 560 or 880.
 *        - \p layernorm_res_mode supports:
 *          - CNNL_TRANSFORMER_PRE_LAYERNORM_NO_RESIDUAL.
 *          - CNNL_TRANSFORMER_PRE_LAYERNORM_INSIDE_RESIDUAL.
 *        - \p mask_mode supports:
 *          - CNNL_TRANSFORMER_SELF_ATTN_ENCODER_MASK.
 *          - CNNL_TRANSFORMER_SELF_ATTN_MIX_MASK.
 *      - If CNNL_TRANSFORMER_SELF_ATTN_PACK_MODE is deployed, the following rules should be met:
 *        - \p batch_size must be 1.
 *        - \p max_seq_len must be in range of [1, 384].
 *        - \p head_num must be 12.
 *        - \p head_size_qk and head_size_v must be 64.
 *        - \p hidden_size = head_num * head_size_v.
 *        - \p in_channel = hidden_size.
 *      - \p Layernorm_res_mode supports CNNL_TRANSFORMER_PRE_LAYERNORM_OUTSIDE_RESIDUAL.
 *    - When using cached decoder attention:
 *      - \p beam_size must be in range of [1, 32].
 *      - \p max_decode_length must be in range of [1, 513].
 *      - \p head_size_qk and \p head_size_v must be equal, only supporting 32 and 64.
 *      - \p head_num must be 8 when \p head_size_qk is 64, and must be 16 when \p head_size_qk is 32.
 *      - \p hidden_size = head_num * head_size_v.
 *      - \p in_channel = head_num * Head_size_qk.
 *      - \p ngroup must be in range of [1, max_decode_length] and less than 6.
 *      - \p curr_idx must be in range of [0, max_decode_length).
 *      - \p layernorm_res_mode must be:
 *        - CNNL_TRANSFORMER_NO_LAYERNORM_WITH_RESIDUAL.
 *        - CNNL_TRANSFORMER_PRE_LAYERNORM_INSIDE_RESIDUAL.
 *  - On MLU500 series:
 *    - When using encoder attention:
 *     - \p batch_size has no limits.
 *     - \p seq_len must be in range of [1, 1024].
 *     - \p head_num must be in range of [1, 128].
 *     - \p head_size_qk and \p head_size_v must be equal, and must be in range of [16, 128].
 *     - \p hidden_size = head_num * head_size_v.
 *     - \p in_channel = hidden_size.
 *     - \p layernorm_res_mode supports:
 *       - CNNL_TRANSFORMER_NO_LAYERNORM_NO_RESIDUAL.
 *       - CNNL_TRANSFORMER_NO_LAYERNORM_WITH_RESIDUAL.
 *       - CNNL_TRANSFORMER_PRE_LAYERNORM_NO_RESIDUAL.
 *       - CNNL_TRANSFORMER_POST_LAYERNORM_NO_RESIDUAL.
 *       - CNNL_TRANSFORMER_PRE_LAYERNORM_INSIDE_RESIDUAL.
 *       - CNNL_TRANSFORMER_PRE_LAYERNORM_OUTSIDE_RESIDUAL.
 *       - CNNL_TRANSFORMER_POST_LAYERNORM_OUTSIDE_RESIDUAL.
 *     - \p mask_mode supports:
 *       - CNNL_TRANSFORMER_SELF_ATTN_NO_MASK.
 *       - CNNL_TRANSFORMER_SELF_ATTN_ENCODER_MASK.
 *       - CNNL_TRANSFORMER_SELF_ATTN_MIX_MASK.
 *       - CNNL_TRANSFORMER_SELF_ATTN_SELF_GEN_ENCODER_MASK.
 *    - When using cached decoder attention:
 *     - \p batch_size has no limits.
 *     - \p max_decode_length must be in range of [1, 1024].
 *     - \p head_num must be in range of [1, 128].
 *     - \p head_size_qk and \p head_size_v must be equal, and must be in range of [16, 128].
 *     - \p hidden_size = head_num * head_size_v.
 *     - \p in_channel = head_num * Head_size_qk.
 *     - \p ngroup must be 1.
 *     - \p curr_idx must be in range of [0, max_decode_length).
 *     - \p layernorm_res_mode must be:
 *       - CNNL_TRANSFORMER_NO_LAYERNORM_NO_RESIDUAL.
 *       - CNNL_TRANSFORMER_NO_LAYERNORM_WITH_RESIDUAL.
 *       - CNNL_TRANSFORMER_PRE_LAYERNORM_NO_RESIDUAL.
 *       - CNNL_TRANSFORMER_POST_LAYERNORM_NO_RESIDUAL.
 *       - CNNL_TRANSFORMER_PRE_LAYERNORM_INSIDE_RESIDUAL.
 *       - CNNL_TRANSFORMER_PRE_LAYERNORM_OUTSIDE_RESIDUAL.
 *       - CNNL_TRANSFORMER_POST_LAYERNORM_OUTSIDE_RESIDUAL.
 *
 *  @par Performance Optimization
 *  - For encoder attention, the performance is best when [batch_size] dot [seq_len] is less than 1024,
 *    seq_len is less than 128, and data types of input and filter are half.
 *  - For cached decoder attention, the performance is the best when batch >= 16, data types of
 *    input, cache and filter are half, and max_decode_length is 129.
 *
 *  @par API Dependency
 *  - You need to call the ::cnnlCreateTransformerSelfAttnDescriptor() to
 *    create a descriptor and use ::cnnlSetTransformerSelfAttnDescriptor_v2()
 *    to set its parameters before calling this function.
 *  - You need to call the ::cnnlDestroyTransformerSelfAttnDescriptor()
 *    function after calling this function.
 *  @par Reference
 *  - https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
 *
 *  @par Example
 *  - When using \p encoder \p attention:
      @verbatim
      hidden_size: head_num * head_size_v

      Dimension of input: [batch_size, seq_len, in_channel]

      Dimension of qfilter: [hidden_size, in_channel]

      Dimension of kfilter: [hidden_size, in_channel]

      Dimension of vfilter: [hidden_size, in_channel]

      Dimension of outfilter: [hidden_size, hidden_size]

      Dimension of qbias: [hidden_size]

      Dimension of kbias: [hidden_size]

      Dimension of vbias: [hidden_size]

      Dimension of outbias: [hidden_size]

      Dimension of norm_scale: [in_channel]

      Dimension of norm_bias: [in_channel]

      When mask_mode is CNNL_TRANSFORMER_SELF_ATTN_DECODER_MASK
      Dimension of mask_vec: [seq_len, seq_len]
      When mask_mode is CNNL_TRANSFORMER_SELF_ATTN_ENCODER_MASK
      Dimension of mask_vec: [batch_size, seq_len]
      When mask_mode is CNNL_TRANSFORMER_SELF_ATTN_MIX_MASK
      Dimension of mask_vec: [batch_size, seq_len, seq_len]
      When mask_mode is CNNL_TRANSFORMER_SELF_ATTN_SELF_GEN_ENCODER_MASK
      Dimension of valid_token: [batch_size]

      Then we will get the output:

      output: [batch_size, seq_len, hidden_size]
      @endverbatim
 *  - When using \p encoder \p attention pack mode:
      @verbatim
      hidden_size: head_num * head_size_v

      for i in batch_size: total_seq_len = total_seq_len + valid_token[i]

      Dimension of input: [1, total_seq_len, in_channel]

      Dimension of qfilter: [hidden_size, in_channel]

      Dimension of kfilter: [hidden_size, in_channel]

      Dimension of vfilter: [hidden_size, in_channel]

      Dimension of outfilter: [hidden_size, hidden_size]

      Dimension of qbias: [hidden_size]

      Dimension of kbias: [hidden_size]

      Dimension of vbias: [hidden_size]

      Dimension of outbias: [hidden_size]

      Dimension of norm_scale: [in_channel]

      Dimension of norm_bias: [in_channel]

      When mask_mode is CNNL_TRANSFORMER_SELF_ATTN_PACK_MODE
      Dimension of valid_token: [batch_size]

      Then we will get the output:

      output: [1, total_seq_len, hidden_size]
      @endverbatim
 *  - When using \p cached \p decoder \p attention:
      @verbatim
      hidden_size: head_num * head_size_v

      Dimension of input: [batch_size, beam_size, in_channel]

      Dimension of qfilter: [hidden_size, in_channel]

      Dimension of kfilter: [hidden_size, in_channel]

      Dimension of vfilter: [hidden_size, in_channel]

      Dimension of outfilter: [hidden_size, hidden_size]

      Dimension of qbias: [hidden_size]

      Dimension of kbias: [hidden_size]

      Dimension of vbias: [hidden_size]

      Dimension of outbias: [hidden_size]

      Dimension of norm_scale: [hidden_size]

      Dimension of norm_bias: [hidden_size]

      Dimension of curr_idx: [1]

      Dimension of key_cache: In MLU270 and MLU500 series mode [batch_size, beam_size, head_num, max_decode_length, head_size]
                              In MLU300 mode [batch_size, beam_size, max_decode_length, hidden_size]

      Dimension of value_cache: In MLU270 and MLU500 series mode [batch_size, beam_size, head_num, max_decode_length, head_size]
                                In MLU300 mode [batch_size, beam_size, max_decode_length, hidden_size]

      Dimension of best_beams_cache: [batch_size, ngroup, beam_size]

      Then we will get the output:

      output: [batch_size, beam_size, hidden_size]
      @endverbatim
 */
cnnlStatus_t CNNL_WIN_API
cnnlTransformerSelfAttn(cnnlHandle_t handle,
                        const cnnlTransformerSelfAttnDescriptor_t self_attn_desc,
                        const cnnlTensorDescriptor_t desc_input,
                        const void *input,
                        const cnnlTensorDescriptor_t desc_qfilter,
                        const void *qfilter,
                        const cnnlTensorDescriptor_t desc_kfilter,
                        const void *kfilter,
                        const cnnlTensorDescriptor_t desc_vfilter,
                        const void *vfilter,
                        const cnnlTensorDescriptor_t desc_qbias,
                        const void *qbias,
                        const cnnlTensorDescriptor_t desc_kbias,
                        const void *kbias,
                        const cnnlTensorDescriptor_t desc_vbias,
                        const void *vbias,
                        const cnnlTensorDescriptor_t desc_outfilter,
                        const void *outfilter,
                        const cnnlTensorDescriptor_t desc_outbias,
                        const void *outbias,
                        const cnnlTensorDescriptor_t desc_mask,
                        const void *mask_vec,
                        const cnnlTensorDescriptor_t desc_valid_token,
                        const void *valid_token,
                        const cnnlTensorDescriptor_t desc_norm_scale,
                        const void *norm_scale,
                        const cnnlTensorDescriptor_t desc_norm_bias,
                        const void *norm_bias,
                        const cnnlTensorDescriptor_t desc_curr_idx,
                        void *curr_idx,
                        const cnnlTensorDescriptor_t desc_key_cache,
                        void *key_cache,
                        const cnnlTensorDescriptor_t desc_value_cache,
                        void *value_cache,
                        const cnnlTensorDescriptor_t desc_best_beams_cache,
                        void *best_beams_cache,
                        void *workspace,
                        size_t workspace_size,
                        const cnnlTensorDescriptor_t desc_output,
                        const void *output);

// Group:Transformer Self Attn
/*!
 *  @brief Creates a descriptor pointed by \p self_attn_desc
 *  for the transformerselfattn operation and allocate memory for it.
 *
 *  @param[out]  self_attn_desc
 *    Output. The descriptor of transformerselfattn operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_ALLOC_FAILED
 *    The function failed to allocate memory space.
 *
 *  @par API Dependency
 *  - After calling this function, you can call the ::cnnlSetTransformerSelfAttnDescriptor_v2
 *  function to initialize and set the information to the descriptor.
 *  You need to call the ::cnnlDestroyTransformerSelfAttnDescriptor function
 *  to destroy the descriptor at the end of the context.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCreateTransformerSelfAttnDescriptor(
  cnnlTransformerSelfAttnDescriptor_t *self_attn_desc);

// Group:Transformer Self Attn
/*!
 *  @brief Destroys the descriptor of the
 *  transformerselfattn operation and free memory for it.
 *
 *  @param[in]  self_attn_desc
 *    Input. The descriptor of transformerselfattn operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One of the following conditions are met:
 *    - The value of self_attn_desc is NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlDestroyTransformerSelfAttnDescriptor(
  cnnlTransformerSelfAttnDescriptor_t self_attn_desc);

// Group:Transformer Self Attn
/*!
 *  @brief Assigns transformerselfattn descriptor with parameters.
 *
 *  @param[in,out] self_attn_desc
 *    Input/output. Descriptor of TransformerSelfAttn operation.
 *  @param[in]  head_num
 *    Input. The number of heads in self attention.
 *  @param[in]  head_size_qk
 *    Input. The size of one head in self attention for query and key.
 *  @param[in]  head_size_v
 *    Input. The size of one head in self attention for value.
 *  @param[in]  hidden_size
 *    Input. The size of feature for one token.
 *  @param[in]  query_factor
 *    Input. The scaling coefficient of query.
 *  @param[in]  query_out_pos
 *    Input. The query quantization position.
 *  @param[in]  query_out_scale
 *    Input. The query quantization scaling coefficient.
 *  @param[in]  key_out_pos
 *    Input. The key quantization position.
 *  @param[in]  key_out_scale
 *    Input. The key quantization scaling coefficient.
 *  @param[in]  value_out_pos
 *    Input. The value quantization position.
 *  @param[in]  value_out_scale
 *    Input. The value quantization scaling coefficient.
 *  @param[in]  qkv_out_pos
 *    Input. The zout quantization position.
 *  @param[in]  qkv_out_scale
 *    Input. The zout quantization scaling coefficient.
 *  @param[in]  layernorm_res_mode
 *    Input. The implementation mode of layernorm and residual in the transformerselfattn.
 *           See ::cnnlTransformerLayernormResidualStructure_t for details.
 *  @param[in]  mask_mode
 *    Input. The mask adding mode. See ::cnnlTransformerSelfAttnMaskMode_t for details.
 *  @param[in]  use_quantify_scale
 *    Input. A Boolean value indicates whether to use scale in quantization.
 *  @param[in]  use_bias
 *    Input. A Boolean value indicates whether to use bias when computing query, key, value and attention output.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    The value of \p self_attn_desc is NULL.
 *
 *  @par API Dependency
 *  - Before this function, you need to use
 *  ::cnnlCreateTransformerSelfAttnDescriptor() to create a descriptor.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetTransformerSelfAttnDescriptor_v2(
            cnnlTransformerSelfAttnDescriptor_t self_attn_desc,
            const int head_num,
            const int head_size_qk,
            const int head_size_v,
            const int hidden_size,
            const float query_factor,
            const int16_t query_out_pos,
            const float query_out_scale,
            const int16_t key_out_pos,
            const float key_out_scale,
            const int16_t value_out_pos,
            const float value_out_scale,
            const int16_t qkv_out_pos,
            const float qkv_out_scale,
            const cnnlTransformerLayernormResidualStructure_t layernorm_res_mode,
            const cnnlTransformerSelfAttnMaskMode_t mask_mode,
            const bool use_quantify_scale,
            const bool use_bias);

// Group:Transformer Self Attn
/*!
 *  @brief Assigns transformerselfattn descriptor with
 *  \p enable_encoder_key_output and \p enable_encoder_value_output parameters
 *   and set values for the parameters.
 *
 *  @param[out] self_attn_desc
 *    Output. Descriptor of transformerselfattn operation.
 *  @param[in]  enable_encoder_key_output
 *    Input. A Boolean value indicates whether to enable encoder key output. This parameter is not supported yet.
 *  @param[in]  enable_encoder_value_output
 *    Input. A Boolean value indicates whether to enable encoder value output.
 *    When this parameter is true, \p head_num can be only set to 4.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_INVALIDPARAM
 *    - The value of \p self_attn_desc is NULL.
 *  @par API Dependency
 *  - Before this function, you need to use
 *  ::cnnlCreateTransformerSelfAttnDescriptor() to create a descriptor.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetTransformerSelfAttnEncoderKeyValueOutputMode(
            cnnlTransformerSelfAttnDescriptor_t self_attn_desc,
            const bool enable_encoder_key_output,
            const bool enable_encoder_value_output);

// Group:Transformer Self Attn
/*!
 *  @brief Assigns transformerselfattn descriptor with
 *  \p softmax_out_pos and \p softmax_out_scale parameters.
 *
 *  @param[out] self_attn_desc
 *    Output. Descriptor of TransformerSelfAttn operation.
 *  @param[in] softmax_out_pos
 *    Input. The position of softmax output quantization in the per-tensor mode.
 *  @param[in]  softmax_out_scale
 *    Input. The scaling coefficient of softmax output quantization in the per-tensor mode.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_INVALIDPARAM
 *    - The value of self_attn_desc is NULL.
 *
 *  @par API Dependency
 *  - Before calling this function, you need to use
 *  ::cnnlCreateTransformerSelfAttnDescriptor() to create a descriptor.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetTransformerSelfAttnSoftmaxOutPosAndScale(
            cnnlTransformerSelfAttnDescriptor_t self_attn_desc,
            const int softmax_out_pos,
            const float softmax_out_scale);

// Group:Transformer Self Attn
/*!
 *  @brief Assigns transformerselfattn descriptor with \p max_seq_len parameter.
 *
 *  @param[out] self_attn_desc
 *    Output. Descriptor of transformerselfattn operation.
 *  @param[in] max_seq_len
 *    Input. The maximum sequence length of the current dataset in pack mode.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_INVALIDPARAM
 *    - The value of \p self_attn_desc is NULL.
 *  @par API Dependency
 *  - You need to use ::cnnlCreateTransformerSelfAttnDescriptor()
 *   to create a descriptor and use ::cnnlSetTransformerSelfAttnPackModeMaxToken()
 *   to set its parameters.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetTransformerSelfAttnPackModeMaxToken(
            cnnlTransformerSelfAttnDescriptor_t self_attn_desc,
            const int max_seq_len);

// Group:Transformer Self Attn
/*!
 *  @brief Gets extra space size required in
 *  transformerselfattn operation.
 *
 *  @param[in]  self_attn_desc
 *    Input.  Descriptor of transformerselfattn operation.
 *  @param[in]  input_desc
 *    Input. Descriptor of input, containing dimension and the layout of input.
 *  @param[out] size
 *    Output. Size of workspace.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One of the following conditions are met:
 *    - The value of self_attn_desc or size is NULL.
 *  @par API Dependency
 *  - You need to use ::cnnlCreateTransformerSelfAttnDescriptor() to create a descriptor
 *  and use ::cnnlSetTransformerSelfAttnDescriptor_v2() to set its parameters.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetTransformerSelfAttnWorkspaceSize(
  const cnnlTransformerSelfAttnDescriptor_t self_attn_desc,
  const cnnlTensorDescriptor_t input_desc,
  size_t *size);

// Group:Transformer Self Attn
/*!
 *  @brief Assigns transformerselfattn descriptor with \p scaled_after_q_dot_k parameter.
 *
 *  @param[out] self_attn_desc
 *    Output. Descriptor of transformerselfattn operation.
 *  @param[in] scaled_after_q_dot_k
 *    Input.  A Boolean value indicates whether to scale the result of q_dot_k.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_INVALIDPARAM
 *    - The value of \p self_attn_desc is NULL.
 *  @par API Dependency
 *  - You need to use ::cnnlCreateTransformerSelfAttnDescriptor() to create a descriptor
 *  and use ::cnnlSetTransformerSelfAttnDescriptor_v2() to set its parameters.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetTransformerSelfAttnFactorPosition(
            cnnlTransformerSelfAttnDescriptor_t self_attn_desc,
            bool scaled_after_q_dot_k);

// Group:Transformer Self Attn
/*!
 *  @brief Assigns transformerselfattn with parameters in the quantization scenario.
 *
 *  @param[out] self_attn_desc
 *    Output. Descriptor of transformerselfattn operation.
 *  @param[in]  head_num
 *    Input. The number of heads in self attention.
 *  @param[in]  head_size_qk
 *    Input. The size of one head in self attention for query and key.
 *  @param[in]  head_size_v
 *    Input. The size of one head in self attention for value.
 *  @param[in]  hidden_size
 *    Input. The size of feature for one token.
 *  @param[in]  query_factor
 *    Input. The scaling coefficient multiplied to query or q_dot_k.
 *  @param[in]  layernorm_eps
 *    Input. The eps value of layernorm.
 *  @param[in]  use_bias
 *    Input. A Boolean value indicates whether to use bias when computing query, key, value and attention output.
 *  @param[in]  use_hp_active
 *    Input. A Boolean value indicates whether to use high-precision activation. There are the following rules:
 *           - When \p compute_dtype = FLOAT and \p use_hp_active = true, spss activation is used.
 *           - When \p compute_dtype = FLOAT and \p use_hp_active = false, float-precision activation table is used.
 *           - When \p compute_dtype = HALF and \p use_hp_active = true, spss activation is used.
 *           - When \p compute_dtype = HALF and \p use_hp_active = false, half-precision activation table is used.
 *  @param[in]  attention_output_quant
 *    Input. A Boolean value indicates whether to quantize the output of attention before adding residual.
 *           The quantization is performed by converting the data type of the output of attention from
 *           float to int8 to float or from half to int8 to half.
 *  @param[in]  residual_input_quant
 *    Input. A Boolean value indicates whether to quantize the input of residual before adding residual.
 *           The quantization is performed by converting the data type of the input of residual from
 *           float to int8 to float or from half to int8 to half.
 *  @param[in]  use_query_aftergemm_quant
 *    Input. A Boolean value indicates whether to use query_aftergemm_pos and query_aftergemm_scale when dequantizing query.
 *  @param[in]  use_key_aftergemm_quant
 *    Input. A Boolean value indicates whether to use key_aftergemm_pos and key_aftergemm_scale when dequantizing key.
 *  @param[in]  use_value_aftergemm_quant
 *    Input. A Boolean value indicates whether to use value_aftergemm_pos and value_aftergemm_scale when dequantizing value.
 *  @param[in]  use_qk_aftergemm_quant
 *    Input. A Boolean value indicates whether to use qk_aftergemm_pos and qk_aftergemm_scale when dequantizing q_dot_k.
 *  @param[in]  use_qkv_aftergemm_quant
 *    Input. A Boolean value indicates whether to use qkv_aftergemm_pos and qkv_aftergemm_scale when dequantizing qk_dot_v.
 *  @param[in]  use_o_aftergemm_quant
 *    Input. A Boolean value indicates whether to use o_aftergemm_pos and o_aftergemm_scale when dequantizing output.
 *  @param[in]  quant_per_tensor
 *    Input. A Boolean value indicates whether to use per-tensor mode for quantization.
 *           - Per-tensor mode(default): Converts a float tensor to quantized tensor with the given position and scale.
 *           - Per-channel mode: Converts a float tensor to per-channel quantized tensor with given positions and scales.
 *  @param[in]  use_quantify_scale
 *    Input. A Boolean value indicates whether to use scaling coefficient in quantization.
 *  @param[in]  use_quantify_offset
 *    Input. A Boolean value indicates whether to use offset in quantization.
 *  @param[in]  online_quantify
 *    Input. A Boolean value indicates whether to enable online quantization.
 *           Quantization information must be provided when it is false.
 *  @param[in]  compute_dtype
 *    Input. The data type used in the computing, which includes half and float.
 *  @param[in]  layernorm_res_mode
 *    Input. The implementation mode of layernorm and residual in the TransformerSelfAttn.
 *           See ::cnnlTransformerLayernormResidualStructure_t for details.
 *  @param[in]  mask_mode
 *    Input. The mask mode used in the TransformerSelfAttn. See ::cnnlTransformerSelfAttnMaskMode_t for details.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    The value of \p self_attn_desc is NULL.
 *
 *  @par API Dependency
 *  - Before this function, you need to use
 *  ::cnnlCreateTransformerSelfAttnDescriptor() to create a descriptor.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetTransformerSelfAttnQATParameters(
        cnnlTransformerSelfAttnDescriptor_t self_attn_desc,
        const int head_num,
        const int head_size_qk,
        const int head_size_v,
        const int hidden_size,
        const float query_factor,
        const float layernorm_eps,
        const bool use_bias,
        const bool use_hp_active,
        const bool attention_output_quant,
        const bool residual_input_quant,
        const bool use_query_aftergemm_quant,
        const bool use_key_aftergemm_quant,
        const bool use_value_aftergemm_quant,
        const bool use_qk_aftergemm_quant,
        const bool use_qkv_aftergemm_quant,
        const bool use_o_aftergemm_quant,
        const bool quant_per_tensor,
        const bool use_quantify_scale,
        const bool use_quantify_offset,
        const bool online_quantify,
        const cnnlDataType_t compute_dtype,
        const cnnlTransformerLayernormResidualStructure_t layernorm_res_mode,
        const cnnlTransformerSelfAttnMaskMode_t mask_mode);

// Group:Transformer Self Attn
/*!
 *  @brief Assigns transformerselfattn descriptor with parameters.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlSetTransformerSelfAttnQuantizeParams instead.
 *
 *  @param[out] self_attn_desc
 *    Output. Descriptor of transformerselfattn operation.
 *  @param[in]  qfilter_pos
 *    Input. The position of qfilter quantization in the per-channel mode.
 *  @param[in]  qfilter_scale
 *    Input. The scaling coefficient of qfilter quantization in the per-channel mode.
 *  @param[in]  qfilter_offset
 *    Input. The offset of qfilter quantization in the per-channel mode.
 *  @param[in]  kfilter_pos
 *    Input. The position of kfilter quantization in the per-channel mode.
 *  @param[in]  kfilter_scale
 *    Input. The scaling coefficient of kfilter quantization in the per-channel mode.
 *  @param[in]  kfilter_offset
 *    Input. The offset of kfilter quantization in the per-channel mode.
 *  @param[in]  vfilter_pos
 *    Input. The position of vfilter quantization in the per-channel mode.
 *  @param[in]  vfilter_scale
 *    Input. The scaling coefficient of vfilter quantization in the per-channel mode.
 *  @param[in]  vfilter_offset
 *    Input. The offset of vfilter quantization in the per-channel mode.
 *  @param[in]  ofilter_pos
 *    Input. The position of ofilter quantization in the per-channel mode.
 *  @param[in]  ofilter_scale
 *    Input. The scaling coefficient of ofilter quantization in the per-channel mode.
 *  @param[in]  ofilter_offset
 *    Input. The offset of ofilter quantization in the per-channel mode.
 *  @param[in]  query_out_pos
 *    Input. The position of query quantization in the per-tensor mode.
 *  @param[in]  query_out_scale
 *    Input. The scaling coefficient of query quantization in the per-tensor mode.
 *  @param[in]  query_out_offset
 *    Input. The offset of query quantization in the per-tensor mode.
 *  @param[in]  key_out_pos
 *    Input. The position of key quantization in the per-tensor mode.
 *  @param[in]  key_out_scale
 *    Input. The scaling coefficient of key quantization in the per-tensor mode.
 *  @param[in]  key_out_offset
 *    Input. The offset of key quantization in the per-tensor mode.
 *  @param[in]  value_out_pos
 *    Input. The position of value quantization in the per-tensor mode.
 *  @param[in]  value_out_scale
 *    Input. The scaling coefficient of value quantization in the per-tensor mode.
 *  @param[in]  value_out_offset
 *    Input. The offset of value quantization in the per-tensor mode.
 *  @param[in] softmax_out_pos
 *    Input. The position of softmax output quantization in the per-tensor mode.
 *  @param[in]  softmax_out_scale
 *    Input. The scaling coefficient of softmax output quantization in the per-tensor mode.
 *  @param[in]  softmax_out_offset
 *    Input. The offset of softmax output quantization in the per-tensor mode.
 *  @param[in]  qkv_out_pos
 *    Input. The position of qk_dot_v quantization in the per-tensor mode.
 *  @param[in]  qkv_out_scale
 *    Input. The scaling coefficient of qk_dot_v quantization in the per-tensor mode.
 *  @param[in]  qkv_out_offset
 *    Input. The offset of qk_dot_v quantization in the per-tensor mode.
 *  @param[in]  o_out_pos
 *    Input. The position of output quantization in the per-tensor mode.
 *  @param[in]  o_out_scale
 *    Input. The scaling coefficient of output quantization in the per-tensor mode.
 *  @param[in]  o_out_offset
 *    Input. The offset of output quantization in the per-tensor mode.
 *  @param[in]  query_aftergemm_pos
 *    Input. The position of query dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in]  query_aftergemm_scale
 *    Input. The scaling coefficient of query dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in]  query_aftergemm_offset
 *    Input. The offset of query dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in]  key_aftergemm_pos
 *    Input. The position of key dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in]  key_aftergemm_scale
 *    Input. The scaling coefficient of key dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in]  key_aftergemm_offset
 *    Input. The offset of key dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in]  value_aftergemm_pos
 *    Input. The position of value dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in]  value_aftergemm_scale
 *    Input. The scaling coefficient of value dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in]  value_aftergemm_offset
 *    Input. The offset of value dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in]  qk_aftergemm_pos
 *    Input. The position of q_dot_k dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in]  qk_aftergemm_scale
 *    Input. The scaling coefficient of q_dot_k dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in]  qk_aftergemm_offset
 *    Input. The offset of q_dot_k dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in]  qkv_aftergemm_pos
 *    Input. The position of qk_dot_v dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in]  qkv_aftergemm_scale
 *    Input. The scaling coefficient of qk_dot_v dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in]  qkv_aftergemm_offset
 *    Input. The offset of qk_dot_v dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in]  o_aftergemm_pos
 *    Input. The position of output dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in]  o_aftergemm_scale
 *    Input. The scaling coefficient of output dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in]  o_aftergemm_offset
 *    Input. The offset of output dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    The value of \p self_attn_desc is NULL.
 *
 *  @par API Dependency
 *  - Before using this function, you need to call
 *  ::cnnlCreateTransformerSelfAttnDescriptor() to create a descriptor.
 */
CNNL_DEPRECATED_FOR(cnnlSetTransformerSelfAttnQuantizeParams)
cnnlStatus_t CNNL_WIN_API
cnnlSetTransformerSelfAttnQATQuantifyParams(
  const cnnlTransformerSelfAttnDescriptor_t self_attn_desc,
  const void *qfilter_pos,
  const void *qfilter_scale,
  const void *qfilter_offset,
  const void *kfilter_pos,
  const void *kfilter_scale,
  const void *kfilter_offset,
  const void *vfilter_pos,
  const void *vfilter_scale,
  const void *vfilter_offset,
  const void *ofilter_pos,
  const void *ofilter_scale,
  const void *ofilter_offset,
  const int16_t query_out_pos,
  const float query_out_scale,
  const int16_t query_out_offset,
  const int16_t key_out_pos,
  const float key_out_scale,
  const int16_t key_out_offset,
  const int16_t value_out_pos,
  const float value_out_scale,
  const int16_t value_out_offset,
  const int16_t softmax_out_pos,
  const float softmax_out_scale,
  const int16_t softmax_out_offset,
  const int16_t qkv_out_pos,
  const float qkv_out_scale,
  const int16_t qkv_out_offset,
  const int16_t o_out_pos,
  const float o_out_scale,
  const int16_t o_out_offset,
  const int16_t query_aftergemm_pos,
  const float query_aftergemm_scale,
  const int16_t query_aftergemm_offset,
  const int16_t key_aftergemm_pos,
  const float key_aftergemm_scale,
  const int16_t key_aftergemm_offset,
  const int16_t value_aftergemm_pos,
  const float value_aftergemm_scale,
  const int16_t value_aftergemm_offset,
  const int16_t qk_aftergemm_pos,
  const float qk_aftergemm_scale,
  const int16_t qk_aftergemm_offset,
  const int16_t qkv_aftergemm_pos,
  const float qkv_aftergemm_scale,
  const int16_t qkv_aftergemm_offset,
  const int16_t o_aftergemm_pos,
  const float o_aftergemm_scale,
  const int16_t o_aftergemm_offset);

// Group:Transformer Self Attn
/*!
 *  @brief Assigns transformerselfattn descriptor with quantization parameters in per-tensor mode.
 *
 *  @param[out] desc
 *    Output. Descriptor of transformerselfattn operation.
 *  @param[in]  input_pos
 *    Input. The position of input quantization in the per-tensor mode.
 *  @param[in]  input_scale
 *    Input. The scaling coefficient of input quantization in the per-tensor mode.
 *  @param[in]  input_offset
 *    Input. The offset of input quantization in the per-tensor mode.
 *  @param[in]  qfilter_pos
 *    Input. The position of qfilter quantization in the per-tensor mode.
 *  @param[in]  qfilter_scale
 *    Input. The scaling coefficient of qfilter quantization in the per-tensor mode.
 *  @param[in]  qfilter_offset
 *    Input. The offset of qfilter quantization in the per-tensor mode.
 *  @param[in]  kfilter_pos
 *    Input. The position of kfilter quantization in the per-tensor mode.
 *  @param[in]  kfilter_scale
 *    Input. The scaling coefficient of kfilter quantization in the per-tensor mode.
 *  @param[in]  kfilter_offset
 *    Input. The offset of kfilter quantization in the per-tensor mode.
 *  @param[in]  vfilter_pos
 *    Input. The position of vfilter quantization in the per-tensor mode.
 *  @param[in]  vfilter_scale
 *    Input. The scaling coefficient of vfilter quantization in the per-tensor mode.
 *  @param[in]  vfilter_offset
 *    Input. The offset of vfilter quantization in the per-tensor mode.
 *  @param[in]  ofilter_pos
 *    Input. The position of ofilter quantization in the per-tensor mode.
 *  @param[in]  ofilter_scale
 *    Input. The scaling coefficient of ofilter quantization in the per-tensor mode.
 *  @param[in]  ofilter_offset
 *    Input. The offset of ofilter quantization in the per-tensor mode.
 *  @param[in]  query_out_pos
 *    Input. The position of query quantization in the per-tensor mode.
 *  @param[in]  query_out_scale
 *    Input. The scaling coefficient of query quantization in the per-tensor mode.
 *  @param[in]  query_out_offset
 *    Input. The offset of query quantization in the per-tensor mode.
 *  @param[in]  key_out_pos
 *    Input. The position of key quantization in the per-tensor mode.
 *  @param[in]  key_out_scale
 *    Input. The scaling coefficient of key quantization in the per-tensor mode.
 *  @param[in]  key_out_offset
 *    Input. The offset of key quantization in the per-tensor mode.
 *  @param[in]  value_out_pos
 *    Input. The position of value quantization in the per-tensor mode.
 *  @param[in]  value_out_scale
 *    Input. The scaling coefficient of value quantization in the per-tensor mode.
 *  @param[in]  value_out_offset
 *    Input. The offset of value quantization in the per-tensor mode.
 *  @param[in] softmax_out_pos
 *    Input. The position of softmax output quantization in the per-tensor mode.
 *  @param[in]  softmax_out_scale
 *    Input. The scaling coefficient of softmax output quantization in the per-tensor mode.
 *  @param[in]  softmax_out_offset
 *    Input. The offset of softmax output quantization in the per-tensor mode.
 *  @param[in]  qkv_out_pos
 *    Input. The position of qk_dot_v quantization in the per-tensor mode.
 *  @param[in]  qkv_out_scale
 *    Input. The scaling coefficient of qk_dot_v quantization in the per-tensor mode.
 *  @param[in]  qkv_out_offset
 *    Input. The offset of qk_dot_v quantization in the per-tensor mode.
 *  @param[in]  o_out_pos
 *    Input. The position of output quantization in the per-tensor mode.
 *  @param[in]  o_out_scale
 *    Input. The scaling coefficient of output quantization in the per-tensor mode.
 *  @param[in]  o_out_offset
 *    Input. The offset of output quantization in the per-tensor mode.
 *  @param[in]  query_aftergemm_pos
 *    Input. The position of query dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in]  query_aftergemm_scale
 *    Input. The scaling coefficient of query dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in]  query_aftergemm_offset
 *    Input. The offset of query dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in]  key_aftergemm_pos
 *    Input. The position of key dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in]  key_aftergemm_scale
 *    Input. The scaling coefficient of key dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in]  key_aftergemm_offset
 *    Input. The offset of key dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in]  value_aftergemm_pos
 *    Input. The position of value dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in]  value_aftergemm_scale
 *    Input. The scaling coefficient of value dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in]  value_aftergemm_offset
 *    Input. The offset of value dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in]  qk_aftergemm_pos
 *    Input. The position of q_dot_k dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in]  qk_aftergemm_scale
 *    Input. The scaling coefficient of q_dot_k dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in]  qk_aftergemm_offset
 *    Input. The offset of q_dot_k dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in]  qkv_aftergemm_pos
 *    Input. The position of qk_dot_v dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in]  qkv_aftergemm_scale
 *    Input. The scaling coefficient of qk_dot_v dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in]  qkv_aftergemm_offset
 *    Input. The offset of qk_dot_v dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in]  o_aftergemm_pos
 *    Input. The position of output dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in]  o_aftergemm_scale
 *    Input. The scaling coefficient of output dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in]  o_aftergemm_offset
 *    Input. The offset of output dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    The value of \p self_attn_desc is NULL.
 *
 *  @par API Dependency
 *  - Before using this function, you need to call
 *  ::cnnlCreateTransformerSelfAttnDescriptor() to create a descriptor.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetTransformerSelfAttnQuantizeParams(
    const cnnlTransformerSelfAttnDescriptor_t desc,
    int input_pos,
    float input_scale,
    int input_offset,
    int qfilter_pos,
    float qfilter_scale,
    int qfilter_offset,
    int kfilter_pos,
    float kfilter_scale,
    int kfilter_offset,
    int vfilter_pos,
    float vfilter_scale,
    int vfilter_offset,
    int ofilter_pos,
    float ofilter_scale,
    int ofilter_offset,
    int query_out_pos,
    float query_out_scale,
    int query_out_offset,
    int key_out_pos,
    float key_out_scale,
    int key_out_offset,
    int value_out_pos,
    float value_out_scale,
    int value_out_offset,
    int softmax_out_pos,
    float softmax_out_scale,
    int softmax_out_offset,
    int qkv_out_pos,
    float qkv_out_scale,
    int qkv_out_offset,
    int o_out_pos,
    float o_out_scale,
    int o_out_offset,
    int query_aftergemm_pos,
    float query_aftergemm_scale,
    int query_aftergemm_offset,
    int key_aftergemm_pos,
    float key_aftergemm_scale,
    int key_aftergemm_offset,
    int value_aftergemm_pos,
    float value_aftergemm_scale,
    int value_aftergemm_offset,
    int qk_aftergemm_pos,
    float qk_aftergemm_scale,
    int qk_aftergemm_offset,
    int qkv_aftergemm_pos,
    float qkv_aftergemm_scale,
    int qkv_aftergemm_offset,
    int o_aftergemm_pos,
    float o_aftergemm_scale,
    int o_aftergemm_offset);

// Group:Transformer Self Attn
/*!
 *  @brief Sets the data type used in the computation of transformerselfattn.
 *
 *  @param[out] self_attn_desc
 *    Output. The descriptor of transformerselfattn operation.
 *  @param[in]  use_hp_active
 *    Input. A Boolean value indicates whether to use high-precision activation. There are the following rules:
 *           - When \p compute_dtype = FLOAT and \p use_hp_active = true, spss activation is used.
 *           - When \p compute_dtype = FLOAT and \p use_hp_active = false, float-precision activation table is used.
 *           - When \p compute_dtype = HALF and \p use_hp_active = true, spss activation is used.
 *           - When \p compute_dtype = HALF and \p use_hp_active = false, half-precision activation table is used.
 *  @param[in]  compute_dtype
 *    Input. The data type used in the computing, which includes half and float.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    The value of \p self_attn_desc is NULL.
 *
 *  @par API Dependency
 *  - Before this function, you need to use
 *  ::cnnlCreateTransformerSelfAttnDescriptor() to create a descriptor
 *  and use ::cnnlSetTransformerSelfAttnDescriptor_v2() to set its parameters.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetTransformerSelfAttnComputeType(
        cnnlTransformerSelfAttnDescriptor_t self_attn_desc,
        const bool use_hp_active,
        const cnnlDataType_t compute_dtype);

/******************************************************************************
 * Cambricon CNNL OP: TransformerEncoderOutput
 ******************************************************************************/
/*!
 * @brief The descriptor of the ::cnnlTransformerEncoderOutput_v2 operation.
 * You can use ::cnnlCreateTransformerEncoderOutputDescriptor, ::cnnlSetTransformerEncoderOutputDescriptor_v3
 * and ::cnnlDestroyTransformerEncoderOutputDescriptor to create, set and destroy the descriptor
 * respectively.
 *
 * It is deprecated and will be removed in future release.
 */
typedef struct cnnlTransformerEncOutputStruct *cnnlTransformerEncoderOutputDescriptor_t;

// Group:Transformer Encoder Output
/*!
 *  @brief Creates a descriptor pointed by \p desc
 *  for the transformer_encoder_output operation.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[out]  desc
 *    Output. A pointer to the transformer_encoder_output descriptor that holds
 *   information about the transformer_encoder_output operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_ALLOC_FAILED
 *    The function failed to malloc memory.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlCreateTransformerEncoderOutputDescriptor(cnnlTransformerEncoderOutputDescriptor_t *desc);

// Group:Transformer Encoder Output
/*!
 *  @brief Destroys the descriptor of the transformer_encoder_output operation.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[in]  desc
 *    Input. Descriptor of the transformer_encoder_output operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    The value of \p desc is NULL.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlDestroyTransformerEncoderOutputDescriptor(cnnlTransformerEncoderOutputDescriptor_t desc);

// Group:Transformer Encoder Output
/*!
 *  @brief Assigns the transformer_encoder_output descriptor with parameters.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @par API Dependency
 *  - Before calling this function, you need to call ::cnnlCreateTransformerEncoderOutputDescriptor
 *    to create a descriptor.
 *  - You need to call ::cnnlDestroyTransformerEncoderOutputDescriptor to destroy the descriptor
 *    at the end of the context.
 *
 *  @param[in,out]  desc
 *    Input/output. Descriptor of the transformer_encoder_output operation.
 *  @param[in]  layernorm_mode
 *    Input. The layernorm position. See ::cnnlTransformerLayernormMode_t for details.
 *  @param[in]  use_bias
 *    Input. A Boolean value indicating whether to use bias.
 *  @param[in]  decoder_layer_num
 *    Input. Layer number of decoder, which falls in range of [1, 6].
 *  @param[in]  head_num
 *    Input. Number of heads in self attention.
 *  @param[in]  head_size
 *    Input. The head size.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    The value of \p desc is NULL.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlSetTransformerEncoderOutputDescriptor_v2(cnnlTransformerEncoderOutputDescriptor_t desc,
                                             const cnnlTransformerLayernormMode_t layernorm_mode,
                                             const bool use_bias,
                                             const int decoder_layer_num,
                                             const int head_num,
                                             const int head_size);
// Group:Transformer Encoder Output
/*!
 *  @brief Assigns the transformer_encoder_output descriptor with parameters.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @par API Dependency
 *  - Before calling this function, you need to call ::cnnlCreateTransformerEncoderOutputDescriptor
 *    to create a descriptor.
 *  - You need to call ::cnnlDestroyTransformerEncoderOutputDescriptor to destroy the descriptor
 *    at the end of the context.
 *
 *  @param[in,out]  desc
 *    Input/output. Descriptor of the transformer_encoder_output operation.
 *  @param[in]  layernorm_mode
 *    Input. The layernorm position. See ::cnnlTransformerLayernormMode_t for details.
 *  @param[in]  use_bias
 *    Input. A Boolean value indicating whether to use bias.
 *  @param[in]  decoder_layer_num
 *    Input. Layer number of decoder, which falls in range of [1, 6].
 *  @param[in]  head_num
 *    Input. Number of heads in self attention.
 *  @param[in]  head_size
 *    Input. The head size.
 *  @param[in] input_quant_desc
 *    Input. A quantization descriptor that holds quantization information of input tensor.
 *           For detailed information, see cnnlQuantizeExDescriptor_t.
 *  @param[in] weight_k_quant_desc
 *    Input. A quantization descriptor that holds quantization information of weight_k tensor.
 *           For detailed information, see cnnlQuantizeExDescriptor_t.
 *  @param[in] weight_v_quant_desc
 *    Input. A quantization descriptor that holds quantization information of weight_v tensor.
 *           For detailed information, see cnnlQuantizeExDescriptor_t.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    The value of \p desc is NULL.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlSetTransformerEncoderOutputDescriptor_v3(cnnlTransformerEncoderOutputDescriptor_t desc,
                                             const cnnlTransformerLayernormMode_t layernorm_mode,
                                             const bool use_bias,
                                             const int decoder_layer_num,
                                             const int head_num,
                                             const int head_size,
                                             cnnlQuantizeExDescriptor_t input_quant_desc,
                                             cnnlQuantizeExDescriptor_t weight_k_quant_desc,
                                             cnnlQuantizeExDescriptor_t weight_v_quant_desc);

// Group:Transformer Encoder Output
/*!
 *  @brief Performs the output post-processing operation of encoder layer in the transformer network.
 *
 *  This function initializes K/V matrices shared by
 *  different layers of decoder based on the results of encoder. Since decoding
 *  process will be repeated until the terminate-token is reached, K/V matrices
 *  used by enc-dec-attn can be pre-computed before the decoding process
 *  by this encoder output operation.
 *
 *  This operation performs with the following steps:
 *
 *  1. Pre-Layernorm if layernorm_mode = CNNL_TRANSFORMER_PRE_LAYERNORM.
 *
 *    input_normed = layernorm(input, beta, gamma) or just input.
 *
 *  2. Compute K/V matrices.
 *
 *    For decoder_layer_num layers of decoder:
 *
 *      K = fc(input_normed, filter_k) + bias_k.
 *
 *      V = fc(input_normed, filter_v) + bias_v.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[in]  handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *           transformer_encoder_output operation.
 *  @param[in]  encoder_output_desc
 *    Input. A descriptor of the transformer_encoder_output operation.
 *  @param[in]  input_desc
 *    Input. A descriptor of input tensor.
 *  @param[in]  input
 *    Input. Pointer to the MLU memory that stores the input data.
 *  @param[in]  weight_k_desc
 *    Input. An array of cnnlTensorDescriptors which holds dimension, data type,
 *    and layout of filter_k tensor. The number of descriptors must be decoder_layer_num, and the shape of
 *    filter is [head_num * head_size, Ci].
 *  @param[in]  weight_k
 *    Input. Pointer to the host memory that stores an array of pointers to the device memory of
 *    K (key) filter data. The element order of this filter array must be consistent with the element
 *    order of the cnnlTensorDescriptors array.
 *  @param[in]  weight_v_desc
 *    Input. An array of cnnlTensorDescriptors which holds dimension, data type,
 *    and layout of filter_v tensor. The number of descriptors must be decoder_layer_num, and the shape of
 *    filter is [head_num * head_size, Ci].
 *  @param[in]  weight_v
 *    Input. Pointer to the host memory that stores an array of pointers to the device memory of
 *    V (value) filter data. The element order of this filter array must be consistent with the element
 *    order of the cnnlTensorDescriptors array.
 *  @param[in]  bias_k_desc
 *    Input. An array of cnnlTensorDescriptors which holds dimension, data type and layout of
 *    bias_k tensor. The number of descriptors must be decoder_layer_num, and the shape of bias is
 *    [head_num * head_size].
 *  @param[in]  bias_k
 *    Input. Pointer to the host memory that stores an array of pointers to the device memory of
 *    K (key) bias data. The element order of this bias array must be consistent with the element
 *    order of the cnnlTensorDescriptors array.
 *  @param[in]  bias_v_desc
 *    Input. An array of cnnlTensorDescriptors which holds dimension, data type and layout of
 *    bias_v tensor. The number of descriptors must be decoder_layer_num, and the shape of bias is
 *    [head_num * head_size].
 *  @param[in]  bias_v
 *    Input. Pointer to the host memory that stores an array of pointers to the device memory of
 *    V (value) bias data. The element order of this bias array must be consistent with the element
 *    order of the cnnlTensorDescriptors array.
 *  @param[in]  gamma_desc
 *    Input. A descriptor of gamma tensor used in layernorm.
 *  @param[in]  gamma
 *    Input. Pointer to the MLU memory that stores the gamma tensor.
 *  @param[in]  beta_desc
 *    Input. A descriptor of beta tensor used in layernorm.
 *  @param[in]  beta
 *    Input. Pointer to the MLU memory that stores the beta tensor.
 *  @param[out]  output_k_desc
 *    Output. An array of cnnlTensorDescriptors which holds dimension, data type and layout of
 *    output_k tensors. The number of descriptors must be decoder_layer_num.
 *    The shape of each output_k is [batch, head_num, token, head_size].
 *  @param[out]  output_k
 *    Output. Pointer to the host memory that stores an array of pointers to the device memory of
 *    K (key) output data. The element order of this output_k array must be consistent with the element
 *    order of the cnnlTensorDescriptors array.
 *  @param[out]  output_v_desc
 *   Output. An array of cnnlTensorDescriptors which holds dimension, data type and layout of
 *   output_v tensors. The number of descriptors must be decoder_layer_num.
 *   The shape of each output_v is [batch, head_num, token, head_size].
 *  @param[out]  output_v
 *   Output. Pointer to the host memory that stores an array of pointers to the device memory of
 *   v (value) output. The element order of this output_v array must be consistent with the element
 *   order of the cnnlTensorDescriptors array.
 *  @param[in] workspace
 *   Input. Pointer to the MLU memory that stores the extra workspace for the operation.
 *  @param[in] workspace_size
 *   Input. The size of workspace needed in the operation.
 *  @retval  CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval  CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are satisfied:
 *    - \p handle is NULL.
 *    - \p input is NULL.
 *    - \p output is NULL.
 *    - \p filter is NULL.
 *    - \p layernorm_gamma or layernorm beta is NULL when layernorm_mode > 0.
 *    - Dimensions are invalid.
 *  @par Data Type
 *  - On MLU300 series:
 *   - input: float, half.
 *   - filter: half.
 *   - bias: float, half.
 *   - output: float, half.
 *  - On MLU500 series:
 *   - All inputs and outputs must have the same data type, either float or half.
 *
 * @par Scale Limitation
 *  - On MLU500 series:
 *    - \p head_num must be in range of [1, 128].
 *    - \p head_size must be in range of [16, 128].
 *    - Ci = Co = head_num * head_size.
 *    - token must be in range of [1, 1024].
 *  - On MLU300 series:
 *    - head_num supports 4, 8, 12 or 16.
 *    - head_size supports 32, 64 or 96.
 *    - Ci = Co = head_num * head_size.
 *    - token supports [1, 1024].
 *    - \p decoder_layer_num supports [1, 128].
 *
 *  @par Performance Optimization
 *  - When the dimension of input meets N*T = 512*n (n=1,2,3..), the performance is best.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlTransformerEncoderOutput_v2(cnnlHandle_t handle,
                                const cnnlTransformerEncoderOutputDescriptor_t encoder_output_desc,
                                const cnnlTensorDescriptor_t input_desc,
                                const void *input,
                                const cnnlTensorDescriptor_t *weight_k_desc,
                                void **weight_k,
                                const cnnlTensorDescriptor_t *weight_v_desc,
                                void **weight_v,
                                const cnnlTensorDescriptor_t *bias_k_desc,
                                void **bias_k,
                                const cnnlTensorDescriptor_t *bias_v_desc,
                                void **bias_v,
                                const cnnlTensorDescriptor_t gamma_desc,
                                const void *gamma,
                                const cnnlTensorDescriptor_t beta_desc,
                                const void *beta,
                                const cnnlTensorDescriptor_t *output_k_desc,
                                void **output_k,
                                const cnnlTensorDescriptor_t *output_v_desc,
                                void **output_v,
                                void *workspace,
                                size_t workspace_size);

// Group:Transformer Encoder Output
/*!
 *  @brief Retrieves extra space size needed in the transformer_encoder_output operation.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *           transformer_encoder_output operations.
 *  @param[in] desc
 *    Input. The descriptor of the transformer_encoder_output operation.
 *  @param[in] input_desc
 *    Input. The descriptor of the input tensor of transformer_encoder_output operation.
 *  @param[out] size_workspace
 *    Output. The size of workspace needed in the transformer_encoder_output operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - The \p handle is NULL.
 *    - The \p input_desc is NULL.
 *  @par API Dependency
 *  - Before calling this function, you need to call ::cnnlCreateTransformerEncoderOutputDescriptor to
 *  create a descriptor and call ::cnnlSetTransformerEncoderOutputDescriptor_v3 to set information to the
 *  descriptor.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlGetTransformerEncoderOutputWorkspaceSize_v2(cnnlHandle_t handle,
                                                const cnnlTransformerEncoderOutputDescriptor_t desc,
                                                const cnnlTensorDescriptor_t input_desc,
                                                size_t *size_workspace);
/******************************************************************************
 * Cambricon CNNL OP: TransformerBeamRearrange
 ******************************************************************************/
// Group:Transformer Beam Rearrange
/*!
 *  @brief Implements in-place beam rearrange of cached
 *  decoder key and value according to beam search result.
 *
 *  It also records history result of beam search to restore the correct order of decoder cache.
 *  For example, when the shape of each cache is [batch, beam, head_num, seq_len, head_size]:
    @verbatim
    1. Rearrange each cache according to best_beams
    for i in range(0, layer_num * 2):
      for batch in range(0, batch_size):
        for beam in range(0, beam_size):
          decoder_states[i, batch, beam, :, 0:nframe-2, :] = decoder_states[i, batch, best_beam[batch, beam], :, 0:nframe-2, :]

    2. Store best_beams to best_beams_cache
    best_beams_cache[:, nframe-1] = best_beam
    @endverbatim
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation.
 * @param[in]  workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace
 *   for the operation.
 * @param[in]  best_beam_desc
 *   Input.  Descriptor of \p best_beam. Shape must be [batch_size, beam_size], and the data type
 *   must be INT8.
 * @param[in]  best_beam
 *   Input.  Pointer to the MLU memory that stores the \p best_beam tensor.
 *   \p best_beam is provided by the beam search operation and stores the top beam_size indices
 *   of last frame.
 * @param[in]  nframe_desc
 *   Input.  Descriptor of \p nframe. Shape must be [1], and data type must be INT32.
 * @param[in]  nframe
 *   Input.  Pointer to the MLU memory that stores the \p nframe tensor. It is the number of the current frame.
 * @param[in]  single_cache_desc
 *   Input. Descriptor of \p single_cache. Its data type must be float or half. Shape must be
 *   [batch_size, beam_size, head_num, max_decode_length, head_size] or
 *   [batch_size, beam_size, max_decode_length, hidden_size], where hidden_size equals to
 *   head_num * head_size.
 * @param[in,out]  caches_array
 *   Input/output. Pointer to the host memory that stores an array of pointers to the device memory.
 *   It must contain \p caches_num elements, where each pointer points
 *   to the data address of one cache tensor. All tensors must have the same shape and data type as
 *   \p single_cache_desc described.
 *   After each frame, they are rearranged in the beam_size dimension
 *   according to \p best_beams_cache.
 * @param[in]  caches_num
 *   Input. An integer indicating how many caches need to be rearranged. Usually it equals to
 *   decoder_layer_num * 2.
 * @param[in]  best_beams_cache_desc
 *   Input. Descriptor of \p best_beams_cache. Shape must be [batch_size, ngroup, beam_size], and
 *   data type must be INT8.
 * @param[out]  best_beams_cache
 *   Output. Pointer to the MLU memory that stores the \p best_beams_cache tensor, which holds
 *   the previous beam indicies of each group to concat groups in correct order.
 * @par Return
 * - CNNL_STATUS_SUCCESS, CNNL_STATUS_BAD_PARAM
 *
 * @par Reference
 * - https://github.com/tensorflow/models/blob/r1.13.0/official/transformer/model/beam_search.py
 *
 * @par Data Type
 * - best_beam and best_beams_cache: int8.
 * - nframe: int32.
 * - decoder_states: float, half.
 *
 * @par Scale Limitation
 * - The dimensions of tensors must meet following requirements:
 *   * 1 <= batch_size.
 *   * 1 <= beam_size <= 16.
 *   * 1 <= max_decode_length <= 10240.
 *   * 1 <= caches_num <= 1024.
 *   * 1 <= head_num <= 32.
 *   * 1 <= head_size <= 128.
 *   * 1 <= ngroup <= max_decode_length, and ngroup <= 60.
 *   * 1 <= \p nframe <= max_decode_length, since value of \p nframe is already
 *     incremented by beam search operation.
 *   * Size of decoder_states must be no larger than 2GB.
 *
 * @note
 * - In order to avoid large memory copy of decoder_states, beam rearrange does not really move
 * memory like above expression. Each layer of cache is divided into ngroup groups in dimension
 * of max_decode_length and each time only the current group is rearranged. Other groups' beam
 * position are recorded into \p best_beams_cache like a linked list. with \p best_beams_cache,
 * other operations like self_attn are able to restore the correct decoder_states.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlTransformerBeamRearrangeWithSplitedCache(cnnlHandle_t handle,
                             void *workspace,
                             const cnnlTensorDescriptor_t best_beam_desc,
                             const void *best_beam,
                             const cnnlTensorDescriptor_t nframe_desc,
                             const void *nframe,
                             const cnnlTensorDescriptor_t single_cache_desc,
                             void **caches_array,
                             int caches_num,
                             const cnnlTensorDescriptor_t best_beams_cache_desc,
                             void *best_beams_cache);

// Group:Transformer Beam Rearrange
/*!
 *  @brief Gets the extra space size needed in the
 *  transformer_beam_rearrange operation.
 *
 *  @param[in]  handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *    queues in the operation. For detailed information, see cnnlHandle_t.
 *  @param[in]  caches_num
 *    Input. An integer indicating how many caches need to be rearranged. Usually it equals to
 *    \p decoder_layer_num * 2.
 *  @param[out] size_workspace
 *    Output. Size of workspace.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One of the following conditions is met:
 *    - The value of \p size_workspace is NULL.
 *    - The value of \p caches_num is invalid.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetTransformerBeamRearrangeWorkspaceSize(cnnlHandle_t handle,
                                             int caches_num,
                                             size_t *size_workspace);

// Group:Transformer Beam Rearrange
/*!
 *  @brief Implements in-place beam rearrange of cached
 *  decoder key and value according to beam search result.
 *
 *  It also records history result of beam search to restore the correct order of decoder cache.
 *  For example, when the shape of each cache is [batch, beam, head_num, seq_len, head_size]:
    @verbatim
    1. Rearrange each cache according to best_beams
    for i in range(0, layer_num * 2):
      for batch in range(0, batch_size):
        for beam in range(0, beam_size):
          decoder_states[i, batch, beam, :, 0:nframe-2, :] = decoder_states[i, batch, best_beam[batch, beam], :, 0:nframe-2, :]

    2. Store best_beams to best_beams_cache
    best_beams_cache[:, nframe-1] = best_beam
    @endverbatim
 *
 *  @param[in]  handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *    queues in the operation. For detailed information, see cnnlHandle_t.
 *  @param[in]  best_beam_desc
 *    Input.  Descriptor of the \p best_beam. Shape must be [batch_size, beam_size], and data type
 *    must be INT8.
 *  @param[in]  best_beam
 *    Input.  Pointer to the MLU memory that stores the \p best_beam tensor.
 *    \p best_beam is provided by the beam search operation and stores the top beam_size indicies
 *    of last frame.
 *  @param[in]  nframe_desc
 *    Input.  Descriptor of \p nframe. Shape must be [1], and data type must be INT32.
 *  @param[in]  nframe
 *    Input.  Pointer to the MLU memory that stores \p nframe tensor. It is the number of the current frame.
 *  @param[in]  decoder_states_desc
 *    Input. Descriptor of \p decoder_states. Its data type must be float or half. Shape must be
 *    [layer_num * 2, batch_size, beam_size, head_num, max_decode_length, head_size] or
 *    [layer_num * 2, batch_size, beam_size, max_decode_length, hidden_size], where hidden_size equals to
 *    head_num * head_size.
 *  @param[in,out]  decoder_states
 *    Input/Output. Pointer to the MLU memory that stores the \p decoder_states tensor.
 *    It is combination of history buffers of key and value
 *    in all decoder self attention layers.
 *    After each frame, they are rearranged in the beam_size dimension
 *    according to \p best_beams_cache.
 *  @param[in]  best_beams_cache_desc
 *    Input. Descriptor of \p best_beams_cache. Shape must be [batch_size, ngroup, beam_size], and
 *    data type must be INT8.
 *  @param[out]  best_beams_cache
 *    Output. Pointer to the MLU memory that stores the \p best_beams_cache tensor, which holds
 *    the previous beam indicies of each group to concat groups in the correct order.
 *  @par Return
 *  - CNNL_STATUS_SUCCESS, CNNL_STATUS_BAD_PARAM
 *
 *  @par Reference
 *  - https://github.com/tensorflow/models/blob/r1.13.0/official/transformer/model/beam_search.py
 *
 *  @par Data Type
 *  - best_beam and best_beams_cache: int8.
 *  - nframe: int32.
 *  - decoder_states: float or half.
 *
 *  @par Scale Limitation
 *  - The dimensions of tensors must meet following requirements:
 *    - 1 <= batch_size.
 *    - 1 <= beam_size <= 16.
 *    - 1 <= max_decode_length <= 10240.
 *    - 1 <= head_num <= 32.
 *    - 1 <= head_size <= 128.
 *    - 1 <= ngroup <= max_decode_length, and ngroup <= 60.
 *    - 1 <= value of \p nframe <= max_decode_length, since value of \p nframe is already
 *      incremented by beam search operation.
 *
 *  - Total count of elements of each input and output should be less than or equal to INT32_MAX.
 *
 *  @note
 *  - In order to avoid large memory copy of decoder_state, beam rearrange does not really move
 *  memory like above expression. Each layer of cache is divided into ngroup groups in dimension
 *  of max_decode_length and each time only the current group is rearranged. Other groups' beam
 *  position are recorded into \p best_beams_cache like a linked list. With \p best_beams_cache
 *  you can restore the correct decoder_state.
 *
 *  @par Requirements
 *  - None.
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlTransformerBeamRearrange(cnnlHandle_t handle,
                             const cnnlTensorDescriptor_t best_beam_desc,
                             const void *best_beam,
                             const cnnlTensorDescriptor_t nframe_desc,
                             const void *nframe,
                             const cnnlTensorDescriptor_t decoder_states_desc,
                             void *decoder_states,
                             const cnnlTensorDescriptor_t best_beams_cache_desc,
                             void *best_beams_cache);

/******************************************************************************
 * Cambricon CNNL OP: Roialign
 ******************************************************************************/
/*! The descriptor of the ::cnnlRoialign operation.
 *
 *  You need to call the ::cnnlCreateRoialignDescriptor function to create a
 *  descriptor, and call the ::cnnlSetRoialignDescriptor function to set the
 *  information of the operation to the descriptor. Also, you need to destroy
 *  the Cambricon CNNL context at the end with the ::cnnlDestroyRoialignDescriptor function.
 */
typedef struct cnnlRoialignStruct *cnnlRoialignDescriptor_t;

// Group:Roialign
/*!
 *  @brief Applies roialign in feature pyramid network.
 *
 *    Given following input tensors:
 *    - input_rois: (batch_size)(num_rois, roi_offset)
 *    - inputs(level)(batch_size, level_h, level_w, channels)
 *
 *    This function performs with the following steps:
 *
 *    1. For multi-level feature maps, compute roi(roi_1, roi_2, roi_3, roi_4) iterate through num_rois rois,
 *       get which level current roi belongs to.
 *    level = level_map(canonical_level, canonical_scale, finest_scale, rois)
 *    spatial_scale = spatial_scales[level]
 *    input = inputs[level]
 *
 *    2. Get the roi coordinates.
 *    offset = aligned ? -0.5 : 0
 *    roi_coord.w_start = roi_x1 * spatial_scale + offset
 *    roi_coord.h_start = roi_y1 * spatial_scale + offset
 *    roi_coord.w_end = roi_x2 * spatial_scale + offset
 *    roi_coord.h_end = roi_y2 * spatial_scale + offset
 *
 *    3. Get the bins according to output feature map.
 *    bins_coord(pooled_height, pooled_width, 4) = bin(roi_coord, pooled_height, pooled_width)
 *    bin.x_min = roi_coord.x + (roi_coord.w / pooled_width) * pooled_width_index
 *    bin.y_min = roi_coord.y + (roi_coord.h / pooled_height) * pooled_height_index
 *
 *    4. Sample 4 points for every bin.
 *    sample_coord(pooled_height, pooled_width, 4) = sample(bins_coord, sampling_ratio)
 *    sample.x = bin.x_min + ((sample_width_index + 0.5) * (roi_coord.w / pooled_width) / sampling_ratio);
 *    sample.y = bin.y_min + ((sample_height_index + 0.5) * (roi_coord.h / pooled_height) / sampling_ratio);
 *
 *    5. Get every sample point data with bilinear interpolation.
 *    sample_value(pooled_height, pooled_width, 4, channels) = bilinear(sample_coord, input)
 *    sample.value = inter_1 * w1 + inter_2 * w2 + inter_3 * w3 + inter_4 * w4;
 *
 *    6. Get bin data with avgpool.
 *    bin_value(pooled_height, pooled_width, cahnnels) = avgpool(sample_value)
 *
 *    7. Get the following output tensor:
 *    output: (total_rois_num, pooled_height, pooled_width, cahnnels)
 *
 *  @param[in]  handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *           roialign operation.
 *  @param[in]  roialign_desc
 *    Input. The descriptor of roialign operation.
 *  @param[in]  input_desc
 *    Input. An array of tensor descriptors which holds dimension,
 *           data type and layout of different level feature map tensor. The shape of tensor must be
 *           [batch_size, level_h, level_w, channels].
 *  @param[in]  input
 *    Input. Pointer to the host memory that stores elements of a 1-D array pointing to different
 *           level feature map tensors on device. The element order of this input array must be
 *           consistent with element order of the cnnlTensorDescriptors array.
 *  @param[in]  input_rois_desc
 *    Input. An array of tensor descriptors which holds dimension,
 *           data type and layout of different batch rois information tensor. The shape of tensor varies
 *           based on different task and layout of tensor:
 *           - fpn roialign: The layout must be ARRAY and the shape of the tensor must be [roi_num, 4].
 *           - roialign: When the layout is NCHW, the shape of tensor must be [roi_num, param_num].
 *           When the layout is NHWC, the shape of tensor must be [param_num, roi_num].
 *           The number and order of roi varies based on different rois_type:
 *           - CNNL_ROI_CORNER:         The shape must be [roi_x_start, roi_y_start, roi_x_end, roi_y_end].
 *           - CNNL_ROI_CORNER_BATCHID: The shape must be [roi_x_start, roi_y_start, roi_x_end, roi_y_end, batch_id].
 *           - CNNL_ROI_BATCHID_CORNER: The shape must be [batch_id, roi_x_start, roi_y_start, roi_x_end, roi_y_end].
 *           - CNNL_ROI_CENTER:         The shape must be [roi_x_start, roi_y_start, roi_x_end, roi_y_end].
 *           - CNNL_ROI_CENTER_BATCHID: The shape must be [roi_x_start, roi_y_start, roi_x_end, roi_y_end, batch_id].
 *           - CNNL_ROI_BATCHID_CENTER: The shape must be [batch_id, roi_x_start, roi_y_start, roi_x_end, roi_y_end].
 *           - CNNL_ROI_CORNER_SCORE:   The shape must be [roi_x_start, roi_y_start, roi_x_end, roi_y_end, score].
 *           - CNNL_ROI_SCORE_CORNER:   The shape must be [score, roi_x_start, roi_y_start, roi_x_end, roi_y_end].
 *           - CNNL_ROI_CENTER_SCORE:   The shape must be [roi_x_start, roi_y_start, roi_x_end, roi_y_end, score].
 *           - CNNL_ROI_SCORE_CENTER:  The shape must be [score, roi_x_start, roi_y_start, roi_x_end, roi_y_end].
 *           Note that for fpn roialign, rois_type must be CNNL_ROI_CENTER. For roialign, rois_type must be
 *           CNNL_ROI_CORNER_SCORE or CNNL_ROI_SCORE_CORNER.
 *  @param[in]  input_rois
 *    Input. Pointer to the host memory that stores elements of a 1-D array pointing to different batch rois
 *           information tensors on device. The element order of this input array must be consistent
 *           with element order of the cnnlTensorDescriptors array.
 *  @param[in]  workspace
 *    Input. Pointer to data address of workspace.
 *  @param[in]  workspace_size
 *    Input. The size of workspace.
 *  @param[in]  output_desc
 *    Input. The descriptor of the output tensor that holds dimension,
 *           data type and layout information. The shape of tensor must be [total_rois_num,
 *           pooled_height, pooled_width, cahnnels].
 *  @param[out] output
 *    Output. Pointer to MLU memory that stores the output tensor.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are encountered:
 *    - \p handle is NULL.
 *    - Pointer is NULL.
 *    - Parameters do not meet scale limitation.
 *  @retval CNNL_STATUS_ARCH_MISMATCH
 *    This function is run on the hardware platform that is not supported.
 *
 *  @par Data Type
 *  - input: float, half.
 *  - input_rois: float, half.
 *  - output: float, half.
 *  - The data type of input, input_rois and output must be same.
 *
 *  @par Data Layout
 *  - input: NHWC.
 *  - input_rois: ARRAY (for fpn roialign), NHWC/NCHW (for roialign).
 *  - output: NHWC.
 *
 *  @par Scale Limitation
 *  - The \p num_level must be between 1 and 10.
 *  - The \p batch_size must be 1 for fpn roialign and must be between 1 and 1024
 *    for roialign and multi-level roialign.
 *  - The \p sampling_ratio must be 2 for fpn roialign.
 *  - The \p canonical_scale must be 224.
 *  - The \p canonical_level must be 4.
 *  - The \p finest_scale must be greater than 0 for multi-level roialign.
 *  - The channels must be between 1 and 2048, and must be divisible by 64 for
 *    multi-level roialign.
 *  - The \p pooled_height and \p pooled_width must be between 1 and 512.
 *  - The \p img_h and \p img_w must be between 1 and 4096.
 *  - The \p img_h(img_w) must be greater than any height(width) of input size.
 *  - The \p pool_mode must be CNNL_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING.
 *  - The \p rois_type must be CNNL_ROI_CENTER for fpn roialgin.
 *  - The \p rois_type must be CNNL_ROI_CORNER_SCORE and CNNL_ROI_SCORE_CORNER for roialign.
 *  - The \p rois_type must be CNNL_ROI_BATCHID_CORNER for multi-level roialign.
 *  - The \p compute_dtype must be CNNL_DTYPE_FLOAT or CNNL_DTYPE_HALF.
 *  - For multi-level roialign, the \p compute_dtype must be CNNL_DTYPE_FLOAT when
 *    the data type of input is float.
 *
 *  @par Performance Optimization
 *  - When the channels meet channels % 64 = 0, the performance is the best.
 *
 *  @par API Dependency
 *  - You need to call the ::cnnlCreateRoialignDescriptor() and ::cnnlGetRoialignWorkspaceSize()
 *   functions before calling this function.
 *  - You need to call the cnnlDestroyRoialignDescriptor() function after calling this function.
 *
 *  @note
 *  - The number of elements in every tensor must be smaller than 2^31.
 *
 *  @par Example
    @verbatim
     Dimension of every input: [batch_size, height, width, channels]

     Dimension of every input_rois: [num_rois, roi_offset]

     Then get the output:

     Dimension of output: [total_num_rois, pooled_height, pooled_width, channels]
    @endverbatim
 */
cnnlStatus_t CNNL_WIN_API
cnnlRoialign(cnnlHandle_t handle,
             const cnnlRoialignDescriptor_t roialign_desc,
             const cnnlTensorDescriptor_t input_desc[],
             const void *const input[],
             const cnnlTensorDescriptor_t input_rois_desc[],
             const void *const input_rois[],
             void *workspace,
             size_t workspace_size,
             cnnlTensorDescriptor_t output_desc,
             void *output);

// Group:Roialign
/*!
 *  @brief Creates a descriptor pointed by \p desc for the
 *  roialign operation and allocate memory for it.
 *
 *  @param[out]  desc
 *    Output. Descriptor of roialign operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_ALLOC_FAILED
 *    The function failed to allocate memory space.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCreateRoialignDescriptor(cnnlRoialignDescriptor_t *desc);

// Group:Roialign
/*!
 *  @brief Destroys the descriptor of roialign and free memory for it.
 *
 *  @param[in]  desc
 *    Input. Descriptor of roialign operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One of the following conditions are met:
 *    - The value of \p desc is NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlDestroyRoialignDescriptor(cnnlRoialignDescriptor_t desc);

// Group:Roialign
/*!
 *  @brief Assigns fpn roialign descriptor with parameters.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[in,out]  desc
 *    Input/output. Descriptor of roialign operation.
 *  @param[in]  num_level
 *    Input. The number of feature maps.
 *  @param[in]  pooled_height
 *    Input. The height of output feature map.
 *  @param[in]  pooled_width
 *    Input. The width of output feature map.
 *  @param[in]  sampling_ratio
 *    Input. The sample ratio of every bin.
 *  @param[in]  canonical_scale
 *    Input. The scale parameter to compute which feature does the current roi belongs to.
 *  @param[in]  canonical_level
 *    Input. The level parameter to compute which feature does the current roi belongs to.
 *  @param[in]  img_h
 *    Input. The height of input image.
 *  @param[in]  img_w
 *    Input. The width of input image.
 *  @param[in]  pool_mode
 *    Input. The mode of pooling. Currently only
 *           CNNL_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING is supported.
 *  @param[in]  rois_type
 *    Input. The mode of regions of interest.  Currently only
 *           CNNL_ROI_CORNER is supported.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    - The value of \p desc is NULL.
 *
 *  @par API Dependency
 *   - Before using this function, you need to call ::cnnlCreateRoialignDescriptor() to create a descriptor.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlSetRoialignDescriptor(cnnlRoialignDescriptor_t desc,
                          const int num_level,
                          const int pooled_height,
                          const int pooled_width,
                          const int sampling_ratio,
                          const int canonical_scale,
                          const int canonical_level,
                          const int img_h,
                          const int img_w,
                          const cnnlPoolingMode_t pool_mode,
                          const cnnlRoiLayoutType_t rois_type);

// Group:Roialign
/*!
 *  @brief Assigns roialign descriptor with parameters.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[in,out]  desc
 *    Input/output. Descriptor of roialign operation.
 *  @param[in]  sampling_ratio
 *    Input. The sample ratio of every bin.
 *  @param[in]  spatial_scale
 *    Input. The spatial scale for roi.
 *  @param[in]  pool_mode
 *    Input. The mode of pooling. Currently only
 *    CNNL_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING is supported.
 *  @param[in]  rois_type
 *    Input. The mode of regions of interest. Currently only
 *    CNNL_ROI_CORNER_SCORE or CNNL_ROI_SCORE_CORNER is supported.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    - The value of \p desc is NULL.
 *
 *  @par API Dependency**
 *  - Before calling this function, you need to call
 *   ::cnnlCreateRoialignDescriptor() to create a descriptor.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlSetRoialignDescriptorV2(cnnlRoialignDescriptor_t desc,
                            const int sampling_ratio,
                            const float spatial_scale,
                            const cnnlPoolingMode_t pool_mode,
                            const cnnlRoiLayoutType_t rois_type);

// Group:Roialign
/*!
 *  @brief Assigns multi-level roialign descriptor with parameters.
 *
 *  @param[in]  desc
 *    Input. Descriptor of roialign operation.
 *  @param[in]  num_level
 *    Input. The number of feature maps. It is must be between 1 and 10.
 *  @param[in]  sampling_ratio
 *    Input. The sample ratio of every bin.
 *  @param[in]  finest_scale
 *    Input. The scale parameter to compute which feature does the current roi
 *    belong to. It must be greater than 0.
 *  @param[in]  roi_scale_factor
 *    Input. The scaling factor of the roi on the feature map. It must be greater
 *    than or equal to 0.
 *  @param[in]  aligned
 *    Input. A Boolean value indicating whether to subtract 0.5 when calculating
 *    the roi coordinates.
 *  @param[in]  pool_mode
 *    Input. The mode of pooling. Currently only
 *    CNNL_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING is supported.
 *  @param[in]  rois_type
 *    Input. The mode selection for regions of interest. Currently only
 *    CNNL_ROI_BATCHID_CORNER is supported.
 *  @param[in]  compute_dtype
 *    Input. The data type used in computing bilinear interpolation.
 *  @param[in]  featmap_strides
 *    Input. An array of stride parameters to calculate spatial scale for every
 *    level feature map.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    The value of \p desc is NULL.
 *
 *  @par API Dependency
 *  - Before calling this function, you need to use ::cnnlCreateRoialignDescriptor()
 *  to create the descriptor.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetMultiLevelRoialignDescriptor(cnnlRoialignDescriptor_t desc,
                                    const int num_level,
                                    const int sampling_ratio,
                                    const int finest_scale,
                                    const float roi_scale_factor,
                                    const bool aligned,
                                    const cnnlPoolingMode_t pool_mode,
                                    const cnnlRoiLayoutType_t rois_type,
                                    const cnnlDataType_t compute_dtype,
                                    float featmap_strides[]);

// Group:Roialign
/*!
 *  @brief Gets the extra space size needed in roialign operation.
 *
 *  @param[in]  handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *           roialign operation.
 *  @param[in]  input_desc
 *    Input. An array of tensor descriptors which holds dimension,
 *           data type and layout of different level feature map tensor. The shape of tensor must be
 *           [batch_size, level_h, level_w, channels].
 *  @param[in]  output_desc
 *    Input. The descriptor of the output tensor that holds dimension,
 *           data type and layout information. The shape of tensor must be [total_rois_num,
 *           pooled_height, pooled_width, cahnnels].
 *  @param[out] size
 *    Output. Pointer to the size of workspace.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    The value of \p input_desc or \p output_desc is NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetRoialignWorkspaceSize(cnnlHandle_t handle,
                             const cnnlTensorDescriptor_t input_desc[],
                             const cnnlTensorDescriptor_t output_desc,
                             size_t *size);

/******************************************************************************
* Cambricon CNNL OP: TransformerFcTopk
******************************************************************************/
/*! @brief Enumeration variables describing the implementation modes
 * of the ::cnnlTransformerFcTopk operation.
 */
typedef enum {
  CNNL_TRANSFORMER_BEAM_TOPK_SOFTMAX = 0,
  /*!< Performs softmax before performing beam topk.*/

  CNNL_TRANSFORMER_SOFTMAX_BEAM_TOPK = 1,
  /*!< Performs softmax after performing beam topk.*/

  CNNL_TRANSFORMER_BATCH_TOPK_SOFTMAX = 2,
  /*!< Performs softmax before performing batch topk.*/

  CNNL_TRANSFORMER_SOFTMAX_BATCH_TOPK = 3,
  /*!< Performs softmax after performing batch topk.*/
} cnnlTransformerFcTopkMode_t;

/*!
 * @brief Enumeration variables describing the softmax mode.
 *
 * It is deprecated and will be removed in future release.
 */
typedef enum {
  CNNL_TRANSFORMER_SOFTMAX = 0,
  /*!< Performs softmax operation.*/

  CNNL_TRANSFORMER_LOG_SOFTMAX = 1,
  /*!< Performs log softmax operation. */
} cnnlTransformerSoftmaxMode_t;


/*!
 *  @brief The descriptor of the ::cnnlTransformerFcTopk operation.
 *  You need to call the ::cnnlCreateTransformerFcTopkDescriptor function
 *  to create a descriptor, and call the ::cnnlSetTransformerFcTopkDescriptor_v2
 *  function to set the tensor information to the descriptor.
 *  Also, you need to destroy the descriptor at the end with the
 *  ::cnnlDestroyTransformerFcTopkDescriptor function.
 *
 *  It is deprecated and will be removed in future release.
 */
typedef struct cnnlTransformerFcTopkStruct *cnnlTransformerFcTopkDescriptor_t;

// Group:Transformer Fc Topk
/* transformer fc-topk of encoder operation start */
/*!
 *  @brief Performs the fc-topk layer in transformer decoder network.
 *  It realizes fully connected and topk function.
 *
 *  This function performs with the following steps:
 *
 *  1. Generate fc_out
 *
 *    Compute fc_out from input tensor.
 *
 *    fc_out = mlp(input_tensor, attr_kernel)
 *
 *  2. Generate topk
 *
 *    Compute topk from fc_out.
 *
 *    output_value = topk(fc_out)
 *
 *    output_index = topk_index(fc_out)
 *
 *  3. Softmax for output_value
 *
 *    output_value = softmax(output_value)
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[in]  handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *           transformer_fc_topk operation.
 *  @param[in]  desc
 *    Input. The descriptor of the transformer_fc_topk operation.
 *  @param[in]  input_desc
 *    Input. Descriptor of input, containing data type, dimensions, layout and
 *    quantization information.
 *  @param[in]  input
 *    Input. Pointer to the MLU memory that stores the input tensor.
 *  @param[in]  filter_desc
 *    Input. Descriptor of filter, containing data type, dimensions, layout
 *    and quantization information.
 *  @param[in]  filter
 *    Input. Pointer to the MLU memory that stores the filter tensor.
 *  @param[in]  bias_desc
 *    Input. Descriptor of bias, containing data type, dimensions and layout.
 *  @param[in]  bias
 *    Input. Pointer to the MLU memory that stores the bias tensor.
 *  @param[in]  accumulated_logits_desc
 *    Input. Descriptor of accumulated_logits tensor, containing dimension
 *    of [batch_size, beam_size].
 *  @param[in]  accumulated_logits
 *    Input. Pointer to the MLU memory that stores the accumulated_logits tensor.
 *    It is used for point accumulation.
 *  @param[in]  desc_norm_scale
 *    Input. Descriptor of norm_scale tensor, containing dimension of [hidden_size].
 *  @param[in]  norm_scale
 *    Input. Pointer to the MLU memory that stores the nrom_scale tensor. It is the scale factor
 *    used in layernorm.
 *  @param[in]  desc_norm_bias
 *    Input. Descriptor of norm_bias tensor, containing dimension of [hidden_size].
 *  @param[in]  norm_bias
 *    Input. Pointer to the MLU memory that stores the nrom_bias tensor. It is bias parameter
 *    used in layernorm.
 *  @param[in]  workspace
 *    Input. The pointer to data address of workspace, which is extra alloced by users
 *    for storing fully connected data.
 *  @param[in]  workspace_size
 *    Input. The size value of workspace (bytes).
 *  @param[in] output_value_desc
 *    Input. Descriptor of output_value, containing data type, dimensions and layout.
 *  @param[out] output_value
 *    Input. Pointer to the MLU memory that stores the output tensor.
 *  @param[in] output_index_desc
 *    Input. Descriptor of output_index, containing data type, dimensions and layout.
 *  @param[out] output_index
 *    Input. Pointer to the MLU memory that stores the index of the output.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are encountered:
 *    - \p handle is NULL.
 *    - pointer is NULL.
 *    - parameters do not meet scale limitation.
 *  @retval CNNL_STATUS_ARCH_MISMATCH
 *    One or more of the following conditions are encountered:
 *    - This function is run on the hardware platform that is not supported.
 *
 *  @par Data Type
 *  - On MLU300 series:
 *    - input: half, float.
 *    - filter: half, float.
 *    - output: float(topk_value), float(topk_index).
 *
 *  @par Data Layout
 *  - input: CNNL_LAYOUT_ARRAY.
 *  - filter: CNNL_LAYOUT_ARRAY.
 *  - bias: CNNL_LAYOUT_ARRAY.
 *  - layernorm: CNNL_LAYOUT_ARRAY.
 *  - output: CNNL_LAYOUT_ARRAY.
 *
 *  @par Scale Limitation
 *  - On MLU300 and MLU500 series, when the data type of filter is half and fctopk mode
 *    is ::CNNL_TRANSFORMER_SOFTMAX_BATCH_TOPK:
 *    - batch_size must be greater than 0, beam_size must be in range of [1, 16],
 *      k must be in range of [2, 16], hidden_size must be in range of [128, 2048],
 *      hidden_size % 64 = 0 and dict_len must be in range of [256, 90000].
 *  - On MLU300 series, when the data type of filter is half and fctopk mode is
 *    ::CNNL_TRANSFORMER_BEAM_TOPK_SOFTMAX:
 *    - batch_size must be in range of [1, 64], beam_size must be in range of [1, 12],
 *      k must be in range of [2, 12], hidden_size must be 512, 768, or 1024, and
 *      dict_len must be in range of [256, 90000].
 *  - On MLU300 and MLU500 series, when the data type of filter is float and fctopk mode is
 *    ::CNNL_TRANSFORMER_SOFTMAX_BATCH_TOPK:
 *    - batch_size must be greater than 0, beam_size must be in range of [1, 16],
 *      k must be in range of [2, 16], hidden_size must be in range of [128, 2048],
 *      hidden_size % 64 = 0 and dict_len must be in range of [256, 90000].
 *  - On MLU300 series, when the data type of filter is float and fctopk mode is
 *    ::CNNL_TRANSFORMER_BEAM_TOPK_SOFTMAX:
 *    - batch_size * beam_size must be in range of [1, 32],
 *      k must be in range of [2, 12], hidden_size must be less than 1536,
 *      hidden_size % 128 = 0, and dict_len must be in range of [64, 18000].
 *
 *  @par Performance Optimization
 *  - When batch_size * beam_size is less than 64, the performance is the best.
 *
 *  @par API Dependency
 *  - You need to call the ::cnnlCreateTransformerFcTopkDescriptor() to
 *    create a descriptor and call ::cnnlSetTransformerFcTopkDescriptor_v2
 *    to set its parameters before calling this function.
 *  - You need to call the ::cnnlDestroyTransformerFcTopkDescriptor
 *    function after calling this function.
 *  @note
 *    - Data type of input, layernorm_bias must be the same.
 *    - Data type of fc_type, bias and accumulated_logits must be the same.
 *    - Input and output cannot be homologous operand.
 *    - The content of input is not modified by fc-topk layer.
 *    - On MLU300 series, the type of input must be the same as the type of filter.
 *  @par Reference
 *  - http://github.com/master/FasterTransformer
 *
 *  @par Example
    @verbatim
     input: a tensor of [batch_size, beam_size, hidden_size]

     filter: a tensor of [dict_len, hidden_size]

     layernorm_scale: a tensor of [hidden_size]

     layernorm_bias: a tensor of [hidden_size]

     Then we will get the output:

     output_value: a tensor of [batch_size, beam_size, k]

     output_index: a tensor of [batch_size, beam_size, k]
     @endverbatim
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlTransformerFcTopk(cnnlHandle_t handle,
                                     const cnnlTransformerFcTopkDescriptor_t desc,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const void *input,
                                     const cnnlTensorDescriptor_t filter_desc,
                                     const void *filter,
                                     const cnnlTensorDescriptor_t bias_desc,
                                     const void *bias,
                                     const cnnlTensorDescriptor_t accumulated_logits_desc,
                                     const void *accumulated_logits,
                                     const cnnlTensorDescriptor_t desc_norm_scale,
                                     const void *norm_scale,
                                     const cnnlTensorDescriptor_t desc_norm_bias,
                                     const void *norm_bias,
                                     void *workspace,
                                     size_t workspace_size,
                                     const cnnlTensorDescriptor_t output_value_desc,
                                     void *output_value,
                                     const cnnlTensorDescriptor_t output_index_desc,
                                     void *output_index);

// Group:Transformer Fc Topk
/*!
 *  @brief Creates a descriptor pointed by \p desc
 *  for the TransformerFcTopk operation and allocate memory for it.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[in,out]  desc
 *    Input/output. Descriptor of the TransformerFcTopk operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_ALLOC_FAILED
 *    The function failed to allocate memory space.
 *
 *  @par API Dependency
 *  - After calling this function, you can call the ::cnnlSetTransformerFcTopkDescriptor_v2
 *  function to initialize and set the information to the descriptor.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlCreateTransformerFcTopkDescriptor(cnnlTransformerFcTopkDescriptor_t *desc);

// Group:Transformer Fc Topk
/*!
 *  @brief Destroys the descriptor of TransformerFcTopk
 *  and free memory for it.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[in]  desc
 *    Input. Descriptor of the TransformerFcTopk operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One of the following conditions are met:
 *    - The value of \p desc is NULL.
 *
 *  @par API Dependency
 *  - You need to call this function after calling the ::cnnlTransformerFcTopk function.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlDestroyTransformerFcTopkDescriptor(cnnlTransformerFcTopkDescriptor_t desc);

// Group:Transformer Fc Topk
/*!
 *  @brief Assigns TransformerFcTopk descriptor with parameters.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[out] desc
 *    Output. Descriptor of TransformerFcTopk operation.
 *  @param[in]  layernorm
 *    Input. A Boolean value indicating whether to enable normalization of input.
 *  @param[in]  fc_type
 *    Input. The data type of fully connected output (half, float).
 *  @param[in]  fctopk_mode
 *    Input. The mode of the operation. See ::cnnlTransformerFcTopkMode_t for details.
 *  @param[in]  softmax_mode
 *    Input. The softmax mode. See ::cnnlTransformerSoftmaxMode_t for details.
 *  @retval CNNL_STATUS_SUCCESS
 *    The object was set successfully.
 *  @par API Dependency
 *  - Before calling this function, you need to call
 *  ::cnnlCreateTransformerFcTopkDescriptor() to create a descriptor.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlSetTransformerFcTopkDescriptor_v2(
        const cnnlTransformerFcTopkDescriptor_t desc,
        const bool layernorm,
        cnnlDataType_t fc_type,
        cnnlTransformerFcTopkMode_t fctopk_mode,
        cnnlTransformerSoftmaxMode_t softmax_mode);

// Group:Transformer Fc Topk
/*!
 *  @brief Gets extra space size needed in the TransformerFcTopk operation.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @par API Dependency
 *  - Before calling this function, you need to call ::cnnlCreateTransformerFcTopkDescriptor()
 *   to create a descriptor and call ::cnnlSetTransformerFcTopkDescriptor_v2() to set its parameters.
 *
 *  @param[in]  desc
 *    Input.  The descriptor of the TransformerFcTopk operation.
 *  @param[in]  input_desc
 *    Input. Descriptor of input, containing dimension and the layout of input.
 *  @param[in]  filter_desc
 *    Input. Descriptor of filter.
 *  @param[out] size
 *    Output. Size of workspace.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    - The value of \p desc or \p size is NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetTransformerFcTopkWorkspaceSize(const cnnlTransformerFcTopkDescriptor_t desc,
                                      const cnnlTensorDescriptor_t input_desc,
                                      const cnnlTensorDescriptor_t filter_desc,
                                      size_t *size);

/******************************************************************************
 * Cambricon CNNL OP: TransformerBeamSearch
 ******************************************************************************/

/*! The descriptor of the ::cnnlTransformerBeamSearch operation.
 * You can use ::cnnlCreateTransformerBeamSearchDescriptor(),::cnnlSetTransformerBeamSearchDescriptor_v2()
 * and ::cnnlDestroyTransformerBeamSearchDescriptor() to create, set and destroy the descriptor.
 */
typedef struct cnnlTransformerBeamSearchStruct *cnnlTransformerBeamSearchDescriptor_t;

// Group:Transformer Beam Search
/*!
 *  @brief Creates a descriptor of TransformerBeamSearch and allocates host memory for it.
 *
 *  @param[out]  desc
 *    Output. Description of TransformerBeamSearch operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_ALLOC_FAILED
 *    The function failed to allocate memory.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCreateTransformerBeamSearchDescriptor(cnnlTransformerBeamSearchDescriptor_t *desc);

// Group:Transformer Beam Search
/*!
 *  @brief Destroys a descriptor of TransformerBeamSearch and frees memory for it.
 *
 *  @param[in]  desc
 *    Input. Description of TransformerBeamSearch operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p desc is NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlDestroyTransformerBeamSearchDescriptor(cnnlTransformerBeamSearchDescriptor_t desc);

// Group:Transformer Beam Search
/*!
 *  @brief Sets a TransformerBeamSearch descriptor with values.
 *
 *  @param[in,out]  desc
 *    Input/output. Description of TransformerBeamSearch operation.
 *  @param[in]  k
 *    Input. A factor of FC-Topk operation. For each batch, FC-Topk outputs \p k * beam
 *    possible tokens. Currently only supports 2.
 *  @param[in]  alpha
 *    Input. The constant used to calculate length_normalization.
 *  @param[in]  eos_index
 *    Input. Index of EOS token in the embedding vocabulary.
 *  @param[in]  vocab_length
 *    Input. Length of embedding vocabulary.
 *  @param[in]  fctopk_mode
 *    Input. Mode of FcTopk operation for beam search to choose compatible algorithm.
 *  @param[in]  softmax_mode
 *    Input. Softmax mode of FcTopk operation for beam search to choose compatible algorithm.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    The values of one or more parameters are invalid.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetTransformerBeamSearchDescriptor_v2(cnnlTransformerBeamSearchDescriptor_t desc,
                                         int k,
                                         float alpha,
                                         int eos_index,
                                         int vocab_length,
                                         cnnlTransformerFcTopkMode_t fctopk_mode,
                                         cnnlTransformerSoftmaxMode_t softmax_mode);

// Group:Transformer Beam Search
/*!
 *  @brief Performs beam search logic based on greedy algorithm.
 *
 *  @param[in]  handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *           operation.
 *  @param[in]  op_desc
 *    Input. Description of TransformerBeamSearch operation.
 *  @param[in]  topk_indices_desc
 *    Input. Descriptor of \p topk_indices_seq. Shape must be [batch, k * beam] or [batch, beam, k].
 *           Data type must be float.
 *  @param[in]  topk_indices_seq
 *    Input. Device pointer points to topk_indices_seq tensor space, holding indices of the
 *           top-(k*beam) possible tokens.
 *  @param[in]  topk_log_probs_desc
 *    Input. Descriptor of \p topk_log_probs_seq. Shape must be [batch, k * beam] or
 *           [batch, beam, k]. Data type must be float.
 *  @param[in]  topk_log_probs_seq
 *    Input. Device pointer points to topk_log_probs_seq tensor space, holding probabilities of the
 *           top-(k*beam) possible tokens.
 *  @param[in]  nframe_desc
 *    Input. Descriptor of \p nframe. Shape must be [1]. Data type must be int32.
 *  @param[in,out]  nframe
 *    Input and Output. Device pointer points to nframe tensor space, holding index of current frame.
 *           Its value will be incremented by 1 during beam search.
 *  @param[in]  state_alive_seq_desc
 *    Input. Descriptor of \p state_alive_seq. Shape must be [batch, beam, max_decode_len + 1].
 *           Data type must be int32.
 *  @param[in,out]  state_alive_seq
 *    Input and Output. Device pointer points to state_alive_seq tensor space,
 *           holding possible token sequences.
 *  @param[in]  state_alive_log_probs_desc
 *    Input. Descriptor of \p state_alive_log_probs. Shape must be [batch, beam].
 *           Data type must be float.
 *  @param[in,out]  state_alive_log_probs
 *    Input and Output. Device pointer points to state_alive_log_probs tensor space,
 *           holding probabilities of corresponding sequences of \p state_alive_seq.
 *  @param[in]  state_finished_seq_desc
 *    Input. Descriptor of \p state_finished_seq. Shape must be [batch, beam, max_decode_len + 1].
 *           Data type must be int32.
 *  @param[in,out]  state_finished_seq
 *    Input and Output. Device pointer points to state_finished_seq tensor space,
 *           holding possible token sequences.
 *  @param[in]  state_finished_scores_desc
 *    Input. Descriptor of \p state_finished_scores. Shape must be [batch, beam].
 *           Data type must be float.
 *  @param[in,out]  state_finished_scores
 *    Input and Output. Device pointer points to state_finished_scores tensor space,
 *           holding probabilities of corresponding sequences of \p state_finished_seq.
 *  @param[in]  state_finished_flags_desc
 *    Input. Descriptor of \p state_finished_flags. Shape must be [batch, beam].
 *           Data type must be bool.
 *  @param[in,out]  state_finished_flags
 *    Input and Output. Device pointer points to state_finished_flags tensor space.
 *           Each element determines whether corresponding sequence of \p state_finished_seq has
 *           finished.
 *  @param[in]  output_topk_indicies_alive_desc
 *    Input. Descriptor of \p state_finished_flags. Shape must be [batch, beam].
 *           Data type must be int8.
 *  @param[out]  output_topk_indicies_alive
 *    Output. Device pointer points to state_finished_flags tensor space.
 *           Each element determines which beam is the corresponding sequence of
 *           \p state_finished_seq forked from. It will be used in beam rearrange operation.
 *  @param[in]  latest_token_alive_desc
 *    Input. Descriptor of \p latest_token_alive. Shape must be [batch, beam].
 *           Data type must be int32. This parameter is optional.
 *  @param[out]  latest_token_alive
 *    Output. Device pointer points to latest_token_alive tensor space.
 *           These tokens will be embedded in the next decoding frame. This parameter is optional.
 *  @param[in]  continue_search_desc
 *    Input. Descriptor of \p continue_search. Shape must be [batch].
 *           Data type must be bool. This parameter is optional.
 *  @param[out]  continue_search
 *    Output. Device pointer points to continue_search tensor space.
 *           Each element determines whether this batch should continue decoding.
 *           This parameter is optional.
 *  @par Return
 *  - CNNL_STATUS_SUCCESS, CNNL_STATUS_BAD_PARAM
 *
 *  @par Data Type
 *  - \p state_alive_seq, \p state_finished_seq, \p nframe, \p latest_token_alive: INT32.
 *  - \p topk_indices_seq, \p topk_log_probs_seq: FLOAT.
 *  - \p state_alive_log_probs, \p state_finished_scores: FLOAT or HALF.
 *  - \p state_finished_flags, \p continue_search: BOOL.
 *  - \p output_topk_indicies_alive: INT8.
 *
 *  @par Scale Limitation
 *  - The dimensions of tensors must meet following requirements:
 *    - 1 <= batch_size.
 *    - 1 <= beam_size <= 16.
 *    - 1 <= max_decode_len <= 1024.
 *    - k == 2
 *    - Shapes of \p topk_indices_seq and \p topk_log_probs_seq must be the same.
 *    - In batch-topk mode, values of \p topk_indices_seq must be in [0, beam_size * vocab_length).
 *    - In beam-topk mode, values of \p topk_indices_seq must be in [0, vocab_length).
 *
 *
 *  @par Reference
 *  - https://github.com/tensorflow/models/blob/r1.13.0/official/transformer/model/beam_search.py
 *
 *  @par Note
 *  - This operation supports batch-topk mode and beam-topk mode.
 *    If \p op_desc is set by cnnlSetTransformerBeamSearchDescriptor_v2 function, the topk mode is
 *    determined by the \p fctopk_mode parameter. Otherwise, it is determined by the dimensions of
 *    \p topk_indices_seq and \p topk_log_probs_seq.
 *    - If the dimension of \p topk_indices_seq and \p topk_log_probs_seq is 2, the batch-topk mode is used.
 *    - If the dimension of \p topk_indices_seq and \p topk_log_probs_seq is 3, the beam-topk mode is used.
 *  - Batch-topk mode or beam-topk mode must be consistent with topk mode of the FcTopk operation.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlTransformerBeamSearch(cnnlHandle_t handle,
                          cnnlTransformerBeamSearchDescriptor_t op_desc,
                          cnnlTensorDescriptor_t topk_indices_desc,
                          const void *topk_indices_seq,
                          cnnlTensorDescriptor_t topk_log_probs_desc,
                          const void *topk_log_probs_seq,
                          cnnlTensorDescriptor_t nframe_desc,
                          void *nframe,
                          cnnlTensorDescriptor_t state_alive_seq_desc,
                          void *state_alive_seq,
                          cnnlTensorDescriptor_t state_alive_log_probs_desc,
                          void *state_alive_log_probs,
                          cnnlTensorDescriptor_t state_finished_seq_desc,
                          void *state_finished_seq,
                          cnnlTensorDescriptor_t state_finished_scores_desc,
                          void *state_finished_scores,
                          cnnlTensorDescriptor_t state_finished_flags_desc,
                          void *state_finished_flags,
                          cnnlTensorDescriptor_t output_topk_indicies_alive_desc,
                          void *output_topk_indicies_alive,
                          cnnlTensorDescriptor_t latest_token_alive_desc,
                          void *latest_token_alive,
                          cnnlTensorDescriptor_t continue_search_desc,
                          void *continue_search);

/******************************************************************************
 * Cambricon CNNL OP: Transformer Embedding
 ******************************************************************************/
/*!
 * @brief Enumeration variables describing whether to add embedding for token_ids.
 *
 * It is deprecated and will be removed in future release.
 */
typedef enum {
    CNNL_EMBEDDING_TOKEN_UNUSED = 0,   /*!<The embedding for token_ids is not added.*/
    CNNL_EMBEDDING_TOKEN_USED = 1,     /*!<The embedding for token_ids is added.*/
} cnnlTransformerEmbeddingTokenMode_t;

/*!
 * @brief Enumeration variables describing whether to apply the position encoding dictionary
 * after the embedding in the ::cnnlTransformerEmbedding operation.
 *
 * It is deprecated and will be removed in future release.
 */
typedef enum {
    CNNL_EMBEDDING_ENCODING_UNUSED = 0,
    /*!< No position encoding is applied.*/
    CNNL_EMBEDDING_POSITION_CACHED = 1,
    /*!< The row of the position encoding dictionary of the current token is applied. */
    CNNL_EMBEDDING_POSITION_UNCACHED = 2,
    /*!< All rows of the position encoding dictionary are applied. */
} cnnlTransformerEmbeddingEncodingMode_t;

// Group:Transformer Embedding
/*!
 * @brief Adds the results of embedding and position encoding to \p output.
 *
 * This function performs with the following steps:
 *
 * mask = indices != 0
 *
 * output = filter[indices, :]
 *
 * If CNNL_EMBEDDING_ENCODING_UNUSED is set: output *= mask
 *
 * output *= alpha
 *
 * if CNNL_EMBEDDING_POSITION_UNCACHED is set: output += position
 *
 * else: output = output;
 *
 * @deprecated
 *   This function is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   position embedding operation.
 * @param[in] filter_desc
 *   Input. The descriptor of the filter tensor to store the information of vocab dictionary.
 * @param[in] filter
 *   Input. A Pointer to the MLU memory that stores the filter tensor. Shape should be
 *   [vocab_size, embedding_size].
 * @param[in] indices_desc
 *   Input. The descriptor of the index tensor used to store the index of convolution filter which
 *   corresponds to each row of \p output. The number of indices dimensions should be 2.
 *   The most common shape is [batch_size, seq_len].
 * @param[in] indices
 *   Input. A pointer to the MLU memory that stores the index of convolution filter.
 * @param[in] position_desc
 *   Input. The descriptor of the position encoding tensor.
 * @param[in] position
 *   Input. A Pointer to the MLU memory that stores the information of position encoding.
 *   Shape should be [position_size, embedding_size].
 * @param[in] length_desc
 *   Input. The descriptor of the seq_len to express offset and acquire the information of
 *   position encoding in cached mode.
 * @param[in] length
 *   Input. A Pointer to the MLU memory that stores the offset. Shape should be 1.
 * @param[in] token_dic_desc
 *   Input. The descriptor of the token_dic tensor to store the information of token dictionary.
 * @param[in] token_dic
 *   Input. A Pointer to the MLU memory that stores the token tensor. Shape should be
 *   [token_dic_size, embedding_size].
 * @param[in] token_ids_desc
 *   Input. The descriptor of the index tensor used to store the index of \p token_dic. The number
 *   of indices dimensions should be 2. The most common shape is [batch_size, seq_len].
 * @param[in] token_ids
 *   Input. A pointer to the MLU memory that stores the index of \p token_dic. Must be
 *   specified if  \p use_token_type is set to 1.
 * @param[in] alpha
 *   Input. A const float scalar used in embedding, usually set to the sqrt of embedding_size.
 * @param[in] use_token_type
 *   Input. A const enum type. Indicates whether to use token embedding. Set to 0 in order
 *   to reserve the interface.
 * @param[in] use_position_encoding
 *   Input. A const enum type. Indicates whether to use position encoding after embedding.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor to achieve embedding and position encoding.
 *   The number of output dimensions should be 3. The most common shape is
 *   [batch_size, seq_len, embedding_size]. In the CNNL_EMBEDDING_POSITION_CACHED mode
 *   the seq_len must be equal to 1.
 * @param[out] output
 *   Output. A pointer to the MLU memory that stores the output tensor.
 * @retval CNNL_STATUS_SUCCESS
 *   The function ends normally.
 * @retval CNNL_STATUS_ARCH_MISMATCH
 *   One or more of the following conditions are encountered:
 *   - This function is run on the hardware platform that is not supported.
 * @retval CNNL_STATUS_BAD_PARAM
 *   The values of one or more parameters are invalid.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for index tensor
 *   \p indices, filter tensor \p filter, position tensor \p position, length tensor \p length,
 *   token_dic tensor \p token_dic, token_ids tensor \p token_ids, output tensor \p output
 *   and \p alpha.
 *   <b>Note that the data type of filter tensor, position tensor, token_dic tensor and output
 *   tensor must be the same.</b>
 *   - filter tensor: half, float.
 *   - index tensor: int32.
 *   - position tensor: half, float.
 *   - token_dic tensor: half float.
 *   - token_ids tensor: int32.
 *   - alpha: float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the filter tensor, index tensor, position tensor,
 *   token_dic tensor, token_ids and output tensor are as follows:
 *   <b>Note that the data layout of filter tensor, index tensor, position tensor
 *   token_dic tensor, token_ids and output tensor must be same.</b>
 *   - filter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - index tensor: \p CNNL_LAYOUT_ARRAY.
 *   - position tensor: \p CNNL_LAYOUT_ARRAY.
 *   - token_dic tensor: \p CNNL_LAYOUT_ARRAY.
 *   - token_ids tensor: \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The filter tensor, index tensor, position tensor, length tensor and output tensor must meet
 *   the following requirements:
 *   - filter tensor: The number of filter dimensions must be 2.
 *     The shape of filter tensor is [vocab_size, embedding_size].
 *     When use_position_encoding is not CNNL_EMBEDDING_ENCODING_UNUSED:
 *     If the data type of filter tensor is float, the embedding_size is a multiple of 32.
 *     If the data type of filter tensor is half, the embedding_size is a multiple of 64.
 *   - indices tensor: The number of indices dimensions must be 2.
 *     The shape of indices tensor is [batch_size, seq_len].
 *   - 1 <= batch_size <= 4096.
 *   - 1 <= seq_len <= 4096.
 *   - position tensor: It can be set to a nullptr if use_position_encoding is
 *     CNNL_EMBEDDING_ENCODING_UNUSED. The number of position dimensions must be 2 if
 *     use_position_encoding is CNNL_EMBEDDING_ENCODING_USED and the total element
 *     numbers of position tensor must be less than 8,388,608.
 *   - token_dic tensor: It can be set to a nullptr if use_token_type is
 *     CNNL_EMBEDDING_TOKEN_UNUSED. The number of token_dic dimensions must be 2 if
 *     use_token_type is CNNL_EMBEDDING_TOKEN_USED.
 *   - token_ids tensor: It can be set to a nullptr if use_token_type is
 *     CNNL_EMBEDDING_TOKEN_UNUSED. The number of token_ids dimensions must be 2 if
 *     use_token_type is CNNL_EMBEDDING_TOKEN_USED.
 *   - length tensor: It can be set to a nullptr if use_position_encoding is not
 *     CNNL_EMBEDDING_POSITION_CACHED.
 *   - embedding_size: If use_position_encoding is CNNL_EMBEDDING_ENCODING_UNUSED,
 *     embedding_size <= 2048.
 *   - output tensor: The dimension must be indices dimension plus 1.
 *     The total element numbers of output tensor must be less than (2^31 -1),
 *     that is,
 *   - 1 <= batch_size * seq_len * embedding_size * sizeof(data type) <= (2^31 -1).
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Reference
 * - https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
 *
 * @par Example
 * - The example of the embedding operation is as follows:
     @verbatim
     input five arrays by 3 * 3, 2 * 2, 2 * 3, 3 * 3 and 2 * 2, alpha equals to 1.0,
     use_token_type equals to CNNL_EMBEDDING_TOKEN_UNUSED, use_position_encoding equals to
     CNNL_EMBEDDING_POSITION_UNCACHED.

     --> filter: [[0.3472, -0.1983, 0.8744], [0.5356, 1.5739, -0.4864], [-0.6622, -0.4790, 0.8539]]

     --> indices: [[1, 2], [2, 1]]

     --> position: [[0.5578, 0.3548, 0.2574], [0.2546, 0.4695, 0.5784]]

    --> token_dic: [[0.3543, 0.2587, -0.1245], [0.4569, 1.2596, 0.2589], [-0.2333, -0.5467, 0.2356]]

     --> token_ids: [[1, 3], [2, 3]]

     output array by 2 * 2 * 3

     --> output: [[[1.0934,  1.9287, -0.2290], [-0.4076, -0.0095,  1.4323]],
                  [[-0.1044, -0.1242,  1.1113], [0.7902,  2.0434,  0.0920]]]
     @endverbatim
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlTransformerEmbedding(cnnlHandle_t handle,
                                const cnnlTensorDescriptor_t filter_desc,
                                const void *filter,
                                const cnnlTensorDescriptor_t indices_desc,
                                const int *indices,
                                const cnnlTensorDescriptor_t position_desc,
                                const void *position,
                                const cnnlTensorDescriptor_t length_desc,
                                const int *length,
                                const cnnlTensorDescriptor_t token_dic_desc,
                                const void *token_dic,
                                const cnnlTensorDescriptor_t token_ids_desc,
                                const int* token_ids,
                                float alpha,
                                cnnlTransformerEmbeddingTokenMode_t use_token_type,
                                cnnlTransformerEmbeddingEncodingMode_t use_position_encoding,
                                const cnnlTensorDescriptor_t output_desc,
                                void *output);

/******************************************************************************
 * Cambricon CNNL OP: Transformer Position Encoding
 ******************************************************************************/
/*!
 * @brief Enumeration variables describing the modes of calculating the position dictionary
 * in the ::cnnlTransformerPositionEncoding operation.
 *
 * It is deprecated and will be removed in future release.
 */
typedef enum {
    CNNL_POSITION_ENCODING_CACHED,
    /*!<Decoder cached mode, which calculates the information of the
     * position of the current token.
     */
    CNNL_POSITION_ENCODING_UNCACHED,
    /*!<Encoder or decoder uncached mode, which calculates the information of all positions. */
} cnnlTransformerPositionEncodingMode_t;

/*!
 * @brief Enumeration variables describing the modes of concatenating
 * sin and cosine in the ::cnnlTransformerPositionEncoding operation.
 *
 * It is deprecated and will be removed in future release.
 */
typedef enum {
    CNNL_POSITION_ENCODING_CONTINUOUS,
    /*!< Concatenates cosine after sin along the last dimension.*/
    CNNL_POSITION_ENCODING_DISCRETED,
    /*!< Concatenates sin and cosine in an alternative way of one sin (odd)
     * and then one cosine (even).
     */
} cnnlTransformerPositionEncodingConcateMode_t;

// Group:Transformer Position Encoding
/*!
 * @brief Obtains position encoding of transformer and stores
 * the result into \p output according to \p length and \p embedding_size.
 *
 * This function performs with the following steps:
 *
 * num_time_scale = embedding_size / 2
 *
 * log_time_scale = -log(max_time_scale / min_time_scale / (num_time_scale - 1))
 *
 * inv_time_scale = min_time_scale * exp(range(num_time_scale) * log_time_scale)
 *
 * scaled_time = matmul(position, inv_time_scale)
 *
 * output = concate([sin(scaled_time), cos(scaled_time)], axis = 1)
 *
 * @deprecated
 *   This function is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the position
 *   encoding operation.
 * @param[in] length
 *   Input. A const int scalar, which indicates the first dimension of output.
 * @param[in] embedding_size
 *   Input. A const int scalar, which indicates the second dimension of output.
 * @param[in] min_time_scale
 *   Input. A const float scalar, which is the minimum scale applied at each position. Default as 1.0.
 * @param[in] max_time_scale
 *   Input. A const float scalar, which is the maximum scale applied at each position. Default as 10000.0.
 * @param[in] position_encoding_mode
 *   Input. The mode of the operation. See ::cnnlTransformerPositionEncodingMode_t for details.
 * @param[in] concate_mode
 *   Input. The concat mode. See ::cnnlTransformerPositionEncodingConcateMode_t for details.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. Shape should be [length, embedding_size].
 *   If position_encoding_mode is CNNL_POSITION_ENCODING_CACHED, shape could be [1, embedding_size]
 *   and only the first row is computed.
 * @param[out] output
 *   Output. A pointer to the MLU memory that stores the output tensor.
 * @retval CNNL_STATUS_SUCCESS
 *   The function ends normally.
 * @retval CNNL_STATUS_ARCH_MISMATCH
 *   One or more of the following conditions are encountered:
 *   - This function is run on the hardware platform that is not supported.
 *
 * @par Data Type
 *  - output tensor: half, float.
 *
 * @par Data Layout
 *  - output tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NCHW.
 *
 * @par Scale Limitation
 * - output tensor: The dimension must be 2. The element number of all dimensions
 *   must be less than \p 1024.
 * - min_time_scale: The value of min_time_scale must be larger than 0 and no larger than 2.0.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Reference
 * - https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
 *
 * @par Example
 * - The example of the position encoding operation is as follows:
     @verbatim
     length equals to 128 and embedding_size equals to 512, position_encoding_mode equals to
     CNNL_POSITION_ENCODING_UNCACHED

     output array by 128 * 512
     @endverbatim
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlTransformerPositionEncoding(
                              cnnlHandle_t handle,
                              int length,
                              int embedding_size,
                              float min_time_scale,
                              float max_time_scale,
                              cnnlTransformerPositionEncodingMode_t position_encoding_mode,
                              cnnlTransformerPositionEncodingConcateMode_t concate_mode,
                              const cnnlTensorDescriptor_t output_desc,
                              void *output);

/******************************************************************************
 * Cambricon CNNL OP: ContinuationIndicator.
 ******************************************************************************/
// Group:Continuation Indicator
/*!
 *  @brief Generates continuous 0 and 1. The first batch data is 0,
 *  and the rest batch data is 1.
 *
 *  This computation logic of this function is as follows:
    @verbatim
    Set the value of the output:

    for (int t = 0; t < time_step; t++) {

      for (int b = 0; b < batch_size; b++) {

        output[t * batch_size + b] = t == 0 ? 0 : 1;

      }

    }
    @endverbatim
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *           queues in the continuation_indicator operation.
 *  @param[in] output_data_desc
 *    Input. Descriptor of output tensor, containing dimension of [time_step, batch_size].
 *  @param[out] output_data
 *    Output. Pointer to MLU memory that stores the output data.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - The handle is NULL.
 *    - The output_data_desc is NULL.
 *    - The output_data_desc->dim != 2.
 *    - The output_data is NULL.
 *    - The time_step(output_data_desc->dims[0]) is smaller than 0.
 *    - The batch_size(output_data_desc->dims[1]) < 0.
 *    - The output data type is not equal than CNNL_DTYPE_HALF and CNNL_DTYPE_FLOAT.
 *    - The time_step(output_data_desc->dims[0]) * batch_size(output_data_desc->dims[1]) > INT32_MAX.
 *
 *  @par Data Type
 *  - output_data: half(float16), float32.
 *
 *  @par Scale Limitation
 *   - time_step(output_data_desc->dims[0]) >= 0.
 *   - 0 <= batch_size(output_data_desc->dims[1]).
 *   - time_step(output_data_desc->dims[0]) * batch_size(output_data_desc->dims[1]) <= INT32_MAX.
 *
 *  @par Reference
 *  - https://github/xmfbit/warpctc-caffe.
 *
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlContinuationIndicator(
    cnnlHandle_t handle,
    const cnnlTensorDescriptor_t output_data_desc,
    void *output_data);

/******************************************************************************
 * Cambricon CNNL OP: Tacotron2Decoder
 ******************************************************************************/
/*!
 *  @brief The descriptor of the ::cnnlTacotron2Decoder operation.
 *  You need to call the ::cnnlCreateTacotron2DecoderDescriptor function
 *  to create a descriptor, and call the ::cnnlSetTacotron2DecoderDescriptor
 *  function to set the information to the descriptor.
 *  Also, you need to destroy the descriptor at the end with the
 *  ::cnnlDestroyTacotron2DecoderDescriptor function.
 *
 *  It is deprecated and will be removed in future release.
 */
typedef struct cnnlTacotron2DecoderStruct *cnnlTacotron2DecoderDescriptor_t;

// Group:Tacotron2
/*!
 *  @brief Creates a descriptor pointed by \p desc
 *   for tacotron2 decoder operation and allocate memory for it.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[out]  desc
 *    Output. Descriptor of tacotron2 decoder operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_ALLOC_FAILED
 *    The function failed to allocate memory.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlCreateTacotron2DecoderDescriptor(cnnlTacotron2DecoderDescriptor_t *desc);

// Group:Tacotron2
/*!
 *  @brief Destroys the descriptor of tacotron2 decoder and free
 *   memory for it.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[in]  desc
 *    Input. Descriptor of tacotron2 decoder operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    The value of \p desc is NULL.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlDestroyTacotron2DecoderDescriptor(cnnlTacotron2DecoderDescriptor_t desc);

// Group:Tacotron2
/*!
 *  @brief Assigns tacotron2 decoder descriptor with parameters.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[in,out]  desc
 *    Input/output. Descriptor of tacotron2 decoder operation.
 *  @param[in]  prenet_dropout_rate
 *    Input. dropout rate of prenet.
 *  @param[in]  auto_stop
 *    Input. An integer describing whether to enable auto-stop decoding.
 *    1 means enabling auto-stop decoding, while 0 means disabling auto-stop decoding.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    The value of \p desc is NULL.
 *  @par API Dependency
 *  - Before using this API, users need to call ::cnnlCreateTacotron2DecoderDescriptor
 *   to create the descriptor.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlSetTacotron2DecoderDescriptor(cnnlTacotron2DecoderDescriptor_t desc,
                                  float prenet_dropout_rate,
                                  int auto_stop);

// Group:Tacotron2
/*!
 *  @brief Retrieves the extra space size required in tacotron2 decoder operation.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[in]  handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *           tacotron2 decoder operation.
 *  @param[out]  size
 *    Output. Pointer to the workspace size required in tacotron2 decoder operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - The value of \p handle is NULL.
 *    - The value of \p size is NULL.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlGetTacotron2DecoderWorkspaceSize(cnnlHandle_t handle,
                                                               size_t *size);

// Group:Tacotron2
/*!
 *  @brief Builds the tacotron2 decoder network, which is a decoder
 *  used in speech synthesis to significantly improve the efficiency and quality on speech synthesis.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the tacotron2
 *           decoder operations.
 *  @param[in] desc
 *    Input. Descriptor of tacotron2 decoder operation.
 *  @param[in] decoder_input_desc
 *    Input.  Descriptor of the decoder input tensor, which holds dimension, data type and layout.
 *            The shape of tensor must be [batch, 80], and data type must be float or half.
 *  @param[in] decoder_input
 *    Input. Pointer to the MLU memory that stores the decoder input tensor.
 *  @param[in] lstm1_hidden_desc
 *    Input.  Descriptor of the lstm1 hidden tensor, which holds dimension, data type and layout.
 *            The shape of tensor must be [batch, 1024], and data type must be float or half.
 *  @param[in] lstm1_hidden
 *    Input. Pointer to the MLU memory that stores the lstm1 hidden tensor.
 *  @param[in] lstm1_cell_desc
 *    Input.  Descriptor of the lstm1 cell tensor, which holds dimension, data type and layout.
 *            The shape of tensor must be [batch, 1024], and data type must be float or half.
 *  @param[in] lstm1_cell
 *    Input. Pointer to the MLU memory that stores the lstm1 cell tensor.
 *  @param[in] lstm2_hidden_desc
 *    Input.  Descriptor of the lstm2 hidden tensor, which holds dimension, data type and layout.
 *            The shape of tensor must be [batch, 1024], and data type must be float or half.
 *  @param[in] lstm2_hidden
 *    Input. Pointer to the MLU memory that stores the lstm2 hidden tensor.
 *  @param[in] lstm2_cell_desc
 *    Input.  Descriptor of the lstm2 cell tensor, which holds dimension, data type and layout.
 *            The shape of tensor must be [batch, 1024], and data type must be float or half.
 *  @param[in] lstm2_cell
 *    Input. Pointer to the MLU memory that stores the lstm2 cell tensor.
 *  @param[in] attention_desc
 *    Input.  Descriptor of the attention tensor, which holds dimension, data type and layout.
 *            The shape of tensor must be [batch, sequence_length], and data type must be float or half.
 *  @param[in] attention
 *    Input. Pointer to the MLU memory that stores the attention tensor.
 *  @param[in] attention_cum_desc
 *    Input.  Descriptor of the attention accumulation tensor, which holds dimension, data type and layout.
 *            The shape of tensor must be [batch, sequence_length], and data type must be float or half.
 *  @param[in] attention_cum
 *    Input. Pointer to the MLU memory that stores the attention accumulation tensor.
 *  @param[in] attention_context_desc
 *    Input.  Descriptor of the attention context tensor, which holds dimension, data type and layout.
 *            The shape of tensor must be [batch, 512], and data type must be float or half.
 *  @param[in] attention_context
 *    Input. Pointer to the MLU memory that stores the attention context tensor.
 *  @param[in] memory_desc
 *    Input.  Descriptor of the memory tensor, which holds dimension, data type and layout.
 *            The shape of tensor must be [batch, sequence_length, 512], and data type must be float or half.
 *  @param[in] memory
 *    Input. Pointer to the MLU memory that stores the memory tensor.
 *  @param[in] processed_memory_desc
 *    Input.  Descriptor of the processed memory tensor, which holds dimension, data type and layout.
 *            The shape of tensor must be [batch, sequence_length, 128], and data type must be float or half.
 *  @param[in] processed_memory
 *    Input. Pointer to the MLU memory that stores the processed memory tensor.
 *  @param[in] mask_desc
 *    Input.  Descriptor of the mask tensor, which holds dimension, data type and layout.
 *            The shape of tensor must be [batch, sequence_length], and data type must be int8.
 *  @param[in] mask
 *    Input. Pointer to the MLU memory that stores the mask tensor.
 *  @param[in] prenet0_filter_desc
 *    Input.  Descriptor of the prenet0 filter tensor, which holds dimension, data type and layout.
 *            The shape of tensor must be [Co, Ci], where Co = 256 and Ci = 80, and data type must be half.
 *  @param[in] prenet0_filter
 *    Input. Pointer to the MLU memory that stores the prenet0 filter tensor.
 *  @param[in] prenet1_filter_desc
 *    Input.  Descriptor of the prenet1 filter tensor, which holds dimension, data type and layout.
 *            The shape of tensor must be [Co, Ci], where Co = 256 and Ci = 256, and data type must be half.
 *  @param[in] prenet1_filter
 *    Input. Pointer to the MLU memory that stores the prenet1 filter tensor.
 *  @param[in] lstm1_input_filters_desc
 *    Input.  Descriptor of the lstm1 input filters tensor, which holds dimension, data type and layout.
 *            The shape of tensor must be [Co, Ci], where Co = 4096 and Ci = 768, and data type must be half.
 *  @param[in] lstm1_input_filters
 *    Input. Pointer to the MLU memory that stores the lstm1 input filters tensor.
 *  @param[in] lstm1_hidden_filters_desc
 *    Input.  Descriptor of the lstm1 hidden filters tensor, which holds dimension, data type and layout.
 *            The shape of tensor must be [Co, Ci], where Co = 4096 and Ci = 1024, and data type must be half.
 *  @param[in] lstm1_hidden_filters
 *    Input. Pointer to the MLU memory that stores the lstm1 hidden filters tensor.
 *  @param[in] lstm1_bias_desc
 *    Input.  Descriptor of the lstm1 bias tensor, which holds dimension, data type and layout.
 *            The shape of tensor must be [8192], and data type must be float or half.
 *  @param[in] lstm1_bias
 *    Input. Pointer to the MLU memory that stores the lstm1 bias tensor.
 *  @param[in] lstm2_input_filters_desc
 *    Input.  Descriptor of the lstm2 input filters tensor, which holds dimension, data type and layout.
 *            The shape of tensor must be [Co, Ci], where Co = 4096 and Ci = 1536, and data type must be half.
 *  @param[in] lstm2_input_filters
 *    Input. Pointer to the MLU memory that stores the lstm2 input filters tensor.
 *  @param[in] lstm2_hidden_filters_desc
 *    Input.  Descriptor of the lstm2 hidden filters tensor, which holds dimension, data type and layout.
 *            The shape of tensor must be [Co, Ci], where Co = 4096 and Ci = 1024, and data type must be half.
 *  @param[in] lstm2_hidden_filters
 *    Input. Pointer to the MLU memory that stores the lstm2 hidden filters tensor.
 *  @param[in] lstm2_bias_desc
 *    Input.  Descriptor of the lstm2 bias tensor, which holds dimension, data type and layout.
 *            The shape of tensor must be [8192], and data type must be float or half.
 *  @param[in] lstm2_bias
 *    Input. Pointer to the MLU memory that stores the lstm2 bias tensor.
 *  @param[in] attention_query_filter_desc
 *    Input.  Descriptor of the attention query filter tensor, which holds dimension, data type and layout.
 *            The shape of tensor must be [Co, Ci], where Co = 128 and Ci = 1024, and data type must be half.
 *  @param[in] attention_query_filter
 *    Input. Pointer to the MLU memory that stores the attention query filter tensor.
 *  @param[in] attention_conv_filter_desc
 *    Input.  Descriptor of the attention conv filter tensor, which holds dimension, data type and layout.
 *            The shape of tensor must be [Co, T, Ci], where Co = 32, T = 31 and Ci = 2, and data type must be half.
 *  @param[in] attention_conv_filter
 *    Input. Pointer to the MLU memory that stores the attention conv filter tensor.
 *  @param[in] attention_full_filter_desc
 *    Input.  Descriptor of the attention full filter tensor, which holds dimension, data type and layout.
 *            The shape of tensor must be [Co, Ci], where Co = 128 and Ci = 32, and data type must be half.
 *  @param[in] attention_full_filter
 *    Input. Pointer to the MLU memory that stores the attention full filter tensor.
 *  @param[in] attention_out_filter_desc
 *    Input.  Descriptor of the attention output filter tensor, which holds dimension, data type and layout.
 *            The shape of tensor must be [Co, Ci], where Co = 1 and Ci = 128, and data type must be half.
 *  @param[in] attention_out_filter
 *    Input. Pointer to the MLU memory that stores the attention output filter tensor.
 *  @param[in] prj_mel_filter_desc
 *    Input.  Descriptor of the filter tensor for the mel output of the projection layer, which holds dimension, data type and layout.
 *            The shape of tensor must be [Co, Ci], where Co = 80 and Ci = 1536, and data type must be half.
 *  @param[in] prj_mel_filter
 *    Input. Pointer to the MLU memory that stores the filter tensor for the mel output of the projection layer.
 *  @param[in] prj_mel_bias_desc
 *    Input.  Descriptor of the bias tensor for the mel output of the projection layer, which holds dimension, data type and layout.
 *            The shape of tensor must be [80], and data type must be float or half.
 *  @param[in] prj_mel_bias
 *    Input. Pointer to the MLU memory that stores the bias tensor for the mel output of the projection layer.
 *  @param[in] prj_gate_filter_desc
 *    Input.  Descriptor of the filter tensor for the gate of the projection layer, which holds dimension, data type and layout.
 *            The shape of tensor must be [Co, Ci], where Co = 1 and Ci = 1536, and data type must be half.
 *  @param[in] prj_gate_filter
 *    Input. Pointer to the MLU memory that stores the filter tensor for the gate of the projection layer.
 *  @param[in] prj_gate_bias_desc
 *    Input.  Descriptor of the bias tensor for the gate of the projection layer, which holds dimension, data type and layout.
 *            The shape of tensor must be [1], and data type must be float or half.
 *  @param[in] prj_gate_bias
 *    Input. Pointer to the MLU memory that stores the bias tensor for the gate of the projection layer.
 *  @param[in] workspace
 *    Input.  Pointer to the MLU memory that is used as an extra workspace.
 *  @param[in] workspace_size
 *    Input.  The size of extra workspace.
 *  @param[in] output_mel_desc
 *    Input.  Descriptor of output mel tensor, which holds dimension, data type and layout.
 *            The shape of tensor must be [max_frame_length, batch, 80], and data type must be float or half.
 *  @param[out] output_mel
 *    Output. Pointer to the MLU memory that stores the output mel data.
 *  @param[in] output_lengths_desc
 *    Input.  Descriptor of output lengths tensor, which holds dimension, data type and layout.
 *            The shape of tensor must be [batch, 1], and data type must be int32.
 *  @param[out] output_lengths
 *    Output. Pointer to the MLU memory that stores the output lengths data.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are encountered:
 *    - handle is NULL.
 *    - pointer is NULL.
 *    - parameters do not meet scale limitation.
 *  @retval CNNL_STATUS_ARCH_MISMATCH
 *    One or more of the following conditions are encountered:
 *    - This function is run on the hardware platform that is not supported.
 *
 *  @par Data Type
 *  - input: float, half.
 *  - filter: half.
 *  - bias: float, half.
 *
 *  @par Scale Limitation
 *   - The value of batch must be greater than 0.
 *   - The value of sequence_length must be greater than 0.
 *   - The value of max_frame_length must be greater than 0.
 *   - The shape of decoder_input must be [batch, 80].
 *   - The shape of lstm1_hidden must be [batch, 1024].
 *   - The shape of lstm1_cell must be [batch, 1024].
 *   - The shape of lstm2_hidden must be [batch, 1024].
 *   - The shape of lstm2_cell must be [batch, 1024].
 *   - The shape of attention must be [batch, sequence_length].
 *   - The shape of attention_cum must be [batch, sequence_length].
 *   - The shape of attention_context must be [batch, 512].
 *   - The shape of memory must be [batch, sequence_length, 512].
 *   - The shape of processed_memory must be [batch, sequence_length, 128].
 *   - The shape of mask must be [batch, sequence_length].
 *   - The shape of prenet1_filter must be [Co, Ci], where Co = 256 and Ci = 80.
 *   - The shape of prenet1_filter must be [Co, Ci], where Co = 256 and Ci = 256.
 *   - The shape of lstm1_input_filters must be [Co, Ci], where Co = 4096 and Ci = 768.
 *   - The shape of lstm1_hidden_filters must be [Co, Ci], where Co = 4096 and Ci = 1024.
 *   - The shape of lstm1_bias must be [8192].
 *   - The shape of lstm2_input_filters must be [Co, Ci], where Co = 4096 and Ci = 1536.
 *   - The shape of lstm2_hidden_filters must be [Co, Ci], where Co = 4096 and Ci = 1024.
 *   - The shape of lstm2_bias must be [8192].
 *   - The shape of attention_query_filter must be [Co, Ci], where Co = 128 and Ci = 1024.
 *   - The shape of attention_conv_filter must be [Co, T, Ci], where Co = 32, T = 31 and Ci = 2.
 *   - The shape of attention_full_filter must be [Co, Ci], where Co = 128 and Ci = 32.
 *   - The shape of attention_out_filter must be [Co, Ci], where Co = 1 and Ci = 128.
 *   - The shape of prj_mel_filter must be [Co, Ci], where Co = 80 and Ci = 1536.
 *   - The shape of prj_mel_bias must be [80].
 *   - The shape of prj_gate_filter must be [Co, Ci], where Co = 1 and Ci = 1536.
 *   - The shape of prj_gate_bias must be [1].
 *   - The shape of output_mel must be [max_frame_length, batch, 80].
 *   - The shape of output_lengths must be [batch, 1].
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlTacotron2Decoder(cnnlHandle_t handle,
                     const cnnlTacotron2DecoderDescriptor_t desc,
                     const cnnlTensorDescriptor_t decoder_input_desc,
                     void *decoder_input,
                     const cnnlTensorDescriptor_t lstm1_hidden_desc,
                     void *lstm1_hidden,
                     const cnnlTensorDescriptor_t lstm1_cell_desc,
                     void *lstm1_cell,
                     const cnnlTensorDescriptor_t lstm2_hidden_desc,
                     void *lstm2_hidden,
                     const cnnlTensorDescriptor_t lstm2_cell_desc,
                     void *lstm2_cell,
                     const cnnlTensorDescriptor_t attention_desc,
                     void *attention,
                     const cnnlTensorDescriptor_t attention_cum_desc,
                     void *attention_cum,
                     const cnnlTensorDescriptor_t attention_context_desc,
                     void *attention_context,
                     const cnnlTensorDescriptor_t memory_desc,
                     void *memory,
                     const cnnlTensorDescriptor_t processed_memory_desc,
                     void *processed_memory,
                     const cnnlTensorDescriptor_t mask_desc,
                     void *mask,
                     const cnnlTensorDescriptor_t prenet0_filter_desc,
                     void *prenet0_filter,
                     const cnnlTensorDescriptor_t prenet1_filter_desc,
                     void *prenet1_filter,
                     const cnnlTensorDescriptor_t lstm1_input_filters_desc,
                     void *lstm1_input_filters,
                     const cnnlTensorDescriptor_t lstm1_hidden_filters_desc,
                     void *lstm1_hidden_filters,
                     const cnnlTensorDescriptor_t lstm1_bias_desc,
                     void *lstm1_bias,
                     const cnnlTensorDescriptor_t lstm2_input_filters_desc,
                     void *lstm2_input_filters,
                     const cnnlTensorDescriptor_t lstm2_hidden_filters_desc,
                     void *lstm2_hidden_filters,
                     const cnnlTensorDescriptor_t lstm2_bias_desc,
                     void *lstm2_bias,
                     const cnnlTensorDescriptor_t attention_query_filter_desc,
                     void *attention_query_filter,
                     const cnnlTensorDescriptor_t attention_conv_filter_desc,
                     void *attention_conv_filter,
                     const cnnlTensorDescriptor_t attention_full_filter_desc,
                     void *attention_full_filter,
                     const cnnlTensorDescriptor_t attention_out_filter_desc,
                     void *attention_out_filter,
                     const cnnlTensorDescriptor_t prj_mel_filter_desc,
                     void *prj_mel_filter,
                     const cnnlTensorDescriptor_t prj_mel_bias_desc,
                     void *prj_mel_bias,
                     const cnnlTensorDescriptor_t prj_gate_filter_desc,
                     void *prj_gate_filter,
                     const cnnlTensorDescriptor_t prj_gate_bias_desc,
                     void *prj_gate_bias,
                     void *workspace,
                     size_t workspace_size,
                     const cnnlTensorDescriptor_t output_mel_desc,
                     void *output_mel,
                     const cnnlTensorDescriptor_t output_lengths_desc,
                     void *output_lengths);

/******************************************************************************
 * Cambricon CNNL OP: FlatSearch
 ******************************************************************************/

/*!
 * @brief Enumeration variables describing the calculation modes of FlatSearch.
 */
typedef enum {
  CNNL_DISTANCE_L2 = 0,
  /*!< Computes euclidean metric between query vector and lib vector.*/

  CNNL_DISTANCE_IP = 1
  /*!< Computes inner product between query vector and lib vector.*/
} cnnlDistanceMode_t;

/*! The descriptor of FlatSearch operation that holds the information
 *  including distance mode.
 *
 *  You need to call the ::cnnlCreateFlatSearchDescriptor() function
 *  to create a descriptor, and call the ::cnnlSetFlatSearchDescriptor()
 *  function to set the input information to the descriptor.
 *  Also, you need to destroy the descriptor at the end with the
 *  ::cnnlDestroyFlatSearchDescriptor() function.
 */
typedef struct cnnlFlatSearchStruct *cnnlFlatSearchStruct_t;

// Group:Flat Search
/*!
 *  @brief Creates a descriptor pointed by
 *  \p flat_search_desc for the FlatSearch operation.
 *  @param[out] flat_search_desc
 *  Output. A pointer to the FlatSearch descriptor that holds information about the
 *  FlatSearch operation.
 *  @retval CNNL_STATUS_SUCCESS
 *  The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *  \p flat_search_desc is NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCreateFlatSearchDescriptor(cnnlFlatSearchStruct_t* flat_search_desc);

// Group:Flat Search
/*!
 *  @brief Assigns a FlatSearch descriptor with parameters.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlSetFlatSearchDescriptor_v2 instead.
 *
 *  @param[in,out] flat_search_desc
 *    Input/output. The descriptor of the FlatSearch operation. For detailed information,
 *    see ::cnnlFlatSearchStruct_t.
 *  @param[in] distance_mode
 *    Input. The distance mode of FlatSearch. Currently only ::CNNL_DISTANCE_L2
 *           or ::CNNL_DISTANCE_IP is supported.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One of the following conditions is met:
 *      - \p flat_search_desc is NULL.
 *      - One or more parameters are not supported.
 */
CNNL_DEPRECATED_FOR(cnnlSetFlatSearchDescriptor_v2)
cnnlStatus_t CNNL_WIN_API
cnnlSetFlatSearchDescriptor(cnnlFlatSearchStruct_t flat_search_desc,
                            cnnlDistanceMode_t distance_mode);

// Group:Flat Search
/*!
 *  @brief Assigns a FlatSearch descriptor with parameters.
 *
 *  @param[in,out] flat_search_desc
 *    Input/output. The descriptor of the FlatSearch operation. For detailed information,
 *    see ::cnnlFlatSearchStruct_t.
 *  @param[in] distance_mode
 *    Input. The distance mode of FlatSearch. Currently only ::CNNL_DISTANCE_L2
 *           or ::CNNL_DISTANCE_IP is supported.
 *  @param[in] lib_quant_desc
 *    Input. A quantization descriptor that holds quantization information of lib tensor.
 *           For detailed information, see cnnlQuantizeExDescriptor_t.
 *  @param[in] query_quant_desc
 *    Input. A quantization descriptor that holds quantization information of qeury
 *           tensor. For detailed information, see cnnlQuantizeExDescriptor_t.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One of the following conditions is met:
 *      - \p flat_search_desc is NULL.
 *      - One or more parameters are not supported.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetFlatSearchDescriptor_v2(cnnlFlatSearchStruct_t flat_search_desc,
                               cnnlDistanceMode_t distance_mode,
                               cnnlQuantizeExDescriptor_t lib_quant_desc,
                               cnnlQuantizeExDescriptor_t query_quant_desc);

// Group:Flat Search
/*!
 *  @brief Destroys the descriptor of FlatSearch and free
 * memory for it.
 *
 *  @param[in] flat_search_desc
 *    Input. Descriptor of FlatSearch operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p flat_search_desc is NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlDestroyFlatSearchDescriptor(cnnlFlatSearchStruct_t flat_search_desc);

// Group:Flat Search
/*!
 * @brief Retrieves the extra space size needed in FlatSearch operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *          queues in the FlatSearch operation.
 * @param[in] flat_search_desc
 *   Input. The descriptor of the FlatSearch operation. For detailed information,
 *   see ::cnnlFlatSearchStruct_t.
 * @param[in] lib_desc
 *   Input. Descriptor of the lib tensor.
 * @param[in] topk_distance_desc
 *   Input. Descriptor of the topk_distance tensor.
 * @param[in] topk_index_desc
 *   Input. Descriptor of the topk_index_tensor.
 * @param[out] workspace_size
 *   Output. Pointer to the MLU memory that stores the extra space size (bytes)
 *   needed in FlatSearch operation.
 * @retval CNNL_STATUS_SUCCESS
 *   The function ends normally.
 * @retval CNNL_STATUS_BAD_PARAM
 *   One or more of the following conditions must be met:
 *   - \p handle is NULL.
 *   - \p flat_search_desc is NULL.
 *   - \p lib_desc is NULL.
 *   - \p topk_distance_desc is NULL.
 *   - \p topk_index_desc is NULL.
 *   - \p workspace_size is NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetFlatSearchWorkspaceSize_v2(cnnlHandle_t handle,
                                 cnnlFlatSearchStruct_t flat_search_desc,
                                 const cnnlTensorDescriptor_t lib_desc,
                                 const cnnlTensorDescriptor_t topk_distance_desc,
                                 const cnnlTensorDescriptor_t topk_index_desc,
                                 size_t *workspace_size);

// Group:Flat Search
/*!
 * @brief Computes the topk minimum distance and corresponding index
 * between query vectors and lib vectors if distance mode is ::CNNL_DISTANCE_L2,
 * and computes the topk maximum distance and corresponding index between query
 * vectors and lib vectors if distance mode is ::CNNL_DISTANCE_IP.
 *
 * This operation performs with the following steps:
 *
 * 1. Compute distance.
 *
 *    Compute euclidean metric or inner product between query vectors and lib vectors.
 *
 * 2. Compute topk.
 *
 *    Selects the most relevant [nk] ones from the [nlib] lib according to the
 *    distance. nk represents the number of probed lib. nlib represents the
 *    number of all lib vectors.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *          queues in the FlatSearch operation.
 * @param[in] flat_search_desc
 *   Input. The descriptor of the FlatSearch operation. For detailed information,
 *   see ::cnnlFlatSearchStruct_t.
 * @param[in] lib_desc
 *   Input. Descriptor of the lib tensor, which contains the dimension, data
 *   type and layout of the tensor. The dimension is [nlib, d].
 * @param[in] lib
 *   Input. Pointer to the MLU memory that stores the lib tensor. \p lib stores all
 *   the lib vectors.
 * @param[in] query_desc
 *   Input. Descriptor of the query tensor, which contains the dimension, data
 *   type and layout of the tensor. The dimension is [nq, d], where nq represents the
 *   number of all the query vectors.
 * @param[in] query
 *   Input. Pointer to the MLU memory that stores the query tensor. \p query stores
 *   all the query vectors.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that stores some temporary results.
 *  @param[in] workspace_size
 *    Input. The value of workspace size.
 * @param[in] topk_distance_desc
 *   Input. Descriptor of the topk_distance tensor, which contains the
 *   dimension, data type and layout of the tensor. The dimension is [nq, nk].
 * @param[out] topk_distance
 *   Output. Pointer to the MLU memory that stores the topk_distance tensor.
 *   \p topk_distance stores the topk-distance between query vectors and lib vectors.
 *   If \p topk_distance is NULL, only the result of \p topk_index is stored.
 * @param[in] topk_index_desc
 *   Input. Descriptor of the topk_index tensor, which contains the dimension,
 *   data type and layout of the tensor. The dimension is [nq, nk].
 * @param[out] topk_index
 *   Output. Pointer to the MLU memory that stores the topk_index tensor.
 *   \p topk_index stores the topk-index with respect to query.
 * @retval CNNL_STATUS_SUCCESS
 *   The function ends normally.
 * @retval CNNL_STATUS_BAD_PARAM
 *   One or more of the following conditions must be met:
 *   - \p handle is NULL.
 *   - \p flat_search_desc is NULL.
 *   - \p lib_desc is NULL.
 *   - \p lib is NULL.
 *   - \p query_desc is NULL.
 *   - \p query is NULL.
 *   - \p workspace is NULL.
 *   - \p workspace_size is too small.
 *   - \p topk_distance_desc is NULL.
 *   - \p topk_index_desc is NULL.
 *   - \p topk_index is NULL.
 *   - Tensor dimension or data type does not match.
 *  @retval CNNL_STATUS_ARCH_MISMATCH
 *   This function is run on the hardware platform that is not supported.
 *
 * @par Data Type
 *  - lib and \p query: float when distance mode is ::CNNL_DISTANCE_L2; float, int8, or half
 *    when distance mode is ::CNNL_DISTANCE_IP.
 *  - topk_distance: float.
 *  - topk_index: int32.
 *
 * @note
 * - This function supports MLU300 and MLU500 series when distance mode is ::CNNL_DISTANCE_IP.
 * - This function only supports MLU300 series when distance mode is ::CNNL_DISTANCE_L2.
 * @par Scale Limitation
 * - nk cannot be greater than 1500 and cannot be greater than nlib when distance mode
 *   is ::CNNL_DISTANCE_IP.
 * - nk cannot be greater than 2000 when distance mode is ::CNNL_DISTANCE_L2.
 * - d(lib_desc->dim[1]) only supports 256, 512, 768, or 1024
 *   when distance mode is ::CNNL_DISTANCE_IP.
 * - d(lib_desc->dim[1]) cannot be greater than 30000
 *   when distance mode is ::CNNL_DISTANCE_L2.
 *
 * @par Reference
 * - http://github.com/facebookresearch/faiss
 */
cnnlStatus_t CNNL_WIN_API
cnnlFlatSearch_v2(cnnlHandle_t handle,
                 cnnlFlatSearchStruct_t flat_search_desc,
                 const cnnlTensorDescriptor_t lib_desc,
                 const void *lib,
                 const cnnlTensorDescriptor_t query_desc,
                 const void *query,
                 const cnnlTensorDescriptor_t topk_distance_desc,
                 void *topk_distance,
                 const cnnlTensorDescriptor_t topk_index_desc,
                 void *topk_index,
                 void *workspace,
                 size_t workspace_size);

/******************************************************************************
 * Cambricon CNNL OP: FSMN
 ******************************************************************************/
/*!
 *  @brief The descriptor of the ::cnnlFSMN operation.
 *  You need to call the ::cnnlCreateFSMNDescriptor function to create a descriptor,
 *  and call the ::cnnlSetFSMNDescriptor function to set the information of the fsmn
 *  operation to the descriptor. Also, you need to destroy the descriptor at the
 *  end with the ::cnnlDestroyFSMNDescriptor function.
 *
 *  It is deprecated and will be removed in future release.
 */
typedef struct cnnlFSMNStruct *cnnlFSMNDescriptor_t;

// Group:FSMN
/*!
 *  @brief Creates a descriptor pointed by \p desc
 *   for the FSMN operation and allocate memory for it.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[out]  desc
 *    Output. Pointer to the FSMN descriptor.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_ALLOC_FAILED
 *    The function failed to allocate memory space.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlCreateFSMNDescriptor(cnnlFSMNDescriptor_t *desc);

// Group:FSMN
/*!
 *  @brief Destroys the descriptor of FSMN and free memory for it.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[in]  desc
 *    Input. The descriptor of FSMN operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    The value of \p desc is NULL.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlDestroyFSMNDescriptor(cnnlFSMNDescriptor_t desc);

// Group:FSMN
/*!
 *  @brief Assigns FSMN descriptor with parameters.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[out]  desc
 *    Output. The descriptor of the FSMN operation.
 *  @param[in]  look_back_step
 *    Input. Number of look back step of the current token.
 *  @param[in]  look_ahead_step
 *    Input. Number of look ahead step of the current token.
 *  @param[in]  use_residual
 *    Input. A Boolean value indicating whether to use residual when computing output.
 *  @param[in]  use_mask
 *    Input. A Boolean value indicating whether to use mask.
 *  @param[in]  mask
 *    Input. Pointer to the MLU memory that stores the mask tensor.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    The value of \p desc is NULL.
 *
 *  @par API Dependency
 *  - Before calling this function, you need to call
 *  ::cnnlCreateFSMNDescriptor() to create a descriptor.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlSetFSMNDescriptor(cnnlFSMNDescriptor_t desc,
                      const int look_back_step,
                      const int look_ahead_step,
                      const bool use_residual,
                      const bool use_mask,
                      const void *mask);

// Group:FSMN
/*!
 *  @brief Performs the FSMN operation in SCAMA network.
 *
 *  This function performs with the following steps:
 *
 *  1. depthwise conv
 *
 *    intermediate = depthwise(input, filter)
 *
 *  2. add residual
 *
 *    output = add(intermediate, residual)
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[in]  handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *           FSMN operation.
 *  @param[in] desc
 *    Input. The descriptor of FSMN operation.
 *  @param[in]  input_desc
 *    Input. Descriptor of input, containing dimension and the layout of input.
 *  @param[in]  input
 *    Input. Pointer to the MLU memory that stores the input tensor.
 *  @param[in]  filter_desc
 *    Input. Descriptor of filter, containing dimension and the layout of filter.
 *  @param[in]  filter
 *    Input. Pointer to the MLU memory that stores the filter tensor.
 *  @param[in]  residual_desc
 *    Input. Descriptor of residual, containing dimension and the layout of residual.
 *  @param[in]  residual
 *    Input.  Pointer to the MLU memory that stores the residual tensor.
 *  @param[out]  output_desc
 *    Input. Descriptor of output, containing dimension and the layout of output.
 *  @param[out]  output
 *    Input. Pointer to the MLU memory that stores the output tensor.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are encountered:
 *    - handle is NULL.
 *    - pointer is NULL.
 *    - parameters do not meet scale limitation.
 *  @retval CNNL_STATUS_ARCH_MISMATCH
 *    This function is run on the hardware platform that is not supported.
 *
 *  @par Data Type
 *  - input: float, half.
 *  - filter: float, half.
 *  - residual: float, half.
 *  - output: float, half.
 *
 *  @note
 *  - The data type of input, filter, residual and output must be the same.
 *
 *  @par Scale Limitation
 *  - The number of dimensions of input, residual and output tensors must match.
 *  - The fsmn descriptor parameter should meet:
 *      0 <= look_back_step <= filter_size
 *      0 <= look_ahead_step <= filter_size
 *  - \p batch_size and \p seq_len cannot be greater than the maximum value of int32.
 *  - \p seq_len cannot be greater than 512.
 *  - \p filter_size cannot be greater than 32.
 *  - \p hidden_size must be dividable by 64 and not be greater than 512.
 *  - If \p seq_len is greater than 384, \p hidden_size cannot be greater than 384.
 *  - If \p hidden_size is greater than 384, \p seq_len cannot be greater than 384.
 *
 *  @par API Dependency
 *  - You need to call the ::cnnlCreateFSMNDescriptor() before calling this function.
 *  - You need to call the ::cnnlDestroyFSMNDescriptor() function after
 *   calling this function.
 *
 *  @par Example
    @verbatim
     Dimension of input: [batch_size, seq_len, hidden_size]

     Dimension of filter: [filter_size, hidden_size]

     Dimension of mask: [batch_size, seq_len]

     Dimension of residual: [batch_size, seq_len, hidden_size]

     Dimension of output: [batch_size, seq_len, hidden_size]
    @endverbatim
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t cnnlFSMN(
    cnnlHandle_t handle,
    const cnnlFSMNDescriptor_t desc,
    const cnnlTensorDescriptor_t input_desc,
    const void *input,
    const cnnlTensorDescriptor_t filter_desc,
    const void *filter,
    const cnnlTensorDescriptor_t residual_desc,
    const void *residual,
    cnnlTensorDescriptor_t output_desc,
    void *output);

/******************************************************************************
 * Cambricon CNNL OP: SiamRPNPost
 ******************************************************************************/
/*!
 *  @brief The descriptor of the ::cnnlSiamRPNPost operation.
 *  You need to call the ::cnnlCreateSiamRPNPostDescriptor() function
 *  to create a descriptor, and call the ::cnnlSetSiamRPNPostDescriptor()
 *  function to set the tensor information to the descriptor.
 *  Also, you need to destroy the SiamRPNPost descriptor at the end with the
 *  ::cnnlDestroySiamRPNPostDescriptor() function.
 *
 *  It is deprecated and will be removed in future release.
 */
typedef struct cnnlSiamRPNPostStruct *cnnlSiamRPNPostDescriptor_t;

// Group:SiamRPNPost
/*!
 *  @brief Performs the one-shot detection and proposal selection process in
 *  Siamese Region Proposal Network. It is used to get the bbox in object
 *  tracking.
 *
 *  This operation performs with the following three steps:
 *
 *  1. Update anchors
 *
 *    delta = (dx, dy, dw, dh)
 *
 *    anchor = (x, y, w, h)
 *
 *    tmp_x = ori_w + w_const * stride + dx * w
 *
 *    tmp_y = ori_h + h_const * stride + dy * h
 *
 *    tmp_w = w * exp(dw)
 *
 *    tmp_h = h * exp(dh)
 *
 *    where ori_w = -(width / 2) * stride, ori_h = -(height / 2) * stride.
 *
 *  2. Compute penalty
 *
 *    penalty = exp(penalty_k * max(r / r', r' / r) * max(s / s', s' / s))
 *
 *    r = tmp_w / tmp_h, r' = ratio
 *
 *    (tmp_w + p) * (tmp_h + p) = s^2
 *
 *    p = (tmp_w + tmp_h) / 2
 *
 *    s' = scale
 *
 *    where ratio and scale are saved in target_scale_ratio.
 *
 *  3. Compute score
 *
 *    score_neg = score[:, 0:num_anchor]
 *
 *    score_pos = scores[:, num_anchor:]
 *
 *    scores = sigmoid(score_pos - score_neg)
 *
 *    scores1 = scores * penalty
 *
 *    scores2 = scores1 * (1.0 - window_influence) + cos_window * window_influence
 *
 *    offset = argmax(scores2)
 *
 *    output = (tmp_x[offset], tmp_y[offset], tmp_w[offset], tmp_h[offset],
 *              scores[offset], scores1[offset] * lr_attr)
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[in]  handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *           SiamRPNPost operation.
 *  @param[in] siam_rpn_post_desc
 *    Input. The descriptor of SiamRPNPost operation.
 *  @param[in]  desc_scores
 *    Input. Descriptor of scores, containing dimensions of [batch_size, 2 * num_anchor, height, width].
 *  @param[in]  scores
 *    Input. Pointer to the MLU memory that stores the scores tensor.
 *  @param[in]  desc_delta
 *    Input. Descriptor of delta, containing dimensions of [batch_size, 4 * num_anchor, height, width].
 *  @param[in]  delta
 *    Input. Pointer to the MLU memory that stores the delta tensor.
 *  @param[in]  desc_target_scale_ratio
 *    Input. Descriptor of target_scale_ratio, containing dimensions of [batch_size, 2].
 *  @param[in]  target_scale_ratio
 *    Input. Pointer to the MLU memory that stores the target_scale_ratio tensor.
 *  @param[in]  desc_anchor
 *    Input. Descriptor of anchor, containing dimensions of [4, num_anchor], where anchor is described by (x, y, w, h).
 *  @param[in]  anchor
 *    Input. Pointer to the MLU memory that stores the anchor tensor.
 *  @param[in]  desc_h_const
 *    Input. Descriptor of h_const, containing dimensions of [height, width].
 *  @param[in]  h_const
 *    Input. Pointer to the MLU memory that stores the h_const tensor.
 *    It is the ordinate used to generate the grid.
 *  @param[in]  desc_w_const
 *    Input. Descriptor of w_const, containing dimensions of [height, width].
 *  @param[in]  w_const
 *    Input. Pointer to the MLU memory that stores the w_const tensor.
 *    It is the abscissa used to generate the grid.
 *  @param[in]  desc_cos_window
 *    Input. Descriptor of cos_window, containing dimensions of [height, width].
 *  @param[in]  cos_window
 *    Input. Pointer to the MLU memory that stores the cos_window tensor.
 *    cos_window is a cosine window used for filtering in spatial positions.
 *  @param[in]  workspace
 *    Input. The pointer to data address of workspace.
 *  @param[in]  workspace_size
 *    Input. The size of workspace.
 *  @param[out]  desc_output
 *    Output. Descriptor of output, containing data type, dimensions and layout.
 *    Shape must be [batch_size, 6].
 *  @param[out]  output
 *    Output. Pointer to the MLU memory that stores the output tensor.
 *
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are encountered:
 *    - handle is NULL.
 *    - pointer is NULL.
 *    - parameters is invalid.
 *  @retval CNNL_STATUS_ARCH_MISMATCH
 *    One or more of the following conditions are encountered:
 *    - This function is run on the hardware platform that is not supported.
 *
 *  @par Data Type
 *  - On MLU270 & MLU300 series:
 *    - scores: float, half.
 *    - delta: float, half.
 *    - target_scale_ratio: float, half.
 *    - anchor: float, half.
 *    - h_const: float, half.
 *    - w_const: float, half.
 *    - cos_window: float, half.
 *    - output: float, half.
 *
 *  @par Data Layout
 *  - scores: CNNL_LAYOUT_NCHW
 *  - delta: CNNL_LAYOUT_NCHW
 *
 *  @note
 *    - The data type of inputs and outputs must be same.
 *
 *  @par Scale Limitation
 *  - hw_align = PAD_UP(height * width, 64)
 *  - ahw_align = hw_align * num_anchor
 *  - num_anchor_align = PAD_UP(num_anchor, 64)
 *  - (3 * hw_align + 15 * ahw_align + 4 * num_anchor_align + PAD_UP(2 * batch_size, 64) + 64) * sizeof(dtype) < 384KB
 *
 *  @par API Dependency
 *  - You need to call ::cnnlCreateSiamRPNPostDescriptor() to
 *    create a descriptor and call the ::cnnlSetSiamRPNPostDescriptor()
 *    to set its parameters before calling this function.
 *  - You need to call the ::cnnlDestroySiamRPNPostDescriptor()
 *    function after calling this function.
 *  @par Reference
 *  - https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_High_Performance_Visual_CVPR_2018_paper.pdf
 *  @par Example
    @verbatim
    scores : a tensor of [batch_size, 2 * num_anchor, height, width]

    delta : a tensor of [batch_size, 4 * num_anchor, height, width]

    target_scale_ratio : a tensor of [batch_size, 2]

    anchor : a tensor of [4, num_anchor], anchor's layout is (x, y, w, h)

    h_const : a tensor of [height, width]

    w_const : a tensor of [height, width]

    cos_window : a tensor of [height, width]

    Then get the output:
    output : a tensor of [batch_size, 6]
    @endverbatim
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlSiamRPNPost(cnnlHandle_t handle,
                const cnnlSiamRPNPostDescriptor_t siam_rpn_post_desc,
                const cnnlTensorDescriptor_t desc_scores,
                const void *scores,
                const cnnlTensorDescriptor_t desc_delta,
                const void *delta,
                const cnnlTensorDescriptor_t desc_target_scale_ratio,
                const void *target_scale_ratio,
                const cnnlTensorDescriptor_t desc_anchor,
                const void *anchor,
                const cnnlTensorDescriptor_t desc_h_const,
                const void *h_const,
                const cnnlTensorDescriptor_t desc_w_const,
                const void *w_const,
                const cnnlTensorDescriptor_t desc_cos_window,
                const void *cos_window,
                void *workspace,
                size_t workspace_size,
                const cnnlTensorDescriptor_t desc_output,
                const void *output);

// Group:SiamRPNPost
/*!
 *  @brief Creates a descriptor pointed by \p desc
 *   for the SiamRPNPost operation and allocate memory for it.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[out]  desc
 *    Output. Descriptor of SiamRPNPost operation. The information of the operation
 *    is defined by ::cnnlSiamRPNPostDescriptor_t.
 *
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_ALLOC_FAILED
 *    The function failed to allocate memory space.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlCreateSiamRPNPostDescriptor(cnnlSiamRPNPostDescriptor_t *desc);

// Group:SiamRPNPost
/*!
 *  @brief Destroys the descriptor of SiamRPNPost
 *  and free memory for it.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[in]  desc
 *    Input. Descriptor of SiamRPNPost operation.
 *
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    The value of \p desc is NULL.
 *
 *  @par API Dependency
 *  - Before using this function, you need to call
 *  ::cnnlCreateSiamRPNPostDescriptor() function to create a descriptor.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlDestroySiamRPNPostDescriptor(cnnlSiamRPNPostDescriptor_t desc);

// Group:SiamRPNPost
/*!
 *  @brief Assigns SiamRPNPost descriptor with parameters.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[in,out] siam_rpn_post_desc
 *    Input/output. Descriptor of SiamRPNPost operation.
 *  @param[in]  stride
 *    Input. The size of stride in SiamRPNPost, used to generate anchors.
 *  @param[in]  penalty_k
 *    Input. The penalty coefficient in SiamRPNPost. It is a hyper-parameter.
 *  @param[in]  window_influence
 *    Input. It is used for filtering in spatial positions with cos_window. The formula is:
 *    scores2 = scores1 * (1.0 - window_influence) + cos_window * window_influence.
 *  @param[in]  lr_attr
 *    Input. A parameter for linear interpolation. After the final bounding box
 *    is selected, target size is updated by linear interpolation to keep the shape
 *    changing smoothly.
 *
 *  @retval CNNL_STATUS_SUCCESS
 *    The object was set successfully.
 *  @retval CNNL_STATUS_INVALIDPARAM
 *    Parameters are invalid.
 *
 *  @par API Dependency
 *  - Before using this function, you need to call
 *  ::cnnlCreateSiamRPNPostDescriptor() to create a descriptor.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlSetSiamRPNPostDescriptor(
    cnnlSiamRPNPostDescriptor_t siam_rpn_post_desc,
    const int stride,
    const float penalty_k,
    const float window_influence,
    const float lr_attr);

// Group:SiamRPNPost
/*!
 *  @brief Gets extra space size needed in SiamRPNPost
 *  operation.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[in]  handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *           SiamRPNPost operation.
 *  @param[in]  siam_rpn_post_desc
 *    Input. Descriptor of SiamRPNPost operation.
 *  @param[in]  desc_scores
 *    Input. Descriptor of \p scores tensor, containing dimensions and the layout of scores.
 *  @param[in]  desc_delta
 *    Input. Descriptor of \p delta tensor, containing dimensions and the layout of delta.
 *  @param[in]  desc_target_scale_ratio
 *    Input. Descriptor of \p target_scale_ratio tensor, containing dimensions and the layout of target_scale_ratio.
 *  @param[in]  desc_anchor
 *    Input. Descriptor of \p anchor tensor, containing dimensions and the layout of anchor.
 *  @param[in]  desc_h_const
 *    Input. Descriptor of \p h_const tensor, containing dimensions and the layout of h_const.
 *  @param[in]  desc_w_const
 *    Input. Descriptor of \p w_const tensor, containing dimensions and the layout of w_const.
 *  @param[in]  desc_cos_window
 *    Input. Descriptor of \p cos_window tensor, containing dimensions and the layout of cos_window.
 *  @param[out] size
 *    Output. Pointer to the host memory that stores the workspace size.
 *
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    The value of a descriptor or size is NULL.
 *
 *  @par API Dependency
 *  - Before using this function, you need to call
 *  ::cnnlCreateSiamRPNPostDescriptor to create a descriptor
 *  and call ::cnnlSetSiamRPNPostDescriptor to set its parameters.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlGetSiamRPNPostWorkspaceSize(cnnlHandle_t handle,
                                const cnnlSiamRPNPostDescriptor_t siam_rpn_post_desc,
                                const cnnlTensorDescriptor_t desc_scores,
                                const cnnlTensorDescriptor_t desc_delta,
                                const cnnlTensorDescriptor_t desc_target_scale_ratio,
                                const cnnlTensorDescriptor_t desc_anchor,
                                const cnnlTensorDescriptor_t desc_h_const,
                                const cnnlTensorDescriptor_t desc_w_const,
                                const cnnlTensorDescriptor_t desc_cos_window,
                                size_t *size);

/******************************************************************************
 * Cambricon CNNL OP: ctc greedy decoder
 ******************************************************************************/
/*!
 *  @brief The descriptor of the ::cnnlCtcGreedyDecoder operation.
 *  You need to call the ::cnnlCreateCtcGreedyDecoderDescriptor() function
 *  to create a descriptor, and call the ::cnnlSetCtcGreedyDecoderDescriptor()
 *  function to set the information to the descriptor.
 *  Also, you need to destroy the CtcGreedyDecoder descriptor at the end with the
 *  ::cnnlDestroyCtcGreedyDecoderDescriptor function.
 *
 *  It is deprecated and will be removed in future release.
 */
typedef struct cnnlCtcGreedyDecoderStruct *cnnlCtcGreedyDecoderDescriptor_t;

// Group:Ctc Greedy Decoder
/*!
 *  @brief Creates a ctc greedy decoder descriptor
 *   pointed by \p ctc_greedy_decoder_desc.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[out] ctc_greedy_decoder_desc
 *    Output.  A pointer to the struct of ctc greedy decoder descriptor.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p ctc_greedy_decoder_desc is NULL.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlCreateCtcGreedyDecoderDescriptor(cnnlCtcGreedyDecoderDescriptor_t *ctc_greedy_decoder_desc);

// Group:Ctc Greedy Decoder
/*!
 *  @brief Assigns ctc greedy decode descriptor with parameters.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[in,out]  ctc_greedy_decoder_desc
 *     Input/output. Descriptor of ctc greedy decode operation.
 *  @param[in]  merge_repeated
 *     Input. Bool type value indicating whether to remove the flag of duplicate label. Only support
 *            merge_repeated is true.
 *  @retval CNNL_STATUS_SUCCESS
 *     The function ends normally.
 *
 *  @par API Dependency
 *  - Before calling this function, you need to call ::cnnlCreateCtcGreedyDecoderDescriptor to
 *    create a descriptor.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlSetCtcGreedyDecoderDescriptor(cnnlCtcGreedyDecoderDescriptor_t ctc_greedy_decoder_desc,
                                  bool merge_repeated);

// Group:Ctc Greedy Decoder
/*!
 *  @brief Destroys ctc greedy decode descriptor.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[in] ctc_greedy_decoder_desc
 *    Input.  A pointer to the struct of ctc greedy decode descriptor.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_EXECUTION_FAILED
 *    \p ctc_greedy_decoder_desc is NULL.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlDestroyCtcGreedyDecoderDescriptor(cnnlCtcGreedyDecoderDescriptor_t ctc_greedy_decoder_desc);

// Group:Ctc Greedy Decoder
/*!
 * @brief Performs greedy decoding on the logits given in input.
 *
 * This operation performs with the following steps:
 *
 * 1. Compute index of the maximum value.
 *
 *    The length of input sequence is seq_len * num_classes and calculate the index of
 *    the maximum value for each num_clasess data.
 *
 * 2. Remap the maximum index of seq_len.
 *
 *    Each element is compared with its previous one. If the result is same, the
 *    element will be removed. Otherwise, it will be retained.
 *
 * 3. Remove delimiters from reserved elements.
 *
 * 4. Connect the remaining elements of each sequence in step 3 as output.
 *
 * @deprecated
 *   This function is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *          queues in the ctc greedy decoder operation.
 * @param[in] ctc_greedy_decoder_desc
 *   Input. Descriptor of ctc greedy decoder, which includes parameters of \p num_batch,
 *          \p num_classes, \p seq_len and \p merge_repeated.
 * @param[in] input_desc
 *   Input. Descriptor of input sequence. The dimension is [num_batch, seq_len,
 *          num_classes].
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input data.
 * @param[out] output_desc
 *   Output. Descriptor of output. The dimension is [num_batch, seq_len].
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output data.
 * @retval CNNL_STATUS_BAD_PARAM
 *   One or more of the following conditions must be met:
 *   - \p handle is NULL.
 *   - \p ctc_greedy_decoder_desc is NULL.
 *   - \p lib is NULL.
 *   - \p input_desc is NULL.
 *   - \p input is NULL.
 *   - \p output_desc is NULL.
 *   - \p distance is NULL.
 *   - \p output is NULL.
 *   - Tensor dimension or data type does not match.
 * @retval CNNL_STATUS_SUCCESS
 *   The function ends normally.
 *
 * @par Data Type
 *  - input: float.
 *  - output: float.
 *
 * @par Scale Limitation
 *  - Element number of input tensor cannot exceed INT32_MAX, i.e., 2147483647.
 *
 * @par Reference
 *  - http://distill.pub/2017/ctc/
 *  - http://gist.github.com/awni.56369a90d03953e370f3964c826ed4b0
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlCtcGreedyDecoder(cnnlHandle_t handle,
                     const cnnlCtcGreedyDecoderDescriptor_t ctc_greedy_decoder_desc,
                     const cnnlTensorDescriptor_t input_desc,
                     const void *input,
                     const cnnlTensorDescriptor_t output_desc,
                     void *output);

/******************************************************************************
 * Cambricon CNNL OP: Relative position multi-head attention.
 ******************************************************************************/
/*!
 * @brief Enumeration variables describing the inputs layouts used in multi-head attention.
 *
 */
typedef enum {
  CNNL_ATTN_TENSOR_LAYOUT_PACKED = 0,
  /*!< Data is stored as packed mode, where, dim == 3.*/

  CNNL_ATTN_TENSOR_LAYOUT_B_S_HN_HS = 1,
  /*!< Data is stored in [batch, sequence, head_num, head_size].*/

  CNNL_ATTN_TENSOR_LAYOUT_B_HN_S_HS = 2,
  /*!< Data is stored in [batch, head_num, sequence, head_size].*/

  CNNL_ATTN_TENSOR_LAYOUT_S_B_HN_HS = 3,
  /*!< Data is stored in [sequence, batch, head_num, head_size].*/

  CNNL_ATTN_TENSOR_LAYOUT_S_HN_B_HS = 4,
  /*!< Data is stored in [sequence, head_num, batch, head_size].*/

  CNNL_ATTN_TENSOR_LAYOUT_HN_B_S_HS = 5,
  /*!< Data is stored in [head_num, batch, sequence, head_size].*/

  CNNL_ATTN_TENSOR_LAYOUT_HN_S_B_HS = 6,
  /*!< Data is stored in [head_num, sequence, batch, head_size].*/
} cnnlAttentionTensorLayout_t;

/*!
 * @brief Enumeration variables describing the mode of mask used in multi-head attention.
 *
 */
typedef enum {
  CNNL_ATTN_MASK_NONE = 0,
  /*!< No mask is applied in the multi-head attention.*/

  CNNL_ATTN_MASK_NT = 1,
  /*!< The mask mode is NT, which represents that the shape of the mask is [batch, seq_len_k]. */

  CNNL_ATTN_MASK_TT = 2,
  /*!< The mask mode is TT,
   * which represents that the shape of the mask is [seq_len_q, seq_len_k].
   */

  CNNL_ATTN_MASK_NTT = 3,
  /*!< The mask mode is NTT,
   * which represents that the shape of the mask is [batch, seq_len_q, seq_len_k].
   */

  CNNL_ATTN_MASK_N = 4,
  /*!< The mask mode is N, which represents that the shape of the mask is [batch].
   * Each value of the mask tensor denotes the length of valid sequence.
   */

  CNNL_ATTN_MASK_NHTT = 5,
  /*!< The mask mode is NHTT, which represents that the shape of the mask is
   * [batch, head_num, seq_len_q, seq_len_k].
   */

  CNNL_ATTN_MASK_CAUSAL = 6,
  /*!< The mask mode is CAUSAL, which represents that the causal mask is applied internally.
   * A query at position q_index will only attend to keys between [0, q_index + seq_k - seq_q]
   * inclusive, where seq_k is the sequence length of key,
   * and seq_q is the sequence length of query.
   */

  CNNL_ATTN_MASK_CAUSAL_TOP_LEFT = 7,
  /*!< The mask mode is CAUSAL from top left, which represents that the causal mask is
   * applied internally.
   * A query at position q_index will only attend to keys between [0, q_index] inclusive.
   */

  CNNL_ATTN_MASK_LOCAL_TOP_LEFT = 8,
  /*!< The mask mode is LOCAL ATTENTION from top left, which represents that the local mask is
   * applied internally.
   * If window_size != (-1, -1), a query at position q_index will only attend to keys between
   * [q_index - window_size[0], q_index + window_size[1]] inclusive.
   */

  CNNL_ATTN_MASK_LOCAL_BOTTOM_RIGHT = 9,
  /*!< The mask mode is LOCAL ATTENTION from bottom right, which represents that the local mask is
   * applied internally.
   * If window_size != (-1, -1), a query at position q_index will only attend to keys between
   * [q_index + seq_k - seq_q - window_size[0], q_index + seq_k - seq_q + window_size[1]] inclusive,
   * where seq_k is the sequence length of key,
   * and seq_q is the sequence length of query.
   */
} cnnlAttentionMaskMode_t;

/*!
 * @brief Enumeration variables describing the implementation types of mask operation
 * of relative position multi-head attention.
 */
typedef enum {
  CNNL_REL_POS_ATTN_MASK_SOFTMAX_FILLZERO = 0,
  /*!< Mask tensor contains only 0 or -1, and mask operation is performed as:
       @verbatim
       scores = scores + mask * maskfill_value
       scores = softmax(scores)
       scores = scores * (mask + 1)
       @endverbatim
  */

  CNNL_REL_POS_ATTN_MASK_SOFTMAX = 1,
  /*!< Mask operation is performed as:
       @verbatim
       scores = scores + mask * maskfill_value
       scores = softmax(scores)
       @endverbatim
  */
} cnnlRelPosAttnMaskImplType_t;

/*! The descriptor of the ::cnnlRelPositionMultiHeadAttention operation.
 *
 *  You need to call the ::cnnlCreateRelPositionMultiHeadAttentionDescriptor() function
 *  to create a descriptor, and call the ::cnnlSetRelPositionMultiHeadAttentionDescriptor()
 *  function to set the input information to the descriptor.
 *  Also, you need to destroy the descriptor at the end with the
 *  ::cnnlDestroyRelPositionMultiHeadAttentionDescriptor() function.
 */
typedef struct cnnlRelPositionMultiHeadAttentionStruct
    *cnnlRelPositionMultiHeadAttentionDescriptor_t;

// Group:Relative Position Multi-head Attention
/*!
 *  @brief Creates a descriptor pointed by \p desc
 *   for the relative position multi-head attention operation.
 *  @param[out] desc
 *    Output. A pointer to the host memory that stores the relative position multi-head attention
 *            descriptor.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p desc is NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCreateRelPositionMultiHeadAttentionDescriptor(
    cnnlRelPositionMultiHeadAttentionDescriptor_t *desc);

// Group:Relative Position Multi-head Attention
/*!
 *  @brief Assigns the relative position multi-head attention
 *   descriptor with parameters.
 *
 *  @param[in,out] desc
 *    Input/output. The relative position multi-head attention descriptor to be set.
 *  @param[in] maskfill_value
 *    Input. The scalar value that will be multiplied with mask tensor before softmax operation.
 *           If \p mask_mode is CNNL_REL_POS_ATTN_MASK_SOFTMAX_FILLZERO,
 *           it should be a large positive value so that maskfill_value * mask is small
 *           enough to ensure that masked parts are zero after softmax operation.
 *  @param[in] mask_mode
 *    Input. Mask mode of relative position multi-head attention. Currently only CNNL_ATTN_MASK_NTT
 *           is supported.
 *  @param[in] mask_impl
 *    Input. Implementation mode of mask operation of relative position multi-head attention. If \p mask_impl
 *           is set to CNNL_REL_POS_ATTN_MASK_SOFTMAX, you are recommended to set \p maskfill_value to 1
 *           and pre-process mask tensor for better performance.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One of the following conditions are met:
 *      - \p desc is NULL.
 *      - One or more parameters are unsupported.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetRelPositionMultiHeadAttentionDescriptor(
    cnnlRelPositionMultiHeadAttentionDescriptor_t desc,
    float maskfill_value,
    cnnlAttentionMaskMode_t mask_mode,
    cnnlRelPosAttnMaskImplType_t mask_impl);

// Group:Relative Position Multi-head Attention
/*!
 *  @brief Sets TensorFloat-32 mode for a relative position multi-head
 *  attention with parameter.
 *
 *  @param[in] desc
 *    Input. The relative position multi-head attention descriptor to be set.
 *  @param[in] allow_tf32
 *    Input. The scalar value is used as a flag for whether to open TensorFloat-32 mode. It can only be
 *           0 or 1. 0 means not using TensorFloat-32 mode, while 1 means using TensorFloat-32 mode.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One of the following conditions are met:
 *      - \p desc is NULL.
 *      - One or more parameters are unsupported.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetRelPositionMultiHeadAttentionDescriptorAllowTF32(
    cnnlRelPositionMultiHeadAttentionDescriptor_t desc,
    int allow_tf32);

// Group:Relative Position Multi-head Attention
/*!
 *  @brief Destroys the descriptor of the relative
 *   position multi-head attention operation.
 *  @param[in] desc
 *    Input.  The relative position multi-head attention descriptor to be destroyed.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *   \p desc is NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlDestroyRelPositionMultiHeadAttentionDescriptor(
    cnnlRelPositionMultiHeadAttentionDescriptor_t desc);

// Group:Relative Position Multi-head Attention
/*!
 * @brief Handles the multi-head attention with relative position algorithm.
 *
 * This operation performs with the following steps:
 *
 * 1. Generate score
 *
 * q_with_bias_u = (query + pos_bias_u)
 *
 * q_with_bias_v = (query + pos_bias_v)
 *
 * matrix_ac = batchdot(q_with_bias_u, key)
 *
 * matrix_bd = batchdot(q_with_bias_v, key_relative_position)
 *
 * scores = softmax((matrix_ac + matrix_bd) / sqrt(head_size) + mask * op_desc->maskfill_value)
 *
 * if (op_desc->mask_impl == CNNL_REL_POS_ATTN_MASK_SOFTMAX_FILLZERO) {
 *   scores = scores * (mask + 1)
 * }
 *
 * 2. Compute attention
 *
 * qkv = batchdot(scores, value)
 *
 * 3. Compute linear layer
 *
 * output = matmul(qkv, out_filter) + out_bias
 *
 * @deprecated
 *   This function is deprecated and will be removed in future release.
 *   Use ::cnnlRelPositionMultiHeadAttention_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *          queues in the operation.
 * @param[in] op_desc
 *   Input. Descriptor of relative position multi-head attention operation.
 * @param[in] workspace
 *   Input. A pointer to the MLU memory that is used as an extra workspace. Reserved for future
 *          use.
 * @param[in] workspace_size
 *   Input. The size of extra space. Reserved for future use.
 * @param[in] query_desc
 *   Input. Descriptor of \p query tensor. Shape must be [batch, seq_len_q, head_num, head_size].
 * @param[in] query
 *   Input. A pointer to the MLU memory that stores the query data.
 * @param[in] key_desc
 *   Input. Descriptor of \p key tensor. Shape must be [batch, seq_len_kv, head_num, head_size].
 * @param[in] key
 *   Input. A pointer to the MLU memory that stores the key data.
 * @param[in] value_desc
 *   Input. Descriptor of \p value tensor. Shape must be [batch, seq_len_kv, head_num, head_size].
 * @param[in] value
 *   Input. A pointer to the MLU memory that stores the value data.
 * @param[in] key_relative_position_desc
 *   Input. Descriptor of \p key_relative_position tensor. Shape must be
 *          [seq_len_kv, head_num, head_size].
 * @param[in] key_relative_position
 *   Input. A pointer to the MLU memory that stores the key_relative_position data.
 * @param[in] value_relative_position_desc
 *   Input. Descriptor of \p value_relative_position tensor. Reserved for future use.
 * @param[in] value_relative_position
 *   Input. Reserved for future use.
 * @param[in] mask_desc
 *   Input. Descriptor of \p mask tensor. Shape must be [batch, seq_len_q, seq_len_kv].
 * @param[in] mask
 *   Input. A pointer to the MLU memory that stores the mask data. It must only contain 0 and -1,
 *          where 0 means corresponding token is valid, and -1 means corresponding token is meaningless
 *          padded token.
 * @param[in] position_bias_u_desc
 *   Input. Descriptor of \p position_bias_u tensor. Shape must be [head_num * head_size].
 * @param[in] position_bias_u
 *   Input. A pointer to the MLU memory that stores the position_bias_u data.
 * @param[in] position_bias_v_desc
 *   Input. Descriptor of \p position_bias_v tensor. Shape must be [head_num * head_size].
 * @param[in] position_bias_v
 *   Input. A pointer to the MLU memory that stores the position_bias_v data.
 * @param[in] out_filter_desc
 *   Input. Descriptor of \p out_filter tensor. Shape must be
 *          [head_num * head_size, head_num * head_size].
 * @param[in] out_filter
 *   Input. A pointer to the MLU memory that stores the filter data of linear layer.
 * @param[in] out_bias_desc
 *   Input. Descriptor of \p out_bias tensor. Shape must be [head_num * head_size].
 * @param[in] out_bias
 *   Input. A pointer to the MLU memory that stores the bias data of linear layer.
 * @param[out] output_desc
 *   Input. Descriptor of \p output tensor. Shape must be [batch, seq_len_q, head_num, head_size].
 * @param[out] output
 *   Input. A pointer to the MLU memory that stores the output data.
 * @retval CNNL_STATUS_BAD_PARAM
 *   One or more of the following errors occurred:
 *     - One or more required arguments are NULL.
 *     - Scale limitation is not satisfied.
 * @retval CNNL_STATUS_ARCH_MISMATCH
 *   One or more of the following errors occurred:
 *     - Device type is not MLU300 series.
 *     - Device has less than 4 visible clusters.
 *     - Device has less or more than 4 cores per cluster.
 *     - Device does not support Union4 task.
 * @retval CNNL_STATUS_SUCCESS
 *   The function ends normally.
 *
 * @par Data Type
 *  - All inputs, filter, and outputs must have the same data type, either half or float.
 *
 * @par Scale Limitation
 *   - 0 < batch
 *   - 0 < seq_len_q <= 512
 *   - 0 < seq_len_kv <= 512
 *   - head_num = 4 or 8 or 12 or 16
 *   - 0 < head_size <= 64
 *
 * @par Reference
 *   - https://github.com/wenet-e2e/wenet/blob/main/wenet/transformer/attention.py
 */
CNNL_DEPRECATED_FOR(cnnlRelPositionMultiHeadAttention_v2)
cnnlStatus_t CNNL_WIN_API
cnnlRelPositionMultiHeadAttention(
    const cnnlHandle_t handle,
    const cnnlRelPositionMultiHeadAttentionDescriptor_t op_desc,
    void *workspace,
    size_t workspace_size,
    const cnnlTensorDescriptor_t query_desc,
    const void *query,
    const cnnlTensorDescriptor_t key_desc,
    const void *key,
    const cnnlTensorDescriptor_t value_desc,
    const void *value,
    const cnnlTensorDescriptor_t key_relative_position_desc,
    const void *key_relative_position,
    const cnnlTensorDescriptor_t value_relative_position_desc,
    const void *value_relative_position,
    const cnnlTensorDescriptor_t mask_desc,
    const void *mask,
    const cnnlTensorDescriptor_t position_bias_u_desc,
    const void *position_bias_u,
    const cnnlTensorDescriptor_t position_bias_v_desc,
    const void *position_bias_v,
    const cnnlTensorDescriptor_t out_filter_desc,
    const void *out_filter,
    const cnnlTensorDescriptor_t out_bias_desc,
    const void *out_bias,
    const cnnlTensorDescriptor_t output_desc,
    void *output);

// Group:Relative Position Multi-head Attention
/*!
 * @brief Handles the multi-head attention with relative position algorithm.
 *
 * Compared with ::cnnlRelPositionMultiHeadAttention, this function has two main differences:
 * firstly, the layouts of \p query, \p key and \p value change from [batch, seq_len_q, head_num, head_size]
 * to [batch, head_num, seq_len_q, head_size], and the layout of \p key_relative_position changes
 * from [seq_len_kv, head_num, head_size] to [head_num, seq_len_kv, head_size]; secondly, the final
 * linear layer is removed from this function.
 *
 * This operation performs the following steps:
 *
 * 1. Generate score
 *
 * q_with_bias_u = (query + pos_bias_u)
 *
 * q_with_bias_v = (query + pos_bias_v)
 *
 * matrix_ac = batchdot(q_with_bias_u, key)
 *
 * matrix_bd = batchdot(q_with_bias_v, key_relative_position)
 *
 * scores = softmax((matrix_ac + matrix_bd) / sqrt(head_size) + mask * op_desc->maskfill_value)
 *
 * if (op_desc->mask_impl == CNNL_REL_POS_ATTN_MASK_SOFTMAX_FILLZERO) {
 *   scores = scores * (mask + 1)
 * }
 *
 * 2. Compute attention
 *
 * qkv = batchdot(scores, value).transpose(1, 2)
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *          queues in the operation.
 * @param[in] op_desc
 *   Input. Descriptor of relative position multi-head attention operation.
 * @param[in] workspace
 *   Input. A pointer to the MLU memory that is used as an extra workspace. Reserved for future
 *          use.
 * @param[in] workspace_size
 *   Input. The size of extra space. Reserved for future use.
 * @param[in] query_desc
 *   Input. Descriptor of \p query tensor. Shape must be [batch, head_num, seq_len_q, head_size].
 * @param[in] query
 *   Input. A pointer to the MLU memory that stores the query data.
 * @param[in] key_desc
 *   Input. Descriptor of \p key tensor. Shape must be [batch, head_num, seq_len_kv, head_size].
 * @param[in] key
 *   Input. A pointer to the MLU memory that stores the key data.
 * @param[in] value_desc
 *   Input. Descriptor of \p value tensor. Shape must be [batch, head_num, seq_len_kv, head_size].
 * @param[in] value
 *   Input. A pointer to the MLU memory that stores the value data.
 * @param[in] key_relative_position_desc
 *   Input. Descriptor of \p key_relative_position tensor. Shape must be
 *          [head_num, seq_len_kv, head_size].
 * @param[in] key_relative_position
 *   Input. A pointer to the MLU memory that stores the key_relative_position data.
 * @param[in] value_relative_position_desc
 *   Input. Descriptor of \p value_relative_position tensor. Reserved for future use.
 * @param[in] value_relative_position
 *   Input. Reserved for future use.
 * @param[in] mask_desc
 *   Input. Descriptor of \p mask tensor. Shape must be [batch, seq_len_q, seq_len_kv].
 * @param[in] mask
 *   Input. A pointer to the MLU memory that stores the mask data. It must only contain 0 and -1,
 *          where 0 means corresponding token is valid, -1 means corresponding token is meaningless
 *          padded token.
 * @param[in] position_bias_u_desc
 *   Input. Descriptor of \p position_bias_u tensor. Shape must be [head_num * head_size].
 * @param[in] position_bias_u
 *   Input. A pointer to the MLU memory that stores the position_bias_u data. This bias would be
 *          added into \p query before computing batchdot between \p query and \p key.
 * @param[in] position_bias_v_desc
 *   Input. Descriptor of \p position_bias_v tensor. Shape must be [head_num * head_size].
 * @param[in] position_bias_v
 *   Input. A pointer to the MLU memory that stores the position_bias_v data. This bias would be
 *          added into \p query before computing batchdot between \p query and
 *          \p key_relative_position.
 * @param[out] output_desc
 *   Input. Descriptor of \p output tensor. Shape must be [batch, seq_len_q, head_num, head_size].
 * @param[out] output
 *   Input. A pointer to the MLU memory that stores the output data.
 * @retval CNNL_STATUS_BAD_PARAM
 *   One or more of the following errors occurred:
 *     - One or more required arguments are NULL.
 *     - Scale limitation is not satisfied.
 * @retval CNNL_STATUS_ARCH_MISMATCH
 *   One or more of the following errors occurred:
 *     - Device type is not one of MLU500 series.
 *     - Device has less or more than 4 cores per cluster.
 * @retval CNNL_STATUS_SUCCESS
 *   The function ends normally.
 *
 * @par Data Type
 *   - All inputs, filter, and outputs must have the same data type, either half or float.
 *
 * @par Scale Limitation
 *   - Only MLU500 series are supported.
 *   - 0 < batch
 *   - 0 < seq_len_q <= 512
 *   - 0 < seq_len_kv <= 512
 *   - 0 < head_num <= 16
 *   - 0 < head_size <= 64
 *
 * @par Reference
 * - https://github.com/wenet-e2e/wenet/blob/main/wenet/transformer/attention.py
 */
cnnlStatus_t CNNL_WIN_API
cnnlRelPositionMultiHeadAttention_v2(
    const cnnlHandle_t handle,
    const cnnlRelPositionMultiHeadAttentionDescriptor_t op_desc,
    void *workspace,
    size_t workspace_size,
    const cnnlTensorDescriptor_t query_desc,
    const void *query,
    const cnnlTensorDescriptor_t key_desc,
    const void *key,
    const cnnlTensorDescriptor_t value_desc,
    const void *value,
    const cnnlTensorDescriptor_t key_relative_position_desc,
    const void *key_relative_position,
    const cnnlTensorDescriptor_t value_relative_position_desc,
    const void *value_relative_position,
    const cnnlTensorDescriptor_t mask_desc,
    const void *mask,
    const cnnlTensorDescriptor_t position_bias_u_desc,
    const void *position_bias_u,
    const cnnlTensorDescriptor_t position_bias_v_desc,
    const void *position_bias_v,
    const cnnlTensorDescriptor_t output_desc,
    void *output);

/******************************************************************************
 * Cambricon CNNL OP: PadPackedSequence
 ******************************************************************************/
/*! The descriptor of the ::cnnlPadPackedSequence operation.
 * You can use ::cnnlCreatePadPackedSequenceDescriptor() and
 * ::cnnlDestroyPadPackedSequenceDescriptor() to create and destroy the descriptor
 * respectively.
 */
typedef struct cnnlPadPackedSequenceStruct *cnnlPadPackedSequenceDescriptor_t;

// Group:PadPackedSequence
/*!
 *  @brief Creates a descriptor pointed by \p desc
 *   for the pad_packed_sequence operation.
 *  @param[out]  desc
 *    Output. Descriptor of the pad_packed_sequence operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_ALLOC_FAILED
 *    The function fails to allocate memory.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCreatePadPackedSequenceDescriptor(cnnlPadPackedSequenceDescriptor_t *desc);

// Group:PadPackedSequence
/*!
 *  @brief Assigns the the pad_packed_sequence descriptor with parameters.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[in,out]  desc
 *    Input/output. Descriptor of the pad_packed_sequence operation.
 *  @param[in]  enforce_sorted
 *    Input. A Boolean value indicating whether to enforce the seq_lengths to be
 *    sorted.
 *    - True: \p seq_lengths is enforced to be sorted.
 *    - False: \p seq_lengths is not enforced to be sorted.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    The value of \p desc is NULL.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlSetSortedPadPackedSequenceDescriptor(cnnlPadPackedSequenceDescriptor_t *desc,
                                         bool enforce_sorted);

// Group:PadPackedSequence
/*!
 *  @brief Destroys the descriptor of the pad_packed_sequence operation.
 *
 *  @param[in]  desc
 *    Input. Descriptor of the pad_packed_sequence operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    The value of desc is NULL.
 */

cnnlStatus_t CNNL_WIN_API
cnnlDestroyPadPackedSequenceDescriptor(cnnlPadPackedSequenceDescriptor_t desc);

// Group:PadPackedSequence
/*!
 *  @brief Pads the sequence length to \p max_seq_len with the specified pad
 *  values.
 *
 *  This operation performs with the following steps:
 *
 *  1. Pad \p seq_len to \p max_seq_len with pad value if \p max_seq_len is not less than \p seq_len.
 *
 *  2. Output \p max_seq_len if \p max_seq_len is less than \p seq_len.
 *
 *  @param[in]  handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *           pad_packed_sequence operation.
 *  @param[in]  desc
 *    Input. The descriptor of pad_packed_sequence operation.
 *  @param[in]  packed_tensor_desc
 *    Input. The descriptor of packed_tensor.
 *  @param[in]  packed_tensor
 *    Input. Pointer to the MLU memory that stores the packed data.
 *  @param[in]  pad_value_desc
 *    Input. The descriptor of pad_value tensor.
 *  @param[in]  pad_value
 *    Input. Pointer to the MLU memory that stores the padding value.
 *  @param[in]  seq_lengths_desc
 *    Input. The descriptor of seq_lengths tensor.
 *  @param[in,out]  seq_lengths
 *    Input. Pointer to the MLU memory that stores the sequence length.
 *  @param[in]  batch_sizes_desc
 *    Input. The descriptor of batch_sizes tensor.
 *  @param[in]  batch_sizes
 *    Input. Pointer to the MLU memory that stores the batch sizes.
 *  @param[in]  unsorted_indices_desc
 *    Input. The descriptor of unsorted_indices tensor.
 *  @param[in]  unsorted_indices
 *    Input. Pointer to the MLU memory that stores the unsorted indices.
 *  @param[in]  workspace
 *    Input. The pointer to data address of workspace.
 *  @param[in]  workspace_size
 *    Input. The size of workspace.
 *  @param[out]  padded_tensor_desc
 *    Output. An array of cnnlTensorDescriptors which holds dimension, data type and layout of
 *    padded_tensor. The shape of padded_tensor is [batch, max_seq_len, hidden_size].
 *  @param[out]  padded_tensor
 *    Output. Pointer to the MLU memory that stores the output data.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are encountered:
 *    - \p handle is NULL.
 *    - \p packed_tensor is NULL.
 *    - \p pad_value is NULL.
 *    - \p padded_tensor is NULL.
 *    - Dims are invalid.
 *
 *  @par Data Type
 *  - packed_tensor: float, half, int16, int8.
 *  - pad_value: float, half, int16, int8.
 *  - seq_lengths: int32.
 *  - batch_sizes: int32.
 *  - unsorted_indices: int32.
 *  - output: float, half, int16, int8.
 *
 *  @par Scale Limitation
 *  - Mode 0:
 *    - \p hidden_size supports [1, 1024].
 *    - \p max_seq_len is not less than \p seq_length in any batch.
 *  - Mode 1:
 *    - \p batch_num supports [1, 12288].
 *    - \p max_seq_len supports [1, 24576].
 *    - \p hidden_size supports [1, 24576].
 *    - \p max_seq_len is not less than \p seq_length in any batch.
 *
 *  @par Example
    @verbatim
    Mode 0:

    Dimension of packed_tensor: [1, valid_word_num, hidden_size].

    Dimension of pad_value: [1].

    Dimension of seq_lengths: [batch_num].

    Dimension of batch_sizes: [max_seq_len].

    Dimension of unsorted_indices: [batch_num].

    Dimension of output: [batch_num, max_seq_len, hidden_size].

    Mode 1:

    Dimension of packed_tensor: [valid_word_num, 1, hidden_size].

    Dimension of pad_value: [1].

    Dimension of batch_sizes: [batch_sizes_len].

    Dimension of unsorted_indices: [batch_num].

    Dimension of seq_lengths: [batch_num].

    Dimension of padded_tensor: [batch_num, total_length, hidden_size] or
                                 [total_length, batch_num, hidden_size].
    @endverbatim
 */
cnnlStatus_t CNNL_WIN_API
cnnlPadPackedSequence(
    cnnlHandle_t handle,
    const cnnlPadPackedSequenceDescriptor_t desc,
    const cnnlTensorDescriptor_t packed_tensor_desc,
    const void *packed_tensor,
    const cnnlTensorDescriptor_t pad_value_desc,
    const void *pad_value,
    const cnnlTensorDescriptor_t seq_lengths_desc,
    void *seq_lengths,
    const cnnlTensorDescriptor_t batch_sizes_desc,
    const void *batch_sizes,
    const cnnlTensorDescriptor_t unsorted_indices_desc,
    const void *unsorted_indices,
    const cnnlTensorDescriptor_t padded_tensor_desc,
    void *padded_tensor,
    void *workspace,
    size_t workspace_size);

// Group:PadPackedSequence
/*!
 *  @brief Retrieves the extra space size needed in the pad_packed_sequence operation
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *           pad_packed_sequence operation.
 *  @param[in] desc
 *    Input. The descriptor of pad_packed_sequence operation.
 *  @param[out] size
 *    Output. Extra space size needed in the operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    The handle is NULL.
 *  @par API Dependence
 *  - Before using this function, you need to call ::cnnlCreatePadPackedSequenceDescriptor to create a
 *  descriptor and call ::cnnlSetSortedPadPackedSequenceDescriptor to set the descriptor.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetPadPackedSequenceWorkspaceSize(cnnlHandle_t handle,
                                      const cnnlPadPackedSequenceDescriptor_t desc,
                                      size_t *size);
/******************************************************************************
 * Cambricon CNNL OP: Pq Search
 ******************************************************************************/

// Group:Ivfpq
/*!
 *  @brief This function is one part of the IVFPQ (Invert File System Product Quantization) algorithm.
 *  IVFPQ is used to search and select the nearest feature vectors from the database based on query
 *  vectors. It contains three sub-operators: flat search, residual-computing and pq search.
 *  This function implements the pq search with residual-computing and the pq search without
 *  residual-computing algorithm on MLU.
 *
 *  This function performs with the following steps:
 *
 *  1. Reorder(optional)
 *
 *    Arrange nprobe buckets in descending order according to the number of vectors in each bucket.
 *
 *  2. Compute residual(optional)
 *
 *    Subtract coarse centers from the query vector to obtain the corresponding
 *    residual vectors.
 *
 *  3. Generate look-up table.
 *
 *    Compute L2-distance between input query (or residual query) and input code_book,
 *    then store the distance as the look-up table:
 *
 *      look_up_table = computeL2Distance(query, code_book).
 *
 *  4. Search and find nearest k-features from lib.
 *
 *    In previous step of IVFPQ training, original database is quantified into [lib] based on
 *    [code_book]. [lib] consists of [nlist] coarse centers. Flat search selects
 *    the most relevant [nprobe] centers from the [lib], using the input [query].
 *    This operation computes the distance between [residual_query] and features related to those selected
 *    centers, and output the indices and distances of the nearest k-features from [ntotal] features.
 *
 *      topk_output = searchAndTopk(look_up_table, lib)
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *           IVFPQ operation.
 *  @param[in] query_desc
 *    Input. Descriptor of query. The dimension of query is [batch, D].
 *  @param[in] query
 *    Input. Pointer to the MLU memory that stores the query data.
 *  @param[in] residual_query_desc
 *    Input. Descriptor of residual query. The dimension of residual_query is [nprobe, D].
 *  @param[in] residual_query
 *    Input. Pointer to the MLU memory that stores the residual query data.
 *  @param[in] coarse_center_desc
 *    Input. Descriptor of coarse center. The dimension of coarse_center is [nlist, D].
 *  @param[in] coarse_center
 *    Input. Pointer to the MLU memory that stores the coarse center data.
 *  @param[in] code_book_desc
 *    Input. Descriptor of codebook. The dimension of code_book is [ksub, m, dsub].
 *  @param[in] code_book
 *    Input. Pointer to the MLU memory that stores the codebook data.
 *  @param[in] lib_index_size_desc
 *    Input. Descriptor of the size of nlist buckets. The dimension of lib_index_size is [nlist].
 *  @param[in] lib_index_size
 *    Input. Pointer to the MLU memory that stores the nlist bucket size data.
 *  @param[in] lib_desc
 *    Input. Descriptor of the lib data of nlist bucket. The dimension of lib is [nlist].
 *  @param[in] lib
 *    Input. Pointer to the MLU memory that stores the lib data. \p lib is the database that contains all
 *           feature vectors to search. \p lib is divided into [nlist] segments with different sizes
 *           during training. The size of every lib segments must be equal to the product of
 *           m and the size of corresponding segment of lib_index_size.
 *  @param[in] lib_index_desc
 *    Input. Descriptor of lib index. The dimension of lib_index is [nlist].
 *  @param[in] lib_index
 *    Input. Pointer to the MLU memory that stores the lib index data. \p lib_index stores the index of
 *           every lib feature in the original database. Each feature has a unique index.
 *           \p lib_index is also divided into [nlist] segments according to the partition of lib.
 *           \p lib_index_size is defined to store the size of every lib_index segments.
 *  @param[in] probe_index_desc
 *    Input. Descriptor of probe index. The dimension of probe_index is [batch, nprobe].
 *  @param[in] probe_index
 *    Input. Pointer to the MLU memory that stores the probe index data. \p probe_index stores the indices of
 *           selected segments from coarse center.
 *  @param[in] workspace
 *    Input. Pointer to the MLU memory that stores the workspace.
 *  @param[in] workspace_size
 *    Input. The size of workspace.
 *  @param[out] topk_output_value_desc
 *    Output. Descriptor of topk output value. The dimension of topk_output_value is
 *            [batch, topk_k].
 *  @param[out] topk_output_value
 *    Output. Pointer to the MLU memory that stores the topk output value data. \p topk_output_value stores
 *            the nearest topk_k distances with respect to query.
 *  @param[out] topk_output_index_desc
 *    Output. Descriptor of topk output index. The dimension of topk_output_index is
 *            [batch, topk_k].
 *  @param[out] topk_output_index
 *    Output. Pointer to the MLU memory that stores the topk output index data. \p topk_output_index stores
 *            the indexes of topk_k nearest features from lib_index.
 *  @param[in] search_mode
 *    Input. The search mode. \p search_mode can be 0 (pq_search_with_residual),
 *           1 (pq_search_without_residual), or 2 (ivfpq_search_with_residual).
 *  @param[in] ntotal
 *    Input. The total number of features in lib.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ended normally.
 *  @retval CNNL_STATUS_ARCH_MISMATCH
 *    - MLU task type is not supported (currently only union1/2/4/8 task is supported).
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions must be met:
 *    - \p handle is NULL.
 *    - \p query_desc is NULL.
 *    - \p query is NULL.
 *    - \p residual_query_desc is NULL.
 *    - \p residual_query is NULL.
 *    - \p coarse_center_desc is NULL.
 *    - \p coarse_center is NULL.
 *    - \p code_book_desc is NULL.
 *    - \p code_book is NULL.
 *    - \p lib_index_size_desc is NULL.
 *    - \p lib_index_size is NULL.
 *    - \p lib_desc is NULL.
 *    - \p lib is NULL.
 *    - \p lib_index_desc is NULL.
 *    - \p lib_index is NULL.
 *    - \p nprobe_index_desc is NULL.
 *    - \p nprobe_index is NULL.
 *    - \p topk_output_value_desc is NULL.
 *    - \p topk_output_value is NULL.
 *    - \p topk_output_index_desc is NULL.
 *    - \p topk_output_index is NULL.
 *    - \p search_mode is not equal to 0/1/2.
 *    - tensor dim does not match.
 *
 *  @par Data Type
 *  - Input: float(query), float(residual_query), float(coarse_center), float(code_book).
 *  - Input: int32(lib_index_size) int32(lib), int32(lib_index).
 *  - Input: int32(probe_index), int32(search_mode).
 *  - Output: float(topk_output_value), int32(topk_output_index).
 *
 *  @par Scale Limitation
 *  - ntotal represents the number of features in lib, and must be no larger than
 *    (pow(2,30) - 1) / 32.
 *  - nlist represents the number of segments that lib is divided into during training, and must be
 *    no larger than [ntotal].
 *  - code_book_desc->dims[0] represents the number of cluster centers(ksub), and must be 256.
 *  - query_desc->dims[1] represents the length of query and code_book vector(D),
 *    and can be 256, 512, 768, or 1024.
 *  - code_book_desc->dims[1] represents the number of segments of code_book vector(m),
 *    and can be 32 and 64.
 *  - code_book_desc->dims[2] represents the sub segment length of code_book(dsub).
 *    The product of m and dsub must be equal to D.
 *  - Users must cnrtMalloc np * D * sizeof(float) memory in device for residual_query at least,
 *    which means np must be the maximum of nprobe and the number of valid cores of
 *    the current board.
 *  - When search_mode is 0 or 1, nlist and nprobe must be 1.
 *  - When search_mode is 2, both nlist and nprobe must be greater than 1,
 *    and nlist must be greater than nprobe.
 *
 *  @par API Dependency
 *  - The function ::cnnlGetPqSearchWorkspaceSize() needs to be called
 *    before this function.
 *
 *  @par Reference
 *  - http://github.com/facebookresearch/faiss
 */
cnnlStatus_t CNNL_WIN_API
cnnlPqSearch_v2(cnnlHandle_t handle,
                const cnnlTensorDescriptor_t query_desc,
                const void *query,
                const cnnlTensorDescriptor_t residual_query_desc,
                void *residual_query,
                const cnnlTensorDescriptor_t coarse_center_desc,
                const void *coarse_center,
                const cnnlTensorDescriptor_t code_book_desc,
                const void *code_book,
                const cnnlTensorDescriptor_t lib_index_size_desc,
                const void *lib_index_size,
                const cnnlTensorDescriptor_t lib_desc,
                const void **lib,
                const cnnlTensorDescriptor_t lib_index_desc,
                const void **lib_index,
                const cnnlTensorDescriptor_t probe_index_desc,
                void *probe_index,
                void *workspace,
                size_t workspace_size,
                const cnnlTensorDescriptor_t topk_output_value_desc,
                void *topk_output_value,
                const cnnlTensorDescriptor_t topk_output_index_desc,
                void *topk_output_index,
                int search_mode,
                int ntotal);

// Group:Ivfpq
/*!
 * @brief Retrieves the extra space size needed in PqSearch operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *          queues in the PqSearch operation.
 * @param[in] probe_index_desc
 *   Input. Descriptor of probe_index tensor. The dimension of \p probe_index is [batch, nprobe].
 * @param[in] topk_output_value_desc
 *   Input. Descriptor of topk_output_value tensor. The dimension of \p topk_output_value is [batch, topk_k].
 * @param[in] topk_output_index_desc
 *   Input. Descriptor of topk_output_index tensor. The dimension of \p topk_output_index is [batch, topk_k].
 * @param[out] workspace_size
 *   Output. Pointer to the MLU memory that stores the extra space size (bytes)
 *   needed in PqSearch operation.
 *  @param[in] search_mode
 *    Input. The search mode. \p search_mode can be 0 (pq_search_with_residual),
 *           1 (pq_search_without_residual), or 2 (ivfpq_search_with_residual).
 * @retval CNNL_STATUS_SUCCESS
 *   The function ended normally.
 * @retval CNNL_STATUS_BAD_PARAM
 *   One or more of the following conditions must be met:
 *   - \p handle is NULL.
 *   - \p probe_index_desc is NULL.
 *   - \p topk_output_value_desc is NULL.
 *   - \p topk_output_index_desc is NULL.
 *   - \p workspace_size is NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetPqSearchWorkspaceSize(cnnlHandle_t handle,
                             const cnnlTensorDescriptor_t probe_index_desc,
                             const cnnlTensorDescriptor_t topk_output_value_desc,
                             const cnnlTensorDescriptor_t topk_output_index_desc,
                             size_t *workspace_size,
                             int search_mode);

/******************************************************************************
 * Cambricon CNNL OP: PackPaddedSequence
 ******************************************************************************/
/*! The descriptor of the ::cnnlPackPaddedSequence operation.
 *
 * You can use ::cnnlCreatePackPaddedSequenceDescriptor() and
 * ::cnnlDestroyPackPaddedSequenceDescriptor() to create and destroy the descriptor
 * respectively.
 */
typedef struct cnnlPackPaddedSequenceStruct *cnnlPackPaddedSequenceDescriptor_t;

// Group:PackPaddedSequence
/*!
 *  @brief Creates a descriptor pointed by \p desc
 *   for the pack_padded_sequence operation.
 *  @param[out]  desc
 *    Output. Descriptor of the pack_padded_sequence operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_ALLOC_FAILED
 *    The function fails to allocate memory for the descriptor.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCreatePackPaddedSequenceDescriptor(cnnlPackPaddedSequenceDescriptor_t *desc);

// Group:PackPaddedSequence
/*!
 *  @brief Assigns the pack_padded_sequence descriptor with parameters.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[in,out]  desc
 *    Input/output. Descriptor of the pack_padded_sequence operation.
 *  @param[in]  enforce_sorted
 *    Input. A Boolean value indicating whether to enforce the seq_lengths to be
 *    sorted.
 *    - True: The \p seq_lengths is enforced to be sorted.
 *    - False: The \p seq_lengths is not enforced to be sorted.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    The value of \p desc is NULL.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlSetSortedPackPaddedSequenceDescriptor(cnnlPackPaddedSequenceDescriptor_t *desc,
                                          bool enforce_sorted);

// Group:PackPaddedSequence
/*!
 *  @brief Destroys the descriptor of
 *   the pack_padded_sequence operation.
 *  @param[in]  desc
 *    Input. Descriptor of the pack_padded_sequence operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    - The value of desc is NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlDestroyPackPaddedSequenceDescriptor(cnnlPackPaddedSequenceDescriptor_t desc);

// Group:PackPaddedSequence
/*!
 *  @brief Gets valid tokens of a padded tensor and rearranges to a packed tensor.
 *
 *  It packs \p seq_lengths to \p seq_len if \p seq_lengths is not more than \p seq_len.
 *
 *  @param[in]  handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *           pack_padded_sequence operation.
 *  @param[in]  desc
 *    Input. A descriptor of pack_padded_sequence operation.
 *  @param[in]  padded_tensor_desc
 *    Input. A descriptor of padded_tensor.
 *  @param[in]  padded_tensor
 *    Input. Pointer to the MLU memory that stores the padded data.
 *  @param[in]  seq_lengths_desc
 *    Input. A descriptor of seq_lengths tensor.
 *  @param[in]  seq_lengths
 *    Input. Pointer to the MLU memory that stores the sequence lengths.
 *  @param[in]  batch_sizes_desc
 *    Input. A descriptor of batch_sizes tensor. This parameter is reserved.
 *  @param[out]  batch_sizes
 *    Output. Pointer to the MLU memory that stores the batch sizes. This parameter is reserved.
 *  @param[in]  sorted_indices_desc
 *    Input. A descriptor of sorted_indices tensor. This parameter is reserved.
 *  @param[out]  sorted_indices
 *    Output. Pointer to the MLU memory that stores the sorted indices. This parameter is reserved.
 *  @param[in]  unsorted_indices_desc
 *    Input. A descriptor of unsorted_indices tensor. This parameter is reserved.
 *  @param[out]  unsorted_indices
 *    Output. Pointer to the MLU memory that stores the unsorted indices. This parameter is reserved.
 *  @param[in]  workspace
 *    Input. The pointer to data address of workspace.
 *  @param[in]  workspace_size
 *    Input. The size of workspace.
 *  @param[in]  packed_tensor_desc
 *    Input. An array of cnnlTensorDescriptors which holds dimension, data type and layout of
 *    packed_tensor. The shape of packed_tensor is [1, valid_word_num, hidden_size].
 *  @param[out]  packed_tensor
 *    Output. Pointer to the MLU memory that stores the packed tensors.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are encountered:
 *    - \p handle is NULL.
 *    - \p packed_tensor is NULL.
 *    - \p padded_tensor is NULL.
 *    - Dims are invalid.
 *
 * @par Data Type
 *  - padded_tensor: float, half, int16, int8.
 *  - seq_lengths: int32.
 *  - batch_sizes: int32.
 *  - sorted_indices: int32.
 *  - unsorted_indices: int32.
 *  - packed_tensor: float, half, int16, int8.
 *
 *  @note
 *  - The data types of padded_tensor and packed_tensor must be the same.
 *  - The dimensions of padded_tensor and pakced_tensor must be the same, which
 *    is three-dimensional.
 *  - The layouts of padded_tensor and packed_tensor must be same in mode 0, which
 *    is NTC layout.
 *
 *  @par Scale Limitation
 *  - Mode 0:
 *    - \p batch_num supports [1, 1024].
 *    - \p seq_len is not less than seq_lengths in any batch.
 *  -  Mode 1:
 *    - \p batch_num supports [1, 12288].
 *    - \p seq_len supports [1, 24576].
 *    - \p hidden_size supports [1, 24576].
 *    - \p valid_word_num is not less than \p batch_num.
 *    - \p seq_len is not less than \p seq_lengths in any batch.
 *  @par Example
    @verbatim
    Mode 0:
    Dimension of padded_tensor: [batch_num, seq_len, hidden_size].

    Dimension of seq_lengths: [batch_num].

    Dimension of batch_sizes: [seq_len].

    Dimension of sorted_indices: [batch_num].

    Dimension of unsorted_indices: [batch_num].

    Dimension of packed_tensor: [1, valid_word_num, hidden_size].

    Mode 1:
    Dimension of padded_tensor: [batch_num, seq_len, hidden_size] or
                                [seq_len, batch_num, hidden_size].

    Dimension of seq_lengths: [batch_num].

    Dimension of batch_sizes: [seq_len].

    Dimension of sorted_indices: [batch_num].

    Dimension of unsorted_indices: [batch_num].

    Dimension of packed_tensor: [valid_word_num, 1, hidden_size].
    @endverbatim
 */
cnnlStatus_t CNNL_WIN_API
cnnlPackPaddedSequence(
    cnnlHandle_t handle,
    const cnnlPackPaddedSequenceDescriptor_t desc,
    const cnnlTensorDescriptor_t padded_tensor_desc,
    const void *padded_tensor,
    const cnnlTensorDescriptor_t seq_lengths_desc,
    const void *seq_lengths,
    const cnnlTensorDescriptor_t packed_tensor_desc,
    void *packed_tensor,
    const cnnlTensorDescriptor_t batch_sizes_desc,
    void *batch_sizes,
    const cnnlTensorDescriptor_t sorted_indices_desc,
    void *sorted_indices,
    const cnnlTensorDescriptor_t unsorted_indices_desc,
    void *unsorted_indices,
    void *workspace,
    size_t workspace_size);

// Group:PackPaddedSequence
/*!
 *  @brief Retrieves extra space size needed in the pack_padded_sequence operation.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *           pack_padded_sequence operation.
 *  @param[in] desc
 *    Input. A descriptor of the pack_padded_sequence operation.
 *  @param[out] size
 *    Output. Extra space size needed in the pack_padded_sequence operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    The handle is NULL.
 *
 *  @par API Dependency
 *  - Before using this function, you need to use ::cnnlCreatePackPaddedSequenceDescriptor
 *   to create a descriptor and use ::cnnlSetSortedPackPaddedSequenceDescriptor to set
 *   the parameter.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetPackPaddedSequenceWorkspaceSize(cnnlHandle_t handle,
                                       const cnnlPackPaddedSequenceDescriptor_t desc,
                                       size_t *size);

/******************************************************************************
 * Cambricon CNNL OP: PqAdd
 ******************************************************************************/
// Group:Ivfpq
/*!
 *  @brief This function is one part of the IVFPQ-ADD (Invert File System Product Quantization) algorithm.
 *  IVFPQ-ADD is used to add features to the library.
 *
 *  This operation performs with the following steps:
 *
 *  1. Get residual vector.
 *
       @verbatim
        for batch_idx in range(len(add)):
            get corresponding sub code index as nlist_idx
            residual_add[batch_idx] = add[batch_idx] - coarse_center[nlist_idx]
       @endverbatim
 *
 *  2. Encode the residual vector.
 *
 *    Compute L2-distance between input queries and input codebook, then select minimum value's index
 *    as code:
 *
      @verbatim
       distances = computeL2Distance(insert_vec, codebook).
       code = argmin(distances).
      @endverbatim
 *
 *  3. Add codes to library.
 *
 *    [src_lib] consists of [nlist] Level 1 cluster centroids. Codes will be added to corresponding
 *    sub library:
 *
 *      src_lib[nlist_index].push_back(codes).
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *           PqAdd operation.
 *  @param[in] add_desc
 *    Input. The descriptor of add tensor, the dimension is [batch, d].
 *  @param[in] add
 *    Input. Pointer to the MLU memory that stores the input data to be inserted.
 *  @param[in] residual_add_desc
 *    Input. The descriptor of residual_add tensor. The dimension is same with add tensor.
 *  @param[in] residual_add
 *    Input. Pointer to the MLU memory that stores the residual input data.
 *  @param[in] add_ids_desc
 *    Input. The descriptor of add_ids tensor. The dimension is [batch].
 *  @param[in] add_ids
 *    Input. Pointer to the MLU memory that stores the IDs of input data.
 *    \p add_ids is the ID of each feature to be inserted and each feature has a unique ID.
 *  @param[in] coarse_center_desc
 *    Input. The descriptor of coarse_center. The dimension is [nlist, d].
 *  @param[in] coarse_center
 *    Input. Pointer to the MLU memory that stores the level-1 cluster centroids.
 *  @param[in] code_book_desc
 *    Input. The descriptor of code_book tensor. The dimension is [ksub, m, dsub].
 *  @param[in] code_book
 *    Input. Pointer to the MLU memory that stores the level-2 cluster centroids.
 *  @param[in] src_lib_desc
 *    Input. The descriptor of src_lib tensor. The dimension is [nlist].
 *  @param[in] src_lib
 *    Input. Pointer to the MLU memory that stores an array of pointers to the device memory of src_lib tensor.
 *           \p src_lib is the database that contains all feature vectors. \p src_lib is divided into [nlist] segments
 *           with different sizes. Each segment should has shape as [m,align(n)], where align value depends on user.
 *  @param[in] src_lib_ids_desc
 *    Input. The descriptor of src_lib_ids tensor. The dimension is [nlist].
 *  @param[in] src_lib_ids
 *    Input. Pointer to the MLU memory that stores an array of pointers to the device memory of src_lib_ids tensor.
 *           \p src_lib_ids stores the IDs of every feature vector in the src_lib, and each feature has a unique ID.
 *           \p src_lib_ids is also divided into [nlist] segments according to the partition of code lib and
 *            \p nlist_id_size is defined to store the size of every code ID segments. Each segment should has
 *           shape as [align(n)], where align value should be the same as src_lib.
 *  @param[in] dst_lib_desc
 *    Input. The descriptor of dst_lib tensor. The dimension is same with src_lib_desc.
 *  @param[out] dst_lib
 *    Output. Pointer to the MLU memory that stores an array of pointers to the device memory of dst_lib tensor.
 *           The shape of \p dst_lib is same with src lib, but its device pointer may differ from src_lib.
 *           dst_lib is divided into [nlist] segments same as src_lib, and each segment size depends on src_lib code size
 *           and inserted data size.
 *  @param[in] dst_lib_ids_desc
 *    Input. The descriptor of dst_lib_ids tensor. The dimension is same with src_lib_ids_desc.
 *  @param[out] dst_lib_ids
 *    Output. Pointer to the MLU memory that stores an array of pointers to the device memory of dst_lib_ids tensor.
 *           The shape of dst_lib_ids is same with \p src_lib_ids, but its device pointer may differ \p src_lib_ids.
 *           \p dst_lib_ids is divided into [nlist] segments same as \p src_lib_ids, and each segment size depends
 *           on p src_lib_ids size and inserted data size.
 *  @param[in] list_ids_for_add_desc
 *    Input. The descriptor of list_ids_for_add tensor. The dimension is [list_num_for_add], and \p list_num_for_add
 *           is the number of the buckets to be inserted into feature data.
 *  @param[in] list_ids_for_add
 *    Input. Pointer to the MLU memory that contains sub code lib index to insert, which should be sorted from smallest to biggest.
 *  @param[in] insert_num_per_list_desc
 *    Input. The descriptor  of insert_num_per_list tensor. The dimension is [list_num_for_add].
 *  @param[in] insert_num_per_list
 *    Input. Pointer to the MLU memory that contains number for inserted vectors for each sub code lib whose order should be consistent with
 *           \p list_ids_for_add.
 *  @param[in] every_list_size_desc
 *    Input. The descriptor of every_list_size tensor. The dimension is [nlist].
 *  @param[in,out] every_list_size
 *    Input/output. Pointer to the MLU memory that contains the size of each sub code lib.
 *  @param[in] mode
 *    Input. mode should be one of [0, 1, 2].
 *           0 means pq algorithm with residual input calculated inside.
 *           1 means pq algorithm with residual input not calculated inside.
 *           2 means ivfpq algorithm with residual input calculated inside.
 *           If mode equals 0 or 1, nlist should be equal to 1.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ended normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    Parameters are not invalid.
 *
 *  @par Data Type
 *    - input: float(add), float(residual_add), int32(add_ids), float(coarse_center), float(codebook).
 *    - input: uint8(src_lib), int32(src_lib_ids), int32(every_list_size).
 *    - input: int32(list_ids_for_add), int32(insert_num_per_list).
 *    - output: int32(every_list_size),uint8(dst_lib),int32(dst_lib_ids).
 *
 *  @par Scale Limitation
 *    - [ntotal] represents the number of features in codes, and must be no larger than
 *      (pow(2,30) - 1) / 32.
 *    - [nlist] represents the number of segments that codes is divided into during training,
 *      and must be no larger than [ntotal].
 *    - \p code_book_desc->dims[0] represents the number of cluster centroids, and must be 256.
 *    - \p code_book_desc->dims[1] represents the number of segments of code_book vector(m),
 *      and can be 32 and 64.
 *    - \p code_book_desc->dims[2] represents the sub segment length of code_book(dsub).
 *      The product of m and dsub must be equal to D.
 *    - \p add_desc->dims[0] represents the number of data to be inserted, and must be
 *      no more than 57360.
 *    - \p add_desc->dims[1] represents the length of data to be inserted, must be one of [256, 512, 768, 1024].
 *    - ntotal = sum(every_list_size[i]), 0 <= i <= nlist - 1.
 *    - add_desc->dims[0] = add_ids_desc->dims[0].
 *    - residual_add_desc and resisual_add not be NULL if mode == 1
 *
 *  @note
 *  - This function supports MLU300 and MLU500 series.
 *  - The addresses of src_lib and dst_lib cannot be the same.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Reference
 *  - http://github.com/facebookresearch/faiss
 */

cnnlStatus_t CNNL_WIN_API
cnnlPqAdd(cnnlHandle_t handle,
             const cnnlTensorDescriptor_t add_desc,
             const void *add,
             const cnnlTensorDescriptor_t residual_add_desc,
             const void *residual_add,
             const cnnlTensorDescriptor_t add_ids_desc,
             const void *add_ids,
             const cnnlTensorDescriptor_t coarse_center_desc,
             const void *coarse_center,
             const cnnlTensorDescriptor_t code_book_desc,
             const void *code_book,
             const cnnlTensorDescriptor_t src_lib_desc,
             void **src_lib,
             const cnnlTensorDescriptor_t src_lib_ids_desc,
             void **src_lib_ids,
             const cnnlTensorDescriptor_t dst_lib_desc,
             void **dst_lib,
             const cnnlTensorDescriptor_t dst_lib_ids_desc,
             void **dst_lib_ids,
             const cnnlTensorDescriptor_t list_ids_for_add_desc,
             const void *list_ids_for_add,
             const cnnlTensorDescriptor_t insert_num_per_list_desc,
             const void *insert_num_per_list,
             const cnnlTensorDescriptor_t every_list_size_desc,
             void *every_list_size,
             int mode);

/******************************************************************************
 * Cambricon CNNL OP: PqRemove
 ******************************************************************************/
// Group:Ivfpq
/*!
 *  @brief This function is one part of the IVFPQ-REMOVE (Invert File System Product Quantization) algorithm.
 *  IVFPQ-REMOVE is used to delete features from the library. It contains two sub-operators: finding
 *  the ID of the bucket where the data to be deleted is located and the index of the data to be
 *  deleted in the bucket, and updating library based on ID and index found.
 *  This function implements updating library by replacing the code and ID to be removed with last
 *  code & ID in the lib and erasing the original last code and ID.
 *
 * This function performs with the following steps:
 *
 * 1. Replace the code & id to be removed with last code & id in the lib.
 *
      @verbatim
       codes[nlist_idx][idx] = code[nlist_idx][-1]
       code_ids[nlist_idx][idx] = code_ids[nlist_idx][-1]
      @endverbatim
 *
 * 2. Erase the original last code & id.
 *
      @verbatim
       codes = codes[:-1]
       code_ids = code_ids[:-1]
      @endverbatim
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *           PqRemove operation.
 *  @param[in] codes_desc
 *    Input. The descriptor of codes tensor. The dimension is [m, n].
 *  @param[in,out] codes
 *    Input. Pointer to the MLU memory that stores target bucket codes data.
 *  @param[in] code_ids_desc
 *    Input. The descriptor of code_ids tensor. The dimension is [n].
 *  @param[in,out] code_ids
 *    Input. Pointer to the MLU memory that stores IDs of target bucket codes data.
 *  @param[in] every_list_size_desc
 *    Input. The descriptor of every_list_size tensor. The dimension is [nlist].
 *  @param[in,out] every_list_size
 *    Input/output. Pointer to the MLU memory that contains the size of each bucket codes.
 *  @param[in]  nlist_idx
 *    Input. Index of target bucket codes whose feature will be removed. The Index should be in [0, nlist).
 *  @param[in]  offset
 *    Input. Feature offset in target bucket. The offset should be in [0, n).
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ended normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    Parameters are not invalid.
 *
 *  @par Data Type
 *  - input: uint8(codes), int32(code_ids), int32(every_list_size).
 *  - input: int32(nlist_idx), int32(offset).
 *  - output: int32(every_list_size).
 *
 *  @par Scale Limitation
 *  - codes_desc->dims[0] represents the number of segment of one feature vector, and must be 32 or 64.
 *
 *  @note
 *  - This function supports MLU300 and MLU500 series.
 *
 *  @par Reference
 *  - http://github.com/facebookresearch/faiss
 */

cnnlStatus_t CNNL_WIN_API
cnnlPqRemove(cnnlHandle_t handle,
                const cnnlTensorDescriptor_t codes_desc,
                uint8_t *codes,
                const cnnlTensorDescriptor_t code_ids_desc,
                int *code_ids,
                const cnnlTensorDescriptor_t every_list_size_desc,
                int *every_list_size,
                int nlist_idx,
                int offset);

/******************************************************************************
 * Cambricon CNNL OP: dlrm interact
 ******************************************************************************/
/*! The descriptor of the ::cnnlDlrmInteract operation.
 *
 *  You need to call the ::cnnlCreateDlrmInteractDescriptor function
 *  to create a descriptor, and call the ::cnnlSetDlrmInteractDescriptor function
 *  to set the attributes. Also, you need to destroy the descriptor at the end
 *  with the ::cnnlDestroyDlrmInteractDescriptor function.
 */
typedef struct cnnlDlrmInteractStruct *cnnlDlrmInteractDescriptor_t;

/*! @brief Enumeration variables describing the interaction modes of the DLRM interaction operation.
 */
typedef enum {
  CNNL_DLRM_INTERACT_DOT = 0,
  /*!< Dot interaction mode, which performs batched matrix-matrix multiplication and
   *  batched tril (returns batches of the lower triangular part of given matrices)
   *  after concatenating spare features and dense features.
   */
  CNNL_DLRM_INTERACT_CAT = 1,
  /*!< Concat interaction mode, which only concatenates spare features and dense features.*/
} cnnlDlrmInteractMode_t;

// Group:DLRM Interact
/*!
 *  @brief Creates a descriptor pointed by
 *   \p dlrm_interact_desc for the dlrm_interact operation.
 *  @param[out] dlrm_interact_desc
 *    Output.  A pointer to the dlrm_interact descriptor.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p dlrm_interact_desc is NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCreateDlrmInteractDescriptor(cnnlDlrmInteractDescriptor_t *dlrm_interact_desc);

// Group:DLRM Interact
/*!
 *  @brief Destroys the dlrm_interact descriptor.
 *
 *  @param[in] dlrm_interact_desc
 *    Input.  The dlrm interact descriptor.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_EXECUTION_FAILED
 *    \p dlrm_interact_desc is NULL.
 *
 *  @par API Dependency
 *  - This function is used after calling the ::cnnlDlrmInteract function.
 */
cnnlStatus_t CNNL_WIN_API
cnnlDestroyDlrmInteractDescriptor(cnnlDlrmInteractDescriptor_t dlrm_interact_desc);

// Group:DLRM Interact
/*!
 *  @brief Sets the dlrm_interact descriptor with parameters.
 *
 *  @param[in,out] dlrm_interact_desc
 *    Input/output.  The dlrm_interact descriptor.
 *  @param[in] interact_mode
 *    Input.  The operation mode to be applied. See ::cnnlDlrmInteractMode_t for details.
 *  @param[in] enable_self_interact
 *    Input.  A Boolean value indicating whether to include principal diagonal
 *    in the triled select operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - \p dlrm_interact_desc is NULL.
 *    - Parameters do not meet requirements.
 *  @par API Dependency
 *  - Before using this function, you need to call ::cnnlCreateDlrmInteractDescriptor
 *  to create a descriptor.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetDlrmInteractDescriptor(cnnlDlrmInteractDescriptor_t dlrm_interact_desc,
                              const cnnlDlrmInteractMode_t interact_mode,
                              const bool enable_self_interact);

// Group:DLRM Interact
/*!
 *  @brief Gets extra space size needed in dlrm_interact operation.
 *
 *  @param[in] dlrm_interact_desc
 *    Input.  The dlrm_interact descriptor.
 *  @param[in] x_desc
 *    Input.  The descriptor of the \p x tensor.
 *  @param[in] ly_desc
 *    Input.  The descriptor of the \p ly tensor.
 *  @param[in] ly_num
 *    Input.  The number of sparse features.
 *  @param[out] size
 *    Output. Pointer to the size of workspace.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - \p dlrm_interact_desc is NULL.
 *    - \p x_desc is NULL.
 *    - \p ly_desc is NULL.
 *    - \p size is NULL.
 *    - \p ly_num is invalid.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetDlrmInteractWorkspaceSize(const cnnlDlrmInteractDescriptor_t dlrm_interact_desc,
                                 const cnnlTensorDescriptor_t x_desc,
                                 const cnnlTensorDescriptor_t ly_desc,
                                 const int ly_num,
                                 size_t *size);

// Group:DLRM Interact
/*!
 * @brief Performs interactions between the dense feature and sparse
 *   features from DLRM Model.
 *
 *  This function contains two types of operations:
 *
 *  - If interact_mode = CNNL_DLRM_INTERACT_DOT
 *
      @verbatim
       output = concat([x] + lys, axis=1)
      @endverbatim
 *
 *  - If interact_mode = CNNL_DLRM_INTERACT_CAT
 *
      @verbatim
       C = concat([x] + lys, axis=1)
       R = reshape(C, (batch_size, ly_num + 1, embedding_size))
       D = bmm(r, transpose(R, (0, 2, 1))
       S = bts(D)
       output = concat([x, S], axis=1)
       - bmm: Batched Matmul.
       - bts: Batched Triled Select. Selects the tril of each matrix contained in a 3-D tensor.
       The attribute enable_self_interact controls if include the principal diagonal.
      @endverbatim
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *           dlrm_interact operation.
 *  @param[in] interact_desc
 *    Input. The descriptor of the dlrm_interact operation.
 *  @param[in] x_desc
 *    Input. The descriptor of the dense feature tensor \p x.
 *    The shape of \p x is [batch_size, embedding_size].
 *  @param[in] x
 *    Input. Pointer to the MLU memory that stores the x tensor.
 *  @param[in] ly_desc
 *    Input. The descriptor of each sparse feature tensor \p ly.
 *    The shape of \p ly is [batch_size, embedding_size].
 *  @param[in] lys
 *    Input. A host array contains pointers to MLU memory that stores the \p ly tensors.
 *  @param[in] ly_num
 *    Input. The number of sparse features.
 *  @param[in] workspace
 *    Input.  A pointer to the MLU memory that is used as an extra workspace.
 *  @param[in] workspace_size
 *    Input. The size of extra space.
 *  @param[out] output_desc
 *    Output. The descriptor of output tensor.
 *  @param[out] output
 *    Output.  A pointer to the MLU memory that stores the output tensor.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions must be met:
 *    - \p handle is NULL.
 *    - \p interact_desc is NULL.
 *    - \p x_desc is NULL.
 *    - \p x is NULL.
 *    - \p ly_desc is NULL.
 *    - \p lys is NULL.
 *    - \p lys[i] is NULL, where i is in range of [0, ly_num-1].
 *    - \p workspace is invalid.
 *    - \p output_desc is NULL.
 *    - \p output is NULL.
 *  @par Example
 *  @verbatim
     Dimension of x: [batch_size, embedding_size].

     Dimension of each ly: [batch_size, embedding_size].

     Dimension of C: [batch_size, (ly_num + 1) * embedding_size].

     Dimension of R: [batch_size, ly_num + 1, embedding_size].

     Dimension of D: [batch_size, ly_num + 1, ly_num + 1].

     Dimension of S: [batch_size, cs].
       cs = (ly_num + 1) * (ly_num + 2) / 2, if enable_self_interact = true;
       cs = ly_num * (ly_num + 1) / 2, if enable_self_interact = false.

     Dimension of output: [batch_size, embedding_size + cs].
    @endverbatim
 *
 * @par Data Type
 *  - input: float, half.
 *  - output: float, half.
 *
 * @note
 *  - Data type of input and output must be the same.
 *  - Only supports MLU300 and MLU500 series.
 *
 * @par Scale Limitation
 *  - 0 < ly_num <= 128.
 *  - 0 < embedding_size <= 512.
 */
cnnlStatus_t CNNL_WIN_API
cnnlDlrmInteract(cnnlHandle_t handle,
                 const cnnlDlrmInteractDescriptor_t interact_desc,
                 const cnnlTensorDescriptor_t x_desc,
                 const void *x,
                 const cnnlTensorDescriptor_t ly_desc,
                 const void *const lys[],
                 const int ly_num,
                 void *workspace,
                 size_t workspace_size,
                 const cnnlTensorDescriptor_t output_desc,
                 void *output);


/******************************************************************************
 * Cambricon CNNL OP: TransformerFeedForward
 ******************************************************************************/
/*! The descriptor of the ::cnnlTransformerFeedForward operation.
 *
 *  You need to call the ::cnnlCreateTransformerFeedForwardDescriptor function
 *  to create a descriptor, and call the ::cnnlSetTransformerFeedForwardDescriptor
 *  to set the information of the feed_forward operation to the descriptor.
 *  Also, you need to destroy the descriptor at the end with the
 *  ::cnnlDestroyTransformerFeedForwardDescriptor function.
 */
typedef struct cnnlTransformerFeedForwardStruct *cnnlTransformerFeedForwardDescriptor_t;

/*! The descriptor of the ::cnnlTransformerFeedForward operation that holds quantization
 *  information.
 *
 *  You need to call the ::cnnlCreateTransformerFeedForwardQuantizeDescriptor function to create a
 *  descriptor, and call the ::cnnlSetTransformerFeedForwardQuantizeDescriptor_v3 function to set
 *  the quantization information. Also, you need to destroy the descriptor at the end with the
 *  ::cnnlDestroyTransformerFeedForwardQuantizeDescriptor function.
 */
typedef struct cnnlTransformerFeedForwardQuantizeStruct
                *cnnlTransformerFeedForwardQuantizeDescriptor_t;

// Group:Transformer FeedForward
/*!
 *  @brief Executes FeedForwardNetwork in Transformer encoder and decoder network.
 *
 *  This operation performs with the following steps:
 *
 *    1. Previous normalization (optional)
 *
 *       input = norm(input, norm_scale, norm_bias).
 *
 *    2. Fc1
 *
 *       fc1_output = fc(input, fc1_filters)
 *
 *    3. Add bias (optional)
 *
 *       fc1_output = add(fc1_output, fc1_bias)
 *
 *    4. Avtivation
 *
 *       fc1_output = active(fc1_output)
 *
 *    5. Fc1 gate (optional)
 *
 *       gate_output = fc(input, gate_filters)
 *
 *    6. Add bias (optional)
 *
 *       gate_output = add(gate_output, gate_bias)
 *
 *    7. Mul fc1  (optional)
 *
 *       fc1_output = mul(fc1_output, gate_output)
 *
 *    8. Fc2
 *
 *       fc2_output = fc(fc1_output, fc2_filters)
 *
 *    9. Add bias (optional)
 *
 *       fc2_output = add(fc2_output, fc2_bias)
 *
 *    11. Residual add (optional)
 *
 *       output = add(alpha * fc2_output, beta * residual_input).
 *       - Alpha, also known as post_scale.
 *       - When the norm_structure is PRE_LAYERNORM_OUTSIDE_RESIDUAL, the residual_input is
 *         the result of pre_norm, otherwise is the original input.
 *
 *    12. Post normalization (optional)
 *
 *       output = norm(output, norm_scale, norm_bias)
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlTransformerFeedForward_v2 instead.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices
 *    and queues in the transformer_feed_forward operation.
 *  @param[in] desc
 *    Input. The descriptor of the transformer_feed_forward operation. For detailed information,
 *    see ::cnnlTransformerFeedForwardDescriptor_t.
 *  @param[in] activation_desc
 *    Input. The descriptor of the activation in transformer_feed_forward operation.
 *    For detailed information, see cnnlActivationDescriptor_t.
 *  @param[in] quantize_desc
 *    Input. The descriptor of the quantization mode in transformer_feed_forward operation.
 *    For detailed information, see ::cnnlTransformerFeedForwardQuantizeDescriptor_t.
 *  @param[in] input_desc
 *    Input. The descriptor of the input tensor.
 *  @param[in] input
 *    Input. Pointer to the MLU memory that stores the input tensor.
 *  @param[in] fc1_filters_desc
 *    Input. The descriptor of the fc1_filters tensor used as filters in the transformer_feed_forward operation.
 *  @param[in] fc1_filters
 *    Input. Pointer to the MLU memory that stores the fc1_filters tensor.
 *  @param[in] fc1_bias_desc
 *    Input. The descriptor of the fc1_bias tensor used as bias in the transformer_feed_forward operation.
 *  @param[in] fc1_bias
 *    Input. Pointer to the MLU memory that stores the fc1_bias tensor.
 *  @param[in] fc2_filters_desc
 *    Input. The descriptor of the fc2_filters tensor used as filters in the transformer_feed_forward operation.
 *  @param[in] fc2_filters
 *    Input. Pointer to the MLU memory that stores the fc2_filters tensor.
 *  @param[in] fc2_bias_desc
 *    Input. The descriptor of the fc2_bias tensor used as bias in the transformer_feed_forward operation.
 *  @param[in] fc2_bias
 *    Input. Pointer to the MLU memory that stores the fc2_bias tensor.
 *  @param[in] norm_scale_desc
 *    Input. The descriptor of the norm_scale tensor used as the scale of normalization module
 *    in the transformer_feed_forward operation.
 *  @param[in] norm_scale
 *    Input. Pointer to the MLU memory that stores the norm_scale tensor.
 *  @param[in] norm_bias_desc
 *    Input. The descriptor of the norm_bias tensor used as the bias of normalization module
 *    in the transformer_feed_forward operation.
 *  @param[in] norm_bias
 *    Input. Pointer to the MLU memory that stores the norm_bias tensor.
 *  @param[in] workspace
 *    Input. Pointer to the MLU memory that is used as an extra workspace for the
 *    transformer_feed_forward operation.
 *  @param[in] workspace_size
 *    Input. The size of the extra workspace in bytes that needs to be used in
 *    the transformer_feed_forward operation. You can get the size of the workspace with
 *    the ::cnnlGetTransformerFeedForwardWorkspaceSize_v2 function.
 *  @param[out] output_desc
 *    Output. The descriptor of the output tensor.
 *  @param[out] output
 *    Output. Pointer to the MLU memory that stores the output tensor.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are encountered:
 *    - The handle is NULL.
 *    - The pointer is NULL.
 *    - Parameters do not meet scale limitation.
 *  @retval CNNL_STATUS_ARCH_MISMATCH
 *    One or more of the following conditions are encountered:
 *    - This function is run on the hardware platform that is not supported.
 *
 *  @par Data Type
 *  - input: float, half, bfloat16.
 *  - fc1 filters: float, half, int8, bfloat16.
 *  - fc1 bias: float, half, bfloat16.
 *  - fc2 filters: float, half, int8, bfloat16.
 *  - fc2 bias: float, half, bfloat16.
 *  - gate filters: float, half, int8, bfloat16.
 *  - gate bias: float, half, bfloat16.
 *  - norm scale: float, half, bfloat16.
 *  - norm bias: float, half, bfloat16.
 *  - output: float, half, bfloat16.
 *  - The data types of \p fc1_filters, \p fc2_filters and \p gate_filters must be the same.
 *  - The data types of tensor that is unrelated to filters and dequantization param tensor must be the
 *    same.
 *  - When the data type of \p fc1_filters is int8, all tensors except filter tensor must have
 *    the same data type: half, float or bfloat16.
 *  - Only when the quantization mode is CNNL_QUANTIZE_GROUP_WISE, the data type of \p input can be
 *    bfloat16 when the data type of \p fc1_filters is int8.
 *  - When the quantization mode is CNNL_QUANTIZE_GROUP_WISE, the data types of dequantization param tensors
 *    must be the same as that of \p input.
 *
 *  @par Quantization Layout
 *  - When the quantization layout of tensor is CNNL_QUANTIZE_NONE, it means that the tensor will not be
 *    quantized.
 *  - The supported quantization layouts are as follows:
 *    - local_input: CNNL_QUANTIZE_NONE.
 *    - residual_input: CNNL_QUANTIZE_NONE, CNNL_QUANTIZE_PER_TENSOR.
 *    - fc1_filter: CNNL_QUANTIZE_NONE, CNNL_QUANTIZE_PER_TENSOR, CNNL_QUANTIZE_PER_CHANNEL,
 *                  CNNL_QUANTIZE_GROUP_WISE.
 *    - fc1_input: CNNL_QUANTIZE_NONE, CNNL_QUANTIZE_PER_TENSOR, CNNL_QUANTIZE_PER_CHANNEL.
 *    - fc1_after_gemm: CNNL_QUANTIZE_NONE, CNNL_QUANTIZE_PER_TENSOR.
 *    - fc2_input: CNNL_QUANTIZE_NONE, CNNL_QUANTIZE_PER_TENSOR, CNNL_QUANTIZE_PER_CHANNEL.
 *    - gate_filter: CNNL_QUANTIZE_NONE, CNNL_QUANTIZE_PER_TENSOR, CNNL_QUANTIZE_PER_CHANNEL,
 *                   CNNL_QUANTIZE_GROUP_WISE.
 *    - gate_after_gemm: CNNL_QUANTIZE_NONE, CNNL_QUANTIZE_PER_TENSOR.
 *    - fc2_filter: CNNL_QUANTIZE_NONE, CNNL_QUANTIZE_PER_TENSOR, CNNL_QUANTIZE_PER_CHANNEL,
 *                  CNNL_QUANTIZE_GROUP_WISE.
 *    - fc2_after_gemm: CNNL_QUANTIZE_NONE, CNNL_QUANTIZE_PER_TENSOR.
 *
 *    The quantization layouts of the
 *    \p fc1_filters, \p fc2_filters and \p gate_filters should be same.
 *  - When the quantization layout of \p fc1_filters is CNNL_QUANTIZE_GROUP_WISE, the quantization
 *    of \p input must be CNNL_QUANTIZE_NONE.
 *  - When the quantization layout of \p input is CNNL_QUANTIZE_NONE, the quantization layout of
 *    local_input, residual_input, fc1_after_gemm, fc2_input, gate_after_gemm and fc2_after_gemm
 *    must be CNNL_QUANTIZE_NONE.
 *  - When the quantization layout of \p input is CNNL_QUANTIZE_PER_CHANNEL, the quantization layout
 *    of fc2_input and filters must be CNNL_QUANTIZE_PER_CHANNEL, and the quantization layout of
 *    local_input, residual_input, fc1_after_gemm, gate_after_gemm and fc2_after_gemm
 *    must be CNNL_QUANTIZE_NONE.
 *
 *  @note
 *  - This function supports MLU300 and MLU500 series.
 *  - Data type bfloat16 is not supported on MLU300 series.
 *  - \p desc and \p activation_desc are required. Call
 *    ::cnnlCreateTransformerFeedForwardDescriptor and cnnlCreateActivationDescriptor to create the
 *    descriptor, call ::cnnlSetTransformerFeedForwardDescriptor_v2 and
 *    cnnlSetActivationDescriptor_v5 to set the descriptor, and call
 *    ::cnnlDestroyTransformerFeedForwardDescriptor and cnnlDestroyActivationDescriptor to free the
 *    descriptor.
 *  - When \p quantize_desc is necessary,
 *    call ::cnnlCreateTransformerFeedForwardQuantizeDescriptor to create the descriptor,
 *    call ::cnnlSetTransformerFeedForwardQuantizeDescriptor_v3 to set the descriptor,
 *    call ::cnnlDestroyTransformerFeedForwardQuantizeDescriptor to free the descriptor.
 *  - When \p workspace is necessary to complete the function, call
 *    ::cnnlGetTransformerFeedForwardWorkspaceSize_v2 to get the \p workspace_size.
 *  - When gated feed forward is required, set \p gate_filters_desc and \p gate_filters to non-NULL.
 *  - When \p fc1_bias is not required, set \p fc1_bias_desc and \p fc1_bias to NULL.
 *  - When \p fc2_bias is not required, set \p fc2_bias_desc and \p fc2_bias to NULL.
 *  - When \p gate_bias is not required, set \p gate_bias_desc and \p gate_bias to NULL.
 *  - The norm_type only supports CNNL_TRANSFORMER_LAYERNORM, CNNL_TRANSFORMER_RMSNORM and
 *    CNNL_TRANSFORMER_SCALENORM.
 *  - When the data type of \p fc1_filters is CNNL_DTYPE_INT8:
 *    - The quantization mode only supports CNNL_QUANTIZE_POSITION_SCALE.
 *    - The quantization bit width only supports 4 and 8.
 *    - When the quantization layout of \p input is CNNL_QUANTIZE_PER_TENSOR:
 *      - The norm_type only supports CNNL_TRANSFORMER_LAYERNORM.
 *      - The norm_structure only supports CNNL_TRANSFORMER_PRE_LAYERNORM_INSIDE_RESIDUAL and
 *        CNNL_TRANSFORMER_PRE_LAYERNORM_OUTSIDE_RESIDUAL.
 *      - The quantization bit width must be 8.
 *      - When residual_input_quant is enabled, the norm_structure only supports
 *        CNNL_TRANSFORMER_PRE_LAYERNORM_OUTSIDE_RESIDUAL.
 *      - The post_scale must be 1.0f.
 *      - fc1_after_gemm_quant and fc2_after_gemm_quant must be enabled together.
 *    - When the quantization layout of \p input is CNNL_QUANTIZE_NONE:
 *      - The quantization bit width must be 4 and 8.
 *      - When the quantization layout of \p fc1_filters is CNNL_QUANTIZE_GROUP_WISE:
 *        - The data type of dequantization param scale of filters must be the same as that of \p input.
 *        - \p gate_filters cannot be null.
 *        - All the dimensions of the \p fc1_filters, \p fc2_filters and \p gate_filters must be
 *          divided by group_size.
 *        - The group_size only supports 64, 128, 256 or 512.
 *        - Before the operation can process the quantization coefficient tensor, you must perform
 *          a specific preprocessing step, which involves adjusting the scale according to the
 *          formula: scale = pow(2, position) / scale. Note that this operation is agnostic to the
 *          value of position in this scenario.
 *      - The quantization param tensor position and scale cannot be null.
 *    - When the quantization layout of \p input is CNNL_QUANTIZE_PER_CHANNEL:
 *      - The quantization bit width of all tensors must be 8.
 *      - The scale of \p fc1_input_quant_desc will multiply with input after norm if norm exist.
 *      - The scale of \p fc2_input_quant_desc will multiply with input after fc1.
 *      - You should fuse input smooth scale with input_quant_scale into input_quant_desc to quantize
 *        input and smoother input.
 *      - The scale of \p fc1_filter_quant_desc, \p fc2_filter_quant_desc and \p gate_filter_quant_desc
 *        will multiply with the result of each filters.
 *      - You should fuse filter smooth scale, input_dequant_scale and filter_dequant_scale into
 *        filter_quant_desc to dequantize filter and smoother filter.
 *
 *  - The post normalization ability (for example, when norm_structure is CNNL_TRANSFORMER_POST_LAYERNORM_NO_RESIDUAL,
 *    CNNL_TRANSFORMER_POST_LAYERNORM_INSIDE_RESIDUAL or CNNL_TRANSFORMER_POST_LAYERNORM_OUTSIDE_RESIDUAL)
 *    in this function is deprecated and will be removed in future release.
 *    Call ::cnnlFuseNorm to process post normalization instead.
 *
 *  @par Scale Limitation
 *  - The range of the last dimension of the \p input tensor is between 16 and 14336. In addition,
 *    when the last dimension of the \p input tensor is greater than or equal to 2048, or the first
 *    dimension of the \p fc1_filters is greater than 8192, the length of the last dimension of the
 *    \p input tensor must be a multiple of 512.
 *  - The first dimension of the \p fc1_filters has a length greater than or equal to 16 and less
 *    than or equal to a value that ensures the size of the tensor is less than 2G bytes.
 *  - When the quantization layout of \p input is not equal to that of \p fc1_filters,
 *    the last dimension of \p input should not be less than 2048.
 *  - When the quantization layout of \p input is CNNL_QUANTIZE_PER_TENSOR, the last dimension of
 *    the \p input tensor cannot be greater than 2048, and the first dimension of the \p fc1_filters
 *    tensor cannot be greater than 8192.
 *  - When the quantization layout of \p fc1_filters is CNNL_QUANTIZE_GROUP_WISE, the first
 *    dimension of quant_param should be same as the first dimension of the corresponding filters
 *    tensor and the last dimension of quant_param should be same as the last dimension of the
 *    corresponding filters tensor divided by group_size.
 *  - The dimensions of all tensors must meet the following conditions:
 *      - The dimensions of \p input and \p output must be the same.
 *      - The dimensions of \p input can only be 2 or 3.
 *      - The dimensions of \p fc1_filters, \p gate_filters and \p fc2_filters must be 2.
 *      - The dimensions of \p fc1_bias, \p gate_bias, \p fc2_bias, \p norm_scale and
 *      \p norm_bias must be 1.
 *      - The dimension sizes of \p input and \p output along the same dimension must be the same.
 *    - The last dimension of \p input must be the same as the following dimensions of tensors:
 *      - The second dimension of \p fc1_filters.
 *      - The second dimension of \p gate_filters.
 *      - The first dimension of \p fc2_filters.
 *      - The first dimension of \p fc2_bias.
 *      - The first dimension of \p norm_scale.
 *      - The first dimension of \p norm_bias.
 *    - The first dimension of \p fc1_filters must be the same as the following dimensions of tensors:
 *      - The second dimension of \p fc2_filters.
 *      - The first dimension of \p gate_filters.
 *      - The first dimension of \p fc1_bias.
 *
 *  @par Performance Optimization
 *  - When the last dimension of \p input can be divisible by 64, meanwhile the first dimension of \p fc1_filters is 4
 *    times of the last dimension of \p input, feed_forward function has best performance.
 *
 *  @par API Dependency
 *  - You need to call the ::cnnlCreateTransformerFeedForwardDescriptor, ::cnnlGetTransformerFeedForwardWorkspaceSize_v2
 *    and cnnlCreateActivationDescriptor functions before calling this function.
 *  - You need to call the ::cnnlDestroyTransformerFeedForwardDescriptor and cnnlDestroyActivationDescriptor
 *    function after calling this function.
 *
 *  @par Example
    @verbatim
    Dimension of \p input: [batch_num, seq_len, hidden_size]
    Dimension of \p fc1_filters: [filter_size, hidden_size]
    Dimension of \p fc1_bias: [filter_size]
    Dimension of \p fc2_filters: [hidden_size, filter_size]
    Dimension of \p fc2_bias: [hidden_size]
    Dimension of \p norm_scale: [hidden_size]
    Dimension of \p norm_bias: [hidden_size]

    Then we will get the output:

    Dimension of \p output: [batch_num, seq_len, hidden_size]
    @endverbatim

 */
CNNL_DEPRECATED_FOR(cnnlTransformerFeedForward_v2)
cnnlStatus_t CNNL_WIN_API
cnnlTransformerFeedForward(cnnlHandle_t handle,
                           const cnnlTransformerFeedForwardDescriptor_t desc,
                           const cnnlActivationDescriptor_t activation_desc,
                           const cnnlTransformerFeedForwardQuantizeDescriptor_t quantize_desc,
                           const cnnlTensorDescriptor_t input_desc,
                           const void *input,
                           const cnnlTensorDescriptor_t fc1_filters_desc,
                           const void *fc1_filters,
                           const cnnlTensorDescriptor_t fc1_bias_desc,
                           const void *fc1_bias,
                           const cnnlTensorDescriptor_t fc2_filters_desc,
                           const void *fc2_filters,
                           const cnnlTensorDescriptor_t fc2_bias_desc,
                           const void *fc2_bias,
                           const cnnlTensorDescriptor_t norm_scale_desc,
                           const void *norm_scale,
                           const cnnlTensorDescriptor_t norm_bias_desc,
                           const void *norm_bias,
                           void *workspace,
                           size_t workspace_size,
                           cnnlTensorDescriptor_t output_desc,
                           void *output);

// Group:Transformer FeedForward
/*!
 *  @brief Initializes the transformer_feed_forward descriptor \p desc that is previously created
 *  with the ::cnnlCreateTransformerFeedForwardDescriptor function, and sets the information
 *  about the feed-forward-network operation to the transformer_feed_forward descriptor
 *  \p desc. The information includes the value of the normalization epsilon \p norm_eps,
 *  the value of the scale at the end of fc2 \p post_scale, the data type of the computation in
 *  transformer_feed_forward \p compute_dtype and the normalization and residual structure of transformer_feed_forward
 *  \p norm_structure.
 *
 *  @param[in,out] desc
 *    Input/output. The descriptor of the transformer_feed_forward operation. For detailed information,
 *    see ::cnnlTransformerFeedForwardDescriptor_t.
 *  @param[in] norm_eps
 *    Input. The value of epsilon in the normalization module of the transformer_feed_forward operation.
 *    In normalization module, the divisor is the variance of input which could be zero or very close to zero.
 *    \p norm_eps is added to the divisor to prevent divisor from being zero. The value of
 *    \p norm_eps is usually set to 1e-6, and the range of \p norm_eps is [0, 1e-4].
 *  @param[in] alpha
 *    Input. The value of scale factor after fc2 and before residual and post normalization module. Set
 *    \p alpha to 1.0 when it is not needed. The value of \p alpha must be 1.0f when it
 *    is needed in QAT function.
 *  @param[in] beta
 *    Input. The value of scale factor of residual. Set \p beta to 1.0 when it is not needed.
 *           See more details in the formula description of cnnlTransformerFeedForward.
 *  @param[in] compute_dtype
 *    Input. The data type of computation in transformer_feed_forward operation. It controls the
 *    data type used in all the computation modules except fc module whose data type is controlled
 *    by the data type of \p input. When the data type of \p input is float or bfloat16,
 *    \p compute_dtype must be float. When the data type of \p input is half, \p compute_dtype
 *    cannot be bfloat16. For detailed information, see cnnlDataType_t.
 *  @param[in] norm_structure
 *    Input. The normalization and residual structure of transformer_feed_forward operation.
 *    CNNL_TRANSFORMER_POST_LAYERNORM_INSIDE_RESIDUAL is not supported.
 *    See ::cnnlTransformerLayernormResidualStructure_t for details. Only supports
 *    CNNL_TRANSFORMER_PRE_LAYERNORM_INSIDE_RESIDUAL and
 *    CNNL_TRANSFORMER_PRE_LAYERNORM_OUTSIDE_RESIDUAL when QAT is enabled.
 *
 *
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p desc is NULL.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetTransformerFeedForwardDescriptor_v2(cnnlTransformerFeedForwardDescriptor_t desc,
                                           const float norm_eps,
                                           const float alpha,
                                           const float beta,
                                           const cnnlDataType_t compute_dtype,
                                           const cnnlTransformerLayernormResidualStructure_t
                                           norm_structure);

// Group:Transformer FeedForward
/*!
 *  @brief Initializes the transformer_feed_forward descriptor \p desc that is previously created
 *  with the ::cnnlCreateTransformerFeedForwardDescriptor function, and sets the information
 *  about the feed-forward-network operation to the transformer_feed_forward descriptor
 *  \p desc. The information includes the value of the normalization epsilon \p norm_eps,
 *  the value of the scale at the end of fc2 \p post_scale, the data type of the computation in
 *  transformer_feed_forward \p compute_dtype and the normalization and residual structure of transformer_feed_forward
 *  \p norm_structure.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlSetTransformerFeedForwardDescriptor_v2 instead.
 *
 *  @param[in,out] desc
 *    Input/output. The descriptor of the transformer_feed_forward operation. For detailed information,
 *    see ::cnnlTransformerFeedForwardDescriptor_t.
 *  @param[in] norm_eps
 *    Input. The value of epsilon in the normalization module of the transformer_feed_forward operation.
 *    In normalization module, the divisor is the variance of input which could be zero or very close to zero.
 *    \p norm_eps is added to the divisor to prevent divisor from being zero. The value of
 *    \p norm_eps is usually set to 1e-6, and the range of \p norm_eps is [0, 1e-4].
 *  @param[in] post_scale
 *    Input. The value of scale factor after fc2 and before residual and post normalization module. Set
 *    \p post_scale to 1.0 when it is not needed. The value of \p post_scale must be 1.0f when it
 *    is needed in QAT function.
 *  @param[in] compute_dtype
 *    Input. The data type of computation in transformer_feed_forward operation. It controls the
 *    data type used in all the computation modules except fc module whose data type is controlled
 *    by the data type of \p input. When the data type of \p input is float or bfloat16,
 *    \p compute_dtype must be float. When the data type of \p input is half, \p compute_dtype
 *    cannot be bfloat16. For detailed information, see cnnlDataType_t.
 *  @param[in] norm_structure
 *    Input. The normalization and residual structure of transformer_feed_forward operation.
 *    CNNL_TRANSFORMER_POST_LAYERNORM_INSIDE_RESIDUAL is not supported.
 *    See ::cnnlTransformerLayernormResidualStructure_t for details. Only supports
 *    CNNL_TRANSFORMER_PRE_LAYERNORM_INSIDE_RESIDUAL and
 *    CNNL_TRANSFORMER_PRE_LAYERNORM_OUTSIDE_RESIDUAL when QAT is enabled.
 *
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p desc is NULL.
 *
 */
CNNL_DEPRECATED_FOR(cnnlSetTransformerFeedForwardDescriptor_v2)
cnnlStatus_t CNNL_WIN_API
cnnlSetTransformerFeedForwardDescriptor(cnnlTransformerFeedForwardDescriptor_t desc,
                                        const float norm_eps,
                                        const float post_scale,
                                        const cnnlDataType_t compute_dtype,
                                        const cnnlTransformerLayernormResidualStructure_t
                                                             norm_structure);

// Group:Transformer FeedForward
/*!
 *  @brief Creates a descriptor pointed by \p desc for
 *   the transformerfeedforward operation and allocate memory for it.
 *
 *  @param[out]  desc
 *    Output. Descriptor of transformerfeedforward operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_ALLOC_FAILED
 *    The function failed to allocate memory space.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCreateTransformerFeedForwardDescriptor(cnnlTransformerFeedForwardDescriptor_t *desc);


// Group:Transformer FeedForward
/*!
 *  @brief Destroys the descriptor of transformerfeedforward and free
 * memory for it.
 *
 *  @param[in]  desc
 *    Input. Descriptor of transformerfeedforward operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One of the following conditions are met:
 *    - The value of \p desc is NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlDestroyTransformerFeedForwardDescriptor(cnnlTransformerFeedForwardDescriptor_t desc);


// Group:Transformer FeedForward
/*!
 *  @brief Sets the quantization information to the transformerfeedforward quantization
 *  descriptor \p desc that is previously created with the
 *  ::cnnlCreateTransformerFeedForwardQuantizeDescriptor function.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlSetTransformerFeedForwardQuantizeDescriptor_v4 instead.
 *
 *  @param[in,out] desc
 *    Input/output. The quantization descriptor of the transformerfeedforward operation. For detailed
 *    information, see ::cnnlTransformerFeedForwardQuantizeDescriptor_t.
 *  @param[in]  residual_input_quant
 *    Input. A Boolean value indicates whether to quantize the residual input.
 *  @param[in]  local_input_quant
 *    Input. A Boolean value indicates whether to quantize the local input.
 *  @param[in]  use_fc1_after_gemm_quant_params
 *    Input. A Boolean value indicates whether to use fc1_after_gemm_position and
 *    fc1_after_gemm_scale when dequantizing fc1.
 *  @param[in]  use_fc2_after_gemm_quant_params
 *    Input. A Boolean value indicates whether to use fc2_after_gemm_position and
 *    fc2_after_gemm_scale when dequantizing fc2. The value of \p use_fc2_after_gemm_quant_params
 *    must be the same as \p use_fc1_after_gemm_quant_params.
 *  @param[in]  quant_per_tensor
 *    Input. A Boolean value indicates whether to quantize the filters of ffn by tensor.
 *    If this parameter is true, quantization is done by tensor, otherwise by channel.
 *  @param[in]  is_qat_interface
 *    Input. A Boolean value indicates whether QAT is used for quantization.
 *    If this parameter is false, PTQ is used.
 * @param[in]   quantize_mode
 *    Input. The quantization mode. CNNL_QUANTIZE_POSITION_SCALE_OFFSET is not supported.
 *    For details, see cnnlQuantizeMode_t in Cambricon CNNL Developer Guide.
 *  @param[in]  fc1_input_position
 *    Input. The quantization position of the fc1 input.
 *  @param[in]  fc1_input_scale
 *    Input. The quantization scaling factor of the fc1 input.
 *  @param[in]  fc1_input_offset
 *    Input. The quantization offset of the fc1 input.
 *  @param[in]  local_input_position
 *    Input. The quantization position of the local input.
 *  @param[in]  local_input_scale
 *    Input. The quantization scaling factor of the local input.
 *  @param[in]  local_input_offset
 *    Input. The quantization offset of the local input.
 *  @param[in]  fc1_after_gemm_position
 *    Input. The quantization position of the fc1 after gemm.
 *  @param[in]  fc1_after_gemm_scale
 *    Input. The quantization scaling factor of the fc1 after gemm.
 *  @param[in]  fc1_after_gemm_offset
 *    Input. The quantization offset of the fc1 after gemm.
 *  @param[in]  fc2_input_position
 *    Input. The quantization position of the fc2 input.
 *  @param[in]  fc2_input_scale
 *    Input. The quantization scaling factor of the fc2 input.
 *  @param[in]  fc2_input_offset
 *    Input. The quantization offset of the fc2 input.
 *  @param[in]  fc2_after_gemm_position
 *    Input. The quantization position of the fc2 after gemm.
 *  @param[in]  fc2_after_gemm_scale
 *    Input. The quantization scaling factor of the fc2 after gemm.
 *  @param[in]  fc2_after_gemm_offset
 *    Input. The quantization offset of the fc2 after gemm.
 *  @param[in]  fc1_filters_positions
 *    Input. Device pointer points to the quantization positions of the fc1 filters.
 *  @param[in]  fc1_filters_scales
 *    Input. Device pointer points to the quantization scaling factors of the fc1 filters.
 *  @param[in]  fc1_filters_offsets
 *    Input. Device pointer points to the quantization offsets of the fc1 filters.
 *  @param[in]  fc2_filters_positions
 *    Input. Device pointer points to the quantization positions of the fc2 filters.
 *  @param[in]  fc2_filters_scales
 *    Input. Device pointer points to the quantization scaling factors of the fc2 filters.
 *  @param[in]  fc2_filters_offsets
 *    Input. Device pointer points to the quantization offsets of the fc2 filters.
 *
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p desc is NULL.
 */
CNNL_DEPRECATED_FOR(cnnlSetTransformerFeedForwardQuantizeDescriptor_v4)
cnnlStatus_t CNNL_WIN_API
cnnlSetTransformerFeedForwardQuantizeDescriptor(
    cnnlTransformerFeedForwardQuantizeDescriptor_t desc,
    bool residual_input_quant,
    bool local_input_quant,
    bool use_fc1_after_gemm_quant_params,
    bool use_fc2_after_gemm_quant_params,
    bool quant_per_tensor,
    bool is_qat_interface,
    cnnlQuantizeMode_t quantize_mode,
    int fc1_input_position,
    int fc1_input_offset,
    float fc1_input_scale,
    int local_input_position,
    int local_input_offset,
    float local_input_scale,
    int fc1_after_gemm_position,
    int fc1_after_gemm_offset,
    float fc1_after_gemm_scale,
    int fc2_input_position,
    int fc2_input_offset,
    float fc2_input_scale,
    int fc2_after_gemm_position,
    int fc2_after_gemm_offset,
    float fc2_after_gemm_scale,
    void *fc1_filters_positions,
    void *fc1_filters_scales,
    void *fc1_filters_offsets,
    void *fc2_filters_positions,
    void *fc2_filters_scales,
    void *fc2_filters_offsets);

// Group:Transformer FeedForward
/*!
 *  @brief Sets the quantization information to the transformerfeedforward quantization
 *  descriptor \p desc that is previously created with the
 *  ::cnnlCreateTransformerFeedForwardQuantizeDescriptor function.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlSetTransformerFeedForwardQuantizeDescriptor_v4 instead.
 *
 *  @param[in,out] desc
 *    Input/output. The quantization descriptor of the transformerfeedforward operation. For detailed
 *    information, see ::cnnlTransformerFeedForwardQuantizeDescriptor_t.
 *  @param[in]  local_input_quant_desc
 *    Input. The quantization descriptor that holds quantization information of the local input.
 *  @param[in]  fc1_filter_quant_desc
 *    Input. The quantization descriptor that holds quantization information of the fc1 filter.
 *  @param[in]  fc1_input_quant_desc
 *    Input. The quantization descriptor that holds quantization information of the fc1 input.
 *  @param[in]  fc1_after_gemm_quant_desc
 *    Input. The quantization descriptor that holds quantization information of the fc1 after gemm.
 *  @param[in]  fc2_input_quant_desc
 *    Input. The quantization descriptor that holds quantization information of the fc2 input.
 *  @param[in]  gate_filter_quant_desc
 *    Input. The quantization descriptor that holds quantization information of the gate filter.
 *  @param[in]  gate_after_gemm_quant_desc
 *    Input. The quantization descriptor that holds quantization information of the gate after gemm.
 *  @param[in]  fc2_filter_quant_desc
 *    Input. The quantization descriptor that holds quantization information of the fc2 filter.
 *  @param[in]  fc2_after_gemm_quant_desc
 *    Input. The quantization descriptor that holds quantization information of the fc2 after gemm.
 *
 *  @note
 *    - Only when the quantization layout of \p fc1_filter_quant_desc is not equal to that of
 *      \p fc1_input_quant_desc, the quant_bit_size of all quantization descriptors can be set to 4.
 *
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p desc is NULL.
 *
 */
CNNL_DEPRECATED_FOR(cnnlSetTransformerFeedForwardQuantizeDescriptor_v4)
cnnlStatus_t CNNL_WIN_API
cnnlSetTransformerFeedForwardQuantizeDescriptor_v2(
    cnnlTransformerFeedForwardQuantizeDescriptor_t desc,
    const cnnlQuantizeDescriptor_t local_input_quant_desc,
    const cnnlQuantizeDescriptor_t fc1_filter_quant_desc,
    const cnnlQuantizeDescriptor_t fc1_input_quant_desc,
    const cnnlQuantizeDescriptor_t fc1_after_gemm_quant_desc,
    const cnnlQuantizeDescriptor_t fc2_input_quant_desc,
    const cnnlQuantizeDescriptor_t gate_filter_quant_desc,
    const cnnlQuantizeDescriptor_t gate_after_gemm_quant_desc,
    const cnnlQuantizeDescriptor_t fc2_filter_quant_desc,
    const cnnlQuantizeDescriptor_t fc2_after_gemm_quant_desc);

// Group:Transformer FeedForward
/*!
 *  @brief Sets the quantization information to the transformerfeedforward quantization
 *  descriptor \p desc that was previously created with the
 *  ::cnnlCreateTransformerFeedForwardQuantizeDescriptor function.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlSetTransformerFeedForwardQuantizeDescriptor_v4 instead.
 *
 *  @param[in,out] desc
 *    Input/output. The quantization descriptor of the transformerfeedforward operation. For detailed
 *    information, see ::cnnlTransformerFeedForwardQuantizeDescriptor_t.
 *  @param[in]  local_input_quant_desc
 *    Input. The quantization descriptor that holds quantization information of the local input.
 *  @param[in]  residual_input_quant_desc
 *    Input. The quantization descriptor that holds quantization information of the residual input.
 *  @param[in]  fc1_filter_quant_desc
 *    Input. The quantization descriptor that holds quantization information of the fc1 filter.
 *  @param[in]  fc1_input_quant_desc
 *    Input. The quantization descriptor that holds quantization information of the fc1 input.
 *  @param[in]  fc1_after_gemm_quant_desc
 *    Input. The quantization descriptor that holds quantization information of the fc1 after gemm.
 *  @param[in]  fc2_input_quant_desc
 *    Input. The quantization descriptor that holds quantization information of the fc2 input.
 *  @param[in]  gate_filter_quant_desc
 *    Input. The quantization descriptor that holds quantization information of the gate filter.
 *  @param[in]  gate_after_gemm_quant_desc
 *    Input. The quantization descriptor that holds quantization information of the gate after gemm.
 *  @param[in]  fc2_filter_quant_desc
 *    Input. The quantization descriptor that holds quantization information of the fc2 filter.
 *  @param[in]  fc2_after_gemm_quant_desc
 *    Input. The quantization descriptor that holds quantization information of the fc2 after gemm.
 *
 *  @note
 *    - Only when the quantization layout of \p fc1_filter_quant_desc is not equal to that of
 *      \p fc1_input_quant_desc, \p quant_bit_size of all quantization descriptors can be set to 4.
 *
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p desc is NULL.
 *
 */
CNNL_DEPRECATED_FOR(cnnlSetTransformerFeedForwardQuantizeDescriptor_v4)
cnnlStatus_t CNNL_WIN_API
cnnlSetTransformerFeedForwardQuantizeDescriptor_v3(
    cnnlTransformerFeedForwardQuantizeDescriptor_t desc,
    const cnnlQuantizeDescriptor_t local_input_quant_desc,
    const cnnlQuantizeDescriptor_t residual_input_quant_desc,
    const cnnlQuantizeDescriptor_t fc1_filter_quant_desc,
    const cnnlQuantizeDescriptor_t fc1_input_quant_desc,
    const cnnlQuantizeDescriptor_t fc1_after_gemm_quant_desc,
    const cnnlQuantizeDescriptor_t fc2_input_quant_desc,
    const cnnlQuantizeDescriptor_t gate_filter_quant_desc,
    const cnnlQuantizeDescriptor_t gate_after_gemm_quant_desc,
    const cnnlQuantizeDescriptor_t fc2_filter_quant_desc,
    const cnnlQuantizeDescriptor_t fc2_after_gemm_quant_desc);

// Group:Transformer FeedForward
/*!
 *  @brief Sets the quantization information to the transformerfeedforward quantization
 *  descriptor \p desc that was previously created with the
 *  ::cnnlCreateTransformerFeedForwardQuantizeDescriptor function.
 *
 *  Compared with ::cnnlSetTransformerFeedForwardQuantizeDescriptor_v3, this API uses cnnlQuantizeExDescriptor_t to
 *  set the quantization function and parameters, instead of ::cnnlQuantizeDescriptor_t.
 *
 *  @param[in,out] desc
 *    Input/output. The quantization descriptor of the transformerfeedforward operation. For detailed
 *    information, see ::cnnlTransformerFeedForwardQuantizeDescriptor_t.
 *  @param[in]  local_input_quant_desc
 *    Input. The quantization descriptor that holds quantization information of the local input.
 *  @param[in]  residual_input_quant_desc
 *    Input. The quantization descriptor that holds quantization information of the residual input.
 *  @param[in]  fc1_filter_quant_desc
 *    Input. The quantization descriptor that holds quantization information of the fc1 filter.
 *  @param[in]  fc1_input_quant_desc
 *    Input. The quantization descriptor that holds quantization information of the fc1 input.
 *  @param[in]  fc1_after_gemm_quant_desc
 *    Input. The quantization descriptor that holds quantization information of the fc1 after gemm.
 *  @param[in]  fc2_input_quant_desc
 *    Input. The quantization descriptor that holds quantization information of the fc2 input.
 *  @param[in]  gate_filter_quant_desc
 *    Input. The quantization descriptor that holds quantization information of the gate filter.
 *  @param[in]  gate_after_gemm_quant_desc
 *    Input. The quantization descriptor that holds quantization information of the gate after gemm.
 *  @param[in]  fc2_filter_quant_desc
 *    Input. The quantization descriptor that holds quantization information of the fc2 filter.
 *  @param[in]  fc2_after_gemm_quant_desc
 *    Input. The quantization descriptor that holds quantization information of the fc2 after gemm.
 *
 *  @note
 *    - Only when the quantization layout of \p fc1_filter_quant_desc is not equal to that of
 *      \p fc1_input_quant_desc, \p quant_bit_size of all quantization descriptors can be set to 4.
 *
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p desc is NULL.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlSetTransformerFeedForwardQuantizeDescriptor_v4(
  cnnlTransformerFeedForwardQuantizeDescriptor_t desc,
  const cnnlQuantizeExDescriptor_t local_input_quant_desc,
  const cnnlQuantizeExDescriptor_t residual_input_quant_desc,
  const cnnlQuantizeExDescriptor_t fc1_filter_quant_desc,
  const cnnlQuantizeExDescriptor_t fc1_input_quant_desc,
  const cnnlQuantizeExDescriptor_t fc1_after_gemm_quant_desc,
  const cnnlQuantizeExDescriptor_t fc2_input_quant_desc,
  const cnnlQuantizeExDescriptor_t gate_filter_quant_desc,
  const cnnlQuantizeExDescriptor_t gate_after_gemm_quant_desc,
  const cnnlQuantizeExDescriptor_t fc2_filter_quant_desc,
  const cnnlQuantizeExDescriptor_t fc2_after_gemm_quant_desc);


// Group:Transformer FeedForward
/*!
 *  @brief Creates a quantization descriptor of transformerfeedforward and
 *  allocate memory for it.
 *
 *  @param[out]  desc
 *    Output. Quantization description of transformerfeedforward operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_ALLOC_FAILED
 *    The function failed to allocate memory space.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCreateTransformerFeedForwardQuantizeDescriptor(
    cnnlTransformerFeedForwardQuantizeDescriptor_t *desc);

// Group:Transformer FeedForward
/*!
 *  @brief Destroys the quantization descriptor of transformerfeedforward and free
 * memory for it.
 *
 *  @param[in]  desc
 *    Input. Quantization description of transformerfeedforward operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One of the following conditions is met:
 *    - The value of \p desc is NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlDestroyTransformerFeedForwardQuantizeDescriptor(
    cnnlTransformerFeedForwardQuantizeDescriptor_t desc);

// Group:Transformer FeedForward
/*!
 *  @brief Sets TensorFloat-32 mode for transformerFeedForward operation
 *  with parameter.
 *
 *  @param[in] desc
 *    Input. The transformerFeedForward descriptor to be set.
 *  @param[in] allow_tf32
 *    Input. A scalar value indicating whether to enable TensorFloat-32 mode. It can only be
 *           0 or 1. 0 means not using TensorFloat-32 mode, while 1 means using TensorFloat-32 mode.
 *           Only when the data type of filter is CNNL_DTYPE_FLOAT and running on MLU500 series,
 *           the value of \p allow_tf32 can be 1.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One of the following conditions is met:
 *      - \p desc is NULL.
 *      - One or more parameters are unsupported.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetTransformerFeedForwardDescriptorAllowTF32(cnnlTransformerFeedForwardDescriptor_t desc,
                                                 int allow_tf32);

// Group:Transformer FeedForward
/*!
 *  @brief Sets normalization type for transformerFeedForward operation
 *  with parameter.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlFuseNorm_v3 instead.
 *
 *  @param[in] desc
 *    Input. The descriptor of the TransformerFeedForward operation.
 *  @param[in] norm_type
 *    Input. The normalization type of the transformerFeedForward operation.
 *           See ::cnnlTransformerNormType_t for details.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One of the following conditions is met:
 *      - \p desc is NULL.
 *      - One or more parameters are unsupported.
 */
CNNL_DEPRECATED_FOR(cnnlFuseNorm_v3)
cnnlStatus_t CNNL_WIN_API
cnnlSetTransformerFeedForwardDescriptorNormType(cnnlTransformerFeedForwardDescriptor_t desc,
                                                cnnlTransformerNormType_t norm_type);

// Group:Transformer FeedForward
/*!
 *  @brief Sets the descriptors and pointers of gate filters and bias
 *  for transformerFeedForward operation.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlTransformerFeedForward_v2 instead.
 *
 *  @param[in] desc
 *    Input. The descriptor of the TransformerFeedForward operation.
 *  @param[in] gate_filters_desc
 *    Input. The descriptor of the gate_filters tensor used as filters in the TransformerFeedForward operation.
 *  @param[in] gate_filters
 *    Input. Pointer to the MLU memory that stores the gate_filters tensor.
 *  @param[in] gate_bias_desc
 *    Input. The descriptor of the gate_bias tensor used as bias in the TransformerFeedForward operation.
 *  @param[in] gate_bias
 *    Input. Pointer to the MLU memory that stores the gate_bias tensor.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One of the following conditions is met:
 *      - \p desc is NULL.
 *      - One or more parameters are unsupported.
 */
CNNL_DEPRECATED_FOR(cnnlTransformerFeedForward_v2)
cnnlStatus_t CNNL_WIN_API cnnlSetTransformerFeedForwardDescriptorGateFiltersBias(
    cnnlTransformerFeedForwardDescriptor_t desc,
    cnnlTensorDescriptor_t gate_filters_desc, void *gate_filters,
    cnnlTensorDescriptor_t gate_bias_desc, void *gate_bias);

// Group:Transformer FeedForward
/*!
 *  @brief Gets the extra space size needed in transformerfeedforward operation.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlGetTransformerFeedForwardWorkspaceSize_v3 instead.
 *
 *  @par API Dependency
 *  - You need to call the ::cnnlCreateTransformerFeedForwardDescriptor and
 *  ::cnnlSetTransformerFeedForwardDescriptor functions before calling this function.
 *
 *  @param[in]  handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *           transformer_feed_forward operation.
 *  @param[in]  desc
 *    Input. Descriptor of transformer_feed_forward operation.
 *  @param[in]  input_desc
 *    Input. Descriptor of input, containing dimension and the layout of input.
 *  @param[in]  fc1_filters_desc
 *    Input. Descriptor of fc1_filters, containing dimension and the layout of fc1_filters.
 *  @param[out] size
 *    Output. Size of workspace.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    - The value of \p desc is NULL.
 */
CNNL_DEPRECATED_FOR(cnnlGetTransformerFeedForwardWorkspaceSize_v3)
cnnlStatus_t CNNL_WIN_API
cnnlGetTransformerFeedForwardWorkspaceSize(cnnlHandle_t handle,
                                           const cnnlTransformerFeedForwardDescriptor_t desc,
                                           const cnnlTensorDescriptor_t input_desc,
                                           const cnnlTensorDescriptor_t fc1_filters_desc,
                                           size_t *size);

// Group:Transformer FeedForward
/*!
 *  @brief Gets the extra space size needed in transformerfeedforward operation.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlGetTransformerFeedForwardWorkspaceSize_v3 instead.
 *
 *  @param[in]  handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *           transformer_feed_forward operation.
 *  @param[in]  desc
 *    Input. Descriptor of transformer_feed_forward operation.
 *  @param[in]  input_desc
 *    Input. Descriptor of input, containing dimension and the layout of input.
 *  @param[in]  fc1_filters_desc
 *    Input. Descriptor of fc1_filters, containing dimension and the layout of fc1_filters.
 *  @param[in] quantize_desc
 *    Input. The descriptor of the quantization mode in transformer_feed_forward operation.
 *    For detailed information, see ::cnnlTransformerFeedForwardQuantizeDescriptor_t.
 *  @param[in] active_desc
 *    Input. The descriptor of the activation in transformer_feed_forward operation.
 *    For detailed information, see cnnlActivationDescriptor_t.
 *  @param[out] size
 *    Output. Size of workspace.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    - The value of \p desc is NULL.
 *
 *  @par API Dependency
 *  - You need to call the following functions before calling this function:
 *    - ::cnnlCreateTransformerFeedForwardDescriptor
 *    - ::cnnlSetTransformerFeedForwardDescriptor
 *    - ::cnnlCreateTransformerFeedForwardQuantizeDescriptor
 *    - ::cnnlSetTransformerFeedForwardQuantizeDescriptor_v3
 *    - cnnlCreateActivationDescriptor
 *    - cnnlSetActivationDescriptor_v5
 */
CNNL_DEPRECATED_FOR(cnnlGetTransformerFeedForwardWorkspaceSize_v3)
cnnlStatus_t CNNL_WIN_API
cnnlGetTransformerFeedForwardWorkspaceSize_v2(cnnlHandle_t handle,
                                              const cnnlTransformerFeedForwardDescriptor_t desc,
                                              const cnnlTensorDescriptor_t input_desc,
                                              const cnnlTensorDescriptor_t fc1_filters_desc,
                                              const cnnlTransformerFeedForwardQuantizeDescriptor_t
                                                    quantize_desc,
                                              const cnnlActivationDescriptor_t active_desc,
                                              size_t *size);

// Group:Transformer FeedForward
/*!
 *  @brief Gets the extra space size needed in transformerfeedforward operation.
 *
 *  Compared with ::cnnlGetTransformerFeedForwardWorkspaceSize_v2, this API adds \p gate_filters_desc to set Gate
 *  funciton, instead of ::cnnlSetTransformerFeedForwardDescriptorGateFiltersBias.
 *
 *  @param[in]  handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *           transformer_feed_forward operation.
 *  @param[in]  desc
 *    Input. Descriptor of transformer_feed_forward operation.
 *  @param[in]  input_desc
 *    Input. Descriptor of input, containing dimension and the layout of input.
 *  @param[in]  fc1_filters_desc
 *    Input. Descriptor of fc1_filters, containing dimension and the layout of fc1_filters.
 *  @param[in]  gate_filters_desc
 *    Input. Descriptor of gate_filters, containing dimension and the layout of gate_filters.
 *  @param[in] quantize_desc
 *    Input. The descriptor of the quantization mode in transformer_feed_forward operation.
 *    For detailed information, see ::cnnlTransformerFeedForwardQuantizeDescriptor_t.
 *  @param[in] active_desc
 *    Input. The descriptor of the activation in transformer_feed_forward operation.
 *    For detailed information, see cnnlActivationDescriptor_t.
 *  @param[out] size
 *    Output. Size of workspace.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    - The value of \p desc is NULL.
 *  @par API Dependency
 *  - You need to call the following fucntions before calling this function:
 *    - ::cnnlCreateTransformerFeedForwardDescriptor
 *    - ::cnnlSetTransformerFeedForwardDescriptor
 *    - ::cnnlCreateTransformerFeedForwardQuantizeDescriptor
 *    - ::cnnlSetTransformerFeedForwardQuantizeDescriptor_v4
 *    - cnnlCreateActivationDescriptor
 *    - cnnlSetActivationDescAttr
 */
cnnlStatus_t CNNL_WIN_API cnnlGetTransformerFeedForwardWorkspaceSize_v3(
  cnnlHandle_t handle,
  const cnnlTransformerFeedForwardDescriptor_t desc,
  const cnnlTensorDescriptor_t input_desc,
  const cnnlTensorDescriptor_t fc1_filters_desc,
  const cnnlTensorDescriptor_t gate_filters_desc,
  const cnnlTransformerFeedForwardQuantizeDescriptor_t quantize_desc,
  const cnnlActivationDescriptor_t active_desc,
  size_t *size);

// Group:Transformer FeedForward
/*!
 *  @brief Executes FeedForwardNetwork in Transformer encoder and decoder network.
 *
 *  Compared with ::cnnlTransformerFeedForward, this API adds \p gate_filters_desc, \p gate_filters, \p gate_bias_desc and
 *   \p gate_bias to set the gate funciton, instead of ::cnnlSetTransformerFeedForwardDescriptorGateFiltersBias.
 *
 *  This operation performs with the following steps:
 *
 *    1. Previous normalization (optional)
 *
 *       input = norm(input, norm_scale, norm_bias).
 *
 *    2. Fc1
 *
 *       fc1_output = fc(input, fc1_filters)
 *
 *    3. Add bias (optional)
 *
 *       fc1_output = add(fc1_output, fc1_bias)
 *
 *    4. Activation
 *
 *       fc1_output = active(fc1_output)
 *
 *    5. Fc1 gate (optional)
 *
 *       gate_output = fc(input, gate_filters)
 *
 *    6. Add bias (optional)
 *
 *       gate_output = add(gate_output, gate_bias)
 *
 *    7. Mul fc1  (optional)
 *
 *       fc1_output = mul(fc1_output, gate_output)
 *
 *    8. Fc2
 *
 *       fc2_output = fc(fc1_output, fc2_filters)
 *
 *    9. Add bias (optional)
 *
 *       fc2_output = add(fc2_output, fc2_bias)
 *
 *    11. Residual add (optional)
 *
 *       output = add(alpha * fc2_output, beta * residual_input).
 *       - Alpha, also known as post_scale.
 *       - When the norm_structure is PRE_LAYERNORM_OUTSIDE_RESIDUAL, the residual_input is
 *         the result of pre_norm, otherwise is the original input.
 *
 *    12. Post normalization (optional)
 *
 *       output = norm(output, norm_scale, norm_bias)
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices
 *    and queues in the transformer_feed_forward operation.
 *  @param[in] desc
 *    Input. The descriptor of the transformer_feed_forward operation. For detailed information,
 *    see ::cnnlTransformerFeedForwardDescriptor_t.
 *  @param[in] activation_desc
 *    Input. The descriptor of the activation in transformer_feed_forward operation.
 *    For detailed information, see cnnlActivationDescriptor_t.
 *  @param[in] quantize_desc
 *    Input. The descriptor of the quantization mode in transformer_feed_forward operation.
 *    For detailed information, see ::cnnlTransformerFeedForwardQuantizeDescriptor_t.
 *  @param[in] input_desc
 *    Input. The descriptor of the input tensor.
 *  @param[in] input
 *    Input. Pointer to the MLU memory that stores the input tensor.
 *  @param[in] fc1_filters_desc
 *    Input. The descriptor of the fc1_filters tensor used as filters in the transformer_feed_forward operation.
 *  @param[in] fc1_filters
 *    Input. Pointer to the MLU memory that stores the fc1_filters tensor.
 *  @param[in] fc1_bias_desc
 *    Input. The descriptor of the fc1_bias tensor used as bias in the transformer_feed_forward operation.
 *  @param[in] fc1_bias
 *    Input. Pointer to the MLU memory that stores the fc1_bias tensor.
 *  @param[in] gate_filters_desc
 *    Input. The descriptor of the gate_filters tensor used as filters in the transformer_feed_forward operation.
 *  @param[in] gate_filters
 *    Input. Pointer to the MLU memory that stores the gate_filters tensor.
 *  @param[in] gate_bias_desc
 *    Input. The descriptor of the gate_bias tensor used as bias in the transformer_feed_forward operation.
 *  @param[in] gate_bias
 *    Input. Pointer to the MLU memory that stores the gate_bias tensor.
 *  @param[in] fc2_filters_desc
 *    Input. The descriptor of the fc2_filters tensor used as filters in the transformer_feed_forward operation.
 *  @param[in] fc2_filters
 *    Input. Pointer to the MLU memory that stores the fc2_filters tensor.
 *  @param[in] fc2_bias_desc
 *    Input. The descriptor of the fc2_bias tensor used as bias in the transformer_feed_forward operation.
 *  @param[in] fc2_bias
 *    Input. Pointer to the MLU memory that stores the fc2_bias tensor.
 *  @param[in] norm_scale_desc
 *    Input. The descriptor of the norm_scale tensor used as the scale of normalization module
 *    in the transformer_feed_forward operation.
 *  @param[in] norm_scale
 *    Input. Pointer to the MLU memory that stores the norm_scale tensor.
 *  @param[in] norm_bias_desc
 *    Input. The descriptor of the norm_bias tensor used as the bias of normalization module
 *    in the transformer_feed_forward operation.
 *  @param[in] norm_bias
 *    Input. Pointer to the MLU memory that stores the norm_bias tensor.
 *  @param[in] workspace
 *    Input. Pointer to the MLU memory that is used as an extra workspace for the
 *    transformer_feed_forward operation.
 *  @param[in] workspace_size
 *    Input. The size of the extra workspace in bytes that needs to be used in
 *    the transformer_feed_forward operation. You can get the size of the workspace with
 *    the ::cnnlGetTransformerFeedForwardWorkspaceSize_v2 function.
 *  @param[out] output_desc
 *    Output. The descriptor of the output tensor.
 *  @param[out] output
 *    Output. Pointer to the MLU memory that stores the output tensor.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are encountered:
 *    - The handle is NULL.
 *    - The pointer is NULL.
 *    - Parameters do not meet scale limitation.
 *  @retval CNNL_STATUS_ARCH_MISMATCH
 *    One or more of the following conditions are encountered:
 *    - This function is run on the hardware platform that is not supported.
 *
 *  @par Data Type
 *  - input: float, half, bfloat16.
 *  - fc1 filters: float, half, int8, bfloat16.
 *  - fc1 bias: float, half, bfloat16.
 *  - fc2 filters: float, half, int8, bfloat16.
 *  - fc2 bias: float, half, bfloat16.
 *  - gate filters: float, half, int8, bfloat16.
 *  - gate bias: float, half, bfloat16.
 *  - norm scale: float, half, bfloat16.
 *  - norm bias: float, half, bfloat16.
 *  - output: float, half, bfloat16.
 *  - The data types of \p fc1_filters, \p fc2_filters and \p gate_filters must be the same.
 *  - The data types of tensor that is unrelated to filters and dequantization param tensor must be the
 *    same.
 *  - When the data type of \p fc1_filters is int8, all tensors except filter tensor must have
 *    the same data type: half, float or bfloat16.
 *  - Only when the quantization mode is CNNL_QUANTIZE_GROUP_WISE, the data type of \p input can be
 *    bfloat16 when the data type of \p fc1_filters is int8.
 *  - When the quantization mode is CNNL_QUANTIZE_GROUP_WISE, the data types of dequantization param tensors
 *    must be the same as that of \p input.
 *
 *  @par Quantization Layout
 *  - When the quantization layout of tensor is CNNL_QUANTIZE_NONE, it means that the tensor will not be
 *    quantized.
 *  - The supported quantization layouts are as follows:
 *    - local_input: CNNL_QUANTIZE_NONE.
 *    - residual_input: CNNL_QUANTIZE_NONE, CNNL_QUANTIZE_PER_TENSOR.
 *    - fc1_filter: CNNL_QUANTIZE_NONE, CNNL_QUANTIZE_PER_TENSOR, CNNL_QUANTIZE_PER_CHANNEL,
 *                  CNNL_QUANTIZE_GROUP_WISE.
 *    - fc1_input: CNNL_QUANTIZE_NONE, CNNL_QUANTIZE_PER_TENSOR, CNNL_QUANTIZE_PER_CHANNEL.
 *    - fc1_after_gemm: CNNL_QUANTIZE_NONE, CNNL_QUANTIZE_PER_TENSOR.
 *    - fc2_input: CNNL_QUANTIZE_NONE, CNNL_QUANTIZE_PER_TENSOR, CNNL_QUANTIZE_PER_CHANNEL.
 *    - gate_filter: CNNL_QUANTIZE_NONE, CNNL_QUANTIZE_PER_TENSOR, CNNL_QUANTIZE_PER_CHANNEL,
 *                   CNNL_QUANTIZE_GROUP_WISE.
 *    - gate_after_gemm: CNNL_QUANTIZE_NONE, CNNL_QUANTIZE_PER_TENSOR.
 *    - fc2_filter: CNNL_QUANTIZE_NONE, CNNL_QUANTIZE_PER_TENSOR, CNNL_QUANTIZE_PER_CHANNEL,
 *                  CNNL_QUANTIZE_GROUP_WISE.
 *    - fc2_after_gemm: CNNL_QUANTIZE_NONE, CNNL_QUANTIZE_PER_TENSOR.
 *
 *    The quantization layouts of the
 *    \p fc1_filters, \p fc2_filters and \p gate_filters should be same.
 *  - When the quantization layout of \p fc1_filters is CNNL_QUANTIZE_GROUP_WISE, the quantization
 *    of \p input must be CNNL_QUANTIZE_NONE.
 *  - When the quantization layout of \p input is CNNL_QUANTIZE_NONE, the quantization layout of
 *    local_input, residual_input, fc1_after_gemm, fc2_input, gate_after_gemm and fc2_after_gemm
 *    must be CNNL_QUANTIZE_NONE.
 *  - When the quantization layout of \p input is CNNL_QUANTIZE_PER_CHANNEL, the quantization layout
 *    of fc2_input and filters must be CNNL_QUANTIZE_PER_CHANNEL, and the quantization layout of
 *    local_input, residual_input, fc1_after_gemm, gate_after_gemm and fc2_after_gemm
 *    must be CNNL_QUANTIZE_NONE.
 *
 *  @note
 *  - This function supports MLU300 and MLU500 series.
 *  - Data type bfloat16 is not supported on MLU300 series.
 *  - \p desc and \p activation_desc are required. Use
 *    ::cnnlCreateTransformerFeedForwardDescriptor and cnnlCreateActivationDescriptor to create the
 *    descriptor, use ::cnnlSetTransformerFeedForwardDescriptor_v2 and
 *    cnnlSetActivationDescriptor_v5 to set the descriptor, and use
 *    ::cnnlDestroyTransformerFeedForwardDescriptor and cnnlDestroyActivationDescriptor to free the
 *    descriptor.
 *  - When \p quantize_desc is necessary,
 *    use ::cnnlCreateTransformerFeedForwardQuantizeDescriptor to create the descriptor,
 *    use ::cnnlSetTransformerFeedForwardQuantizeDescriptor_v3 to set the descriptor,
 *    and use ::cnnlDestroyTransformerFeedForwardQuantizeDescriptor to free the descriptor.
 *  - When \p workspace is necessary to complete the function, use
 *    ::cnnlGetTransformerFeedForwardWorkspaceSize_v2 to get the \p workspace_size.
 *  - When gated feed forward is required, set \p gate_filters_desc and \p gate_filters to non-NULL.
 *  - When \p fc1_bias is not required, set \p fc1_bias_desc and \p fc1_bias to NULL.
 *  - When \p fc2_bias is not required, set \p fc2_bias_desc and \p fc2_bias to NULL.
 *  - When \p gate_bias is not required, set \p gate_bias_desc and \p gate_bias to NULL.
 *  - The norm_type only supports CNNL_TRANSFORMER_LAYERNORM, CNNL_TRANSFORMER_RMSNORM and
 *    CNNL_TRANSFORMER_SCALENORM.
 *  - When the data type of \p fc1_filters is CNNL_DTYPE_INT8:
 *    - The quantization mode only supports CNNL_QUANTIZE_POSITION_SCALE.
 *    - The quantization bit width only supports 4 and 8.
 *    - When the quantization layout of \p input is CNNL_QUANTIZE_PER_TENSOR:
 *      - The norm_type only supports CNNL_TRANSFORMER_LAYERNORM.
 *      - The norm_structure only supports CNNL_TRANSFORMER_PRE_LAYERNORM_INSIDE_RESIDUAL and
 *        CNNL_TRANSFORMER_PRE_LAYERNORM_OUTSIDE_RESIDUAL.
 *      - The quantization bit width must be 8.
 *      - When residual_input_quant is enabled, the norm_structure only supports
 *        CNNL_TRANSFORMER_PRE_LAYERNORM_OUTSIDE_RESIDUAL.
 *      - The post_scale must be 1.0f.
 *      - fc1_after_gemm_quant and fc2_after_gemm_quant must be enabled together.
 *    - When the quantization layout of \p input is CNNL_QUANTIZE_NONE:
 *      - The quantization bit width must be 4 and 8.
 *      - When the quantization layout of \p fc1_filters is CNNL_QUANTIZE_GROUP_WISE:
 *        - The data type of dequantization param scale of filters must be the same as that of \p input.
 *        - \p gate_filters cannot be null.
 *        - All the dimensions of the \p fc1_filters, \p fc2_filters and \p gate_filters must be
 *          divided by group_size.
 *        - The group_size only supports 64, 128, 256 or 512.
 *        - Before the operation can process the quantization coefficient tensor, you must perform
 *          a specific preprocessing step, which involves adjusting the scale according to the
 *          formula: scale = pow(2, position) / scale. Note that this operation is agnostic to the
 *          value of position in this scenario.
 *      - The quantization param tensor position and scale cannot be null.
 *    - When the quantization layout of \p input is CNNL_QUANTIZE_PER_CHANNEL:
 *      - The quantization bit width of all tensors must be 8.
 *      - The scale of \p fc1_input_quant_desc will multiply with input after norm if norm exist.
 *      - The scale of \p fc2_input_quant_desc will multiply with input after fc1.
 *      - You should fuse input smooth scale with input_quant_scale into input_quant_desc to quantize
 *        input and smoother input.
 *      - The scale of \p fc1_filter_quant_desc, \p fc2_filter_quant_desc and \p gate_filter_quant_desc
 *        will multiply with the result of each filters.
 *      - You should fuse filter smooth scale, input_dequant_scale and filter_dequant_scale into
 *        filter_quant_desc to dequantize filter and smoother filter.
 *
 *  - The post normalization ability (for example, when norm_structure is CNNL_TRANSFORMER_POST_LAYERNORM_NO_RESIDUAL,
 *    CNNL_TRANSFORMER_POST_LAYERNORM_INSIDE_RESIDUAL or CNNL_TRANSFORMER_POST_LAYERNORM_OUTSIDE_RESIDUAL)
 *    in ::cnnlTransformerFeedForward is deprecated and will be removed in future release.
 *    Use ::cnnlFuseNorm to process post normalization instead.
 *
 *  @par Scale Limitation
 *  - The range of the last dimension of the \p input tensor is between 16 and 14336. In addition,
 *    when the last dimension of the \p input tensor is greater than or equal to 2048, or the first
 *    dimension of the \p fc1_filters is greater than 8192, the length of the last dimension of the
 *    \p input tensor must be a multiple of 512.
 *  - The first dimension of the \p fc1_filters has a length greater than or equal to 16 and less
 *    than or equal to a value that ensures the size of the tensor is less than 2G bytes.
 *  - When the quantization layout of \p input is not equal to that of \p fc1_filters,
 *    the last dimension of \p input should not be less than 2048.
 *  - When the quantization layout of \p input is CNNL_QUANTIZE_PER_TENSOR, the last dimension of
 *    the \p input tensor cannot be greater than 2048, and the first dimension of the \p fc1_filters
 *    tensor cannot be greater than 8192.
 *  - When the quantization layout of \p fc1_filters is CNNL_QUANTIZE_GROUP_WISE, the first
 *    dimension of quant_param should be same as the first dimension of the corresponding filters
 *    tensor, and the last dimension of quant_param should be same as the last dimension of the
 *    corresponding filters tensor divided by group_size.
 *  - The dimensions of all tensors must meet the following conditions:
 *      - The dimensions of \p input and \p output must be the same.
 *      - The dimensions of \p input can only be 2 or 3.
 *      - The dimensions of \p fc1_filters, \p gate_filters and \p fc2_filters must be 2.
 *      - The dimensions of \p fc1_bias, \p gate_bias, \p fc2_bias, \p norm_scale and
 *      \p norm_bias must be 1.
 *      - The dimension sizes of \p input and \p output along the same dimension must be the same.
 *    - The last dimension of \p input must be the same as the following dimensions of tensors:
 *      - The second dimension of \p fc1_filters.
 *      - The second dimension of \p gate_filters.
 *      - The first dimension of \p fc2_filters.
 *      - The first dimension of \p fc2_bias.
 *      - The first dimension of \p norm_scale.
 *      - The first dimension of \p norm_bias.
 *    - The first dimension of \p fc1_filters must be the same as the following dimensions of tensors:
 *      - The second dimension of \p fc2_filters.
 *      - The first dimension of \p gate_filters.
 *      - The first dimension of \p fc1_bias.
 *
 *  @par Performance Optimization
 *  - When the last dimension of \p input can be divisible by 64, and
 *  meanwhile the first dimension of \p fc1_filters is 4
 *    times of the last dimension of \p input, feed_forward function has best performance.
 *
 *  @par API Dependency
 *  - Before calling this function, you need to call the ::cnnlCreateTransformerFeedForwardDescriptor, ::cnnlGetTransformerFeedForwardWorkspaceSize_v2
 *    and cnnlCreateActivationDescriptor functions.
 *  - After calling this function, you need to call the ::cnnlDestroyTransformerFeedForwardDescriptor and cnnlDestroyActivationDescriptor
 *    function.
 *
 *  @par Example
    @verbatim
    Dimension of \p input: [batch_num, seq_len, hidden_size]
    Dimension of \p fc1_filters: [filter_size, hidden_size]
    Dimension of \p fc1_bias: [filter_size]
    Dimension of \p fc2_filters: [hidden_size, filter_size]
    Dimension of \p fc2_bias: [hidden_size]
    Dimension of \p norm_scale: [hidden_size]
    Dimension of \p norm_bias: [hidden_size]

    Then we will get the output:

    Dimension of \p output: [batch_num, seq_len, hidden_size]
    @endverbatim

 */
cnnlStatus_t CNNL_WIN_API
cnnlTransformerFeedForward_v2(cnnlHandle_t handle,
                              const cnnlTransformerFeedForwardDescriptor_t desc,
                              const cnnlActivationDescriptor_t activation_desc,
                              const cnnlTransformerFeedForwardQuantizeDescriptor_t quantize_desc,
                              const cnnlTensorDescriptor_t input_desc,
                              const void *input,
                              const cnnlTensorDescriptor_t fc1_filters_desc,
                              const void *fc1_filters,
                              const cnnlTensorDescriptor_t fc1_bias_desc,
                              const void *fc1_bias,
                              const cnnlTensorDescriptor_t gate_filters_desc,
                              const void *gate_filters,
                              const cnnlTensorDescriptor_t gate_bias_desc,
                              const void *gate_bias,
                              const cnnlTensorDescriptor_t fc2_filters_desc,
                              const void *fc2_filters,
                              const cnnlTensorDescriptor_t fc2_bias_desc,
                              const void *fc2_bias,
                              const cnnlTensorDescriptor_t norm_scale_desc,
                              const void *norm_scale,
                              const cnnlTensorDescriptor_t norm_bias_desc,
                              const void *norm_bias,
                              void *workspace,
                              size_t workspace_size,
                              cnnlTensorDescriptor_t output_desc,
                              void *output);

/******************************************************************************
 * CNNLEXTRA OP Information
 ******************************************************************************/

/*!
 * @brief Enumeration variables describing the supported operations.
 */
typedef enum {
  TRANSFORMER_SELF_ATTN = 0,  /*!< The operation transformer_self_attn.*/
  PYTORCH_SELFATTN,  /*!< The operation pytorch_selfattn.*/
  TRANSFORMER_FFN,  /*!< The operation transformer_ffn.*/
  PYTORCH_FFN,  /*!< The operation pytorch_ffn.*/
  BERT_POST CNNL_DEPRECATED_ENUM_FOR(),
  /*!< The operation bertpost.
   *   BERT_POST is depercated and will be removed in future release.*/
  BERT_PRE,  /*!< The operation bertpre.*/
  TRANSFORMER_BEAMSEARCH,  /*!< The operation transformer_beamsearch.*/
  TRANSFORMER_BEAMREARRANGE,  /*!< The operation transformer_beamrearrange.*/
  TRANSFORMER_EMBEDDING,  /*!< The operation transformer_embedding.*/
  TRANSFORMER_ENCODER_OUTPUT,  /*!< The operation transformer_encoder_output.*/
  TRANSFORMER_POSITION_ENCODING,  /*!< The operation transformer_position_encoding.*/
  TRANSFORMER_ENCDECATTN,  /*!< The operation transformer_encdecattn.*/
  TRANSFORMER_FCTOPK,  /*!< The operation transformer_fctopk.*/
  PACK2PAD,  /*!< The operation padpackedsequence.*/
  PAD2PACK, /*!< The operation packpaddedsequence.*/
  TACOTRON2_DECODER CNNL_DEPRECATED_ENUM_FOR(),
  /*!< The operation tacotron2_decoder.
   *   TACOTRON2_DECODER is deprecated and will be removed in future release.*/
  DLRM_INTERACT,  /*!< The operation dlrm_interact.*/
  TRANSFORMER_ATTN_PROJ,  /*!< The operation transformer_attn_proj.*/
  TRANSFORMER_ATTENTION,  /*!< The operation transformer_attention.*/
  TRANSFORMER_FEED_FORWARD,  /*!< The operation transformer_feed_forward.*/
  FUSENORM, /*!< The operation fusenorm. */
  CNNLEXTRA_OP_COUNT,  /*!< The total counts of operations.*/
} cnnlextraSupportOps_t;

/*!
 * @brief Enumeration variables describing the supported devices of an operation.
 */
typedef enum {
  MTP_220_1_CLUSTER = 0,  /*!< The device mtp220 with 1 cluster is supported.*/
  MTP_270_4_CLUSTER,  /*!< The device mtp270 with 4 clusters is supported.*/
  MTP_290_8_CLUSTER,  /*!< The device mtp290 with 8 clusters is supported.*/
  MTP_322_2_CLUSTER,  /*!< The device mtp322 with 2 clusters is supported.*/
  MTP_372_2_CLUSTER,  /*!< The device mtp372 with 2 clusters is supported.*/
  MTP_372_4_CLUSTER,  /*!< The device mtp372 with 4 clusters is supported.*/
  MTP_372_6_CLUSTER,  /*!< The device mtp372 with 6 clusters is supported.*/
  MTP_372_8_CLUSTER,  /*!< The device mtp372 with 8 clusters is supported.*/
  MTP_520_1_CLUSTER,  /*!< The device mtp520 with 1 cluster is supported.*/
  MTP_520_2_CLUSTER,  /*!< The device mtp520 with 2 clusters is supported.*/
  MTP_592_10_CLUSTER,  /*!< The device mtp592 with 10 clusters is supported.*/
  MTP_592_12_CLUSTER,  /*!< The device mtp592 with 12 clusters is supported.*/
  MTP_592_4_CLUSTER,  /*!< The device mtp592 with 4 clusters is supported.*/
  CNNLEXTRA_DEVICE_COUNT,  /*!< The total counts of the supported devices.*/
} cnnlextraSupportDevices_t;

/*!
 * @brief Enumeration variables describing the supported data types of an operation on different devices.
 */
typedef enum {
  DEVICE_SPPT_NONE = 0,
  /*!< No data type is supported.*/
  DEVICE_SPPT_FP16_FP32 = 3,
  /*!< fp16 and fp32 are supported.*/
  DEVICE_SPPT_FP16_FP32_INT8 = 7,
  /*!< fp16, fp32 and int8 are supported.*/
  DEVICE_SPPT_FP16_FP32_INT16 = 11,
  /*!< fp16, fp32 and int16 are supported.*/
  DEVICE_SPPT_FP16_FP32_INT8_INT16 = 15,
  /*!< fp16, fp32, int8 and int16 are supported.*/
  DEVICE_SPPT_FP16_FP32_INT32 = 19,
  /*!< fp16, fp32 and int32 are supported.*/
  DEVICE_SPPT_FP16_FP32_INT8_TF32 = 39,
  /*!< fp16, fp32, int8 and tf32 are supported.*/
} cnnlextraSupportDeviceDataType_t;

/*!
 * @brief Enumeration variables describing the supported job types in Cambricon CNNLEXTRA.
*/
typedef enum {
  CNNLEXTRA_JOBTYPE_NOLIMIT = 0,
  /*!< No job type limitation.*/
  CNNLEXTRA_JOBTYPE_BLOCK,
  /*!< One MLU core is used to execute tasks once.*/
  CNNLEXTRA_JOBTYPE_UNION1,
  /*!< One MLU cluster is used to execute tasks once.*/
  CNNLEXTRA_JOBTYPE_UNION2,
  /*!< Two MLU clusters are used to execute tasks once.*/
  CNNLEXTRA_JOBTYPE_UNION4,
  /*!< Four MLU clusters are used to execute tasks once.*/
  CNNLEXTRA_JOBTYPE_UNION8,
  /*!< Eight MLU clusters are used to execute tasks once.*/
  CNNLEXTRA_JOBTYPE_COUNT,
  /*!< The total count of supported job types.*/
} cnnlextraJobType_t;

// Group:GetSupportDevice
/*!
 *  @brief Receives a certain group of operation names and
 *   then gets the supported devices and data types.
 *  @param[in] kernel_name
 *    Input. The name of the operation.
 *           See ::cnnlextraSupportOps_t to view the supported operations.
 *  @param[out] output
 *    Output. Pointer to the host memory that stores the final data.
 *  @param[out] output_size
 *    Output. The number of output elements. It should be not less than ::CNNLEXTRA_DEVICE_COUNT.
 *  @retval CNNL_STATUS_SUCCESS
      The function ends normally.
    @retval CNNL_STATUS_BAD_PARAM
      The pointer of output is null.
 */
cnnlStatus_t CNNL_WIN_API
cnnlextraGetSupportDevice(cnnlextraSupportOps_t kernel_name,
                          cnnlextraSupportDeviceDataType_t  * output,
                          int output_size);

// Group:GetSupportDevice
/*!
 *  @brief Receives an operator, a device name, an available cluster number,
 *  and a job type parameter, and then returns whether the operator can run under these constraints.
 *  @param[in] kernel_name
 *   Input. The name of the operator.
 *          See ::cnnlextraSupportOps_t to view the supported operators.
 *  @param[in] device_name
 *   Input. The name of the device.
 *          See ::cnnlextraSupportDevices_t to view the supported devices.
 *  @param[in] cluster_num
 *   Input. The count of available clusters.
 *  @param[in] job_type
 *   Input. The required job type.
 *          See ::cnnlextraJobType_t to view the supported job types.
 *  @retval
 *   Whether the operator can run under these constraints.
 *  @par Scale Limitation
 *   This function has following constraints:
 *   - \p kernel_name should be one of TRANSFORMER_SELF_ATTN, TRANSFORMER_FFN, BERT_POST,
 *     BERT_PRE, PACK2PAD, PAD2PACK, TRANSFORMER_ATTN_PROJ, TRANSFORMER_ATTENTION and
 *     TRANSFORMER_FEED_FORWARD.
 *   - \p cluster_num should be one of 2, 4 and 12.
 *   - \p job_type should be CNNLEXTRA_JOBTYPE_NOLIMIT.
 * */
bool CNNL_WIN_API
cnnlextraGetSupportCluster(cnnlextraSupportOps_t kernel_name,
                           cnnlextraSupportDevices_t device_name,
                           int cluster_num,
                           cnnlextraJobType_t job_type);

/******************************************************************************
 * Cambricon CNNL OP: TransformerAttnProj
 ******************************************************************************/
/*! The descriptor of the ::cnnlTransformerAttnProj operation.
 * You can use ::cnnlCreateTransformerAttnProjDescriptor, ::cnnlSetTransformerAttnProjDescriptor,
 * ::cnnlGetTransformerAttnProjDescriptor, and ::cnnlDestroyTransformerAttnProjDescriptor to create,
 * set, get and destroy the descriptor respectively.
 */
typedef struct cnnlTransformerAttnProjStruct *cnnlTransformerAttnProjDescriptor_t;

/*! The descriptor of the ::cnnlTransformerAttnProj operation that holds quantization information.
 * Reserved for future use.
 */
typedef struct cnnlTransformerAttnProjQuantizeStruct *cnnlTransformerAttnProjQuantizeDescriptor_t;

// Group:TransformerAttnProj
/*!
 *  @brief Creates a descriptor pointed by \p attn_proj_desc
 *  for the TransformerAttnProj operation.
 *  @param[out]  attn_proj_desc
 *    Output. Descriptor of TransformerAttnProj operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_ALLOC_FAILED
 *    The function failed to allocate memory space.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCreateTransformerAttnProjDescriptor(cnnlTransformerAttnProjDescriptor_t *attn_proj_desc);

// Group:TransformerAttnProj
/*!
 *  @brief Destroys the descriptor of TransformerAttnProj.
 *
 *  @param[in]  attn_proj_desc
 *    Input. Descriptor of TransformerAttnProj operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p desc is NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlDestroyTransformerAttnProjDescriptor(cnnlTransformerAttnProjDescriptor_t attn_proj_desc);

// Group:TransformerAttnProj
/*!
 *  @brief Retrieves extra space size needed in TransformerAttnProj operation.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
 *  @param[in] attn_proj_desc
 *    Input. Descriptor of TransformerAttnProj operation.
 *  @param[in] attn_proj_quant_desc
 *    Input. Descriptor of TransformerAttnProj operation including quantization related information.
 *    Reserved for future use. Currently attn_proj_quant_desc must be NULL.
 *  @param[in] input_desc
 *    Input. Descriptor of input tensor. The shape is [batch, seq_len, input_size].
 *  @param[in] filter_desc
 *    Input. Descriptor of filter tensor. The shape is [hidden_size, input_size].
 *  @param[in] output_desc
 *    Input. Descriptor of output tensor. The shape is [batch, seq_len, hidden_size] or
 *    [batch, head_num, seq_len, head_size].
 *  @param[out] size
 *    Output. Size of workspace.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more required parameters are NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetTransformerAttnProjWorkspaceSize(cnnlHandle_t handle,
    const cnnlTransformerAttnProjDescriptor_t attn_proj_desc,
    const cnnlTransformerAttnProjQuantizeDescriptor_t attn_proj_quant_desc,
    const cnnlTensorDescriptor_t input_desc,
    const cnnlTensorDescriptor_t filter_desc,
    const cnnlTensorDescriptor_t output_desc,
    size_t *size);

// Group:TransformerAttnProj
/*!
 *  @brief Creates a quantization descriptor pointed
 *  by \p attn_proj_quant_desc for the TransformerAttnProj operation.
 *  @param[out]  attn_proj_quant_desc
 *    Output. Quantization descriptor of TransformerAttnProj operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_ALLOC_FAILED
 *    The function failed to allocate memory space.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCreateTransformerAttnProjQuantifyDescriptor(
          cnnlTransformerAttnProjQuantizeDescriptor_t *attn_proj_quant_desc);

// Group:TransformerAttnProj
/*!
 *  @brief Destroys the Quantization descriptor of TransformerAttnProj.
 *
 *  @param[in]  attn_proj_quant_desc
 *    Input. Quantization descriptor of TransformerAttnProj operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p desc is NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlDestroyTransformerAttnProjQuantifyDescriptor(
    cnnlTransformerAttnProjQuantizeDescriptor_t attn_proj_quant_desc);

// Group:TransformerAttnProj
/*!
 *  @brief Sets a Quantization descriptor
 *  of TransformerAttnProj with values.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlSetTransformerAttnProjQuantizeDescriptor_v3 instead.
 *
 *  @param[in,out] attn_proj_quant_desc
 *    Input/output. Quantization descriptor of transformer_attn_proj operation.
 *    Reserved for future use. Currently must be NULL.
 *  @param[in] q_filter_pos_pc
 *    Input. The position of q_filter quantization in the per-channel mode.
 *    Reserved for future use. Currently must be NULL.
 *  @param[in] q_filter_scale_pc
 *    Input. The scaling coefficient of q_filter quantization in the per-channel mode.
 *    Reserved for future use. Currently must be NULL.
 *  @param[in] q_filter_offset_pc
 *    Input. The offset of q_filter quantization in the per-channel mode.
 *    Reserved for future use. Currently must be NULL.
 *  @param[in] k_filter_pos_pc
 *    Input. The position of k_filter quantization in the per-channel mode.
 *    Reserved for future use. Currently must be NULL.
 *  @param[in] k_filter_scale_pc
 *    Input. The scaling coefficient of k_filter quantization in the per-channel mode.
 *    Reserved for future use. Currently must be NULL.
 *  @param[in] k_filter_offset_pc
 *    Input. The offset of k_filter quantization in the per-channel mode.
 *    Reserved for future use. Currently must be NULL.
 *  @param[in] v_filter_pos_pc
 *    Input. The position of v_filter quantization in the per-channel mode.
 *    Reserved for future use. Currently must be NULL.
 *  @param[in] v_filter_scale_pc
 *    Input. The scaling coefficient of v_filter quantization in the per-channel mode.
 *    Reserved for future use. Currently must be NULL.
 *  @param[in] v_filter_offset_pc
 *    Input. The offset of v_filter quantization in the per-channel mode.
 *    Reserved for future use. Currently must be NULL.
 *  @param[in] input_pos
 *    Input. The position of input quantization in the per-tensor mode.
 *  @param[in] input_scale
 *    Input. The scale of input quantization in the per-tensor mode.
 *  @param[in] input_offset
 *    Input. The offset of input quantization in the per-tensor mode.
 *    Reserved for future use.
 *  @param[in] residual_pos
 *    Input. The position of residual dequantization in the per-tensor mode.
 *  @param[in] residual_scale
 *    Input. The scale of residual dequantization in the per-tensor mode.
 *  @param[in] residual_offset
 *    Input. The offset of residual dequantization in the per-tensor mode.
 *    Reserved for future use.
 *  @param[in] q_filter_pos
 *    Input. The position of q_filter quantization in the per-tensor mode.
 *  @param[in] q_filter_scale
 *    Input. The scale of q_filter quantization in the per-tensor mode.
 *  @param[in] q_filter_offset
 *    Input. The offset of q_filter quantization in the per-tensor mode.
 *    Reserved for future use.
 *  @param[in] k_filter_pos
 *    Input. The position of k_filter quantization in the per-tensor mode.
 *  @param[in] k_filter_scale
 *    Input. The scale of k_filter quantization in the per-tensor mode.
 *  @param[in] k_filter_offset
 *    Input. The offset of k_filter quantization in the per-tensor mode.
 *    Reserved for future use.
 *  @param[in] v_filter_pos
 *    Input. The position of v_filter quantization in the per-tensor mode.
 *  @param[in] v_filter_scale
 *    Input. The scale of v_filter quantization in the per-tensor mode.
 *  @param[in] v_filter_offset
 *    Input. The offset of v_filter quantization in the per-tensor mode.
 *    Reserved for future use.
 *  @param[in] query_out_pos
 *    Input. The position of query output quantization in the per-tensor mode.
 *  @param[in] query_out_scale
 *    Input. The scale of query output quantization in the per-tensor mode.
 *  @param[in] query_out_offset
 *    Input. The offset of query output quantization in the per-tensor mode.
 *    Reserved for future use.
 *  @param[in] key_out_pos
 *    Input. The position of key output quantization in the per-tensor mode.
 *  @param[in] key_out_scale
 *    Input. The scale of key output quantization in the per-tensor mode.
 *  @param[in] key_out_offset
 *    Input. The offset of key output quantization in the per-tensor mode.
 *    Reserved for future use.
 *  @param[in] value_out_pos
 *    Input. The position of value output quantization in the per-tensor mode.
 *  @param[in] value_out_scale
 *    Input. The scale of value output quantization in the per-tensor mode.
 *  @param[in] value_out_offset
 *    Input. The offset of value output quantization in the per-tensor mode.
 *    Reserved for future use.
 *  @param[in] query_aftergemm_pos
 *    Input. The position of query dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in] query_aftergemm_scale
 *    Input. The scaling coefficient of query dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in] query_aftergemm_offset
 *    Input. The offset of query dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in] key_aftergemm_pos
 *    Input. The position of key dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in] key_aftergemm_scale
 *    Input. The scaling coefficient of key dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in] key_aftergemm_offset
 *    Input. The offset of key dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in] value_aftergemm_pos
 *    Input. The position of value dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in] value_aftergemm_scale
 *    Input. The scaling coefficient of value dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in] value_aftergemm_offset
 *    Input. The offset of value dequantization that will be performed with the after_gemm
 *           dequantization parameter after matrix multiplication.
 *  @param[in] query_factor
 *    Input. The scale factor for query output.
 *  @param[in] after_query_factor
 *    Input. A Boolean value indicating whether to do query factor.
 *  @param[in] residual_input_quant
 *    Input. A Boolean value indicating whether the residual is quantized.
 *  @param[in] use_query_aftergemm_quant
 *    Input. A Boolean value indicates whether to use query_aftergemm_pos and query_aftergemm_scale when dequantizing query.
 *  @param[in] use_key_aftergemm_quant
 *    Input. A Boolean value indicates whether to use key_aftergemm_pos and key_aftergemm_scale when dequantizing key.
 *  @param[in] use_value_aftergemm_quant
 *    Input. A Boolean value indicates whether to use value_aftergemm_pos and value_aftergemm_scale when dequantizing value.
 *  @param[in] quant_per_tensor
 *    Input. A Boolean value indicates whether to quantize the filters by tensor.
 *    If this parameter is true, quantization is done by tensor, otherwise by channel.
 *    Currently must be true.
 *  @param[in] use_quantify_scale
 *    Input. A Boolean value indicating whether to use the quantization scale.
 *  @param[in] use_quantify_offset
 *    Input. A Boolean value indicates whether to use the quantization offset.
 *    Currently must be false.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    The value of \p attn_proj_quant_desc is NULL.
 *
 *  @par API Dependency
 *  - Before using this function, you need to call
 *  ::cnnlCreateTransformerAttnProjQuantifyDescriptor() to create a descriptor.
 */
CNNL_DEPRECATED_FOR(cnnlSetTransformerAttnProjQuantizeDescriptor_v3)
cnnlStatus_t CNNL_WIN_API
cnnlSetTransformerAttnProjQuantifyDescriptor(
    cnnlTransformerAttnProjQuantizeDescriptor_t attn_proj_quant_desc,
    const void *q_filter_pos_pc,
    const void *q_filter_scale_pc,
    const void *q_filter_offset_pc,
    const void *k_filter_pos_pc,
    const void *k_filter_scale_pc,
    const void *k_filter_offset_pc,
    const void *v_filter_pos_pc,
    const void *v_filter_scale_pc,
    const void *v_filter_offset_pc,
    const int16_t input_pos,
    const float input_scale,
    const int16_t input_offset,
    const int16_t residual_pos,
    const float residual_scale,
    const int16_t residual_offset,
    const int16_t q_filter_pos,
    const float q_filter_scale,
    const int16_t q_filter_offset,
    const int16_t k_filter_pos,
    const float k_filter_scale,
    const int16_t k_filter_offset,
    const int16_t v_filter_pos,
    const float v_filter_scale,
    const int16_t v_filter_offset,
    const int16_t query_out_pos,
    const float query_out_scale,
    const int16_t query_out_offset,
    const int16_t key_out_pos,
    const float key_out_scale,
    const int16_t key_out_offset,
    const int16_t value_out_pos,
    const float value_out_scale,
    const int16_t value_out_offset,
    const int16_t query_aftergemm_pos,
    const float query_aftergemm_scale,
    const int16_t query_aftergemm_offset,
    const int16_t key_aftergemm_pos,
    const float key_aftergemm_scale,
    const int16_t key_aftergemm_offset,
    const int16_t value_aftergemm_pos,
    const float value_aftergemm_scale,
    const int16_t value_aftergemm_offset,
    const float query_factor,
    const bool after_query_factor,
    const bool residual_input_quant,
    const bool use_query_aftergemm_quant,
    const bool use_key_aftergemm_quant,
    const bool use_value_aftergemm_quant,
    const bool quant_per_tensor,
    const bool use_quantify_scale,
    const bool use_quantify_offset);

// Group:TransformerAttnProj
/*!
 *  @brief Sets a quantization descriptor of TransformerAttnProj with values.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlSetTransformerAttnProjQuantizeDescriptor_v3 instead.
 *
 *  @param[in,out] attn_proj_quant_desc
 *    Input/output. Quantization descriptor of transformer_attn_proj operation.
 *  @param[in] input_quant_desc
 *    Input. The quantization descriptor that holds quantization information of the input.
 *  @param[in] filter_quant_desc
 *    Input. The quantization descriptor array that holds quantization information of the filters.
 *  @param[in] output_quant_desc
 *    Input. The quantization descriptor array that holds quantization information of the outputs.
 *  @param[in] after_gemm_quant_desc
 *    Input. The quantization descriptor array that holds quantization information used in QAT mode.
 *  @param[in] norm_res_quant_desc
 *    Input. The quantization descriptor that holds quantization information of the residual or
 *    the normalization output.
 *  @param[in] after_query_factor
 *    Input. A Boolean value indicating whether to do query factor.
 *  @param[in] query_factor
 *    Input. The scale factor for query output.
 *  @note
 *  - The length of all three arrays of \p filter_quant_desc, \p output_quant_desc and \p after_gemm_quant_desc
 *    must be 3, which means that there are 3 projections. If you do not need 3 projections, you can set
 *    the element in that position to nullptr.
 */
CNNL_DEPRECATED_FOR(cnnlSetTransformerAttnProjQuantizeDescriptor_v3)
cnnlStatus_t CNNL_WIN_API
cnnlSetTransformerAttnProjQuantizeDescriptor_v2(
    cnnlTransformerAttnProjQuantizeDescriptor_t attn_proj_quant_desc,
    const cnnlQuantizeDescriptor_t input_quant_desc,
    const cnnlQuantizeDescriptor_t filter_quant_desc[],
    const cnnlQuantizeDescriptor_t output_quant_desc[],
    const cnnlQuantizeDescriptor_t after_gemm_quant_desc[],
    const cnnlQuantizeDescriptor_t norm_res_quant_desc,
    bool after_query_factor,
    float query_factor);

// Group:TransformerAttnProj
/*!
 *  @brief Sets a quantization descriptor of TransformerAttnProj with values.
 *
 *  Compared with ::cnnlSetTransformerAttnProjQuantizeDescriptor_v2, this function uses cnnlQuantizeExDescriptor_t
 *  instead of cnnlQuantizeDescriptor_t. See cnnlQuantizeExDescriptor_t for details.
 *
 *  @param[in,out] quant_desc
 *    Input/output. Quantization descriptor of the transformer_attn_proj operation.
 *  @param[in] input_quant_desc
 *    Input. The quantization descriptor that holds quantization information of the input.
 *  @param[in] filter_quant_desc
 *    Input. The quantization descriptor array that holds quantization information of the filters.
 *  @param[in] output_quant_desc
 *    Input. The quantization descriptor array that holds quantization information of the outputs.
 *  @param[in] after_gemm_quant_desc
 *    Input. The quantization descriptor array that holds quantization information used in QAT mode.
 *  @param[in] norm_res_quant_desc
 *    Input. The quantization descriptor that holds quantization information of the residual or
 *    the normalization output.
 *  @param[in] after_query_factor
 *    Input. A Boolean value indicating whether to do query factor.
 *  @param[in] query_factor
 *    Input. The scale factor for query output.
 *  @note
 *  - The length of all three arrays of \p filter_quant_desc, \p output_quant_desc and \p after_gemm_quant_desc
 *    must be 3, which means that there are 3 projections. If you do not need 3 projections, you can set
 *    the element in that position to nullptr.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetTransformerAttnProjQuantizeDescriptor_v3(
    cnnlTransformerAttnProjQuantizeDescriptor_t quant_desc,
    const cnnlQuantizeExDescriptor_t input_quant_desc,
    const cnnlQuantizeExDescriptor_t filter_quant_desc[],
    const cnnlQuantizeExDescriptor_t output_quant_desc[],
    const cnnlQuantizeExDescriptor_t after_gemm_quant_desc[],
    const cnnlQuantizeExDescriptor_t norm_res_quant_desc,
    bool after_query_factor,
    float query_factor);

// Group:TransformerAttnProj
/*!
 *  @brief Gets the quantization descriptor of
 *  TransformerAttnProj with values.
 *
 *  @param[in] attn_proj_quant_desc
 *    Input. Quantization descriptor of TransformerAttnProj operation.
 *  @param[out] q_filter_scale_pc
 *    Output. The scaling coefficient of q_filter quantization in the per-channel mode.
 *  @param[out] k_filter_scale_pc
 *    Output. The scaling coefficient of k_filter quantization in the per-channel mode.
 *  @param[out] v_filter_scale_pc
 *    Output. The scaling coefficient of v_filter quantization in the per-channel mode.
 *  @param[out] query_out_pos
 *    Output. The position of query output quantization in the per-tensor mode.
 *  @param[out] query_out_scale
 *    Output. The scale of query output quantization in the per-tensor mode.
 *  @param[out] key_out_pos
 *    Output. The position of key output quantization in the per-tensor mode.
 *  @param[out] key_out_scale
 *    Output. The scale of key output quantization in the per-tensor mode.
 *  @param[out] value_out_pos
 *    Output. The position of value output quantization in the per-tensor mode.
 *  @param[out] value_out_scale
 *    Output. The scale of value output quantization in the per-tensor mode.
 *  @param[out] query_aftergemm_pos
 *    Output. The position of query dequantization that will be performed with the after_gemm
 *            dequantization parameter after matrix multiplication.
 *  @param[out] query_aftergemm_scale
 *    Output. The scale of query dequantization that will be performed with the after_gemm
 *            dequantization parameter after matrix multiplication.
 *  @param[out] key_aftergemm_pos
 *    Output. The position of key dequantization that will be performed with the after_gemm
 *            dequantization parameter after matrix multiplication.
 *  @param[out] key_aftergemm_scale
 *    Output. The scale of key dequantization that will be performed with the after_gemm
 *            dequantization parameter after matrix multiplication.
 *  @param[out] value_aftergemm_pos
 *    Output. The position of value dequantization that will be performed with the after_gemm
 *            dequantization parameter after matrix multiplication.
 *  @param[out] value_aftergemm_scale
 *    Output. The scale of value dequantization that will be performed with the after_gemm
 *            dequantization parameter after matrix multiplication.
 *  @param[out] query_factor
 *    Output. The scale factor for query output.
 *  @param[out] after_query_factor
 *    Output. A Boolean value indicating whether to do query factor.
 *  @param[out] residual_input_quant
 *    Output. A Boolean value indicating whether the residual is quantized.
 *  @param[out] use_query_aftergemm_quant
 *    Output. A Boolean value indicates whether to use query_aftergemm_pos and query_aftergemm_scale when dequantizing query.
 *  @param[out] use_key_aftergemm_quant
 *    Output. A Boolean value indicates whether to use key_aftergemm_pos and key_aftergemm_scale when dequantizing key.
 *  @param[out] use_value_aftergemm_quant
 *    Output. A Boolean value indicates whether to use value_aftergemm_pos and value_aftergemm_scale when dequantizing value.
 *  @param[out] quant_per_tensor
 *    Output. A Boolean value indicates whether to quantize the filters by tensor.
 *  @param[out] use_quantify_scale
 *    Output. A Boolean value indicating whether to use the quantization scale.
 *  @param[out] use_quantify_offset
 *    Output. A Boolean value indicates whether to use the quantization offset.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p attn_proj_desc is NULL, or one or more parameters are not supported.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetTransformerAttnProjQuantifyDescriptor(
    cnnlTransformerAttnProjQuantizeDescriptor_t attn_proj_quant_desc,
    const void **q_filter_scale_pc,
    const void **k_filter_scale_pc,
    const void **v_filter_scale_pc,
    int16_t *query_out_pos,
    float *query_out_scale,
    int16_t *key_out_pos,
    float *key_out_scale,
    int16_t *value_out_pos,
    float *value_out_scale,
    int16_t *query_aftergemm_pos,
    float *query_aftergemm_scale,
    int16_t *key_aftergemm_pos,
    float *key_aftergemm_scale,
    int16_t *value_aftergemm_pos,
    float *value_aftergemm_scale,
    float *query_factor,
    bool *after_query_factor,
    bool *residual_input_quant,
    bool *use_query_aftergemm_quant,
    bool *use_key_aftergemm_quant,
    bool *use_value_aftergemm_quant,
    bool *quant_per_tensor,
    bool *use_quantify_scale,
    bool *use_quantify_offset);

// Group:TransformerAttnProj
/*!
 * @brief Sets TensorFloat-32 mode for TransformerAttnProj operation with parameter.
 *
 * @param[in,out] desc
 *   Input/output. Descriptor of TransformerAttnProj operation.
 * @param[in] allow_tf32
 *   Input. An integer value that determines whether to enable TensorFloat-32.
 *   The value only contains 0 and 1. 1 means TensorFloat-32 is enabled and 0 means TensorFloat-32 is disabled.
 *   TensorFloat-32 is disabled by default.
 * @retval CNNL_STATUS_SUCCESS
 *   The function ends normally.
 * @retval CNNL_STATUS_BAD_PARAM
 *   \p attn_desc is NULL, or one or more parameters are not supported.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetTransformerAttnProjDescriptorAllowTF32(
    cnnlTransformerAttnProjDescriptor_t desc,
    const int allow_tf32);

// Group:TransformerAttnProj
/*!
 * @brief Sets normalization type for TransformerAttnProj operation with parameter.
 *
 * @deprecated
 *   This function is deprecated and will be removed in future release.
 *   Use ::cnnlFuseNorm_v3 instead.
 *
 * @param[inout] desc
 *   Input. Descriptor of TransformerAttnProj operation.
 * @param[in] norm_type
 *   Input. The indicator of normalization type, see ::cnnlTransformerNormType_t for details.
 * @retval CNNL_STATUS_SUCCESS
 *   The function ends normally.
 * @retval CNNL_STATUS_BAD_PARAM
 *   \p attn_desc is NULL, or one or more parameters are not supported.
 */
CNNL_DEPRECATED_FOR(cnnlFuseNorm_v3)
cnnlStatus_t CNNL_WIN_API
cnnlSetTransformerAttnProjDescriptorNormType(
  cnnlTransformerAttnProjDescriptor_t desc,
  cnnlTransformerNormType_t norm_type);

// Group:TransformerAttnProj
/*!
 *  @brief Sets a descriptor of TransformerAttnProj with values.
 *
 *  @param[in] attn_proj_desc
 *    Input. Descriptor of TransformerAttnProj operation.
 *  @param[in] norm_residual_mode
 *    Input. Position relations among matmul, normalization and residual.
 *    Accepted value is
 *    - CNNL_TRANSFORMER_NO_LAYERNORM_NO_RESIDUAL
 *    - CNNL_TRANSFORMER_NO_LAYERNORM_WITH_RESIDUAL
 *    - CNNL_TRANSFORMER_PRE_LAYERNORM_NO_RESIDUAL
 *    - CNNL_TRANSFORMER_POST_LAYERNORM_NO_RESIDUAL
 *    - CNNL_TRANSFORMER_POST_LAYERNORM_INSIDE_RESIDUAL
 *    - CNNL_TRANSFORMER_POST_LAYERNORM_OUTSIDE_RESIDUAL
 *  @param[in] activation_desc
 *    Input. Descriptor of activation. Reserved for future use. Currently must be NULL.
 *  @param[in] math_pre
 *    Input. Onchip computation data type of normalization.
 *  @param[in] q_has_value
 *    Input. A Boolean value indicating whether to involve q(query) in computing.
 *  @param[in] k_has_value
 *    Input. A Boolean value indicating whether to involve k(key) in computing.
 *  @param[in] v_has_value
 *    Input. A Boolean value indicating whether to involve v(value) in computing.
 *  @param[in] has_bias
 *    Input. A Boolean value indicating whether to add bias for q(query)/k(key)/v(value).
 *  @param[in] is_pack_mode
 *    Input. A Boolean value indicating the mode of input. If true, input is packed; if false, input is padded.
 *  @param[in] packed_max_seq_len
 *    Input. Reserved for future use.
 *  @param[in] trans_out
 *    Input. A Boolean value indicating whether to transpose output.
 *  @param[in] store_norm_result
 *    Input. A Boolean value indicating whether to store normalization output before doing matrix multiplication.
 *    When \p store_norm_result is true, \p norm_residual_mode should be
 *    \p CNNL_TRANSFORMER_PRE_LAYERNORM_NO_RESIDUAL.
 *  @param[in] alpha
 *    Input. The value of scale factor after fc and before residual and before post normalization module. Set
 *    \p alpha to 1.0 when it is not needed.
 *  @param[in] beta
 *    Input. The value of scale factor of residual. Set \p beta to 1.0 when it is not needed.
 *    See more details in the formula description of ::cnnlTransformerAttnProj.
 *  @param[in] norm_eps
 *    Input. A small value added to variance when doing normalization. \p norm_eps must be
 *    equal to or greater than 0, and equal to or less than 1e-4.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p attn_proj_desc is NULL, or one or more parameters are not supported.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetTransformerAttnProjDescriptor(
    cnnlTransformerAttnProjDescriptor_t attn_proj_desc,
    const cnnlTransformerLayernormResidualStructure_t norm_residual_mode,
    const cnnlActivationDescriptor_t activation_desc,
    const cnnlDataType_t math_pre,
    const bool q_has_value,
    const bool k_has_value,
    const bool v_has_value,
    const bool has_bias,
    const bool is_pack_mode,
    const int packed_max_seq_len,
    const bool trans_out,
    const bool store_norm_result,
    const float alpha,
    const float beta,
    const float norm_eps);

// Group:TransformerAttnProj
/*!
 *  @brief Gets the descriptor of TransformerAttnProj with values.
 *
 *  @param[in] attn_proj_desc
 *    Input. Descriptor of TransformerAttnProj operation.
 *  @param[out] norm_residual_mode
 *    Output. Position relations among matmul, normalization and residual.
 *  @param[out] activation_desc
 *    Output. Descriptor of activation. Reserved for future use.
 *  @param[out] math_pre
 *    Output. Onchip computation data type of normalization.
 *  @param[out] q_has_value
 *    Output. A Boolean value indicating whether to involve q(query)in computing.
 *  @param[out] k_has_value
 *    Output. A Boolean value indicating whether to involve k(key)in computing.
 *  @param[out] v_has_value
 *    Output. A Boolean value indicating whether to involve v(value)in computing.
 *  @param[out] has_bias
 *    Output. A Boolean value indicating whether to have bias for q(query)/k(key)/v(value).
 *  @param[out] is_pack_mode
 *    Output. A Boolean value describing the pack mode of input.
 *    If true, input is packed; if false, input is padded.
 *  @param[out] packed_max_seq_len
 *    Output. Reserved for future use.
 *  @param[out] trans_out
 *    Output. A Boolean value indicating whether to transpose output.
 *  @param[out] store_norm_result
 *    Output. A Boolean value indicating whether to store normalization output before doing matrix multiplication.
 *  @param[out] alpha
 *    Output. See details in ::cnnlSetTransformerAttnProjDescriptor.
 *  @param[out] beta
 *    Output. See details in ::cnnlSetTransformerAttnProjDescriptor.
 *  @param[out] norm_eps
 *    Output. A small value added to variance when doing normalization.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p attn_proj_desc is NULL, or one or more parameters are not supported.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetTransformerAttnProjDescriptor(
    const cnnlTransformerAttnProjDescriptor_t attn_proj_desc,
    cnnlTransformerLayernormResidualStructure_t* norm_residual_mode,
    cnnlActivationDescriptor_t* activation_desc,
    cnnlDataType_t* math_pre,
    bool* q_has_value,
    bool* k_has_value,
    bool* v_has_value,
    bool* has_bias,
    bool* is_pack_mode,
    int* packed_max_seq_len,
    bool* trans_out,
    bool* store_norm_result,
    float* alpha,
    float* beta,
    float* norm_eps);

// Group:TransformerAttnProj
/*!
 *  @brief Executes attention projection in Transformer encoder and decoder network.
 *
 *  This function performs with the following steps:
 *
 *    if (pre_norm):
 *      x = normalization(x)
 *
 *    if (q_has_value):
 *      q = x * q_filter + q_bias
 *
 *    if (k_has_value):
 *      k = x * k_filter + k_bias
 *
 *    if (v_has_value):
 *      v = x * v_filter + v_bias
 *
 *    if (post_norm):
 *      q = normalization(q)
 *
 *    if (residual):
 *      q = q * aplha + residual * beta
 *
 *  Some operations may not be done or may be done in different order according to attn_proj_desc.
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
 *  @param[in] attn_proj_desc
 *    Input. Descriptor of TransformerAttnProj operation.
 *  @param[in] attn_proj_quant_desc
 *    Input. Descriptor of quantization information of TransformerAttnProj operation.
 *  @param[in] input_desc
 *    Input. Descriptor of input tensor. The shape is [batch, seq_len, input_size].
 *  @param[in] input
 *    Input. Pointer to the MLU memory that stores the input tensor.
 *  @param[in] residual_desc
 *    Input. Descriptor of residual tensor. The shape is [batch, seq_len, hidden_size].
 *  @param[in] residual
 *    Input. Pointer to the MLU memory that stores the residual tensor.
 *  @param[in] q_filter_desc
 *    Input. Descriptor of q_filter tensor. The shape is [hidden_size, input_size].
 *    When \p q_has_value is false, \p q_filter_desc must be NULL.
 *  @param[in] q_filter
 *    Input. Pointer to the MLU memory that stores the q_filter tensor.
 *    When \p q_has_value is false, \p q_filter must be NULL.
 *  @param[in] k_filter_desc
 *    Input. Descriptor of k_filter tensor. The shape is [hidden_size, input_size].
 *    When \p k_has_value is false, \p k_filter_desc must be NULL.
 *  @param[in] k_filter
 *    Input. Pointer to the MLU memory that stores the k_filter tensor.
 *    When \p k_has_value is false, \p k_filter must be NULL.
 *  @param[in] v_filter_desc
 *    Input. Descriptor of v_filter tensor. The shape is [hidden_size, input_size].
 *    When \p v_has_value is false, \p v_filter_desc must be NULL.
 *  @param[in] v_filter
 *    Input. Pointer to the MLU memory that stores the v_filter tensor.
 *    When \p v_has_value is false, \p v_filter must be NULL.
 *  @param[in] q_bias_desc
 *    Input. Descriptor of q_bias tensor. The shape is [hidden_size].
 *    When \p q_has_value is false or \p has_bias is false, \p q_bias_desc must be NULL.
 *  @param[in] q_bias
 *    Input. Pointer to the MLU memory that stores the q_bias tensor.
 *    When \p q_has_value is false, \p q_bias must be NULL.
 *  @param[in] k_bias_desc
 *    Input. Descriptor of k_bias tensor. The shape is [hidden_size].
 *    When \p k_has_value is false or \p has_bias is false, \p k_bias_desc must be NULL.
 *  @param[in] k_bias
 *    Input. Pointer to the MLU memory that stores the k_bias tensor.
 *    When \p k_has_value is false or \p has_bias is false, \p k_bias must be NULL.
 *  @param[in] v_bias_desc
 *    Input. Descriptor of v_bias tensor. The shape is [hidden_size].
 *    When \p v_has_value is false or \p has_bias is false, \p v_bias_desc must be NULL.
 *  @param[in] v_bias
 *    Input. Pointer to the MLU memory that stores the v_bias tensor.
 *    When \p v_has_value is false or \p has_bias is false, \p v_bias must be NULL.
 *  @param[in] valid_token_desc
 *    Input. Descriptor of valid_token tensor. The shape is [batch_size].
 *    When \p is_pack_mode is false, \p valid_token_desc must be NULL.
 *  @param[in] valid_token
 *    Input. Pointer to the MLU memory that stores the valid_token tensor.
 *    When is_pack_mode is false, valid_token must be NULL.
 *  @param[in] norm_scale_desc
 *    Input. Descriptor of norm_scale tensor.
 *    When \p pre_norm is enabled, the shape is [input_size].
 *    When \p post_norm is enabled, the shape is [hidden_size].
 *    When no normalization is enabled, \p norm_scale_desc must be NULL.
 *  @param[in] norm_scale
 *    Input. Pointer to the MLU memory that stores the norm_scale tensor.
 *    When no normalization is enabled, norm_scale must be NULL.
 *  @param[in] norm_bias_desc
 *    Input. Descriptor of norm_bias tensor.
 *    When \p pre_norm is enabled, the shape is [input_size].
 *    When \p post_norm is enabled, the shape is [hidden_size].
 *    When no normalization is enabled, \p norm_bias_desc must be NULL.
 *  @param[in] norm_bias
 *    Input. Pointer to the MLU memory that stores the norm_bias tensor.
 *    When no normalization is enabled, norm_bias must be NULL.
 *  @param[in] workspace
 *    Input. Pointer to the MLU memory that is used as an extra workspace.
 *  @param[in] workspace_size
 *    Input. The size of workspace.
 *  @param[in] q_out_desc
 *    Input. Descriptor of q_out tensor. When trans_out is false, the shape is
 *    [batch, seq_len, hidden_size]. When trans_out is true, the shape is
 *    [batch, head_num, seq_len, head_size].
 *    When q_has_value is false, q_out_desc must be NULL.
 *  @param[out] q_out
 *    Output. Pointer to the MLU memory that stores the q_out tensor
 *    When \p q_has_value is false, \p q_out must be NULL.
 *  @param[in] k_out_desc
 *    Input. Descriptor of k_out tensor. When trans_out is false, the shape is
 *    [batch, seq_len, hidden_size]. When trans_out is true, the shape is
 *    [batch, head_num, seq_len, head_size].
 *    When k_has_value is false, k_out_desc must be NULL.
 *  @param[out] k_out
 *    Output. Pointer to the MLU memory that stores the k_out tensor.
 *    When k_has_value is false, k_out must be NULL.
 *  @param[in] v_out_desc
 *    Input. Descriptor of v_out tensor. When \p trans_out is false, the shape is
 *    [batch, seq_len, hidden_size]. When \p trans_out is true, the shape is
 *    [batch, head_num, seq_len, head_size].
 *    When \p v_has_value is false, \p v_out_desc must be NULL.
 *  @param[out] v_out
 *    Output. Pointer to the MLU memory that stores the v_out tensor.
 *    When \p v_has_value is false, \p v_out must be NULL.
 *  @param[in] norm_out_desc
 *    Input. Descriptor of norm_out tensor. When \p store_norm_result is true, the shape is
 *    [batch, seq_len, input_size].
 *    When \p store_norm_result is false, \p norm_out_desc must be NULL.
 *  @param[out] norm_out
 *    Output. Pointer to the MLU memory that stores the norm_out space.
 *    When store_norm_result is false, norm_out must be NULL.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are encountered:
 *    - handle is NULL.
 *    - One or more required pointers are NULL.
 *    - The scale or data type limitation is not satisfied.
 *  @par Data Type
 *  - \p input: float, half, int8, bfloat16.
 *  - \p filter: float, half, int8, bfloat16.
 *  - \p bias: float, half, bfloat16.
 *  - \p output: float, half, int8, bfloat16.
 *  - \p valid_token: int32.
 *  - When data type of \p input is float, \p math_pre must be float.
 *  - When data type of \p filter is half, bfloat16 or float, the data type of \p input, \p filter, \p output, and \p norm_output
      must be the same.
 *  - When data type of \p filter is int8:
 *    - If data type of \p input is int8, the data type of \p output must be half or float.
 *    - The data type of \p norm_output must be the same as that of \p input or quantized as int8.
 *    - The data type of \p residual must be the same as that of \p output or quantized as int8.
 *    - If data type of \p output is int8, \p alpha and \p beta must be 1.0.
 *  - When data type of \p input is bfloat16, the normalization, bias and residual operations will be calculated using the float data  *     type.
 *  @par Quantization Layout
 *  - When the quantization layout of tensor is CNNL_QUANTIZE_NONE, it means that the tensor will not be
 *    quantized.
 *  - The supported quantization layouts are as follows:
 *    - input: CNNL_QUANTIZE_NONE, CNNL_QUANTIZE_PER_TENSOR, CNNL_QUANTIZE_PER_CHANNEL.
 *    - filter: CNNL_QUANTIZE_NONE, CNNL_QUANTIZE_PER_TENSOR, CNNL_QUANTIZE_PER_CHANNEL,
 *              CNNL_QUANTIZE_GROUP_WISE.
 *    - output: CNNL_QUANTIZE_NONE, CNNL_QUANTIZE_PER_TENSOR.
 *    - after_gemm: CNNL_QUANTIZE_NONE, CNNL_QUANTIZE_PER_TENSOR.
 *    - norm_res: CNNL_QUANTIZE_NONE, CNNL_QUANTIZE_PER_TENSOR.
 *
 *    Note that the quantization layouts of all the filter tensors should be same.
 *  @note
 *  - Currently, attn_proj supports these quantization methods: qat, weight_only, group_wise and smooth quant.
 *   - When the quantization method is qat:
 *     - The quantization layout of \p input should be set to CNNL_QUANTIZE_PERTENSOR.
 *     - The quantization layout of \p filter should be set to CNNL_QUANTIZE_PERTENSOR or CNNL_QUANTIZE_PER_CHANNEL.
 *     - The quantization layout of \p output should be set to CNNL_QUANTIZE_PERTENSOR when \p output needs quantization; Otherwise, set to CNNL_QUANTIZE_NONE.
 *     - The quantization layout of \p norm_res should be set to CNNL_QUANTIZE_PERTENSOR when \p norm_out or \p residual needs quantization; Otherwise, set to CNNL_QUANTIZE_NONE.
 *     - The quantization bit width must be 8.
 *     - The data type of dequant param scale of filters must be CNNL_DTYPE_FLOAT.
 *   - When the quantization method is weight_only:
 *     - The quantization layout of \p input should be set to CNNL_QUANTIZE_NONE.
 *     - The quantization layout of \p filter should be set to CNNL_QUANTIZE_PERTENSOR or CNNL_QUANTIZE_PER_CHANNEL.
 *     - The data type of dequant param scale of filters must be CNNL_DTYPE_FLOAT.
 *     - The quantization bit width must be 4 or 8.
 *   - When the quantization method is group_wise:
 *     - The quantization layout of \p input should be set to CNNL_QUANTIZE_NONE.
 *     - The quantization layout of \p filter should be set to CNNL_QUANTIZE_GROUP_WISE.
 *     - The data type of dequant param scale of filters must be the same as that of \p input.
 *     - The second dimension of filters must be divided by group_size.
 *     - The group_size only supports 64, 128, 256 and 512.
 *     - The quantization bit width must be 4 or 8.
 *   - When the quantization method is smooth_quant:
 *     - The quantization layout of \p input should be set to CNNL_QUANTIZE_PER_CHANNEL.
 *     - The quantization layout of \p filter should be set to CNNL_QUANTIZE_PER_CHANNEL.
 *     - The quantization bit width of all tensors must be 8.
 *     - The data type of all quantization param scale must be CNNL_DTYPE_FLOAT.
 *     - The scale of input_quant_desc needs to be multiplied after \p norm_out if normalization exists; Otherwise, it is multiplied after \p input.
 *     - You should fuse input smooth scale with input_quant_scale into input_quant_desc to quantize
 *       input and smoother input.
 *     - The scale of filter_quant_desc will multiply with the matmul result of each filters.
 *     - You should fuse filter smooth scale, input_dequant_scale and filter_dequant_scale into
 *      filter_quant_desc to dequant filter and smoother filter.
 *  - The post normalization ability (for example, norm_structure is CNNL_TRANSFORMER_POST_LAYERNORM_NO_RESIDUAL,
 *    CNNL_TRANSFORMER_POST_LAYERNORM_INSIDE_RESIDUAL or CNNL_TRANSFORMER_POST_LAYERNORM_OUTSIDE_RESIDUAL)
 *    in this function is deprecated and will be removed in future release.
 *    Call ::cnnlFuseNorm to process post normalization instead.
 *  @note
 *  - Only MLU300 and MLU500 series are supported.
 *  - Data type bfloat16 is not supported on MLU300 series.
 *  - Inputs and outputs cannot be homologous operand.
 *  - The content of all input tensors is not modified.
 *
 *  @par Scale Limitation
 *  - \p batch > 0.
 *  - \p seq_len > 0.
 *  - \p input_size must be in range of [16, 14336].
 *  - \p hidden_size must be in range of [16, 14336].
 *  - \p hidden_size = \p head_num * \p head_size.
 *  - When \p trans_out is true, \p head_num must be in range of [1, 128].
 *  - When \p trans_out is true, \p head_size must be in range of [16, 256].
 *  - \p norm_residual_mode must be:
 *    - CNNL_TRANSFORMER_NO_LAYERNORM_NO_RESIDUAL
 *    - CNNL_TRANSFORMER_NO_LAYERNORM_WITH_RESIDUAL
 *    - CNNL_TRANSFORMER_PRE_LAYERNORM_NO_RESIDUAL
 *    - CNNL_TRANSFORMER_POST_LAYERNORM_NO_RESIDUAL
 *    - CNNL_TRANSFORMER_POST_LAYERNORM_INSIDE_RESIDUAL
 *    - CNNL_TRANSFORMER_POST_LAYERNORM_OUTSIDE_RESIDUAL
 *  - When doing post normalization or adding residual, only one filter is available.
 *  - When \p trans_out is true, post normalization or residual is forbidden.
 *  - When \p store_norm_result is true, \p norm_residual_mode must
 *    contain pre normalization mode.
 *  - When data type of input is int8, pre normalization is forbidden.
 *  - When data type of output is int8, adding residual is forbidden.
 *  - When \p hidden_size or \p input_size is more than 2048, quant mode is forbidden.
 *  - When \p hidden_size or \p input_size is more than 2048, TensorFloat-32 is forbidden.
 *  - When \p is_pack_mode is true, \p batch must be equal to 1, and the values contained in \p valid_token must add up to \p seq_len.
 *  - Total count of elements of each filter should be less than or equal to INT32_MAX.
 *  - The product of \p batch and \p seq_len should be less than or equal to INT32_MAX.
 */
cnnlStatus_t CNNL_WIN_API
cnnlTransformerAttnProj(cnnlHandle_t handle,
                        const cnnlTransformerAttnProjDescriptor_t attn_proj_desc,
                        const cnnlTransformerAttnProjQuantizeDescriptor_t attn_proj_quant_desc,
                        const cnnlTensorDescriptor_t input_desc,
                        const void* input,
                        const cnnlTensorDescriptor_t residual_desc,
                        const void* residual,
                        const cnnlTensorDescriptor_t q_filter_desc,
                        const void* q_filter,
                        const cnnlTensorDescriptor_t k_filter_desc,
                        const void* k_filter,
                        const cnnlTensorDescriptor_t v_filter_desc,
                        const void* v_filter,
                        const cnnlTensorDescriptor_t q_bias_desc,
                        const void* q_bias,
                        const cnnlTensorDescriptor_t k_bias_desc,
                        const void* k_bias,
                        const cnnlTensorDescriptor_t v_bias_desc,
                        const void* v_bias,
                        const cnnlTensorDescriptor_t valid_token_desc,
                        const void* valid_token,
                        const cnnlTensorDescriptor_t norm_scale_desc,
                        const void* norm_scale,
                        const cnnlTensorDescriptor_t norm_bias_desc,
                        const void* norm_bias,
                        void* workspace,
                        size_t workspace_size,
                        const cnnlTensorDescriptor_t q_out_desc,
                        void* q_out,
                        const cnnlTensorDescriptor_t k_out_desc,
                        void* k_out,
                        const cnnlTensorDescriptor_t v_out_desc,
                        void* v_out,
                        const cnnlTensorDescriptor_t norm_out_desc,
                        void* norm_out);

/******************************************************************************
 * Cambricon CNNL OP: TransformerAttention
 ******************************************************************************/
/*! The descriptor of the ::cnnlTransformerAttention operation.
 * You can use ::cnnlCreateTransformerAttentionDescriptor(), ::cnnlSetTransformerAttentionDescriptor()
 * and ::cnnlDestroyTransformerAttentionDescriptor() to create, set and destroy the descriptor
 * respectively.
 */
typedef struct cnnlTransformerAttentionStruct *cnnlTransformerAttentionDescriptor_t;

/*! The descriptor of the ::cnnlTransformerAttention operation that holds quantization information.
 * Reserved for future use.
 */
typedef struct cnnlTransformerAttentionQuantizeStruct *cnnlTransformerAttentionQuantizeDescriptor_t;

// Group:TransformerAttention
/*!
 *  @brief Creates a descriptor pointed by \p desc
 *   for the TransformerAttention operation.
 *  @param[out]  desc
 *    Output. Descriptor of TransformerAttention operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_ALLOC_FAILED
 *    The function failed to allocate memory for the TransformerAttention descriptor.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCreateTransformerAttentionDescriptor(cnnlTransformerAttentionDescriptor_t *desc);

// Group:TransformerAttention
/*!
 *  @brief Destroys the descriptor of TransformerAttention.
 *
 *  @param[in] desc
 *    Input. Descriptor of TransformerAttention operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p desc is NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlDestroyTransformerAttentionDescriptor(cnnlTransformerAttentionDescriptor_t desc);

// Group:TransformerAttention
/*!
 *  @brief Sets the descriptor of TransformerAttention with values.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlSetTransformerAttentionDescriptor_v2 instead.
 *
 *  @param[in,out] desc
 *    Input/output. Descriptor of TransformerAttention operation.
 *  @param[in] compute_dtype
 *    Input. Data type to be used during computation.
 *    - The data types of \p query, \p key and \p value must be the same.
 *    - If data type of \p query is half, \p compute_dtype can be half or float.
 *    - If data type of \p query is float, \p compute_dtype must be float.
 *    - If data type of \p query is bfloat16, \p compute_dtype can be bfloat16 or float.
 *    - If \p compute_dtype is set to CNNL_DTYPE_INVALID, it will be the same as that of \p query.
 *  @param[in] act_pref
 *    Input. The indicator of activation operation preference, including:
 *           - CNNL_ACTIVATION_FAST: Use high-performance for activation operations.
 *           - CNNL_ACTIVATION_HIGH_PRECISION: Use high-precision for activation operations.
 *  @param[in]  mask_mode
 *    Input. The indicator of mask adding mode.
 *    Currently only ::CNNL_ATTN_MASK_NONE, ::CNNL_ATTN_MASK_NT, ::CNNL_ATTN_MASK_NTT,
 *    ::CNNL_ATTN_MASK_N, ::CNNL_ATTN_MASK_NHTT and ::CNNL_ATTN_MASK_CAUSAL are supported. See ::cnnlAttentionMaskMode_t for details.
 *  @param[in]  is_pack_mode
 *    Input. If true, inputs of multi-head attention are packed. If false, inputs are padded.
 *  @param[in]  packed_max_seq_len
 *    Input. The indicator of maximum sequence length of pack mode.
 *           This parameter is only used when \p is_pack_mode is true.
 *           When \p is_pack_mode is false, this parameter is ignored.
 *  @param[in]  query_factor
 *    Input. The scale factor for scaled batchdot of q and k.
 *  @param[in]  is_mul_factor_after_qk
 *    Input. If true, \p query_factor is multiplied after calculating q dot k. If false,
 *           \p query_factor is multiplied on q before performing q dot k.
 *  @param[in]  is_store_qk
 *    Input. Reserved for future use.
 *    Currently only false is supported.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p desc is NULL, or one or more parameters are not supported.
 */
CNNL_DEPRECATED_FOR(cnnlSetTransformerAttentionDescriptor_v2)
cnnlStatus_t CNNL_WIN_API
cnnlSetTransformerAttentionDescriptor(cnnlTransformerAttentionDescriptor_t desc,
                                      cnnlDataType_t compute_dtype,
                                      cnnlActivationPreference_t act_pref,
                                      cnnlAttentionMaskMode_t mask_mode,
                                      bool is_pack_mode,
                                      int packed_max_seq_len,
                                      float query_factor,
                                      bool is_mul_factor_after_qk,
                                      bool is_store_qk);

// Group:TransformerAttention
/*!
 *  @brief Sets the descriptor of TransformerAttention with values. Comparing to ::cnnlSetTransformerAttentionDescriptor,
 *  ::cnnlSetTransformerAttentionDescriptor_v2 updates cnnlActivationPreference_t to cnnlComputationPreference_t.
 *
 *  @param[in,out] desc
 *    Input/output. Descriptor of TransformerAttention operation.
 *  @param[in] compute_dtype
 *    Input. Data type to be used during computation.
 *    - The data types of \p query, \p key and \p value must be the same.
 *    - If data type of \p query is half, \p compute_dtype can be half or float.
 *    - If data type of \p query is float, \p compute_dtype must be float.
 *    - If data type of \p query is bfloat16, \p compute_dtype can be bfloat16 or float.
 *    - If \p compute_dtype is set to CNNL_DTYPE_INVALID, it will be the same as that of \p query.
 *  @param[in] act_pref
 *    Input. The indicator of activation operation preference, including:
 *           - CNNL_COMPUTATION_FAST: Use high-performance for activation operations.
 *           - CNNL_COMPUTATION_HIGH_PRECISION: Use high-precision for activation operations.
 *  @param[in] mask_mode
 *    Input. The indicator of mask adding mode.
 *    Currently only ::CNNL_ATTN_MASK_NONE, ::CNNL_ATTN_MASK_NT, ::CNNL_ATTN_MASK_NTT,
 *    ::CNNL_ATTN_MASK_N, ::CNNL_ATTN_MASK_NHTT and ::CNNL_ATTN_MASK_CAUSAL are supported. See ::cnnlAttentionMaskMode_t for details.
 *  @param[in] is_pack_mode
 *    Input. If true, inputs of multi-head attention are packed. If false, inputs are padded.
 *  @param[in] packed_max_seq_len
 *    Input. The indicator of maximum sequence length of pack mode.
 *           This parameter is only used when \p is_pack_mode is true.
 *           When \p is_pack_mode is false, this parameter is ignored.
 *  @param[in] query_factor
 *    Input. The scale factor for scaled batchdot of q and k.
 *  @param[in] is_mul_factor_after_qk
 *    Input. If true, \p query_factor is multiplied after calculating q dot k. If false,
 *           \p query_factor is multiplied on q before performing q dot k.
 *  @param[in] is_store_qk
 *    Input. Reserved for future use.
 *    Currently only false is supported.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p desc is NULL, or one or more parameters are not supported.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetTransformerAttentionDescriptor_v2(cnnlTransformerAttentionDescriptor_t desc,
                                         cnnlDataType_t compute_dtype,
                                         cnnlComputationPreference_t act_pref,
                                         cnnlAttentionMaskMode_t mask_mode,
                                         bool is_pack_mode,
                                         int packed_max_seq_len,
                                         float query_factor,
                                         bool is_mul_factor_after_qk,
                                         bool is_store_qk);

// Group:TransformerAttention
/*!
 *  @brief Creates a descriptor that holds quantization information of TransformerAttention.
 *
 *  @param[out]  desc
 *    Output. Descriptor of TransformerAttention operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_ALLOC_FAILED
 *    The function fails to allocate memory for the TransformerAttention descriptor.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateTransformerAttentionQuantizeDescriptor(
    cnnlTransformerAttentionQuantizeDescriptor_t *desc);

// Group:TransformerAttention
/*!
 *  @brief Destroys the descriptor that holds quantization information of TransformerAttention.
 *
 *  @param[in]  desc
 *    Input. Descriptor of TransformerAttention operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p desc is NULL.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyTransformerAttentionQuantizeDescriptor(
    cnnlTransformerAttentionQuantizeDescriptor_t desc);

// Group:TransformerAttention
/*!
 *  @brief Sets the quantization descriptor of TransformerAttention with values.
 *
 *  @param[in,out]  desc
 *    Input/output. Descriptor of TransformerAttention operation.
 *  @param[in]  query_out_pos
 *    Input. Quantization position of \p query.
 *  @param[in]  query_out_scale
 *    Input. Quantization scale of \p query.
 *  @param[in]  key_out_pos
 *    Input. Quantization position of \p key.
 *  @param[in]  key_out_scale
 *    Input. Quantization scale of \p key.
 *  @param[in]  value_out_pos
 *    Input. Quantization position of \p value.
 *  @param[in]  value_out_scale
 *    Input. Quantization scale of \p value.
 *  @param[in]  softmax_out_pos
 *    Input. Quantization position of result of softmax.
 *  @param[in]  softmax_out_scale
 *    Input. Quantization scale of result of softmax.
 *  @param[in]  qkv_out_pos
 *    Input. Quantization position of \p output.
 *  @param[in]  qkv_out_scale
 *    Input. Quantization scale of \p output.
 *  @param[in]  use_qk_aftergemm
 *    Input. A Boolean value indicating whether to enable fake quantization on q dot k. Currently only false is supported.
 *  @param[in]  qk_aftergemm_pos
 *    Input. Reserved for future use.
 *  @param[in]  qk_aftergemm_scale
 *    Input. Reserved for future use.
 *  @param[in]  use_qkv_aftergemm
 *    Input. A Boolean value indicating whether to enable fake quantization before \p output. Currently only false is supported.
 *  @param[in]  qkv_aftergemm_pos
 *    Input. Reserved for future use.
 *  @param[in]  qkv_aftergemm_scale
 *    Input. Reserved for future use.
 *  @param[in]  quantize_matmul_sum_dtype
 *    Input. The computation data type of quantized matmul. Currently only \p CNNL_DTYPE_INT32 is supported.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p desc is NULL, or one or more parameters are not supported.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetTransformerAttentionQuantizeDescriptor(cnnlTransformerAttentionQuantizeDescriptor_t desc,
                                              int query_out_pos,
                                              float query_out_scale,
                                              int key_out_pos,
                                              float key_out_scale,
                                              int value_out_pos,
                                              float value_out_scale,
                                              int softmax_out_pos,
                                              float softmax_out_scale,
                                              int qkv_out_pos,
                                              float qkv_out_scale,
                                              bool use_qk_aftergemm,
                                              int qk_aftergemm_pos,
                                              float qk_aftergemm_scale,
                                              bool use_qkv_aftergemm,
                                              int qkv_aftergemm_pos,
                                              float qkv_aftergemm_scale,
                                              cnnlDataType_t quantize_matmul_sum_dtype);

// Group:TransformerAttention
/*!
 *  @brief Sets the decoder kv cache quantization parameters of TransformerAttention with values.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlSingleQueryCachedKVAttn_v2 instead.
 *
 *  @param[in,out]  desc
 *    Input/output. Descriptor of TransformerAttention quantization parameters.
 *  @param[in]  k_cache_quant_desc
 *    Input. Quantization descriptor of k cache.
 *  @param[in]  v_cache_quant_desc
 *    Input. Quantization descriptor of v cache.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p desc is NULL, or one or more parameters are not supported.
 *  @note
 *  - The quantization layout in \p k_cache_quant_desc and \p v_cache_quant_desc must be \p CNNL_QUANTIZE_PER_TOKEN.
 *  - The quantization mode in \p k_cache_quant_desc and \p v_cache_quant_desc must be CNNL_QUANTIZE_POSITION_SCALE. And only
 *    scale will be supported, and position will be ignored.
 *  - The shape of quantization parameters in \p k_cache_quant_desc and \p v_cache_quant_desc must be [batch, max_decode_len].
 *  - The data type of scale in \p k_cache_quant_desc and \p v_cache_quant_desc must be float.
 *  - The \p quant_bit_size must be 8, which means that data type of kv cache should be int8.
 *  - The \p fake_quant must be false.
 */
CNNL_DEPRECATED_FOR(cnnlSingleQueryCachedKVAttn_v2)
cnnlStatus_t CNNL_WIN_API
cnnlSetTransformerAttentionKVCacheQuantizeDescriptor(
    cnnlTransformerAttentionQuantizeDescriptor_t desc,
    const cnnlQuantizeDescriptor_t k_cache_quant_desc,
    const cnnlQuantizeDescriptor_t v_cache_quant_desc);

// Group:TransformerAttention
/*!
 *  @brief Sets TensorFloat-32 mode for TransformerAttention operation with parameter.
 *
 *  @param[in,out] attn_desc
 *    Input/output. Descriptor of TransformerAttention operation.
 *  @param[in] allow_tf32
 *    Input. An integer value that determines whether to enable TensorFloat-32.
 *    The value only contains 0 and 1. 1 means TensorFloat-32 is enabled and 0 means TensorFloat-32 is disabled.
 *    TensorFloat-32 is disabled by default.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p attn_desc is NULL, or one or more parameters are not supported.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetTransformerAttentionDescriptorAllowTF32(
    cnnlTransformerAttentionDescriptor_t attn_desc,
    const int allow_tf32);

/*!
 *  @brief Sets the sequence layout with parameters for the TransformerAttention operation.
 *
 *  @param[in,out] attn_desc
 *  Input/output. Descriptor of the TransformerAttention operation.
 *  @param[in] seq_layout
 *  Input. The layout of query/key/value in TransformerAttention.
 *  The value can only be \p CNNL_SEQDATA_NTBC or \p CNNL_SEQDATA_NBTC.
 *  - When \p seq_layout is \p CNNL_SEQDATA_NBTC, dim of head_num should be ahead of dim of seq_len.
 *    \p query, \p key and \p value must be continuous.
 *  - When \p seq_layout is \p CNNL_SEQDATA_NTBC, dim of head_num should be after dim of seq_len.
 *    \p query, \p key and \p value may not be continuous along other dims except head_size.
 *  - When \p seq_layout is not set, the default value is \p CNNL_SEQDATA_NBTC.
 *  @retval CNNL_STATUS_SUCCESS
 *  - The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *  - \p attn_desc is NULL, or one or more parameters are not supported.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetTransformerAttentionSeqDataLayout(
    cnnlTransformerAttentionDescriptor_t attn_desc,
    cnnlSeqDataLayout_t seq_layout);

// Group:TransformerAttention
/*!
 *  @brief Sets the descriptor of TransformerAttention with position embedding information.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[in,out]  desc
 *    Input/output. Descriptor of TransformerAttention operation.
 *  @param[in]  position_embedding_type
 *    Input. The indicator of position embedding type, see ::cnnlTransformerAttentionPositionEmbeddingType_t for details.
 *  @param[in]  position_embedding_bias_desc
 *    Input. Descriptor of \p position_embedding_bias.
 *           - When \p position_embedding_type is ::CNNL_ATTN_CROSS_ROTARY_EMBEDDING or ::CNNL_ATTN_FOLD_ROTARY_EMBEDDING, the shape
 *           must be [pe_seq_len, rotary_embedding_dim * 2], where pe_seq_len is larger than or equal to [max(\p token_offset) + max(\p seq_len_q, seq_len_k)],
 *           and \p rotary_embedding_dim must be less than or equal to \p head_size.
 *           - When \p position_embedding_type is ::CNNL_ATTN_RELATIVE_POSITION_EMBEDDING or ::CNNL_ATTN_ALIBI_POSITION_EMBEDDING, the shape
 *           must be [head_num, pe_seq_len_q, pe_seq_len_k], where pe_seq_len_q is larger than or equal to seq_len_q,
 *           and pe_seq_len_k is larger than or equal to seq_len_k.
 *           - When \p position_embedding_type is ::CNNL_ATTN_CROSS_ROTARY_EMBEDDING_2D or ::CNNL_ATTN_FOLD_ROTARY_EMBEDDING_2D, the shape
 *           must be [pe_seq_len, \p head_size], where pe_seq_len is larger than or equal to [max(\p token_offset) + max(seq_len_q, seq_len_k)],
 *           and head_size should be divisible by 4.
 *  @param[in]  position_embedding_bias
 *    Input. Pointer to the MLU memory that stores the position embedding bias tensor.
 *  @param[in]  token_offset_desc
 *    Input. Descriptor of \p token_offset. The data type must be int32.
 *           - The shape should be [batch_size] or [batch_size, offset_seq_len] when \p position_embedding_type is ::CNNL_ATTN_CROSS_ROTARY_EMBEDDING or ::CNNL_ATTN_FOLD_ROTARY_EMBEDDING,
 *           where offset_seq_len is equal to max(seq_len_q, seq_len_k).
 *           - The shape must be [batch_size, 2, offset_seq_len] when \p position_embedding_type is
 *           ::CNNL_ATTN_CROSS_ROTARY_EMBEDDING_2D or ::CNNL_ATTN_FOLD_ROTARY_EMBEDDING_2D,
 *           where offset_seq_len is equal to max(seq_len_q, seq_len_k).
 *  @param[in]  token_offset
 *    Input. Pointer to the MLU memory that stores the token offset tensor.
 *  @param[in]  rotary_query_desc
 *    Input. Descriptor of \p rotary_query. In encoder mode, if sequence layout is \p CNNL_SEQDATA_NBTC,
 *           the shape is [batch_size, q_heads, seq_q, head_size], and if sequence layout is \p CNNL_SEQDATA_NTBC,
             the shape is [batch_size, seq_q, q_heads, head_size], when \p position_embedding_type is ::CNNL_ATTN_CROSS_ROTARY_EMBEDDING,
             ::CNNL_ATTN_FOLD_ROTARY_EMBEDDING, ::CNNL_ATTN_CROSS_ROTARY_EMBEDDING_2D or ::CNNL_ATTN_FOLD_ROTARY_EMBEDDING_2D.
 *  @param[in]  rotary_query
 *    Input. Pointer to MLU memory that stores the rotary_query tensor.
 *  @param[in]  rotary_key_desc
 *    Input. Descriptor of \p rotary_key. In encoder mode, if sequence layout is \p CNNL_SEQDATA_NBTC,
 *           the shape is [batch_size, kv_heads, seq_kv, head_size], and if sequence layout is \p CNNL_SEQDATA_NTBC,
             the shape is [batch_size, seq_kv, kv_heads, head_size], when \p position_embedding_type is ::CNNL_ATTN_CROSS_ROTARY_EMBEDDING,
             ::CNNL_ATTN_FOLD_ROTARY_EMBEDDING, ::CNNL_ATTN_CROSS_ROTARY_EMBEDDING_2D or ::CNNL_ATTN_FOLD_ROTARY_EMBEDDING_2D.
 *  @param[in]  rotary_key
 *    Input. Pointer to MLU memory that stores the rotary_key tensor.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p desc is NULL, or one or more parameters are not supported.
 *  @note
 *   - The data type of \p position_embedding_bias must be the same as that of query, key and value if
 *     query, key and value are not quantized; If query, key and value are quantized, and \p mask_mode
 *     is neither ::CNNL_ATTN_MASK_NONE nor ::CNNL_ATTN_MASK_N, the data type of
 *     \p position_embedding_bias must be the same as that of \p mask. Except the cases stated above,
 *     the data type of \p position_embedding_bias should be float, half or bfloat16.
 *   - The sequence layout should be \p CNNL_SEQDATA_NBTC or \p CNNL_SEQDATA_NTBC.
 *   - Position embedding is only used in encoder mode.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t cnnlSetTransformerAttentionDescriptorPositionEmbedding(
    const cnnlTransformerAttentionDescriptor_t desc,
    const cnnlTransformerAttentionPositionEmbeddingType_t position_embedding_type,
    const cnnlTensorDescriptor_t position_embedding_bias_desc,
    void *position_embedding_bias,
    const cnnlTensorDescriptor_t token_offset_desc,
    void *token_offset,
    const cnnlTensorDescriptor_t rotary_query_desc,
    void *rotary_query,
    const cnnlTensorDescriptor_t rotary_key_desc,
    void *rotary_key
);

// Group:TransformerAttention
/*!
 *  @brief Retrieves the extra space size needed in attention operation.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlGetTransformerAttentionWorkspaceSize_v2 instead.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
 *  @param[in]  desc
 *    Input. Descriptor of TransformerAttention operation.
 *  @param[in]  quan_desc
 *    Input. Descriptor of TransformerAttention operation including quantization related information.
 *    Reserved for future use.
 *  @param[in]  query_desc
 *    Input. Descriptor of query tensor.
 *  @param[in]  key_desc
 *    Input. Descriptor of key tensor.
 *  @param[in]  value_desc
 *    Input. Descriptor of value tensor.
 *  @param[out] size
 *    Output. Size of workspace.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more required parameters are NULL.
 */
CNNL_DEPRECATED_FOR(cnnlGetTransformerAttentionWorkspaceSize_v2)
cnnlStatus_t CNNL_WIN_API
cnnlGetTransformerAttentionWorkspaceSize(cnnlHandle_t handle,
                                         cnnlTransformerAttentionDescriptor_t desc,
                                         cnnlTransformerAttentionQuantizeDescriptor_t quan_desc,
                                         cnnlTensorDescriptor_t query_desc,
                                         cnnlTensorDescriptor_t key_desc,
                                         cnnlTensorDescriptor_t value_desc,
                                         size_t *size);

// Group:TransformerAttention
/*!
 *  @brief Retrieves the extra space size needed in attention operation.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
 *  @param[in]  desc
 *    Input. Descriptor of TransformerAttention operation.
 *  @param[in]  quan_desc
 *    Input. Descriptor of TransformerAttention operation including quantization related information.
 *    Reserved for future use.
 *  @param[in]  query_desc
 *    Input. Descriptor of query tensor.
 *  @param[in]  key_desc
 *    Input. Descriptor of key tensor.
 *  @param[in]  value_desc
 *    Input. Descriptor of value tensor.
 *  @param[in]  mask_desc
 *    Input. Descriptor of mask tensor.
 *   @param[in]  output_desc
 *    Input. Descriptor of output tensor.
 *  @param[out] size
 *    Output. Size of workspace.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more required parameters are NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetTransformerAttentionWorkspaceSize_v2(cnnlHandle_t handle,
                                            cnnlTransformerAttentionDescriptor_t desc,
                                            cnnlTransformerAttentionQuantizeDescriptor_t quan_desc,
                                            cnnlTensorDescriptor_t query_desc,
                                            cnnlTensorDescriptor_t key_desc,
                                            cnnlTensorDescriptor_t value_desc,
                                            cnnlTensorDescriptor_t mask_desc,
                                            cnnlTensorDescriptor_t output_desc,
                                            size_t *size);

// Group:TransformerAttention
/*!
 *  @brief Sets discrete cache index for Transformer Attention operation.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlSingleQueryCachedKVAttn_v2 instead.
 *
 *  @param[in,out] desc
 *    Input/output. Descriptor of Transformer Attention operation.
 *  @param[in] enable_discrete_cache
 *    Input. If true, discrete kv cache function is enabled. If false, discrete kv cache function is disabled.
 *  @param[in] discrete_cache_locs_desc
 *    Input. Descriptor of \p discrete_cache_locs. The shape is [batch], and the data type is int32.
 *  @param[in] discrete_cache_locs
 *    Input. Pointer to the MLU memory that stores the \p discrete_cache_locs tensor.
 *    It's a list of index to indicate which sequences will be dealt with.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p desc is NULL, or one or more parameters are not supported.
 *  @note
 *  - This function is only used for transformer attention decoder mode.
 */
CNNL_DEPRECATED_FOR(cnnlSingleQueryCachedKVAttn_v2)
cnnlStatus_t cnnlSetTransformerAttentionDescriptorDiscreteCacheLocs(
    const cnnlTransformerAttentionDescriptor_t desc,
    bool  enable_discrete_cache,
    const cnnlTensorDescriptor_t discrete_cache_locs_desc,
    void *discrete_cache_locs);

// Group:TransformerAttention
/*!
 *  @brief Executes multi-head attention in Transformer encoder and decoder network.
 *
 *  @par API Dependency
 *  - You need to call the ::cnnlGetTransformerAttentionCacheStrategy() function before using decoder self attention.
 *
 *  This function performs with the following steps:
 *
 *  - In Transformer encoder:
 *
 *    if (position_embedding_type == rotary_embedding) then
 *
 *      query = rotate(query, position_embedding_bias)
 *
 *    if (position_embedding_type == rotary_embedding) then
 *
 *      key = rotate(key, position_embedding_bias)
 *
 *    if (is_mul_factor_after_qk == false and query.dtype != int8) then
 *
 *      query = query * query_factor
 *
 *    beta = batch_matmul(query, key)
 *
 *    if (is_mul_factor_after_qk == true) then
 *
 *      beta = beta * query_factor
 *
 *    if (position_embedding_type == alibi_embedding or position_embedding_type == relative_embedding) then
 *
 *      beta = beta + position_embedding_bias
 *
 *    if (has_mask == true) then
 *
 *      beta = beta + mask
 *
 *    alpha = softmax(beta)
 *
 *    output = batch_matmul(alpha, value)
 *
 *  - In Transformer decoder:
 *
 *    if (position_embedding_type == rotary_embedding) then
 *
 *      query = rotate(query, position_embedding_bias)
 *
 *    if (position_embedding_type == rotary_embedding) then
 *
 *      key = rotate(key, position_embedding_bias)
 *
 *    f = curr_idx[0]
 *
 *    key = concat(key_cache[:,:,0:f], key)
 *
 *    value = concat(value_cache[:,:,0:f], value)
 *
 *    if (is_mul_factor_after_qk == false) then
 *
 *      query = query * query_factor
 *
 *    beta = batch_matmul(query, key)
 *
 *    if (is_mul_factor_after_qk == true) then
 *
 *      beta = beta * query_factor
 *
 *    if (position_embedding_type == alibi_embedding or position_embedding_type == relative_embedding) then
 *
 *      beta = beta + position_embedding_bias
 *
 *    alpha = softmax(beta)
 *
 *    output = batch_matmul(alpha, value)
 *
 * @par Data Type
 *    - Data types of \p query, \p key, \p value and \p output must be the same, which support float, bfloat16, half and int8.
 *    - Data type bfloat16 is only supported on MLU500 series.
 *    - If mask_mode is CNNL_ATTN_MASK_N, then data type of \p mask must be int32.
 *    - If mask_mode is not CNNL_ATTN_MASK_N, and data type of \p query is not int8, then data type of \p mask
 *      must be the same as that of \p query. If data type of \p query is int8, \p mask can be float or half.
 *    - If mask_mode is CNNL_ATTN_MASK_CAUSAL, data type of \p query must be half or float.
 *
 * @note
 *  - Only MLU300 and MLU500 series are supported.
 *  - If \p curr_idx is NULL, the operation computes encoder attention.
 *  - If \p curr_idx is not NULL, the operation computes decoder attention.
 *  - Inputs and outputs cannot be homologous operand.
 *  - The content of all input tensors is not modified.
 *  - All input and output tensors must be continuous except \p query, \p key and \p value.
 *  - The sequence layout should be \p CNNL_SEQDATA_NBTC or \p CNNL_SEQDATA_NTBC.
 *  - When seq_layout is \p CNNL_SEQDATA_NBTC.
 *    - In pack mode, the memory layout of \p query, \p key and \p value must be [1, head_num, total_seq_len, head_size].
 *      Total_seq_len equals to the sum of each sequence length in all batches. Actual memory layout of the i-th batch must be [head_num, mask[i], head_size].
 *      All batches are stored adjacently.
 *      The first dimension '1' in \p query_desc, \p key_desc, \p value_desc and \p output_desc can be omitted.
 *      The dimensions 'head_num' and 'total_seq_len' in \p query_desc, \p key_desc and \p value_desc can be squashed as one dimension.
 *    - In pad mode, the memory layout of \p query, \p key and \p value must be [batch, head_num, seq_len, head_size].
 *    - \p query, \p key and \p value must be continuous.
 *  - When seq_layout is \p CNNL_SEQDATA_NTBC.
 *    - In pack mode, the memory layout of \p query, \p key and \p value must be [1, total_seq_len, head_num, head_size].
 *      Total_seq_len equals to the sum of each sequence length in all batches. All batches are stored adjacently.
 *      The first dimension '1' in \p query_desc, \p key_desc, \p value_desc and \p output_desc can be omitted.
 *    - In pad mode, the memory layout of \p query, \p key and \p value must be [batch, seq_len, head_num, head_size].
 *    - \p query, \p key and \p value may not be continuous along other dims except head_size.
 *  - Pack mode is only supported in encoder attention.
 *  - In decoder mode, \p best_beams_cache must be nullptr when \p position_embedding_type is not CNNL_ATTN_NO_POSITION_EMBEDDING.
 *  - In decoder mode, \p mask_mode must be CNNL_ATTN_MASK_NONE.
 *  - In decoder mode, \p query, \p key and \p may not be continuous alone dim of head_num.
 *  - In decoder mode, only \p key, \p value, \p key_cache and \p value_cache can use date type int8 for decoder quantization,
 *    and \p query must be floating point type.
 *
 *  @par Scale Limitation
 *  - \p batch > 0.
 *  - \p seq_q must be in range of [1, 1024] if data type of input is int8. Otherwise \p seq_q > 0.
 *  - \p seq_kv must be in range of [1, 1024] if data type of input is int8. Otherwise \p seq_kv > 0.
 *  - \p q_heads and \p kv_heads must be in range of [1, 128], and \p q_heads % \p kv_heads == 0.
 *    If data type of input is int8, \p q_heads must be equal to \p kv_heads when encoder mode is used.
 *  - In encoder mode, \p head_size must be in range of [16, 512]. When \p head_size > 256, \p head_size can be 320, 384, 448 or 512, which is a multiple of 64.
 *  - In decoder mode, \p head_size must be in range of [16, 256].
 *  - \p compute_dtype must be half, bfloat16 or float. If data type of \p input is float, \p compute_dtype must be float. If data type
 *    of \p input is bfloat16, \p compute_dtype can be bfloat16 or float, however, the operator will use float as the actual \p compute_dtype automatically.
 *    If data type of \p input is half, \p compute_dtype can be half or float.
 *  - \p mask_mode must be one of the following:
 *    - ::CNNL_ATTN_MASK_NONE
 *    - ::CNNL_ATTN_MASK_NT
 *    - ::CNNL_ATTN_MASK_NTT
 *    - ::CNNL_ATTN_MASK_NHTT
 *    - ::CNNL_ATTN_MASK_N
 *    - ::CNNL_ATTN_MASK_CAUSAL
 *  - If \p is_pack_mode is true, \p position_embedding_type must be ::CNNL_ATTN_NO_POSITION_EMBEDDING,
 *    and \p mask_mode could be ::CNNL_ATTN_MASK_N or ::CNNL_ATTN_MASK_CAUSAL. If \p mask_mode is ::CNNL_ATTN_MASK_N,
 *    then the sum of \p mask must be equal to \p total_seq_len.
 *  - \p output_qk must be false.
 *  - \p max_decode_len must be in range of [1, 32768].
 *  - \p curr_idx must be in range of [0, max_decode_len).
 *  - \p beam must be in range of [1, 32].
 *  - \p batch % \p beam must be 0 in decoder attention.
 *  - \p ngroup must be in range of [1, 60].
 *  - The element number of every input or output cannot exceed INT32_MAX, including strides.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
 *  @param[in]  desc
 *    Input. Descriptor of TransformerAttention operation.
 *  @param[in]  quan_desc
 *    Input. Descriptor of TransformerAttention operation including quantization related information.
 *    It is required when data types of \p query, \p key, \p value and \p output are int8 for encoder, and
 *    date types of \p key, \p value, \p key_cache, and \p value_cache are int8 for decoder.
 *  @param[in]  query_desc
 *    Input. Descriptor of query tensor.
 *    In encoder pad mode, if sequence layout is \p CNNL_SEQDATA_NBTC, the shape is [batch, q_heads, seq_q, head_size],
 *    and if sequence layout is \p CNNL_SEQDATA_NTBC, the shape is [batch, q_heads, seq_q, head_size].
 *    In encoder pack mode, if sequence layout is \p CNNL_SEQDATA_NBTC, the shape is [q_heads * total_seq_len, head_size],
 *    and if sequence layout is \p CNNL_SEQDATA_NTBC, the shape is [total_seq_len, q_heads, head_size].
 *    In decoder mode, if sequence layout is \p CNNL_SEQDATA_NBTC, the shape is [batch * beam, q_heads, seq_q, head_size],
 *    and if sequence layout is \p CNNL_SEQDATA_NTBC, the shape is [batch * beam, seq_q, q_heads, head_size].
 *  @param[in]  query
 *    Input. Pointer to the MLU memory that stores the query tensor.
 *  @param[in]  key_desc
 *    Input. Descriptor of key tensor.
 *    In encoder pad mode, if sequence layout is \p CNNL_SEQDATA_NBTC, the shape is [batch, kv_heads, seq_kv, head_size],
 *    and if sequence layout is \p CNNL_SEQDATA_NTBC, the shape is [batch, kv_heads, seq_kv, head_size].
 *    In encoder pack mode, if sequence layout is \p CNNL_SEQDATA_NBTC, the shape is [kv_heads * total_seq_len, head_size],
 *    and if sequence layout is \p CNNL_SEQDATA_NTBC, the shape is [total_seq_len, kv_heads, head_size].
 *    In decoder mode, if sequence layout is \p CNNL_SEQDATA_NBTC, the shape is [batch * beam, kv_heads, seq_kv, head_size],
 *    and if sequence layout is \p CNNL_SEQDATA_NTBC, the shape is [batch * beam, seq_kv, kv_heads, head_size].
 *  @param[in]  key
 *    Input. Pointer to the MLU memory that stores the key tensor.
 *  @param[in]  value_desc
 *    Input. Descriptor of value tensor.
 *    In encoder pad mode, if sequence layout is \p CNNL_SEQDATA_NBTC, the shape is [batch, kv_heads, seq_kv, head_size],
 *    and if sequence layout is \p CNNL_SEQDATA_NTBC, the shape is [batch, kv_heads, seq_kv, head_size].
 *    In encoder pack mode, if sequence layout is \p CNNL_SEQDATA_NBTC, the shape is [kv_heads * total_seq_len, head_size],
 *    and if sequence layout is \p CNNL_SEQDATA_NTBC, the shape is [total_seq_len, kv_heads, head_size].
 *    In decoder mode, if sequence layout is \p CNNL_SEQDATA_NBTC, the shape is [batch * beam, kv_heads, seq_kv, head_size],
 *    and if sequence layout is \p CNNL_SEQDATA_NTBC, the shape is [batch * beam, seq_kv, kv_heads, head_size].
 *  @param[in]  value
 *    Input. Pointer to the MLU memory that stores the value tensor.
 *  @param[in]  mask_desc
 *    Input. Descriptor of mask tensor. The shape and data type must be consistent with mask_mode.
 *  @param[in]  mask
 *    Input. Pointer to the MLU memory that stores the mask tensor.
 *  @param[in]  curr_idx_desc
 *    Input. Descriptor of curr_idx tensor. The shape is [1], and data type is int32.
 *           This parameter is only required in decoder mode.
 *  @param[in]  curr_idx
 *    Input. Pointer to the MLU memory that stores the curr_idx tensor.
 *           This parameter is only required in decoder mode.
 *  @param[in]  key_cache_desc
 *    Input. Descriptor of key_cache tensor. The shape is [batch * beam, kv_heads, max_decode_len, head_size]
 *           or [batch, beam, kv_heads, max_decode_len, head_size].
 *           This parameter is only required in decoder mode.
 *  @param[in]  key_cache
 *    Input. Pointer to the MLU memory that stores the \p key_cache tensor.
 *           This parameter is only required in decoder mode.
 *  @param[in]  value_cache_desc
 *    Input. Descriptor of value_cache tensor. The shape is [batch * beam, kv_heads, max_decode_len, head_size]
 *           or [batch, beam, kv_heads, max_decode_len, head_size].
 *           This parameter is only required in decoder mode.
 *  @param[in]  value_cache
 *    Input. Pointer to the MLU memory that stores the value_cache tensor.
 *           This parameter is only required in decoder mode.
 *  @param[in] best_beams_cache_desc
 *    Input. Descriptor of best_beams_cache tensor. The shape must be [(batch / beam), ngroup, beam].
 *  @param[in] best_beams_cache
 *    Input. Pointer to the MLU memory that stores the best_beams_cache tensor. It is a state
 *           inferred from beam search result, to restore historical K/V cache correctly.
 *           If this is the first frame of decoder, this cache must be initialized with zero
 *           before calling this operation.
 *  @param[in]  workspace
 *    Input. Pointer to the MLU memory that is used as an extra workspace.
 *  @param[in]  workspace_size
 *    Input. Size of workspace.
 *  @param[in]  output_qk_desc
 *    Input. Descriptor of output_qk tensor. The shape is [batch * beam, q_heads, seq_q, seq_k].
 *           Reserved for future use.
 *  @param[in]  output_qk
 *    Input. Pointer to the MLU memory that stores the output_qk tensor. Reserved for future use.
 *  @param[in]  output_desc
 *    Input. Descriptor of output tensor. The shape is [batch * beam, seq_q, q_heads, head_size] in pad mode,
      or [1, total_seq_len, q_heads, head_size] in pack mode.
 *  @param[out]  output
 *    Output. Pointer to the MLU memory that stores the output tensor.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are encountered:
 *    - \p handle is NULL.
 *    - One or more required pointers are NULL.
 *    - The scale or data type limitation is not satisfied.
 */
cnnlStatus_t CNNL_WIN_API
cnnlTransformerAttention(cnnlHandle_t handle,
                         cnnlTransformerAttentionDescriptor_t desc,
                         cnnlTransformerAttentionQuantizeDescriptor_t quan_desc,
                         cnnlTensorDescriptor_t query_desc,
                         const void *query,
                         cnnlTensorDescriptor_t key_desc,
                         const void *key,
                         cnnlTensorDescriptor_t value_desc,
                         const void *value,
                         cnnlTensorDescriptor_t mask_desc,
                         const void *mask,
                         cnnlTensorDescriptor_t curr_idx_desc,
                         const void *curr_idx,
                         cnnlTensorDescriptor_t key_cache_desc,
                         const void *key_cache,
                         cnnlTensorDescriptor_t value_cache_desc,
                         const void *value_cache,
                         cnnlTensorDescriptor_t best_beams_cache_desc,
                         const void *best_beams_cache,
                         void *workspace,
                         const size_t workspace_size,
                         cnnlTensorDescriptor_t output_qk_desc,
                         void *output_qk,
                         cnnlTensorDescriptor_t output_desc,
                         void *output);

// Group:MaskedSoftmax
/*!
 *  @brief Speeds up the Transformer structure.
 *  This operation performs the following steps:
 *
 *  1. Scale the tensor.
 *
 *  2. Apply the mask.
 *
 *     - Add the mask (typically used in BERT models).
 *
 *        The calculation formula is y = softmax(x * scale + mask).
 *
 *     - Apply upper triangular mask (typically used in GPT models).
 *
 *        The calculation formula is y = softmax(maskUpperTriangle(x * scale)).
 *
 *        maskUpperTriangle means that it returns the lower triangular part of the matrices \p x
 *        and the other elements of the result tensor are set to -10000 or -inf.
 *
 *     - Apply masked fill (typically used in Transformer models).
 *
 *        The calculation formula is y = softmax(masked_fill(x * scale, mask, float_min)).
 *
 *        masked_fill means that it fills the elements of \p x with \p float_min where \p mask is true,
 *        and the \p float_min value can be -10000 or -inf.
 *
 *  3. Perform softmax.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
 *  @param[in] masked_softmax_op
 *    Input. The algorithm of the masked softmax operation. See ::cnnlMaskedSoftmaxOp_t for details.
 *  @param[in] axis
 *    Input. A dimension along which softmax will be computed. The axis can be negative.
 *  @param[in] scale
 *    Input. A float value, which is the scaling factor for input \p x.
 *  @param[in]  x_desc
 *   Input. The descriptor of the input tensor \p x. It should be 4D or 5D.
 *  @param[in]  x
 *    Input. Pointer to the MLU memory that stores the \p x tensor.
 *  @param[in]  mask_desc
 *    Input. Descriptor of mask tensor.
 *    This operation supports broadcasting. Each dimension of the mask tensor \p a must match the
 *    corresponding dimension of the tensor \p x, or must be equal to 1.
 *  @param[in]  mask
 *    Input. Pointer to the MLU memory that stores the mask tensor.
 *  @param[in]  y_desc
 *   Input. The descriptor of the output tensor.
 *  @param[out]  y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are encountered:
 *    - \p handle is NULL.
 *    - \p x_desc is NULL.
 *    - \p y_desc is NULL.
 *    - One or more required pointers are NULL.
 *    - The scale or data type limitation is not satisfied.
 *    - Homologous operand is enabled, but the input \p x
 *       and output \p y data types are the different.
 *  @retval CNNL_STATUS_ARCH_MISMATCH
 *    One or more of the following conditions are encountered:
 *    - This function is running on the hardware platform that is not supported.
 *  @retval CNNL_STATUS_NOT_SUPPORTED
 *    - A tensor number is equal to or larger than 2G on MLU300 series.
 *
 *  @par Data Type
 *  - By the order of \p masked_softmax_op - \p x - \p mask, the supported data types are as follows:
 *    - CNNL_MASKED_SOFTMAX_ADD_MASK - half - half
 *    - CNNL_MASKED_SOFTMAX_ADD_MASK - float - float
 *    - CNNL_MASKED_SOFTMAX_ADD_MASK - bfloat16 - bfloat16
 *    - CNNL_MASKED_SOFTMAX_MASKED_FILL - float/half/bfloat16 - bool
 *    - CNNL_MASKED_SOFTMAX_MASKED_FILL_NEG_INF - float/half/bfloat16 - bool
 *    - Otherwise, the \p mask tensor is not required, and the parameter should be passed with a value of nullptr.
 *
 *  - If \p masked_softmax_op is \p CNNL_MASKED_SOFTMAX_UPPER_TRIANGLE_MASK_NEG_INF,
 *    the supported data types are as follows in the order of \p x - \p y:
 *    - half - half
 *    - bfloat16 - bfloat16
 *
 *  - If \p masked_softmax_op is not \p CNNL_MASKED_SOFTMAX_UPPER_TRIANGLE_MASK_NEG_INF,
 *    the supported data types are as follows in the order of \p x - \p y:
 *    - float - float
 *    - half  - half
 *    - half  - float
 *    - bfloat16 - bfloat16
 *
 *  @note
 *  - The shape size of \p x must be 4 or 5.
 *  - The shape of the \p y must be the same as that of the \p x.
 *  - Only when the input \p x and output \p y data types are the same, input \p x and output \p y
 *    can be homologous operand.
 *  - The input and mask tensors only support CNNL_LAYOUT_ARRAY.
 *  - The value of the input data in \p CNNL_MASKED_SOFTMAX_UPPER_TRIANGLE_MASK or
 *    \p CNNL_MASKED_SOFTMAX_MASKED_FILL mode is recommended to be greater than -10000.
 *  - For \p CNNL_MASKED_SOFTMAX_MASKED_FILL_NEG_INF, when all the values in the input tensor \p x
 *    are -inf, the result will be 0.
 *
 *  @par Scale Limitation
 *  - Only MLU300 and MLU500 series are supported.
 *  - Large tensor is only supported on MLU500 series, which means the input tensor number can be equal to or larger than 2G.
 *  - When \p masked_softmax_op is \p CNNL_MASKED_SOFTMAX_ADD_MASK,
 *    the supported dimensions of \p mask tensor are as follows:
 *
 *    - The last dimension of the \p mask must be the same as the \p x,
 *      and the other dimension of the shape of \p mask must be 1.
 *
 *      For example, when the shape of \p x is [B, H, F, T], the shape of \p mask is as follows:
 *
 *      [T], [1, T], [1, 1, T], [1, 1, 1, T]
 *
 *    - The shape of \p mask must be the same as that of the \p x.
 *
 *  - When \p masked_softmax_op is \p CNNL_MASKED_SOFTMAX_MASKED_FILL,
 *    the supported dimensions of mask tensor are as follows:
 *
 *    - The shape of \p mask must be the same as that of the \p x.
 *
 *  - When \p masked_softmax_op is \p CNNL_MASKED_SOFTMAX_UPPER_TRIANGLE_MASK_NEG_INF
 *    or \p CNNL_MASKED_SOFTMAX_MASKED_FILL_NEG_INF, the last dimension of input tensor
 *    must be less than or equal to 2048. When \p masked_softmax_op is
 *    \p CNNL_MASKED_SOFTMAX_UPPER_TRIANGLE_MASK or \p CNNL_MASKED_SOFTMAX_MASKED_FILL,
 *    it must be less than or equal to 608.
 *  - The value of \p axis only supports -1 or the shape size of \p x minus one.
 *
 *  - When \p masked_softmax_op is \p CNNL_MASKED_SOFTMAX_MASKED_FILL_NEG_INF,
 *    the supported dimensions of mask tensor are as follows:
 *
 *    - The shape of \p mask can be the same as that of the \p x.
 *    - The third dimension from the end can be 1. For example, when the shape of \p x is [B, H, F, T],
 *      the shape of \p mask shape of \p mask can be [B, 1, F, T].
 *
 *  @par Example
* - The example of the mask softmax operation is as follows:
     @verbatim
     Example 1:
       input array by 1 * 1 * 4 * 4
       --> x: [[ 0.1103, -1.9906,  1.2256,  1.8663],
               [-1.0390, -0.3271, -0.3401,  0.7607],
               [-0.4324,  1.9866,  0.8293, -0.2430],
               [ 0.0405,  1.1191,  2.2231,  0.5400]]

       param:
         axis: -1
         scale: 1.2
         masked_softmax_op: CNNL_MASKED_SOFTMAX_NO_MASK

       output array by 1 * 1 * 4 * 4
       --> y: [[0.0762, 0.0061, 0.2906, 0.6270],
               [0.0698, 0.1640, 0.1614, 0.6048],
               [0.0400, 0.7283, 0.1816, 0.0502],
               [0.0495, 0.1807, 0.6796, 0.0902]]

     Example 2:
       input array by 1 * 1 * 4 * 4
       --> x: [[ 0.1103, -1.9906,  1.2256,  1.8663],
               [-1.0390, -0.3271, -0.3401,  0.7607],
               [-0.4324,  1.9866,  0.8293, -0.2430],
               [ 0.0405,  1.1191,  2.2231,  0.5400]]

       input array by 1 * 1 * 4 * 4
       --> mask: [[-10000.,     -0., -10000., -10000.],
                  [    -0.,     -0.,     -0., -10000.],
                  [    -0., -10000., -10000.,     -0.],
                  [-10000., -10000., -10000., -10000.]]
       param:
         axis: -1
         scale: 1.2
         masked_softmax_op: CNNL_MASKED_SOFTMAX_ADD_MASK

       output array by 1 * 1 * 4 * 4
       --> y: [[0.0000, 1.0000, 0.0000, 0.0000],
               [0.1766, 0.4149, 0.4085, 0.0000],
               [0.4434, 0.0000, 0.0000, 0.5566],
               [0.0495, 0.1806, 0.6797, 0.0902]]

     Example 3:
       input array by 1 * 1 * 4 * 4
       --> x: [[ 0.1103, -1.9906,  1.2256,  1.8663],
               [-1.0390, -0.3271, -0.3401,  0.7607],
               [-0.4324,  1.9866,  0.8293, -0.2430],
               [ 0.0405,  1.1191,  2.2231,  0.5400]]

       param:
         axis: -1
         scale: 1.2
         masked_softmax_op: CNNL_MASKED_SOFTMAX_UPPER_TRIANGLE_MASK or
                            CNNL_MASKED_SOFTMAX_UPPER_TRIANGLE_MASK_NEG_INF

       output array by 1 * 1 * 4 * 4
       --> y: [[0.0762, 0.0061, 0.2906, 0.6270],
               [0.0698, 0.1640, 0.1614, 0.6048],
               [0.0400, 0.7283, 0.1816, 0.0502],
               [0.0495, 0.1807, 0.6796, 0.0902]]
       --> y: [[1.0000, 0.0000, 0.0000, 0.0000],
               [0.2985, 0.7015, 0.0000, 0.0000],
               [0.0421, 0.7667, 0.1912, 0.0000],
               [0.0495, 0.1807, 0.6796, 0.0902]]

     Example 4:
       input array by 1 * 1 * 4 * 4
       --> x: [[ 0.1103, -1.9906,  1.2256,  1.8663],
               [-1.0390, -0.3271, -0.3401,  0.7607],
               [-0.4324,  1.9866,  0.8293, -0.2430],
               [ 0.0405,  1.1191,  2.2231,  0.5400]]
      input array by 1 * 1 * 4 * 4
       --> mask: [[1, 0, 1, 1],
                  [0, 0, 0, 1],
                  [0, 1, 1, 0],
                  [1, 1, 1, 1]]

       param:
         axis: -1
         scale: 1.2
         masked_softmax_op: CNNL_MASKED_SOFTMAX_MASKED_FILL or
                            CNNL_MASKED_SOFTMAX_MASKED_FILL_NEG_INF

       output array by 1 * 1 * 4 * 4
       --> y: [[0.0000, 1.0000, 0.0000, 0.0000],
               [0.1766, 0.4149, 0.4085, 0.0000],
               [0.4434, 0.0000, 0.0000, 0.5566],
               [0.2500, 0.2500, 0.2500, 0.2500]]
       --> y: [[0.0000, 1.0000, 0.0000, 0.0000],
               [0.1766, 0.4149, 0.4085, 0.0000],
               [0.4434, 0.0000, 0.0000, 0.5566],
               [   nan,    nan,    nan,    nan]]
     @endverbatim
 */
cnnlStatus_t CNNL_WIN_API
cnnlMaskedSoftmax(cnnlHandle_t handle,
                  const cnnlMaskedSoftmaxOp_t masked_softmax_op,
                  const int axis,
                  const float scale,
                  const cnnlTensorDescriptor_t x_desc,
                  const void *x,
                  const cnnlTensorDescriptor_t mask_desc,
                  const void *mask,
                  const cnnlTensorDescriptor_t y_desc,
                  void *y);

// Group:MaskedScaleSoftmaxBackward
/*!
 *  @brief Computes the fusion operator gradient that consists of scale, mask,
 *  softmax and droupout operations. The MaskedScale in the interface name means the dropout mask and
 *  scale by dropout probability.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
 *  @param[in] axis
 *    Input. An integer value, which is the dimension for softmax calculation.
 *  @param[in] scale
 *    Input. A float value, which is the scaling factor for input y.
 *  @param[in]  y_desc
 *    Input. Descriptor of y tensor, which is the output of ::cnnlMaskedSoftmax operation.
 *  @param[in]  y
 *    Input. Pointer to the MLU memory that stores the y tensor.
 *    The shape of y is [B, H, D_q, D_k], where B represents the batch size, H represents the number
 *    of attention heads in the bert-large network, D_q represents the value of the second-to-last
 *    dimension of the query_layer and D_k represents the value of the second-to-last dimension of the
 *    key_layer.
 *  @param[in]  diff_y_desc
 *    Input. Descriptor of diff_y tensor, which is the derivative of the model loss with respect
 *    to the input y.
 *  @param[in]  diff_y
 *    Input. Pointer to the MLU memory that stores the diff_y tensor.
 *    The shape of y is [B, H, D_q, D_k], where B represents the batch size, H represents the number
 *    of attention heads in the bert-large network, D_q represents the value of the second-to-last
 *    dimension of the query_layer and D_k represents the value of the second-to-last dimension of the
 *    key_layer.
 *  @param[in]  mask_desc
 *    Input. Descriptor of mask tensor, which is mask output of the output dropout layer
 *    during forward propagation.
 *  @param[in]  mask
 *    Input. Pointer to the MLU memory that stores the mask tensor.
 *    The shape of y is [B, H, D_q, D_k], where B represents the batch size, H represents the number
 *    of attention heads in the bert-large network, D_q represents the value of the second-to-last
 *    dimension of the query_layer and D_k represents the value of the second-to-last dimension of the
 *    key_layer.
 *  @param[in] p
 *    Input. A float value used as the probability that the value of input \p y element is set to zero.
 *    \p p must be in [0, 1].
 *  @param[in]  diff_x_desc
 *    Input. Descriptor of diff_x tensor, which is the gradients of ::cnnlMaskedSoftmax operation.
 *  @param[out]  diff_x
 *    Output. Pointer to the MLU memory that stores the diff_x tensor.
 *    The shape of y is [B, H, D_q, D_k], where B represents the batch size, H represents the number
 *    of attention heads in the bert-large network, D_q represents the value of the second-to-last
 *    dimension of the query_layer and D_k represents the value of the second-to-last dimension of the
 *    key_layer.
 *
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are encountered:
 *    - handle is NULL.
 *    - One or more required pointers are NULL.
 *    - The scale or data type limitation is not satisfied.
 *    - Homologous operand is enabled, but the input \p diff_y
 *      and output \p diff_x data types are different.
 *    - \p y and \p diff_x homologous operand is enabled.
 *  @retval CNNL_STATUS_ARCH_MISMATCH
 *   This function is running on the hardware platform that is not supported.
 *
 *  @par Data Type
 *  - axis: int32
 *  - scale: float
 *  - y: half/float
 *  - diff_y: half/float
 *  - mask: uint8_t
 *  - p: float
 *  - diff_x: half/float
 *
 *  @par Data Layout
 *  - y tensor: \p CNNL_LAYOUT_ARRAY.
 *  - diff_y tensor: \p CNNL_LAYOUT_ARRAY.
 *  - mask tensor: \p CNNL_LAYOUT_ARRAY.
 *  - diff_x tensor: \p CNNL_LAYOUT_ARRAY.
 *
 *  @par Scale Limitation
 *    - The value of \p axis must be -1 or 3.
 *    - The value of \p p must be in range of [0, 1.f].
 *
 *  @note
 *  - Only MLU300 and MLU500 series are supported.
 *  - The data type of \p diff_y must be equal to the data type of \p y, either float or half.
 *  - The dimensions of \p y tensor, \p diff_y tensor, \p mask tensor and \p diff_x tensor
 *    must be same, and must be equal to 4.
 *  - \p mask cannot be NULL even if the value of \p p is equal to 0.
 *  - Only when the input \p diff_y and output \p diff_x data types are the same, input
 *    \p diff_y and output \p diff_x can be homologous operand.
 *
 *  @par Reference
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlMaskedScaleSoftmaxBackward(cnnlHandle_t handle,
                                                         int axis,
                                                         const float scale,
                                                         const cnnlTensorDescriptor_t y_desc,
                                                         const void *y,
                                                         const cnnlTensorDescriptor_t diff_y_desc,
                                                         const void *diff_y,
                                                         const cnnlTensorDescriptor_t mask_desc,
                                                         const uint8_t *mask,
                                                         const float p,
                                                         const cnnlTensorDescriptor_t diff_x_desc,
                                                         void *diff_x);

// Group:TransformerAttention
/*!
 *  @brief Obtains the merge_head and the best ngroup in decoder self attention.
 *         The merge_head indicates the shape of \p key_cache and \p value_cache parameters in
 *         ::cnnlTransformerAttention(), and the ngroup indicates the number of groups that
 *         max_decode_length is divided.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
 *  @param[in] batch
 *    Input. Number of input batches.
 *  @param[in] beam
 *    Input. Number of input beams.
 *  @param[in] head_num
 *    Input. Number of input heads.
 *  @param[in] head_size
 *    Input. The size of one head.
 *  @param[in] max_decode_len
 *    Input. The longest sequence length of the current dataset.
 *  @param[out] merge_head
 *    Output. Boolean.
 *            True means that the shape of k_cache in decoder self attention
 *            is [batch, beam, max_decode_len, hidden_size].
 *            False means that the shape of k_cache in decoder self attention
 *            is [batch, beam, head_num, max_decode_len, head_size].
 *  @param[out] ngroup
 *    Output. The number of groups that each max_decode_len is divided.
 *
 *  @retval CNNL_STATUS_SUCCESS = 0
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM = 3
 *    One or more of the following conditions are encountered:
 *    - handle is NULL.
 *    - ngroup is NULL.
 *    - merge_head is NULL.
 *    - The scale or data type limitation is not satisfied.
 *
 *  @par Scale Limitation
 *  - \p batch > 0.
 *  - \p beam must be in range of [1, 32].
 *  - \p head_num must be in range of [1, 128].
 *  - \p head_size must be in range of [16, 128].
 *  - \p max_seq_len must be in range of [1, 1024].
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetTransformerAttentionCacheStrategy(cnnlHandle_t handle,
                                         int batch,
                                         int beam,
                                         int head_num,
                                         int head_size,
                                         int max_decode_len,
                                         bool *merge_head,
                                         int * ngroup);

// Group:ScaledMaskedSoftmaxBackward
/*!
 *  @brief Computes the gradients of the ::cnnlMaskedSoftmax operation.
 *  It is the backpropagation process of the ::cnnlMaskedSoftmax operation, which is a fusion
 *  operator consisting of scale, mask and softmax operations.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
 *  @param[in] axis
 *    Input. An integer value, which is the dimension for softmax calculation.
 *  @param[in] scale
 *    Input. A float value, which is the scaling factor for input y.
 *  @param[in]  y_desc
 *    Input. Descriptor of y tensor, which is the output of ::cnnlMaskedSoftmax operation.
 *  @param[in]  y
 *    Input. Pointer to the MLU memory that stores the y tensor.
 *    The shape of y is [B, H, D_q, D_k], where B represents the batch size, H represents the number
 *    of attention heads in the bert-large network, D_q represents the value of the second-to-last
 *    dimension of the query_layer and D_k represents the value of the second-to-last dimension of the
 *    key_layer.
 *  @param[in]  diff_y_desc
 *    Input. Descriptor of diff_y tensor, which is the derivative of the model loss with respect
 *    to the input y.
 *  @param[in]  diff_y
 *    Input. Pointer to the MLU memory that stores the diff_y tensor.
 *    The shape of y is [B, H, D_q, D_k], where B represents the batch size, H represents the number
 *    of attention heads in the bert-large network, D_q represents the value of the second-to-last
 *    dimension of the query_layer and D_k represents the value of the second-to-last dimension of the
 *    key_layer.
 *  @param[in]  diff_x_desc
 *    Input. Descriptor of diff_x tensor, which is the gradients of ::cnnlMaskedSoftmax operation.
 *  @param[out]  diff_x
 *    Output. Pointer to the MLU memory that stores the diff_x tensor.
 *    The shape of y is [B, H, D_q, D_k], where B represents the batch size, H represents the number
 *    of attention heads in the bert-large network, D_q represents the value of the second-to-last
 *    dimension of the query_layer and D_k represents the value of the second-to-last dimension of the
 *    key_layer.
 *
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are encountered:
 *    - handle is NULL.
 *    - One or more required pointers are NULL.
 *    - data type limitation is not satisfied.
 *    - Homologous operand is enabled, but the input \p diff_y
 *      and output \p diff_x data types are different.
 *    - \p y and \p diff_x homologous operand is enabled.
 *  @retval CNNL_STATUS_ARCH_MISMATCH
 *   This function is running on the hardware platform that is not supported.
 *
 *  @par Data Type
 *  - y: half/float/bfloat16
 *  - diff_y: half/float/bfloat16
 *  - diff_x: half/float/bfloat16
 *  The dtype of y must be equal to the dtype of diff_y.
 *  If the dtype of y is bfloat16, the dtype of diff_x must be bfloat16. Otherwise,
 *  the dtype of diff_x can be half or float.
 *  @par Data Layout
 *  - y tensor: \p CNNL_LAYOUT_ARRAY.
 *  - diff_y tensor: \p CNNL_LAYOUT_ARRAY.
 *  - diff_x tensor: \p CNNL_LAYOUT_ARRAY.
 *
 *  @par Scale Limitation
 *    - The value of \p axis must be -1 or 3.
 *
 *  @note
 *  - Only MLU300 and MLU500 series are supported.
 *  - The data type of \p diff_y must be equal to the data type of \p y, either float or half.
 *  - The dimensions of \p y tensor, \p diff_y tensor and \p diff_x tensor must be same, and
 *    must be equal to 4.
 *  - Only when the input \p diff_y and output \p diff_x data types are the same, input
 *    \p diff_y and output \p diff_x can be homologous operand.
 *
 *  @par Reference
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlScaledMaskedSoftmaxBackward(cnnlHandle_t handle,
                                                          int axis,
                                                          const float scale,
                                                          const cnnlTensorDescriptor_t y_desc,
                                                          const void *y,
                                                          const cnnlTensorDescriptor_t diff_y_desc,
                                                          const void *diff_y,
                                                          const cnnlTensorDescriptor_t diff_x_desc,
                                                          void *diff_x);

/******************************************************************************
 * Cambricon CNNL OP: WindowAttention
 ******************************************************************************/
/*! The descriptor of the ::cnnlWindowAttention operation.
 * You can use ::cnnlCreateWindowAttentionDescriptor(), ::cnnlSetWindowAttentionDescriptor_v2()
 * and ::cnnlDestroyWindowAttentionDescriptor() to create, set and destroy the descriptor
 * respectively.
 */
typedef struct cnnlWindowAttentionStruct *cnnlWindowAttentionDescriptor_t;

/*! The descriptor of the ::cnnlWindowAttention operation that holds quantization information.
 * Reserved for future use.
 */
typedef struct cnnlWindowAttentionQuantizeStruct *cnnlWindowAttentionQuantizeDescriptor_t;

// Group:WindowAttention
/*!
 *  @brief Creates a descriptor pointed by \p desc
 *   for the WindowAttention operation.
 *  @param[out]  desc
 *    Output. Descriptor of WindowAttention operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_ALLOC_FAILED
 *    The function failed to allocate memory for the WindowAttention descriptor.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCreateWindowAttentionDescriptor(cnnlWindowAttentionDescriptor_t *desc);

// Group:WindowAttention
/*!
 *  @brief Destroys the descriptor of WindowAttention.
 *
 *  @param[in]  desc
 *    Input. Descriptor of WindowAttention operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p desc is NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlDestroyWindowAttentionDescriptor(cnnlWindowAttentionDescriptor_t desc);

// Group:WindowAttention
/*!
 *  @brief Sets the descriptor of WindowAttention with values.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlSetWindowAttentionDescriptor_v2 instead.
 *
 *  @param[in,out]  desc
 *    Input/output. Descriptor of WindowAttention operation.
 *  @param[in]  compute_dtype
 *    Input. Data type to be used during computation.
 *    - If data type of inputs are half, \p compute_dtype can be half or float.
 *    - If data type of inputs are float, \p compute_dtype must be float.
 *    - If \p compute_dtype is set to CNNL_DTYPE_INVALID, it will be the same as that of inputs.
 *  @param[in]  act_pref
 *    Input. The indicator of activation operation preference, including:
 *    - CNNL_ACTIVATION_FAST: use high-performance for activation operations.
 *    - CNNL_ACTIVATION_HIGH_PRECISION: use high-precision for activation operations.
 *  @param[in]  query_factor
 *    Input. The scale factor for scaled batchdot of q and k.
 *  @param[in]  is_mul_factor_after_qk
 *    Input. If true, \p query_factor is multiplied after calculating q dot k. If false,
 *           \p query_factor is multiplied on q before performing q dot k.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p desc is NULL, or one or more parameters are not supported.
 */
CNNL_DEPRECATED_FOR(cnnlSetWindowAttentionDescriptor_v2)
cnnlStatus_t CNNL_WIN_API
cnnlSetWindowAttentionDescriptor(cnnlWindowAttentionDescriptor_t desc,
                                 cnnlDataType_t compute_dtype,
                                 cnnlActivationPreference_t act_pref,
                                 float query_factor,
                                 bool is_mul_factor_after_qk);

// Group:WindowAttention
/*!
 *  @brief Sets the descriptor of WindowAttention with values.
 *
 *  Compared with ::cnnlSetWindowAttentionDescriptor, this API uses cnnlComputationPreference_t to indicate
 *  the best suited algorithm used for implementation of the activation and accumulation operations.
 *
 *  @param[in,out]  desc
 *    Input/output. Descriptor of WindowAttention operation.
 *  @param[in]  compute_dtype
 *    Input. Data type to be used during computation.
 *    - If data type of inputs are half, \p compute_dtype can be half or float.
 *    - If data type of inputs are float, \p compute_dtype must be float.
 *    - If \p compute_dtype is set to CNNL_DTYPE_INVALID, it will be the same as that of inputs.
 *  @param[in]  act_pref
 *    Input. The indicator of activation operation preference, including:
 *    - CNNL_COMPUTATION_FAST: use fatest algorithm and lower precision.
 *    - CNNL_COMPUTATION_HIGH_PRECISION: use high-precision algorithm regardless of the performance.
 *  @param[in]  query_factor
 *    Input. The scale factor for scaled batchdot of q and k.
 *  @param[in]  is_mul_factor_after_qk
 *    Input. If true, \p query_factor is multiplied after calculating q dot k. If false,
 *           \p query_factor is multiplied on q before performing q dot k.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p desc is NULL, or one or more parameters are not supported.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetWindowAttentionDescriptor_v2(cnnlWindowAttentionDescriptor_t desc,
                                    cnnlDataType_t compute_dtype,
                                    cnnlComputationPreference_t act_pref,
                                    float query_factor,
                                    bool is_mul_factor_after_qk);

// Group:WindowAttention
/*!
 *  @brief Retrieves the extra space size needed in WindowAttention operation.
 *
 *  @param[in]  desc
 *    Input. Descriptor of WindowAttention operation.
 *  @param[in]  qat_desc
 *    Input. Descriptor of WindowAttention operation including quantization related information.
 *    Reserved for future use.
 *  @param[in]  query_desc
 *    Input. Descriptor of query tensor. The shape is [batch, head_num, seq_q, head_size].
 *  @param[in]  key_desc
 *    Input. Descriptor of key tensor. The shape is [batch, head_num, seq_kv, head_size].
 *  @param[in]  value_desc
 *    Input. Descriptor of value tensor. The shape is [batch, head_num, seq_kv, head_size].
 *  @param[out] size
 *    Output. Size of workspace.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more required parameters are NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetWindowAttentionWorkspaceSize(cnnlWindowAttentionDescriptor_t desc,
                                    cnnlWindowAttentionQuantizeDescriptor_t qat_desc,
                                    cnnlTensorDescriptor_t query_desc,
                                    cnnlTensorDescriptor_t key_desc,
                                    cnnlTensorDescriptor_t value_desc,
                                    size_t *size);

// Group:WindowAttention
/*!
 *  @brief Creates a descriptor that holds quantization information of WindowAttention.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[out]  desc
 *    Output. Descriptor of WindowAttention operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_ALLOC_FAILED
 *    The function fails to allocate memory for the WindowAttention descriptor.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlCreateWindowAttentionQuantizeDescriptor(cnnlWindowAttentionQuantizeDescriptor_t *desc);

// Group:WindowAttention
/*!
 *  @brief Destroys the descriptor that holds quantization information of WindowAttention.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[in]  desc
 *    Input. Descriptor of WindowAttention operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p desc is NULL.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlDestroyWindowAttentionQuantizeDescriptor(cnnlWindowAttentionQuantizeDescriptor_t desc);

// Group:WindowAttention
/*!
 *  @brief Sets the quantization descriptor of WindowAttention with values.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[in,out]  desc
 *    Input/output. Descriptor of WindowAttention operation.
 *  @param[in]  query_out_pos
 *    Input. Quantization position of \p query.
 *  @param[in]  query_out_scale
 *    Input. Quantization scale of \p query.
 *  @param[in]  key_out_pos
 *    Input. Quantization position of \p key.
 *  @param[in]  key_out_scale
 *    Input. Quantization scale of \p key.
 *  @param[in]  value_out_pos
 *    Input. Quantization position of \p value.
 *  @param[in]  value_out_scale
 *    Input. Quantization scale of \p value.
 *  @param[in]  softmax_out_pos
 *    Input. Quantization position of result of softmax.
 *  @param[in]  softmax_out_scale
 *    Input. Quantization scale of result of softmax.
 *  @param[in]  qkv_out_pos
 *    Input. Quantization position of \p output.
 *  @param[in]  qkv_out_scale
 *    Input. Quantization scale of \p output.
 *  @param[in]  use_qk_aftergemm
 *    Input. A Boolean value indicating whether to enable fake quantization on q dot k. Currently only false is supported.
 *  @param[in]  qk_aftergemm_pos
 *    Input. Reserved for future use.
 *  @param[in]  qk_aftergemm_scale
 *    Input. Reserved for future use.
 *  @param[in]  use_qkv_aftergemm
 *    Input. A Boolean value indicating whether to enable fake quantization before \p output. Currently only false is supported.
 *  @param[in]  qkv_aftergemm_pos
 *    Input. Reserved for future use.
 *  @param[in]  qkv_aftergemm_scale
 *    Input. Reserved for future use.
 *  @param[in]  quantize_matmul_sum_dtype
 *    Input. The computation data type of quantized matmul. Currently only \p CNNL_DTYPE_INT32 is supported.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p desc is NULL, or one or more parameters are not supported.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlSetWindowAttentionQuantizeDescriptor(cnnlWindowAttentionQuantizeDescriptor_t desc,
                                         int query_out_pos,
                                         float query_out_scale,
                                         int key_out_pos,
                                         float key_out_scale,
                                         int value_out_pos,
                                         float value_out_scale,
                                         int softmax_out_pos,
                                         float softmax_out_scale,
                                         int qkv_out_pos,
                                         float qkv_out_scale,
                                         bool use_qk_aftergemm,
                                         int qk_aftergemm_pos,
                                         float qk_aftergemm_scale,
                                         bool use_qkv_aftergemm,
                                         int qkv_aftergemm_pos,
                                         float qkv_aftergemm_scale,
                                         cnnlDataType_t quantize_matmul_sum_dtype);

// Group:WindowAttention
/*!
 *  @brief Executes window attention.
 *
 *  This function performs with the following steps:
 *
 *  In window attention:
 *
 *    if (is_mul_factor_after_qk == false) then
 *      query = query * query_factor
 *
 *    beta = batch_matmul(query, key)
 *
 *    if (is_mul_factor_after_qk == true) then
 *      beta = beta * query_factor
 *
 *    beta = beta + position
 *
 *    if (has_mask == true) then
 *      beta = beta + mask
 *
 *    alpha = softmax(beta)
 *
 *    output = batch_matmul(alpha, value)
 *
 * @par Data Type
 *    - Data types of \p query, \p key, \p value and \p output must be the same,
 *      which support float and half.
 *
 * @note
 *  - Only MLU300 and MLU500 series are supported.
 *  - Inputs and outputs cannot be homologous operand.
 *  - The contents of all input tensors are not modified.
 *  - WindowAttention does not support quantization yet.
 *
 *  @par Scale Limitation
 *  - \p batch > 0.
 *  - \p seq_q must be in range of [1, 1024].
 *  - \p seq_kv must be in range of [1, 1024].
 *  - \p head_num must be in range of [1, 128].
 *  - \p head_size must be in range of [16, 128].
 *  - \p window_num must be in range of [1, 256].
 *  - \p compute_dtype must be half or float. If data type of input is float, \p compute_dtype must be float.
 *  - \p output_qk must be false.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
 *  @param[in]  desc
 *    Input. Descriptor of WindowAttention operation.
 *  @param[in]  qat_desc
 *    Input. Descriptor of WindowAttention operation including quantization related information.
 *    It is required only when data types of \p query, \p key, \p value and \p output are int8.
 *  @param[in]  query_desc
 *    Input. Descriptor of query tensor. The shape is [batch * window_num, head_num, seq_q, head_size],
 *    or [batch, head_num, seq_q, head_size] if mask does not exist.
 *  @param[in]  query
 *    Input. Pointer to the MLU memory that stores the query tensor.
 *  @param[in]  key_desc
 *    Input. Descriptor of key tensor. The shape is [batch * window_num, head_num, seq_kv, head_size],
 *    or [batch, head_num, seq_kv, head_size] if mask does not exist.
 *  @param[in]  key
 *    Input. Pointer to the MLU memory that stores the key tensor.
 *  @param[in]  value_desc
 *    Input. Descriptor of value tensor. The shape is [batch * window_num, head_num, seq_kv, head_size],
 *    or [batch, head_num, seq_kv, head_size] if mask does not exist.
 *  @param[in]  value
 *    Input. Pointer to the MLU memory that stores the value tensor.
 *  @param[in]  position_desc
 *    Input. Descriptor of position tensor. The shape is [1, head_num, seq_q, seq_kv].
 *  @param[in]  position
 *    Input. Pointer to the MLU memory that stores the position tensor.
 *  @param[in]  mask_desc
 *    Input. Descriptor of mask tensor. The shape is [1, window_num, 1, seq_q, seq_kv].
 *    If mask does not exist, mask_desc and mask must be null.
 *  @param[in]  mask
 *    Input. Pointer to the MLU memory that stores the mask tensor.
 *  @param[in]  workspace
 *    Input. Pointer to the MLU memory that is used as an extra workspace.
 *  @param[in]  workspace_size
 *    Input. Size of workspace.
 *  @param[in]  output_qk_desc
 *    Input. Descriptor of output_qk tensor. The shape is [batch * window_num, head_num, seq_q, seq_kv],
 *    or [batch, head_num, seq_q, seq_kv] if mask does not exist.
 *    Reserved for future use.
 *  @param[out]  output_qk
 *    Output. Pointer to the MLU memory that stores the output_qk tensor. Reserved for future use.
 *  @param[in]  output_desc
 *    Input. Descriptor of output tensor. The shape is [batch * window_num, seq_q, head_num, head_size],
 *    or [batch, seq_q, head_num, head_size] if mask does not exist.
 *  @param[out]  output
 *    Output. Pointer to the MLU memory that stores the output tensor.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are encountered:
 *    - handle is NULL.
 *    - One or more required pointers are NULL.
 *    - The scale or data type limitation is not satisfied.
 */
cnnlStatus_t CNNL_WIN_API
cnnlWindowAttention(cnnlHandle_t handle,
                    cnnlWindowAttentionDescriptor_t desc,
                    cnnlWindowAttentionQuantizeDescriptor_t qat_desc,
                    cnnlTensorDescriptor_t query_desc,
                    void *query,
                    cnnlTensorDescriptor_t key_desc,
                    void *key,
                    cnnlTensorDescriptor_t value_desc,
                    void *value,
                    cnnlTensorDescriptor_t position_desc,
                    void *position,
                    cnnlTensorDescriptor_t mask_desc,
                    void *mask,
                    void *workspace,
                    const size_t workspace_size,
                    cnnlTensorDescriptor_t output_qk_desc,
                    void *output_qk,
                    cnnlTensorDescriptor_t output_desc,
                    void *output);

// Group:ConcateCache
/*!
 *  @brief Retrieves extra space size needed in ConcateCache operation.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
 *  @param[in]  prompt_desc
 *    Input. Descriptor of prompt tensor. The shape is [batch, head_num, prompt_seq_len, head_size].
 *  @param[in]  context_desc
 *    Input. Descriptor of context tensor. The shape is [layer, batch, head_num, context_seq_len, head_size].
 *  @param[in]  cache_desc
 *    Input. Descriptor of cache tensor. The shape is [layer, batch, beam, head_num, cache_seq_len, head_size].
 *  @param[out] size
 *    Output. Size of workspace.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more required parameters are NULL.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlGetConcateCacheWorkspaceSize(cnnlHandle_t handle,
                                 const cnnlTensorDescriptor_t prompt_desc,
                                 const cnnlTensorDescriptor_t context_desc,
                                 const cnnlTensorDescriptor_t cache_desc,
                                 size_t *size);

// Group:ConcateCache
/*!
 *  @brief Appends valid prompt and valid context data in cache.
 *
 *  This function performs with the following steps:
 *
 *  @verbatim
 *    1. Scale the tensor.
 *
 *    2. Concatenate prompt if prompt exists.
 *
 *       if has_prompt:
 *           cache.concate(prompt)
 *
 *    3. Concatenate context.
 *
 *       cache.concate(context)
 *
 *    4. Update cache valid token.
 *
 *       if update_cache_token:
 *           if has_prompt:
 *               cache_valid_token += prompt_valid_token + context_valid_token
 *           else:
 *               cache_valid_token += context_valid_token
    @endverbatim
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *
 *  @param[in]  handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
 *  @param[in]  prompt_desc
 *    Input. Descriptor of prompt tensor. The shape is [batch, head_num, prompt_seq_len, head_size].
 *  @param[in]  prompt
 *    Input. Pointer to the MLU memory that stores the prompt tensor.
 *  @param[in]  prompt_valid_token_desc
 *    Input. Descriptor of prompt_valid_token tensor. The shape is [batch].
 *  @param[in]  prompt_valid_token
 *    Input. Pointer to the MLU memory that stores the prompt_valid_token tensor.
 *  @param[in]  context_desc
 *    Input. Descriptor of context tensor.
 *    The shape is [layer, batch, head_num, context_seq_len, head_size].
 *  @param[in]  context
 *    Input. Pointer to the MLU memory that stores the context tensor.
 *  @param[in]  context_valid_token_desc
 *    Input. Descriptor of context_valid_token tensor. The shape is [batch].
 *  @param[in]  context_valid_token
 *    Input. Pointer to the MLU memory that stores the context_valid_token tensor.
 *  @param[in]  cache_desc
 *    Input. Descriptor of cache tensor.
 *    The shape is [layer, batch, beam, head_num, cache_seq_len, head_size].
 *  @param[in,out]  cache
 *    Input. Pointer to the MLU memory that stores the cache tensor.
 *  @param[in]  cache_valid_token_desc
 *    Input. Descriptor of cache_valid_token tensor. The shape is [batch].
 *  @param[in,out]  cache_valid_token
 *    Input/output. Pointer to the MLU memory that stores the cache_valid_token tensor.
 *  @param[in]  workspace
 *    Input. Pointer to the MLU memory that is used as an extra workspace.
 *  @param[in]  workspace_size
 *    Input. Size of workspace.
 *  @param[in]  update_cache_valid_token
 *    Input. Boolean. False means joining valid prompt (if exists) and valid context data after
 *    cache without updating cache_valid_token. True means cache_valid_token should add
 *    context_valid_token and prompt_valid_token (if exists).
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are encountered:
 *    - handle is NULL.
 *    - One or more required pointers are NULL.
 *    - The scale or data type limitation is not satisfied.
 *
 *  @par Data Type
 *    - Data types of \p prompt, \p context and \p cache must be the same,
 *      which support float and half.
 *    - Data types of \p prompt_valid_token, \p context_valid_token and \p cache_valid_token must be int32.
 *
 *  @note
 *    - This function supports MLU300 and MLU500 series.
 *    - Although the sequence layout of input is imperceptible to the function,
 *      it can still deal with \p CNNL_SEQDATA_NTBC and \p CNNL_SEQDATA_NBTC layouts.
 *      - If sequence layout is \p CNNL_SEQDATA_NBTC, and, for example, the prompt shape is [batch, head_num, prompt_seq_len, head_size],
 *        you don't need to perform any additional operations.
 *      - If sequence layout is \p CNNL_SEQDATA_NTBC and, for example, the prompt shape is [batch, prompt_seq_len, head_num, head_size],
 *        you should change the shape to [batch, head_num, prompt_seq_len, head_size]
 *        and simultaneously modify the stride with the same order.
 *      - You can get/set these information by the cnnlGetTensorDescriptorEx/cnnlSetTensorDescriptorEx function.
 *
 *  @par Scale Limitation
 *    - \p batch > 0.
 *    - \p layer > 0.
 *    - \p beam > 0.
 *    - \p head_num > 0.
 *    - \p head_num <=128.
 *    - \p head_size >= 16.
 *    - \p head_size <= 256.
 *    - \p prompt_seq_len > 0.
 *    - \p context_seq_len > 0.
 *    - \p cache_seq_len > \p prompt_seq_len + \p context_seq_len.
 *    - \p prompt_seq_len * \p head_size * dtype_size <= INT32_MAX.
 *    - \p context_seq_len * \p head_size * dtype_size <= INT32_MAX.
 *    - \p head_num * \p cache_seq_len * \p head_size * dtype_size <= INT32_MAX.
 *    - On MLU300 and MLU500 series, the total count of elements of prompt and context should be less than or equal to INT32_MAX.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlConcateCache(cnnlHandle_t handle,
                 const cnnlTensorDescriptor_t prompt_desc,
                 const void *prompt,
                 const cnnlTensorDescriptor_t prompt_valid_token_desc,
                 const void *prompt_valid_token,
                 const cnnlTensorDescriptor_t context_desc,
                 const void *context,
                 const cnnlTensorDescriptor_t context_valid_token_desc,
                 const void *context_valid_token,
                 const cnnlTensorDescriptor_t cache_desc,
                 void *cache,
                 const cnnlTensorDescriptor_t cache_valid_token_desc,
                 void *cache_valid_token,
                 bool update_cache_valid_token,
                 void *workspace,
                 size_t workspace_size);

/******************************************************************************
 * Cambricon CNNL OP: FuseLayerNorm
 ******************************************************************************/
/*!
 *  @brief The descriptor of the ::cnnlFuseLayerNorm operation.
 *  You need to call the ::cnnlCreateFuseLayerNormDescriptor function
 *  to create a descriptor, and call the ::cnnlSetFuseLayerNormDescriptor function
 *  to set the attributes. Also, you need to destroy the descriptor at the end
 *  with the ::cnnlDestroyFuseLayerNormDescriptor function.
 *
 *  It is deprecated and will be removed in future release.
 */
typedef struct cnnlFuseLayerNormStruct *cnnlFuseLayerNormDescriptor_t;

// Group:FuseLayerNorm
/*!
 *  @brief Creates a descriptor pointed by
 *   \p fuse_layernorm_desc for the FuseLayerNorm operation.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlCreateFuseNormDescriptor instead.
 *
 *  @param[out] fuse_layernorm_desc
 *    Output.  A pointer to the FuseLayerNorm descriptor.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p fuse_layernorm_desc is NULL.
 */
CNNL_DEPRECATED_FOR(cnnlCreateFuseNormDescriptor)
cnnlStatus_t CNNL_WIN_API
cnnlCreateFuseLayerNormDescriptor(cnnlFuseLayerNormDescriptor_t *fuse_layernorm_desc);

// Group:FuseLayerNorm
/*!
 *  @brief Destroys the FuseLayerNorm descriptor.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlDestroyFuseNormDescriptor instead.
 *
 *  @param[in] fuse_layernorm_desc
 *    Input.  The FuseLayerNorm descriptor.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_EXECUTION_FAILED
 *    \p fuse_layernorm_desc is NULL.
 *
 *  @par API Dependency
 *  - This function is used after calling the ::cnnlFuseLayerNorm function.
 */
CNNL_DEPRECATED_FOR(cnnlDestroyFuseNormDescriptor)
cnnlStatus_t CNNL_WIN_API
cnnlDestroyFuseLayerNormDescriptor(cnnlFuseLayerNormDescriptor_t fuse_layernorm_desc);

// Group:FuseLayerNorm
/*!
 *  @brief Sets the FuseLayerNorm descriptor with parameters.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlSetFuseNormDescriptor instead.
 *
 *  @param[in,out] fuse_layernorm_desc
 *    Input/output.  The FuseLayerNorm descriptor.
 *  @param[in]  eps
 *    Input. The epsilon value of layernorm.
 * @param[in] has_bias
 *    Input. A Boolean value indicating whether to add bias for input.
 * @param[in] has_residual
 *    Input. A Boolean value indicating whether to add residual for input.
 * @param[in] store_output_before_layernorm
 *    Input. A Boolean value indicating whether to involve output before layernorm in computing.
 * @param[in] math_pre
 *    Input. Onchip computation data type of FuseLayerNorm operation.
 *  @par API Dependency
 *    - Before using this function, you need to call ::cnnlCreateFuseLayerNormDescriptor
 *    to create a descriptor.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - \p fuse_layernorm_desc is NULL.
 */
CNNL_DEPRECATED_FOR(cnnlSetFuseNormDescriptor)
cnnlStatus_t CNNL_WIN_API
cnnlSetFuseLayerNormDescriptor(cnnlFuseLayerNormDescriptor_t fuse_layernorm_desc,
                               float eps,
                               bool has_bias,
                               bool has_residual,
                               bool store_output_before_layernorm,
                               cnnlDataType_t math_pre);

// Group:FuseLayerNorm
/*!
 * @brief Retrieves extra space size needed in FuseLayerNorm operation.
 *
 * @deprecated
 *   This function is deprecated and will be removed in future release.
 *   Use ::cnnlGetFuseNormWorkspaceSize instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *          queues in the FuseLayerNorm operation.
 * @param[in] fuse_layernorm_desc
 *   Input. The descriptor of the FuseLayerNorm operation. For detailed information,
 *   see ::cnnlFuseLayerNormDescriptor_t.
 * @param[in] input_desc
 *    Input. The descriptor of the \p input tensor, containing the dimension and layout of the tensor.
 * @param[out] workspace_size
 *   Output. Pointer to the MLU memory that stores the extra space size (bytes)
 *   needed in FuseLayerNorm operation.
 * @retval CNNL_STATUS_SUCCESS
 *   The function ends normally.
 * @retval CNNL_STATUS_BAD_PARAM
 *   One or more of the following conditions must be met:
 *   - \p handle is NULL.
 *   - \p fuse_layernorm_desc is NULL.
 *   - \p input_desc is NULL.
 *   - \p workspace_size is NULL.
 */
CNNL_DEPRECATED_FOR(cnnlGetFuseNormWorkspaceSize)
cnnlStatus_t CNNL_WIN_API
cnnlGetFuseLayerNormWorkspaceSize(cnnlHandle_t handle,
                                  const cnnlFuseLayerNormDescriptor_t fuse_layernorm_desc,
                                  const cnnlTensorDescriptor_t input_desc,
                                  size_t *workspace_size);

// Group:FuseLayerNorm
/*!
 *  @brief Performs the FuseLayerNorm operation.
 *
 *  This function performs with the following steps:
 *
 *  1. add residual when the value of has_residual is true.
 *
 *    input = add(input, residual)
 *
 *  2. add bias when the value of has_bias is true.
 *
 *    input = add(input, bias)
 *
 *  3. store the result in output_before_layernorm when the value of store_output_before_layernorm is true.
 *
 *  4. layernorm (input).
 *
 *    output = layernorm(input, layernorm_scale, layernorm_bias, eps)
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlFuseNorm_v3 instead.
 *
 *  @param[in]  handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *           FuseLayerNorm operation.
 *  @param[in] fuse_layernorm_desc
 *    Input. The descriptor of the FuseLayerNorm operation. For detailed information,
 *    see ::cnnlFuseLayerNormDescriptor_t.
 *  @param[in]  input_desc
 *    Input. The descriptor of the \p input tensor, containing the dimension and layout of the tensor.
 *  @param[in]  input
 *    Input. Pointer to the MLU memory that stores the \p input tensor, which are the indices of the input words.
 *    Its shape is [h, c].
 *  @param[in]  layernorm_scale_desc
 *    Input. The descriptor of the \p layernorm_scale tensor, containing the dimension and layout of the tensor.
 *  @param[in]  layernorm_scale
 *    Input. Pointer to the MLU memory that stores the \p layernorm_scale tensor, which is the scaling factor
 *    of the layernorm. Its shape is [c].
 *  @param[in]  layernorm_bias_desc
 *    Input. The descriptor of the \p layernorm_bias tensor, containing the dimension and layout of the tensor.
 *  @param[in]  layernorm_bias
 *    Input. Pointer to the MLU memory that stores the \p layernorm_bias tensor, which is the bias
 *    parameter of the layernorm. Its shape is [c].
 *  @param[in]  residual_desc
 *    Input. The descriptor of the \p residual tensor, containing the dimension and layout of the tensor.
 *  @param[in]  residual
 *    Input. Pointer to the MLU memory that stores the \p residual tensor. Its shape is [h,c].
 *  @param[in]  bias_desc
 *    Input. The descriptor of the \p bias tensor, containing the dimension and layout of the tensor.
 *  @param[in]  bias
 *    Input. Pointer to the MLU memory that stores the \p bias tensor. Its shape is [c].
 *  @param[in]  output_desc
 *    Input. The descriptor of the \p output tensor, containing the dimension and layout of the tensor.
 *  @param[out]  output
 *    Output. Pointer to the MLU memory that stores the \p output tensor. Its shape is [h, c].
 *  @param[in]  output_before_layernorm_desc
 *    Input. The descriptor of the \p output_before_layernorm tensor, containing the dimension and layout of the tensor.
 *  @param[out]  output_before_layernorm
 *    Output. Pointer to the MLU memory that stores the \p output_before_layernorm tensor, which is the output before computing layernorm.
 *    Its shape is [h, c].
 *  @param[in] workspace
 *    Input.  A pointer to an extra workspace required in the operation.
 *  @param[in] workspace_size
 *    Input. The size of the workspace.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are encountered:
 *    - \p handle is NULL.
 *    - \p fuse_layernorm_desc is NULL.
 *    - \p input_desc is NULL.
 *    - \p input is NULL.
 *    - \p layernorm_scale_desc is NULL.
 *    - \p layernorm_scale is NULL.
 *    - \p layernorm_bias_desc is NULL.
 *    - \p layernorm_bias is NULL.
 *    - \p residual_desc is NULL when the value of has_residual is true.
 *    - \p residual is NULL when the value of has_residual is true.
 *    - \p bias_desc is NULL when the value of has_residual is true.
 *    - \p bias is NULL when the value of has_bias is true.
 *    - \p output_desc is NULL.
 *    - \p output is NULL.
 *    - \p output_before_layernorm_desc is NULL when the value of store_output_before_layernorm is true.
 *    - \p output_before_layernorm is NULL when the value of store_output_before_layernorm is true.
 *    - \p has_residual is false, \p has_bias is false and \p store_output_before_layernorm is true.
 *  @retval CNNL_STATUS_ARCH_MISMATCH
 *    - This function is run on the hardware platform that is not supported.
 *  @par Data Type
 *   - input: float, half.
 *   - layernorm_scale: float, half.
 *   - layernorm_bias: float, half.
 *   - residual: float, half.
 *   - bias: float, half.
 *   - output: float, half.
 *   - output_before_layernorm: float, half.
 *   - Data types of \p input, \p layernorm_scale, \p layernorm_bias, \p residual, \p bias, \p output
 *     and \p output_before_layernorm must be same.
 *
 * @note
 * - This function supports MLU300 and MLU500 series.
 * - When the data type of \p input is float, the \p math_pre must be DTYPE_FLOAT.
 * - When the data type of \p input is half, the \p math_pre should be either DTYPE_HALF or DTYPE_FLOAT.
 *  @par Scale Limitation
 *   - The second dimension (C dimension) of \p input cannot be greater than 12800.
 *
 *  @par Example
 *  @verbatim
    Dimension of input: [h, c]

    Dimension of layernorm_scale_desc: [c]

    Dimension of layernorm_bias: [c]

    Dimension of residual: [h, c]

    Dimension of bias: [c]

    Then we will get the output:

    Dimension of output: [h, c]

    Dimension of output_before_layernorm: [h, c]
    @endverbatim
 */
CNNL_DEPRECATED_FOR(cnnlFuseNorm_v3)
cnnlStatus_t CNNL_WIN_API
cnnlFuseLayerNorm(cnnlHandle_t handle,
                  const cnnlFuseLayerNormDescriptor_t fuse_layernorm_desc,
                  const cnnlTensorDescriptor_t input_desc,
                  const void *input,
                  const cnnlTensorDescriptor_t layernorm_scale_desc,
                  const void *layernorm_scale,
                  const cnnlTensorDescriptor_t layernorm_bias_desc,
                  const void *layernorm_bias,
                  const cnnlTensorDescriptor_t residual_desc,
                  const void *residual,
                  const cnnlTensorDescriptor_t bias_desc,
                  const void *bias,
                  const cnnlTensorDescriptor_t output_desc,
                  void *output,
                  const cnnlTensorDescriptor_t output_before_layernorm_desc,
                  void *output_before_layernorm,
                  void *workspace,
                  size_t workspace_size);

/******************************************************************************
 * Cambricon CNNL OP: FuseNorm
 ******************************************************************************/
/*! The descriptor of the ::cnnlFuseNorm operation.
 *
 *  You need to call the ::cnnlCreateFuseNormDescriptor function
 *  to create a descriptor, and call the ::cnnlSetFuseNormDescriptor function
 *  to set the attributes. Also, you need to destroy the descriptor at the end
 *  with the ::cnnlDestroyFuseNormDescriptor function.
 */
typedef struct cnnlFuseNormStruct *cnnlFuseNormDescriptor_t;

/*! The descriptor of the ::cnnlFuseNorm_v2 operation.
 *
 *  You need to call ::cnnlCreateFuseNormQuantizeDescriptor
 *  to create a descriptor, and call the ::cnnlSetFuseNormQuantizeDescriptor function
 *  to set the attributes. Also, you need to destroy the descriptor at the end
 *  with ::cnnlDestroyFuseNormQuantizeDescriptor.
 */
typedef struct cnnlFuseNormQuantizeStruct *cnnlFuseNormQuantizeDescriptor_t;

// Group:FuseNorm
/*!
 *  @brief Creates a descriptor pointed by
 *   \p fuse_norm_desc for the FuseNorm operation.
 *  @param[out] fuse_norm_desc
 *    Output.  A pointer to the FuseNorm descriptor.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p fuse_norm_desc is NULL.
 *  @retval CNNL_STATUS_ALLOC_FAILED
 *    The function quitted because host memory allocation failed.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCreateFuseNormDescriptor(cnnlFuseNormDescriptor_t *fuse_norm_desc);

// Group:FuseNorm
/*!
 *  @brief Creates a descriptor pointed by
 *   \p fuse_norm_quant_desc for the FuseNormQuant operation.
 *  @param[out] fuse_norm_quant_desc
 *    Output.  A pointer to the FuseNormQuantize descriptor.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p fuse_norm_quant_desc is NULL.
 *  @retval CNNL_STATUS_ALLOC_FAILED
 *    The function quitted because host memory allocation failed.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCreateFuseNormQuantizeDescriptor(cnnlFuseNormQuantizeDescriptor_t *fuse_norm_quant_desc);

// Group:FuseNorm
/*!
 *  @brief Destroys the FuseNorm descriptor.
 *  @param[in] fuse_norm_desc
 *    Input.  The FuseNorm descriptor.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_EXECUTION_FAILED
 *    \p fuse_norm_desc is NULL.
 *
 *  @par API Dependency
 *  - This function is used after calling the ::cnnlFuseNorm function.
 */
cnnlStatus_t CNNL_WIN_API
cnnlDestroyFuseNormDescriptor(cnnlFuseNormDescriptor_t fuse_norm_desc);

// Group:FuseNorm
/*!
 *  @brief Destroy the FuseNormQuant descriptor.
 *
 *  @param[in] fuse_norm_quant_desc
 *    Input.  The fuseNormQuantize descriptor.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_EXECUTION_FAILED
 *    \p fuse_norm_quant_desc is NULL.
 *
 *  @par API Dependency
 *  - This function is used after calling ::cnnlFuseNorm_v2.
 */
cnnlStatus_t CNNL_WIN_API
cnnlDestroyFuseNormQuantizeDescriptor(cnnlFuseNormQuantizeDescriptor_t fuse_norm_quant_desc);

// Group:FuseNorm
/*!
 *  @brief Sets the FuseNorm descriptor with parameters.
 *
 *  @param[in, out] fuse_norm_desc
 *    Input/output.  The FuseNorm descriptor.
 *  @param[in] eps
 *    Input. The epsilon value of norm function.
 *  @param[in] scale
 *    Input. The scale value of norm function.
 *  @param[in] has_norm_scale
 *    Input. A Boolean value indicating whether to multiply scale after normalization.
 *    When \p mode is ::CNNL_TRANSFORMER_LAYERNORM, it must be true.
 *  @param[in] has_norm_bias
 *    Input. A Boolean value indicating whether to add bias after normalization.
 *    When \p mode is ::CNNL_TRANSFORMER_LAYERNORM, it must be true.
 *    When \p mode is ::CNNL_TRANSFORMER_SCALENORM, it must be false.
 *  @param[in] has_bias
 *    Input. A Boolean value indicating whether to add bias for input.
 *  @param[in] has_residual
 *    Input. A Boolean value indicating whether to add residual for input.
 *  @param[in] store_output_before_norm
 *    Input. A Boolean value indicating whether to involve output before normalization in computing.
 *  @param[in] math_pre
 *    Input. Onchip computation data type of FuseNorm operation.
 *  @param[in] mode
 *    Input. The normalization type of FuseNorm operation. See ::cnnlTransformerNormType_t for details.
 *  @par API Dependency
 *    - Before using this function, you need to call ::cnnlCreateFuseNormDescriptor
 *    to create a descriptor.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - \p fuse_norm_desc is NULL.
 *    - \p mode has an illegal value.
 *    - \p has_norm_scale or \p has_norm_bias mismatches with \p mode.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetFuseNormDescriptor(cnnlFuseNormDescriptor_t fuse_norm_desc,
                          float eps,
                          float scale,
                          bool has_norm_scale,
                          bool has_norm_bias,
                          bool has_bias,
                          bool has_residual,
                          bool store_output_before_norm,
                          cnnlDataType_t math_pre,
                          cnnlTransformerNormType_t mode);

// Group:FuseNorm
/*!
 *  @brief Sets the FuseNormQuantize descriptor with parameters.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlSetFuseNormQuantizeDescriptor_v2 instead.
 *
 *  @param[in, out] fuse_norm_quant_desc
 *    Input/output.  The FuseNormQuantize descriptor.
 *  @param[in] quant_result_desc
 *    Input. The descriptor of quant.
 *  @par API Dependency
 *    - Before using this function, you need to call ::cnnlCreateFuseNormDescriptor
 *    to create a descriptor.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - \p quant_result_desc is NULL.
 */
CNNL_DEPRECATED_FOR(cnnlSetFuseNormQuantizeDescriptor_v2)
cnnlStatus_t CNNL_WIN_API
cnnlSetFuseNormQuantizeDescriptor(cnnlFuseNormQuantizeDescriptor_t fuse_norm_quant_desc,
                                  const cnnlQuantizeDescriptor_t quant_result_desc);

// Group:FuseNorm
/*!
 *
 *  @brief Sets the quantization information to FuseNorm quantization descriptor \p fuse_norm_desc
 *  that is previously created with the ::cnnlCreateFuseNormQuantizeDescriptor function.
 *
 *  Compared with ::cnnlSetFuseNormQuantizeDescriptor, this API
 *  updates cnnlQuantizeDescriptor_t to cnnlQuantizeExDescriptor_t.
 *
 *  @param[in, out] fuse_norm_quant_desc
 *    Input/output.  The FuseNormQuantize descriptor.
 *  @param[in] quant_result_desc
 *    Input. The quantization descriptor that holds quantization information of quant_result tensor.
 *           For detailed information, see cnnlQuantizeExDescriptor_t.
 *  @par API Dependency
 *    - Before using this function, you need to call ::cnnlCreateFuseNormDescriptor
 *    to create a descriptor.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - \p quant_result_desc is NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetFuseNormQuantizeDescriptor_v2(cnnlFuseNormQuantizeDescriptor_t fuse_norm_quant_desc,
                                     const cnnlQuantizeExDescriptor_t quant_result_desc);

// Group:FuseNorm
/*!
 * @brief Retrieves extra space size needed in FuseNorm operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *          queues in the FuseNorm operation.
 * @param[in] fuse_norm_desc
 *   Input. The descriptor of the FuseNorm operation. For detailed information,
 *   see ::cnnlFuseNormDescriptor_t.
 * @param[in] input_desc
 *    Input. The descriptor of the \p input tensor, containing the dimension and layout of the tensor.
 * @param[out] workspace_size
 *   Output. Pointer to the MLU memory that stores the extra space size (bytes)
 *   needed in FuseNorm operation.
 * @retval CNNL_STATUS_SUCCESS
 *   The function ends normally.
 * @retval CNNL_STATUS_BAD_PARAM
 *   One or more of the following conditions must be met:
 *   - \p handle is NULL.
 *   - \p fuse_norm_desc is NULL.
 *   - \p input_desc is NULL.
 *   - \p workspace_size is NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetFuseNormWorkspaceSize(cnnlHandle_t handle,
                             const cnnlFuseNormDescriptor_t fuse_norm_desc,
                             const cnnlTensorDescriptor_t input_desc,
                             size_t *workspace_size);

// Group:FuseNorm
/*!
 *  @brief Performs the FuseNorm operation.
 *
 *  Compared with ::cnnlFuseLayerNorm, this function allows users to choose the mode of normalization, and
 *  supports larger range of the second dimension (C dimension) of \p input.
 *
 *  This function performs with the following steps:
 *
 *  1. Add residual when the value of has_residual is true.
 *
 *    input = add(input, residual)
 *
 *  2. Add bias when the value of has_bias is true.
 *
 *    input = add(input, bias)
 *
 *  3. Store the result in output_before_norm when the value of store_output_before_norm is true.
 *
 *  4. Normalize input.
 *
 *    input = norm(input)
 *
 *  5. Multiply norm_scale when the value of has_norm_scale is true.
 *
 *    input = multiply(input, norm_scale)
 *
 *  6. Add norm_bias when the value has_norm_bias is true.
 *
 *    input = add(input, norm_bias)
 *
 *  7. Store the result in output.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlFuseNorm_v2 instead.
 *
 *  @param[in]  handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *           FuseNorm operation.
 *  @param[in] fuse_norm_desc
 *    Input. The descriptor of the FuseNorm operation. For detailed information,
 *    see ::cnnlFuseNormDescriptor_t.
 *  @param[in]  input_desc
 *    Input. The descriptor of the \p input tensor, containing the dimension and layout of the tensor.
 *  @param[in]  input
 *    Input. Pointer to the MLU memory that stores the \p input tensor, which are the indices of the input words.
 *    Its shape is [h, c].
 *  @param[in]  norm_scale_desc
 *    Input. The descriptor of the \p norm_scale tensor, containing the dimension and layout of the tensor.
 *  @param[in]  norm_scale
 *    Input. Pointer to the MLU memory that stores the \p norm_scale tensor, which is the scaling factor
 *    of the norm function. Its shape is [c].
 *  @param[in]  norm_bias_desc
 *    Input. The descriptor of the \p norm_bias tensor, containing the dimension and layout of the tensor.
 *  @param[in]  norm_bias
 *    Input. Pointer to the MLU memory that stores the \p norm_bias tensor, which is the bias
 *    parameter of the norm function. Its shape is [c].
 *  @param[in]  residual_desc
 *    Input. The descriptor of the \p residual tensor, containing the dimension and layout of the tensor.
 *  @param[in]  residual
 *    Input. Pointer to the MLU memory that stores the \p residual tensor. Its shape is [t, c].
 *  @param[in]  bias_desc
 *    Input. The descriptor of the \p bias tensor, containing the dimension and layout of the tensor.
 *  @param[in]  bias
 *    Input. Pointer to the MLU memory that stores the \p bias tensor. Its shape is [c].
 *  @param[in] workspace
 *    Input.  A pointer to an extra workspace required in the operation.
 *  @param[in] workspace_size
 *    Input. The size of the workspace.
 *  @param[in]  output_desc
 *    Input. The descriptor of the \p output tensor, containing the dimension and layout of the tensor.
 *  @param[out]  output
 *    Output. Pointer to the MLU memory that stores the \p output tensor. Its shape is [h, c].
 *  @param[in]  output_before_norm_desc
 *    Input. The descriptor of the \p output_before_norm tensor, containing the dimension and layout of the tensor.
 *  @param[out]  output_before_norm
 *    Output. Pointer to the MLU memory that stores the \p output_before_norm tensor, which is the output before computing norm function.
 *    Its shape is [h, c].
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are encountered:
 *    - \p handle is NULL.
 *    - \p fuse_norm_desc is NULL.
 *    - \p input_desc is NULL.
 *    - \p input is NULL.
 *    - \p norm_scale_desc is NULL when the value of has_norm_scale is true.
 *    - \p norm_scale is NULL when the value of has_norm_scale is true.
 *    - \p norm_bias_desc is NULL when the value of has_norm scale is true.
 *    - \p norm_bias is NULL when the value of has_norm scale is true.
 *    - \p residual_desc is NULL when the value of has_residual is true.
 *    - \p residual is NULL when the value of has_residual is true.
 *    - \p bias_desc is NULL when the value of has_residual is true.
 *    - \p bias is NULL when the value of has_bias is true.
 *    - \p output_desc is NULL.
 *    - \p output is NULL.
 *    - \p output_before_norm_desc is NULL when the value of store_output_before_norm is true.
 *    - \p output_before_norm is NULL when the value of store_output_before_norm is true.
 *    - \p has_residual is false, \p has_bias is false and \p store_output_before_layernorm is true.
 *  @retval CNNL_STATUS_ARCH_MISMATCH
 *    - This function is run on the hardware platform that is not supported.
 *  @par Data Type
 *    - input: float, half, bfloat16.
 *    - norm_scale: float, half, bfloat16.
 *    - norm_bias: float, half, bfloat16.
 *    - residual: float, half, bfloat16.
 *    - bias: float, half, bfloat16.
 *    - output: float, half, bfloat16.
 *    - output_before_norm: float, half, bfloat16.
 *    - Data types of \p input, \p norm_scale, \p norm_bias, \p residual, \p bias, \p output
 *      and \p output_before_norm must be same.
 *
 *  @note
 *    - This function only supports MLU300 and MLU500 series.
 *    - When the data type of \p input is float, \p math_pre must be DTYPE_FLOAT.
 *    - When the data type of \p input is half, \p math_pre should be either DTYPE_HALF or DTYPE_FLOAT.
 *    - When the data type of \p input is bfloat16, \p math_pre should be DTYPE_BFLOAT16 or DTYPE_FLOAT.
 *    - This function supports bfloat16 data type only on MLU500 series.
 *
 *  @par Scale Limitation
 *    - The second dimension (C dimension) of \p input cannot be greater than 18688.
 *
 *  @par Example
 *  @verbatim
    Dimension of input: [h, c]

    Dimension of norm_scale: [c]

    Dimension of norm_bias: [c]

    Dimension of residual: [h, c]

    Dimension of bias: [c]

    Then we will get the output:

    Dimension of output: [h, c]

    Dimension of output_before_norm: [h, c]
    @endverbatim
 */
CNNL_DEPRECATED_FOR(cnnlFuseNorm_v3)
cnnlStatus_t CNNL_WIN_API
cnnlFuseNorm(cnnlHandle_t handle,
             const cnnlFuseNormDescriptor_t fuse_norm_desc,
             const cnnlTensorDescriptor_t input_desc,
             const void *input,
             const cnnlTensorDescriptor_t norm_scale_desc,
             const void *norm_scale,
             const cnnlTensorDescriptor_t norm_bias_desc,
             const void *norm_bias,
             const cnnlTensorDescriptor_t residual_desc,
             const void *residual,
             const cnnlTensorDescriptor_t bias_desc,
             const void *bias,
             void *workspace,
             size_t workspace_size,
             const cnnlTensorDescriptor_t output_desc,
             void *output,
             const cnnlTensorDescriptor_t output_before_norm_desc,
             void *output_before_norm);

// Group:FuseNorm
/*!
 *  @brief This function is used to perform the FuseNorm operation. Compared with ::cnnlFuseNorm, this function allows users to choose
 *   the quantization mode of \p output.
 *
 *  This function performs with the following steps:
 *
 *  1. Add residual when the value of has_residual is true.
 *
 *    input = add(input, residual)
 *
 *  2. Add bias when the value of has_bias is true.
 *
 *    input = add(input, bias)
 *
 *  3. Store the result in output before layernorm when the value of store_output_before_layernorm is true.
 *
 *  4. Normalize input.
 *
 *    norm_scale = norm(input)
 *
 *  5. Multiply norm_scale when the value of has_norm_scale is true.
 *
 *    input = multiply(input, norm_scale)
 *
 *  6. Add norm_bias when the value of has_norm_bias is true.
 *
 *    input = add(input, norm_bias)
 *
 *  7. Quantize output when \p fuse_norm_quant_desc has usefull quantization information.
 *
 *    input = round(input * quant_scale)
 *
 *  8. Store the result in output.
 *
 *  @param[in]  handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *           FuseNorm operation.
 * @param[in] fuse_norm_desc
 *   Input. The descriptor of the FuseNorm operation. For detailed information,
 *   see ::cnnlFuseNormDescriptor_t.
  * @param[in] fuse_norm_quant_desc
 *   Input. The descriptor of the FuseNorm quantization operation. For detailed information,
 *   see ::cnnlFuseNormQuantizeDescriptor_t.
 *  @param[in]  input_desc
 *    Input. The descriptor of the \p input tensor, containing the dimension and layout of the tensor.
 *  @param[in]  input
 *    Input. Pointer to the MLU memory that stores the \p input tensor, which is the indices of the input words.
 *    Its shape is [t, c] or [n, t, c].
 *  @param[in]  norm_scale_desc
 *    Input. The descriptor of the \p norm_scale tensor, containing the dimension and layout of the tensor.
 *  @param[in]  norm_scale
 *    Input. Pointer to the MLU memory that stores the \p norm_scale tensor, which is the scaling factor
 *    of the norm function. Its shape is [c].
 *  @param[in]  norm_bias_desc
 *    Input. The descriptor of the \p norm_bias tensor, containing the dimension and layout of the tensor.
 *  @param[in]  norm_bias
 *    Input. Pointer to the MLU memory that stores the \p norm_bias tensor, which is the bias
 *    parameter of the norm function. Its shape is [c].
 *  @param[in]  residual_desc
 *    Input. The descriptor of the \p residual tensor, containing the dimension and layout of the tensor.
 *  @param[in]  residual
 *    Input. Pointer to the MLU memory that stores the \p residual tensor. Its shape is the same as that of \p input.
 *  @param[in]  bias_desc
 *    Input. The descriptor of the \p bias tensor, containing the dimension and layout of the tensor.
 *  @param[in]  bias
 *    Input. Pointer to the MLU memory that stores the \p bias tensor. Its shape is [c].
 *  @param[in] workspace
 *    Input.  Pointer to an extra workspace required in the operation.
 *  @param[in] workspace_size
 *    Input. The size of the workspace.
 *  @param[in]  output_desc
 *    Input. The descriptor of the \p output tensor, containing the dimension and layout of the tensor.
 *  @param[out]  output
 *    Output. Pointer to the MLU memory that stores the \p output tensor. Its shape is the same as that of \p input.
 *  @param[in]  output_before_norm_desc
 *    Input. The descriptor of the \p output_before_norm tensor, containing the dimension and layout of the tensor.
 *  @param[out]  output_before_norm
 *    Output. Pointer to the MLU memory that stores the \p output_before_norm tensor, which is the output before computing norm function.
 *    Its shape is the same as that of \p input.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are encountered:
 *    - \p handle is NULL.
 *    - \p fuse_norm_desc is NULL.
 *    - \p input_desc is NULL.
 *    - \p input is NULL.
 *    - \p norm_scale_desc is NULL when the value of has_norm_scale is true.
 *    - \p norm_scale is NULL when the value of has_norm_scale is true.
 *    - \p norm_bias_desc is NULL when the value of has_norm scale is true.
 *    - \p norm_bias is NULL when the value of has_norm scale is true.
 *    - \p residual_desc is NULL when the value of has_residual is true.
 *    - \p residual is NULL when the value of has_residual is true.
 *    - \p bias_desc is NULL when the value of has_bias is true.
 *    - \p bias is NULL when the value of has_bias is true.
 *    - \p output_desc is NULL.
 *    - \p output is NULL.
 *    - \p output_before_norm_desc is NULL when the value of store_output_before_norm is true.
 *    - \p output_before_norm is NULL when the value of store_output_before_norm is true.
 *    - \p has_residual is false, \p has_bias is false and \p store_output_before_layernorm is true.
 *  @retval CNNL_STATUS_ARCH_MISMATCH
 *    - This function is run on the hardware platform that is not supported.
 *  @par Data Type
 *    - input: float, half, bfloat16.
 *    - norm_scale: float, half, bfloat16.
 *    - norm_bias: float, half, bfloat16.
 *    - residual: float, half, bfloat16.
 *    - bias: float, half, bfloat16.
 *    - output: float, half, bfloat16, int8.
 *    - output_before_norm: float, half, bfloat16.
 *    - Data types of \p input, \p norm_scale, \p norm_bias, \p residual, \p bias,
 *      and \p output_before_norm must be same.
 *    - The data type of \p quant_scale must be float32.
 *
 *  @note
 *    - This function only supports MLU300 and MLU500 series.
 *    - When the data type of \p input is float, \p math_pre must be DTYPE_FLOAT.
 *    - When the data type of \p input is half, \p math_pre should be either DTYPE_HALF or DTYPE_FLOAT.
 *    - When the data type of \p input is bfloat16, \p math_pre should be DTYPE_BFLOAT16 or DTYPE_FLOAT.
 *    - The bfloat16 data type is only supported on MLU500 series.
 *    - Strides of \p input, \p residual, \p output, and \p output_before_norm must be same.
 *    - \p input, \p residual, \p output, and \p output_before_norm can be partially-packed tensor.
 *    - When the value of \p has_bias is false, the value of \p has_residual is false, and the value of \p store_output_before_norm is true,
 *      all tensors must be fully-packed tensors.
 *    - When the data type of \p output is int8, the shape of \p input must be [t, c] and all tensors must be fully-packed tensors.
 *
 *  @par Scale Limitation
 *    - The non-last dimension (n or t dimension) of \p input cannot exceed INT32_MAX, including strides.
 *    - The last dimension (C dimension) of \p input cannot be greater than 18688.
 *
 *  @par Example
 *  @verbatim
    Dimension of input: [h, c]

    Dimension of norm_scale: [c]

    Dimension of norm_bias: [c]

    Dimension of residual: [h, c]

    Dimension of bias: [c]

    Dimension of quant_scale: [c]

    Then we will get the output:

    Dimension of output: [h, c]

    Dimension of output_before_norm: [h, c]
    @endverbatim
 */
CNNL_DEPRECATED_FOR(cnnlFuseNorm_v3)
cnnlStatus_t CNNL_WIN_API
cnnlFuseNorm_v2(cnnlHandle_t handle,
                const cnnlFuseNormDescriptor_t fuse_norm_desc,
                const cnnlFuseNormQuantizeDescriptor_t fuse_norm_quant_desc,
                const cnnlTensorDescriptor_t input_desc,
                const void *input,
                const cnnlTensorDescriptor_t norm_scale_desc,
                const void *norm_scale,
                const cnnlTensorDescriptor_t norm_bias_desc,
                const void *norm_bias,
                const cnnlTensorDescriptor_t residual_desc,
                const void *residual,
                const cnnlTensorDescriptor_t bias_desc,
                const void *bias,
                void *workspace,
                size_t workspace_size,
                const cnnlTensorDescriptor_t output_desc,
                void *output,
                const cnnlTensorDescriptor_t output_before_norm_desc,
                void *output_before_norm);

// Group:FuseNorm
/*!
 *  @brief Performs the FuseNorm operation.
 *
 *  This function performs with the following steps:
 *
 *  1. Add residual when the value of has_residual is true.
 *
 *    input = add(input, residual)
 *
 *  2. Add bias when the value of has_bias is true.
 *
 *    input = add(input, bias)
 *
 *  3. Store the result in output before normalization when the value of store_output_before_norm is true.
 *
 *  4. Normalize input.
 *
 *    norm_scale = norm(input)
 *
 *  5. Multiply norm_scale when norm_scale is not nullptr.
 *
 *    input = multiply(input, norm_scale)
 *
 *  6. Add norm_bias when norm_bias is not nullptr.
 *
 *    input = add(input, norm_bias)
 *
 *  7. Quantize output when \p quant_desc is CNNL_QUANTIZE_PER_CHANNEL or CNNL_QUANTIZE_PER_TOKEN.
 *
 *    input = input * input_scale
 *
 *    if quant_desc is CNNL_QUANTIZE_PER_CHANNEL:
 *
 *      output = round(input)
 *
 *    else if quant_desc is CNNL_QUANTIZE_PER_TOKEN:
 *
 *      output, output_scale = quantize(input)
 *
 *  8. Store the result in output.
 *
 *  @param[in]  handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *           FuseNorm operation.
 *  @param[in]  input_desc
 *    Input. The descriptor of the \p input tensor, containing the dimension and layout of the tensor.
 *  @param[in]  input
 *    Input. Pointer to the MLU memory that stores the \p input tensor.
 *    Its shape is [n * t, c] or [n, t, c].
 *  @param[in]  input_scale_desc
 *    Input. The descriptor of the \p input_scale tensor, containing the dimension and layout of the tensor.
 *  @param[in]  input_scale
 *    Input. Pointer to the MLU memory that stores the \p input_scale tensor. Its shape is [c].
 *  @param[in]  norm_scale_desc
 *    Input. The descriptor of the \p norm_scale tensor, containing the dimension and layout of the tensor.
 *  @param[in]  norm_scale
 *    Input. Pointer to the MLU memory or host memory that stores the \p norm_scale tensor, which is the scaling factor
 *    of the norm function. Its shape is [c] if it points to the MLU memory.
 *  @param[in]  norm_bias_desc
 *    Input. The descriptor of the \p norm_bias tensor, containing the dimension and layout of the tensor.
 *  @param[in]  norm_bias
 *    Input. Pointer to the MLU memory that stores the \p norm_bias tensor, which is the bias
 *    parameter of the norm function. Its shape is [c].
 *  @param[in]  residual_desc
 *    Input. The descriptor of the \p residual tensor, containing the dimension and layout of the tensor.
 *  @param[in]  residual
 *    Input. Pointer to the MLU memory that stores the \p residual tensor. Its shape is the same as that of \p input.
 *  @param[in]  bias_desc
 *    Input. The descriptor of the \p bias tensor, containing the dimension and layout of the tensor.
 *  @param[in]  bias
 *    Input. Pointer to the MLU memory that stores the \p bias tensor. Its shape is [c].
 *  @param[in]  eps
 *    Input. The epsilon value of norm.
 *  @param[in]  quant_scheme
 *    Input. Indicates quantize layout of \p input tensor.
 *  @param[in]  store_output_before_norm
 *    Input. Indicates whether to store output before doing normalization.
 *  @param[in]  norm_type
 *    Input. Indicates norm type.
 *  @param[in]  math_pre
 *    Input. Indicates compute type for normalization.
 *  @param[in] workspace
 *    Input.  Pointer to an extra workspace required in the operation.
 *  @param[in] workspace_size
 *    Input. The size of the workspace.
 *  @param[in]  output_desc
 *    Input. The descriptor of the \p output tensor, containing the dimension and layout of the tensor.
 *  @param[out]  output
 *    Output. Pointer to the MLU memory that stores the \p output tensor. Its shape is the same as that of \p input.
 *  @param[in]  output_before_norm_desc
 *    Input. The descriptor of the \p output_before_norm tensor, containing the dimension and layout of the tensor.
 *  @param[out]  output_before_norm
 *    Output. Pointer to the MLU memory that stores the \p output_before_norm tensor, which is the output before normalization.
 *    Its shape is the same as that of \p input.
 *  @param[in]  output_quant_scale_desc
 *    Input. The descriptor of the \p output_quant_scale tensor, containing the dimension and layout of the tensor.
 *  @param[out]  output_quant_scale
 *    Output. Pointer to the MLU memory that stores the \p output_quant_scale tensor. Its shape is [n * t].
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are encountered:
 *    - \p handle is NULL.
 *    - \p input_desc or \p input is NULL.
 *    - \p input_scale_desc or \p input_scale is NULL when \p quant_scheme is CNNL_QUANTIZE_PER_CHANNEL
 *    - \p norm_scale_desc or \p norm_scale is NULL.
 *    - \p norm_bias_desc or \p norm_bias is NULL when \p norm_type is not CNNL_TRANSFORMER_SCALENORM.
 *    - \p output_desc or \p output is NULL.
 *    - \p output_before_norm_desc or \p output_before_norm is NULL when the value of \p store_output_before_norm is true.
 *    - \p output_quant_scale_desc or \p output_quant_scale is NULL when \p quant_scheme is CNNL_QUANTIZE_PER_TOKEN.
 *    - \p has_residual is false, \p has_bias is false and \p store_output_before_layernorm is true.
 *  @retval CNNL_STATUS_ARCH_MISMATCH
 *    - This function is run on the hardware platform that is not supported.
 *  @par Data Type
 *    - input: float, half, bfloat16.
 *    - input_scale: float.
 *    - norm_scale: float, half, bfloat16.
 *    - norm_bias: float, half, bfloat16.
 *    - residual: float, half, bfloat16.
 *    - bias: float, half, bfloat16.
 *    - output: float, half, bfloat16, int8, float8_e5m2, float8_e4m3fn.
 *    - output_before_norm: float, half, bfloat16.
 *    - output_quant_scale: float.
 *    - Data types of \p input, \p norm_scale, \p norm_bias, \p residual, \p bias,
 *      and \p output_before_norm must be same.
 *
 *    The bfloat16 data type is only supported on MLU500 series.
 *
 *  @note
 *    - This function only supports MLU300 and MLU500 series.
 *    - When the data type of \p input is float, \p math_pre must be DTYPE_FLOAT.
 *    - When the data type of \p input is half, \p math_pre should be either DTYPE_HALF or DTYPE_FLOAT.
 *    - When the data type of \p input is bfloat16, \p math_pre should be DTYPE_BFLOAT16 or DTYPE_FLOAT.
 *    - When the data type of \p output is int8, float8_e5m2 or float8_e4m3fn, \p input, \p residual, \p output and \p output_before_norm must be continuous.
 *    - \p input, \p residual, \p output, and \p output_before_norm can be partially-packed tensor.
 *    - When \p residual and \p bias are NULL, and the value of \p store_output_before_norm is true,
 *      all tensors must be fully-packed tensors.
 *    - When the data type of \p output is int8, the shape of \p input must be [n * t, c] and all tensors must be fully-packed tensors.
 *    - When the data type of \p output is float8_e5m2 or float8_e4m3fn, \p quant_scheme must be CNNL_QUANTIZE_PER_CHANNEL.
 *    - When the data type of \p output is int8, \p quant_scheme must be CNNL_QUANTIZE_PER_CHANNEL or CNNL_QUANTIZE_PER_TOKEN.
 *    - When the data type of \p output is int8, float8_e5m2 or float8_e4m3fn, the shape of \p input must be [n * t, c] and all tensors must be fully-packed tensors.
 *    - When \p norm_type is CNNL_TRANSFORMER_SCALENORM, \p norm_scale should point to the host memory, otherwise, it should point
 *      to the MLU memory.
 *
 *  @par Scale Limitation
 *    - The non-last dimension (n or t dimension) of \p input cannot exceed INT32_MAX, including strides.
 *    - The last dimension (C dimension) of \p input cannot be greater than 18688.
 *
 *  @par Example
 *  @verbatim
    Dimension of input: [n * t, c] or [n, t, c]

    Dimension of input_scale: [c]

    Dimension of norm_scale: [c] if it points to the MLU memory

    Dimension of norm_bias: [c]

    Dimension of residual: [n * t, c] or [n, t, c]

    Dimension of bias: [c]

    Then we will get the output:

    Dimension of output: [n * t, c] or [n, t, c]

    Dimension of output_before_norm: [n * t, c] or [n, t, c]

    Dimension of output_quant_scale: [n * t]
    @endverbatim
 */
cnnlStatus_t CNNL_WIN_API
cnnlFuseNorm_v3(cnnlHandle_t handle,
                const cnnlTensorDescriptor_t input_desc,
                const void *input,
                const cnnlTensorDescriptor_t input_scale_desc,
                const void *input_scale,
                const cnnlTensorDescriptor_t norm_scale_desc,
                const void *norm_scale,
                const cnnlTensorDescriptor_t norm_bias_desc,
                const void *norm_bias,
                const cnnlTensorDescriptor_t residual_desc,
                const void *residual,
                const cnnlTensorDescriptor_t bias_desc,
                const void *bias,
                const float eps,
                const cnnlQuantizeScheme_t quant_scheme,
                const bool store_output_before_norm,
                const cnnlTransformerNormType_t norm_type,
                const cnnlDataType_t math_pre,
                void *workspace,
                size_t workspace_size,
                const cnnlTensorDescriptor_t output_desc,
                void *output,
                const cnnlTensorDescriptor_t output_before_norm_desc,
                void *output_before_norm,
                const cnnlTensorDescriptor_t output_quant_scale_desc,
                void* output_quant_scale);

/******************************************************************************
 * Cambricon CNNL OP: BiasDropoutAddFusedTrain
 ******************************************************************************/
/*! The descriptor of the ::cnnlBiasDropoutAddFusedTrain operation.
 *
 *  You need to call the ::cnnlCreateBiasDropoutAddFusedTrainDescriptor function
 *  to create a descriptor, and call the ::cnnlSetBiasDropoutAddFusedTrainDescriptor
 *  function to set the attributes. Also, you need to destroy the descriptor at
 *  the end with the ::cnnlDestroyBiasDropoutAddFusedTrainDescriptor function.
 */
typedef struct cnnlBiasDropoutAddFusedTrainStruct *cnnlBiasDropoutAddFusedTrainDescriptor_t;

// Group:BiasDropoutAddFusedTrain
/*!
 *  @brief Creates a descriptor pointed by
 *   \p bias_dropout_add_fused_train_desc for the BiasDropoutAddFusedTrain operation.
 *
 *  @param[out] bias_dropout_add_fused_train_desc
 *    Output. A pointer to the BiasDropoutAddFusedTrain descriptor.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p bias_dropout_add_fused_train_desc is NULL.
 *  @retval CNNL_STATUS_ALLOC_FAILED
 *    The function fails to allocate memory for the BiasDropoutAddFusedTrain descriptor.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCreateBiasDropoutAddFusedTrainDescriptor(
    cnnlBiasDropoutAddFusedTrainDescriptor_t *bias_dropout_add_fused_train_desc);

// Group:BiasDropoutAddFusedTrain
/*!
 *  @brief Destroys the BiasDropoutAddFusedTrain descriptor.
 *
 *  @param[in] bias_dropout_add_fused_train_desc
 *    Input. The BiasDropoutAddFusedTrain descriptor.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p bias_dropout_add_fused_train_desc is NULL.
 *
 *  @par API Dependency
 *  - This function is used after calling the ::cnnlBiasDropoutAddFusedTrain function.
 */
cnnlStatus_t CNNL_WIN_API
cnnlDestroyBiasDropoutAddFusedTrainDescriptor(
    cnnlBiasDropoutAddFusedTrainDescriptor_t bias_dropout_add_fused_train_desc);

// Group:BiasDropoutAddFusedTrain
/*!
 *  @brief Sets the BiasDropoutAddFusedTrain descriptor.
 *
 *  @param[in,out] bias_dropout_add_fused_train_desc
 *    Input/output. The BiasDropoutAddFusedTrain descriptor.
 *  @param[in] p
 *    Input. A float value used as the probability of an element to be zeroed.
 *  @param[in] math_pre
 *    Input. Onchip computation data type of BiasDropoutAddFusedTrain operation.
 *    Currently only CNNL_DTYPE_FLOAT is supported.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - \p bias_dropout_add_fused_train_desc is NULL.
 *    - \p p is less than 0 or greater than 1.
 *    - \p math_per is illegal.
 *
 *  @par API Dependency
 *    - Before using this function, you need to call ::cnnlCreateBiasDropoutAddFusedTrainDescriptor
 *    to create a descriptor.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetBiasDropoutAddFusedTrainDescriptor(
    cnnlBiasDropoutAddFusedTrainDescriptor_t bias_dropout_add_fused_train_desc,
    const float p,
    const cnnlDataType_t math_pre);

// Group:BiasDropoutAddFusedTrain
/*!
 *  @brief Retrieves extra space size needed in BiasDropoutAddFusedTrain
 *  operation.
 *
 *  @param[in] handle
 *    Input. Handle to the Cambricon CNNL context that is used to manage MLU devices
 *    and queues in the BiasDropoutAddFusedTrain operation.
 *  @param[in] bias_dropout_add_fused_train_desc
 *    Input. The descriptor of the BiasDropoutAddFusedTrain operation. For detailed
 *    information, see ::cnnlBiasDropoutAddFusedTrainDescriptor_t.
 *  @param[in] input_desc
 *    Input. The descriptor of the \p input tensor, containing the dimension and
 *    layout of the tensor.
 *  @param[out] workspace_size
 *    Output. Pointer to the MLU memory that stores the extra space size (bytes)
 *    needed in BiasDropoutAddFusedTrain operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - \p handle is NULL.
 *    - \p bias_dropout_add_fused_train_desc is NULL.
 *    - \p input_desc is NULL.
 *    - \p workspace_size is NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetBiasDropoutAddFusedTrainWorkspaceSize(
    cnnlHandle_t handle,
    const cnnlBiasDropoutAddFusedTrainDescriptor_t bias_dropout_add_fused_train_desc,
    const cnnlTensorDescriptor_t input_desc,
    size_t *workspace_size);

// Group:BiasDropoutAddFusedTrain
/*!
 *  @brief Performs the BiasDropoutAddFusedTrain operation.
 *
 *  This function performs with the following steps:
 *
 *  1. Add bias when \p bias is not NULL.
 *
 *     input = add(input, bias)
 *
 *  2. Dropout input.
 *
 *     input = dropout(input, p)
 *
 *  3. Add residual.
 *
 *     output = add(input, residual)
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlBiasDropoutAddFusedTrain_v2 instead.
 *
 *  @param[in] handle
 *    Input. Handle to the Cambricon CNNL context that is used to manage MLU devices
 *    and queues in the BiasDropoutAddFusedTrain operation.
 *  @param[in] bias_dropout_add_fused_train_desc
 *    Input. The descriptor of the BiasDropoutAddFusedTrain operation. For detailed
 *    information, see ::cnnlBiasDropoutAddFusedTrainDescriptor_t.
 *  @param[in] generator
 *    Input. The random generator. It currently only supports CNNL_RAND_RNG_PHILOX.
 *    For detailed information, see cnnlRandGenerator_t in Cambricon CNNL Developer
 *    Guide.
 *  @param[in] input_desc
 *    Input. The descriptor of the \p input tensor, containing the dimension and
 *    layout of the tensor.
 *  @param[in] input
 *    Input. Pointer to the MLU memory that stores the \p input tensor. Its shape
 *    is [s, b, h].
 *  @param[in] bias_desc
 *    Input. The descriptor of the \p bias tensor, containing the dimension and
 *    layout of the tensor.
 *  @param[in] bias
 *    Input. Pointer to the MLU memory that stores the \p bias tensor. Its shape
 *    is [h].
 *  @param[in] residual_desc
 *    Input. The descriptor of the \p residual tensor, containing the dimension
 *    and layout of the tensor.
 *  @param[in] residual
 *    Input. Pointer to the MLU memory that stores the \p residual tensor. Its
 *    shape is [s, b, h].
 *  @param[in] workspace
 *    Input. A pointer to an extra workspace required in the operation.
 *  @param[in] workspace_size
 *    Input. The size of the workspace.
 *  @param[in] mask_desc
 *    Input. The descriptor of the \p mask tensor, containing the dimension and
 *    layout of the tensor.
 *  @param[out] mask
 *    Output. Pointer to the MLU memory that stores the \p mask tensor. Its shape
 *    is [s, b, h].
 *  @param[in] output_desc
 *    Input. The descriptor of the \p output tensor, containing the dimension and
 *    layout of the tensor.
 *  @param[out] output
 *    Output. Pointer to the MLU memory that stores the \p output tensor. Its
 *    shape is [s, b, h].
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - \p handle is NULL.
 *    - \p bias_dropout_add_fused_train_desc is NULL.
 *    - \p generator is NULL.
 *    - \p input_desc is NULL.
 *    - \p input is NULL.
 *    - \p bias_desc is NULL when \p bias is not NULL.
 *    - \p bias is NULL when \p bias_desc is not NULL.
 *    - \p residual_desc is NULL.
 *    - \p residual is NULL.
 *    - \p workspace is NULL.
 *    - \p mask_desc is NULL.
 *    - \p mask is NULL.
 *    - \p output_desc is NULL.
 *    - \p output is NULL.
 *  @retval CNNL_STATUS_ARCH_MISMATCH
 *    - This function runs on the hardware platform that is not supported.
 *  @retval CNNL_STATUS_NOT_SUPPORTED
 *    - The requested functionality is not supported in this version.
 *  @par Data Type
 *   - input: float, half, bfloat16.
 *   - bias: float, half, bfloat16.
 *   - residual: float, half, bfloat16.
 *   - mask: uint8, bool.
 *   - output: float, half, bfloat16.
 *   - Data types of \p input, \p bias, \p residual, and \p output must be same.
 *
 * @note
 * - This function supports MLU300 and MLU500 series.
 * - The \p math_pre in ::cnnlBiasDropoutAddFusedTrainDescriptor_t should be CNNL_DTYPE_FLOAT.
 *
 *  @par Example
 *  @verbatim
    Dimension of input: [s, b, h]

    Dimension of bias: [h]

    Dimension of residual: [s, b, h]

    Then we will get the mask and output:

    Dimension of mask: [s, b, h]

    Dimension of output: [s, b, h]
    @endverbatim
 */
CNNL_DEPRECATED_FOR(cnnlBiasDropoutAddFusedTrain_v2)
cnnlStatus_t CNNL_WIN_API
cnnlBiasDropoutAddFusedTrain(cnnlHandle_t handle,
                             const cnnlBiasDropoutAddFusedTrainDescriptor_t
                             bias_dropout_add_fused_train_desc,
                             const cnnlRandGenerator_t generator,
                             const cnnlTensorDescriptor_t input_desc,
                             const void *input,
                             const cnnlTensorDescriptor_t bias_desc,
                             const void *bias,
                             const cnnlTensorDescriptor_t residual_desc,
                             const void *residual,
                             void *workspace,
                             const size_t workspace_size,
                             const cnnlTensorDescriptor_t mask_desc,
                             const void *mask,
                             const cnnlTensorDescriptor_t output_desc,
                             void *output);

// Group:BiasDropoutAddFusedTrain
/*!
 *  @brief Performs the BiasDropoutAddFusedTrain operation.
 *
 *  This function performs with the following steps:
 *
 *  1. Add bias when \p bias is not NULL.
 *
 *     input = add(input, bias)
 *
 *  2. Dropout input.
 *
 *     input = dropout(input, p)
 *
 *  3. Add residual.
 *
 *     output = add(input, residual)
 *
 *  @param[in] handle
 *    Input. Handle to the Cambricon CNNL context that is used to manage MLU devices
 *    and queues in the BiasDropoutAddFusedTrain operation.
 *  @param[in] bias_dropout_add_fused_train_desc
 *    Input. The descriptor of the BiasDropoutAddFusedTrain operation. For detailed
 *    information, see ::cnnlBiasDropoutAddFusedTrainDescriptor_t.
 *  @param[in] rng_type
 *    Input. The type of random generator. Only supports \p CNNL_RAND_RNG_PHILOX.
 *  @param[in] seed
 *    Input. Specify the seed of the Philox algorithm.
 *  @param[in] offset
 *    Input. Specify the offset of the Philox algorithm.
 *  @param[in] input_desc
 *    Input. The descriptor of the \p input tensor, containing the dimension and
 *    layout of the tensor.
 *  @param[in] input
 *    Input. Pointer to the MLU memory that stores the \p input tensor. Its shape
 *    is [s, b, h].
 *  @param[in] bias_desc
 *    Input. The descriptor of the \p bias tensor, containing the dimension and
 *    layout of the tensor.
 *  @param[in] bias
 *    Input. Pointer to the MLU memory that stores the \p bias tensor. Its shape
 *    is [h].
 *  @param[in] residual_desc
 *    Input. The descriptor of the \p residual tensor, containing the dimension
 *    and layout of the tensor.
 *  @param[in] residual
 *    Input. Pointer to the MLU memory that stores the \p residual tensor. Its
 *    shape is [s, b, h].
 *  @param[in] workspace
 *    Input. A pointer to an extra workspace required in the operation.
 *  @param[in] workspace_size
 *    Input. The size of the workspace.
 *  @param[in] mask_desc
 *    Input. The descriptor of the \p mask tensor, containing the dimension and
 *    layout of the tensor.
 *  @param[out] mask
 *    Output. Pointer to the MLU memory that stores the \p mask tensor. Its shape
 *    is [s, b, h].
 *  @param[in] output_desc
 *    Input. The descriptor of the \p output tensor, containing the dimension and
 *    layout of the tensor.
 *  @param[out] output
 *    Output. Pointer to the MLU memory that stores the \p output tensor. Its
 *    shape is [s, b, h].
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - \p handle is NULL.
 *    - \p bias_dropout_add_fused_train_desc is NULL.
 *    - \p generator is NULL.
 *    - \p input_desc is NULL.
 *    - \p input is NULL.
 *    - \p bias_desc is NULL when \p bias is not NULL.
 *    - \p bias is NULL when \p bias_desc is not NULL.
 *    - \p residual_desc is NULL.
 *    - \p residual is NULL.
 *    - \p workspace is NULL.
 *    - \p mask_desc is NULL.
 *    - \p mask is NULL.
 *    - \p output_desc is NULL.
 *    - \p output is NULL.
 *  @retval CNNL_STATUS_ARCH_MISMATCH
 *    - This function runs on the hardware platform that is not supported.
 *  @retval CNNL_STATUS_NOT_SUPPORTED
 *    - The requested functionality is not supported in this version.
 *  @par Data Type
 *   - input: float, half, bfloat16.
 *   - bias: float, half, bfloat16.
 *   - residual: float, half, bfloat16.
 *   - mask: uint8, bool.
 *   - output: float, half, bfloat16.
 *   - Data types of \p input, \p bias, \p residual, and \p output must be same.
 *
 * @note
 * - This function supports MLU300 and MLU500 series.
 * - The \p math_pre in ::cnnlBiasDropoutAddFusedTrainDescriptor_t should be CNNL_DTYPE_FLOAT.
 *
 *  @par Example
 *  @verbatim
    Dimension of input: [s, b, h]

    Dimension of bias: [h]

    Dimension of residual: [s, b, h]

    Then we will get the mask and output:

    Dimension of mask: [s, b, h]

    Dimension of output: [s, b, h]
    @endverbatim
 */
cnnlStatus_t CNNL_WIN_API
cnnlBiasDropoutAddFusedTrain_v2(cnnlHandle_t handle,
                                const cnnlBiasDropoutAddFusedTrainDescriptor_t
                                bias_dropout_add_fused_train_desc,
                                cnnlRandRngType_t rng_type,
                                const uint64_t seed,
                                const uint64_t offset,
                                const cnnlTensorDescriptor_t input_desc,
                                const void *input,
                                const cnnlTensorDescriptor_t bias_desc,
                                const void *bias,
                                const cnnlTensorDescriptor_t residual_desc,
                                const void *residual,
                                void *workspace,
                                const size_t workspace_size,
                                const cnnlTensorDescriptor_t mask_desc,
                                const void *mask,
                                const cnnlTensorDescriptor_t output_desc,
                                void *output);

/******************************************************************************
 * Cambricon CNNL OP: FlashAttentionForward
 ******************************************************************************/
/*! The descriptor of the ::cnnlFlashAttentionForward operation.
 * You can use ::cnnlCreateFlashAttentionDescriptor(), ::cnnlSetFlashAttentionDescriptor()
 * and ::cnnlDestroyFlashAttentionDescriptor() to create, set and destroy the descriptor
 * respectively.
 */
typedef struct cnnlFlashAttentionStruct *cnnlFlashAttentionDescriptor_t;

// Group:FlashAttentionForward
/*!
 *  @brief Creates a descriptor pointed by \p desc
 *   for the FlashAttentionForward operation.
 *  @param[out] desc
 *    Output. Descriptor of the FlashAttentionForward operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_ALLOC_FAILED
 *    The function failed to allocate memory for the FlashAttentionForward descriptor.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCreateFlashAttentionDescriptor(cnnlFlashAttentionDescriptor_t *desc);

// Group:FlashAttentionForward
/*!
 *  @brief Destroys the descriptor of the FlashAttentionForward operation.
 *
 *  @param[in] desc
 *    Input. Descriptor of the FlashAttentionForward operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p desc is NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlDestroyFlashAttentionDescriptor(cnnlFlashAttentionDescriptor_t desc);

// Group:FlashAttentionForward
/*!
 *  @brief Sets the descriptor of the FlashAttentionForward operation with values.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlSetFlashAttentionDescriptor_v2 instead.
 *
 *  @param[in,out] desc
 *    Input/output. Descriptor of the FlashAttentionForward operation.
 *  @param[in] compute_dtype
 *    Input. Data type to be used during computation.
 *    - If data type of inputs is half or bfloat16, \p compute_dtype should be CNNL_DTYPE_FLOAT.
 *    - If \p compute_dtype is set to CNNL_DTYPE_INVALID, it will be the same as that of inputs.
 *  @param[in] act_pref
 *    Input. The indicator of activation operation preference.
 *    Currently only \p CNNL_ACTIVATION_HIGH_PRECISION is supported.
 *  @param[in] mask_mode
 *    Input. The indicator of on-chip masking algorithm.
 *    Currently only ::CNNL_ATTN_MASK_NONE, ::CNNL_ATTN_MASK_CAUSAL and ::CNNL_ATTN_MASK_CAUSAL_TOP_LEFT are supported.
 *    See ::cnnlAttentionMaskMode_t for details.
 *  @param[in] is_pack_mode
 *    Input. If true, inputs of multi-head attention are packed. If false, inputs are padded.
 *  @param[in] is_out_zero
 *    Input. A Boolean value used to indicate whether output tensors are
 *           initialized with zero. Currently only supports false.
 *  @param[in] return_softmax
 *    Input. A Boolean value used to indicate whether to store dropout mask for debugging.
 *  @param[in] max_seq_q
 *    Input. An int32_t value used to indicate the largest sequence length among
 *           query-related sequences.
 *  @param[in] max_seq_kv
 *    Input. An int32_t value used to indicate the largest sequence length among
 *           key-related or value-related sequences.
 *  @param[in] p_dropout
 *    Input. Dropout rate of data.
 *  @param[in] qk_scale
 *    Input. A float32 value used to indicate the scaling factor of Q-K matmul.
 *           A typical value is 1 / sqrt(head_size).
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p desc is NULL, or one or more parameters are not supported.
 */
CNNL_DEPRECATED_FOR(cnnlSetFlashAttentionDescriptor_v2)
cnnlStatus_t CNNL_WIN_API
cnnlSetFlashAttentionDescriptor(cnnlFlashAttentionDescriptor_t desc,
                                cnnlDataType_t compute_dtype,
                                cnnlActivationPreference_t act_pref,
                                cnnlAttentionMaskMode_t mask_mode,
                                bool is_pack_mode,
                                bool is_out_zero,
                                bool return_softmax,
                                int max_seq_q,
                                int max_seq_kv,
                                float p_dropout,
                                float qk_scale);

// Group:FlashAttentionForward
/*!
 *  @brief Sets the descriptor of the FlashAttentionForward operation with values.
 *
 *  Compared with ::cnnlSetFlashAttentionDescriptor, this API adds \p tensor_layout that indicates inputs shape,
 *  and changes the parameters that indicate computation preference.
 *
 *  @param[in,out] desc
 *    Input/output. Descriptor of the FlashAttentionForward operation.
 *  @param[in] compute_dtype
 *    Input. Data type to be used during computation.
 *    - If data type of inputs is half or bfloat16, \p compute_dtype should be CNNL_DTYPE_FLOAT.
 *    - If \p compute_dtype is set to CNNL_DTYPE_INVALID, it will be the same as that of inputs.
 *  @param[in] comp_pref
 *    Input. The indicator of activation operation preference.
 *    Currently only \p CNNL_COMPUTATION_HIGH_PRECISION is supported.
 *  @param[in] mask_mode
 *    Input. The indicator of on-chip masking algorithm.
 *    Currently only ::CNNL_ATTN_MASK_NONE, ::CNNL_ATTN_MASK_CAUSAL and ::CNNL_ATTN_MASK_CAUSAL_TOP_LEFT are supported.
 *    See ::cnnlAttentionMaskMode_t for details.
 *  @param[in] tensor_layout
 *    Input. The tensor layouts of input and output tensors.
 *    Currently only support HS in the last level. It supports the following layouts:
 *    ::CNNL_ATTN_TENSOR_LAYOUT_PACKED,
 *    ::CNNL_ATTN_TENSOR_LAYOUT_B_S_HN_HS,
 *    ::CNNL_ATTN_TENSOR_LAYOUT_B_HN_S_HS,
 *    ::CNNL_ATTN_TENSOR_LAYOUT_S_B_HN_HS,
 *    ::CNNL_ATTN_TENSOR_LAYOUT_S_HN_B_HS,
 *    ::CNNL_ATTN_TENSOR_LAYOUT_HN_B_S_HS,
 *    and ::CNNL_ATTN_TENSOR_LAYOUT_HN_S_B_HS.
 *    See ::cnnlAttentionTensorLayout_t for details.
 *  @param[in] is_pack_mode
 *    Input. If true, inputs of multi-head attention are packed. If false, inputs are padded.
 *  @param[in] is_out_zero
 *    Input. A Boolean value used to indicate whether output tensors are
 *           initialized with zero. Currently only supports false.
 *  @param[in] return_softmax
 *    Input. A Boolean value used to indicate whether to store dropout mask for debugging.
 *  @param[in] max_seq_q
 *    Input. An int32_t value used to indicate the largest sequence length among
 *           query-related sequences.
 *  @param[in] max_seq_kv
 *    Input. An int32_t value used to indicate the largest sequence length among
 *           key-related or value-related sequences.
 *  @param[in] p_dropout
 *    Input. Dropout rate of data.
 *  @param[in] qk_scale
 *    Input. A float32 value used to indicate the scaling factor of Q-K matmul.
 *           A typical value is 1 / sqrt(head_size).
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p desc is NULL, or one or more parameters are not supported.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetFlashAttentionDescriptor_v2(cnnlFlashAttentionDescriptor_t desc,
                                   cnnlDataType_t compute_dtype,
                                   cnnlComputationPreference_t comp_pref,
                                   cnnlAttentionMaskMode_t mask_mode,
                                   cnnlAttentionTensorLayout_t tensor_layout,
                                   bool is_pack_mode,
                                   bool is_out_zero,
                                   bool return_softmax,
                                   int max_seq_q,
                                   int max_seq_kv,
                                   float p_dropout,
                                   float qk_scale);

// Group:FlashAttentionForward
/*!
 *  @brief Sets \p dropout_mask_mode.
 *
 *  @param[in] desc
 *    Input. Descriptor of the FlashAttentionForward/Backward operation.
 *  @param[in] dropout_mask_mode
 *    Input. An int32_t value used to indicate dropout_mask mode. Value 0 represents
 *    storing float-type 0/1 dropout_mask to DRAM. Value 1 represents loading float-type
 *    0/1 dropout_mask from DRAM.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    The \p desc is NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetFlashAttentionDropoutMaskMode(cnnlFlashAttentionDescriptor_t desc,
                                     int dropout_mask_mode);

// Group:FlashAttentionForward
/*!
 *  @brief Retrieves extra space size needed in FlashAttentionForward operation.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlGetFlashAttentionForwardWorkspaceSize_v2 instead.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
 *  @param[in] desc
 *    Input. Descriptor of the FlashAttentionForward operation.
 *  @param[in] query_desc
 *    Input. Descriptor of \p query tensor.
 *    Its shape is [total_q, head_num_q, head_size] when \p is_pack_mode is true.
 *    Its shape is [batch, seqlence_q, head_num_q, head_size] when \p is_pack_mode is false.
 *  @param[in] key_desc
 *    Input. Descriptor of \p key tensor.
 *    Its shape is [total_k, head_num_k, head_size] when \p is_pack_mode is true.
 *    Its shape is [batch, seqlence_k, head_num_k, head_size] when \p is_pack_mode is false.
 *  @param[in] value_desc
 *    Input. Descriptor of \p value tensor.
 *    Its shape is [total_k, head_num_k, head_size_v] when \p is_pack_mode is true.
 *    Its shape is [batch, seqlence_k, head_num_k, head_size_v] when \p is_pack_mode is false.
 *  @param[out] size
 *    Output. Size of workspace.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more required parameters are NULL.
 */
CNNL_DEPRECATED_FOR(cnnlGetFlashAttentionForwardWorkspaceSize_v2)
cnnlStatus_t CNNL_WIN_API
cnnlGetFlashAttentionForwardWorkspaceSize(cnnlHandle_t handle,
                                          cnnlFlashAttentionDescriptor_t desc,
                                          cnnlTensorDescriptor_t query_desc,
                                          cnnlTensorDescriptor_t key_desc,
                                          cnnlTensorDescriptor_t value_desc,
                                          size_t *size);

// Group:FlashAttentionForward
/*!
 *  @brief Retrieves extra space size needed in FlashAttentionForward operation.
 *
 *  Compared with ::cnnlGetFlashAttentionForwardWorkspaceSize, the API is used to allocate device space
 *  when sdpa kernel is launched.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
 *  @param[in] desc
 *    Input. Descriptor of the FlashAttentionForward operation.
 *  @param[in] query_desc
 *    Input. Descriptor of \p query tensor.
 *    Its shape is [total_q, head_num_q, head_size] when \p is_pack_mode is true.
 *    Its shape is [batch, seqlence_q, head_num_q, head_size] when \p is_pack_mode is false.
 *  @param[in] key_desc
 *    Input. Descriptor of \p key tensor.
 *    Its shape is [total_k, head_num_k, head_size] when \p is_pack_mode is true.
 *    Its shape is [batch, seqlence_k, head_num_k, head_size] when \p is_pack_mode is false.
 *  @param[in] value_desc
 *    Input. Descriptor of \p value tensor.
 *    Its shape is [total_k, head_num_k, head_size_v] when \p is_pack_mode is true.
 *    Its shape is [batch, seqlence_k, head_num_k, head_size_v] when \p is_pack_mode is false.
 *  @param[in] seqlens_q_desc
 *    Input. Descriptor of \p seqlens_q tensor. Its shape is [batch + 1].
 *  @param[in] seqlens_k_desc
 *    Input. Descriptor of \p seqlens_k tensor. Its shape is [batch + 1].
 *  @param[in] seqused_k_desc
 *    Input. Descriptor of \p seqused_k tensor. Its shape is [batch]. It can be nullptr.
 *  @param[in] attn_mask_desc
 *    Input. Descriptor of \p attn_mask tensor. Its shape is [batch, head_num_q, max_seq_q, max_seq_k]. It can be nullptr.
 *  @param[in] alibi_slopes_desc
 *    Input. Descriptor of \p alibi_slopes tensor. Its shape can be [batch, head_num_q] or [head_num_q]. It can be nullptr.
 *  @param[in] softmax_lse_desc
 *    Input. Descriptor of \p softmax_lse tensor. The shape is [head_num_q, total_q].
 *  @param[out] size
 *    Output. Size of workspace.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more required parameters are NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetFlashAttentionForwardWorkspaceSize_v2(cnnlHandle_t handle,
                                             cnnlFlashAttentionDescriptor_t desc,
                                             cnnlTensorDescriptor_t query_desc,
                                             cnnlTensorDescriptor_t key_desc,
                                             cnnlTensorDescriptor_t value_desc,
                                             cnnlTensorDescriptor_t seqlens_q_desc,
                                             cnnlTensorDescriptor_t seqlens_k_desc,
                                             cnnlTensorDescriptor_t seqused_k_desc,
                                             cnnlTensorDescriptor_t attn_mask_desc,
                                             cnnlTensorDescriptor_t alibi_slopes_desc,
                                             cnnlTensorDescriptor_t softmax_lse_desc,
                                             size_t *size);

// Group:FlashAttentionForward
/*!
 *  @brief Gets the number of generated random data in FlashAttentionForward operation.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
 *  @param[in] desc
 *    Input. Descriptor of the FlashAttentionForward operation.
 *  @param[in] query_desc
 *    Input. Descriptor of \p query tensor.
 *    Its shape is [total_q, head_num_q, head_size] when \p is_pack_mode is true.
 *    Its shape is [batch, seqlence_q, head_num_q, head_size] when \p is_pack_mode is false.
 *  @param[in] value_desc
 *    Input. Descriptor of \p value tensor.
 *    Its shape is [total_k, head_num_k, head_size_v] when \p is_pack_mode is true.
 *    Its shape is [batch, seqlence_k, head_num_k, head_size_v] when \p is_pack_mode is false.
 *  @param[in] seqlens_q_desc
 *    Input. Descriptor of \p seqlens_q tensor. The shape is [batch + 1].
 *  @param[out] random_numbers
 *    Output. The number of random data generated in flash attention operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more required parameters are NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetFlashAttentionGeneratedRandomNumbers(cnnlHandle_t handle,
                                            cnnlFlashAttentionDescriptor_t desc,
                                            cnnlTensorDescriptor_t query_desc,
                                            cnnlTensorDescriptor_t value_desc,
                                            cnnlTensorDescriptor_t seqlens_q_desc,
                                            size_t *random_numbers);

// Group:FlashAttentionForward
/*!
 *  @brief Executes flash attention in the transformer network.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlFlashAttentionForward_v2 instead.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
 *  @param[in] desc
 *    Input. Descriptor of the FlashAttentionForward operation.
 *  @param[in] query_desc
 *    Input. Descriptor of \p query tensor.
 *    Its shape is [total_q, head_num_q, head_size] when \p is_pack_mode is true.
 *    Its shape is [batch, seqlence_q, head_num_q, head_size] when \p is_pack_mode is false.
 *  @param[in] query
 *    Input. Pointer to the MLU memory that stores the query tensor.
 *  @param[in] key_desc
 *    Input. Descriptor of \p key tensor.
 *    Its shape is [total_k, head_num_k, head_size] when \p is_pack_mode is true.
 *    Its shape is [batch, seqlence_k, head_num_k, head_size] when \p is_pack_mode is false.
 *  @param[in] key
 *    Input. Pointer to the MLU memory that stores the key tensor.
 *  @param[in] value_desc
 *    Input. Descriptor of \p value tensor.
 *    Its shape is [total_k, head_num_k, head_size_v] when \p is_pack_mode is true.
 *    Its shape is [batch, seqlence_k, head_num_k, head_size_v] when \p is_pack_mode is false.
 *  @param[in] value
 *    Input. Pointer to the MLU memory that stores the value tensor.
 *  @param[in] seqlens_q_desc
 *    Input. Descriptor of \p seqlens_q tensor. The shape is [batch + 1].
 *  @param[in] seqlens_q
 *    Input. Pointer to the MLU memory that stores the seqlens_q tensor.
 *  @param[in] seqlens_k_desc
 *    Input. Descriptor of \p seqlens_k tensor. The shape is [batch + 1].
 *  @param[in] seqlens_k
 *    Input. Pointer to the MLU memory that stores the seqlens_k tensor.
 *  @param[in] rng_state
 *    Input. Random seed used by dropout calculation, which is a pointer to the CPU
 *           memory that stores 2 uint64_t value. rng_state[0] represents keys
 *           and rng_state[1] represents offsets.
 *  @param[in] workspace
 *    Input. Pointer to the MLU memory that is used as an extra workspace.
 *  @param[in] workspace_size
 *    Input. Size of workspace.
 *  @param[in] dropout_mask_desc
 *    Input. Descriptor of \p dropout_mask tensor. The shape is [batch, head_num_q, max_seq_q, max_seq_kv].
 *  @param[in,out] dropout_mask
 *    Input. Pointer to the MLU memory that stores the dropout_mask tensor. Reserved for future use.
 *  @param[in] softmax_lse_desc
 *    Input. Descriptor of \p softmax_lse tensor. The shape is [head_num_q, total_q].
 *  @param[out] softmax_lse
 *    Input. Pointer to the MLU memory that stores the softmax_lse tensor.
 *  @param[in] output_desc
 *    Input. Descriptor of \p output tensor.
 *    Its shape is [total_q, head_num_q, head_size_v] when \p is_pack_mode is true.
 *    Its shape is [batch, seqlence_q, head_num_q, head_size_v] when \p is_pack_mode is false.
 *  @param[out] output
 *    Output. Pointer to the MLU memory that stores the output tensor.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are encountered:
 *    - handle is NULL.
 *    - One or more required pointers are NULL.
 *    - Scale or data type limitation is not satisfied.
 *
 *  @par API Dependency
 *    - Before using this function, you need to call ::cnnlCreateFlashAttentionDescriptor
 *      to create a descriptor.
 *
 *  @par Data Type
 *    - Data types of \p query, \p key, \p value and \p output must be the same, which support half and bfloat16.
 *    - Data types of \p seqlens_q and \p seqlens_k must be the same, which support int32.
 *    - Data type of \p softmax_lse must be float.
 *
 *  @note
 *    - Only MLU300 and MLU500 series are supported.
 *    - Inputs and outputs cannot be homologous operand.
 *    - The content of all input tensors is not modified.
 *    - In pack mode, the memory layout of \p query must be [total_q, head_num_q, head_size],
 *      \p key must be [total_k, head_num_k, head_size], and \p value must be [total_k, head_num_k, head_size_v].
 *    - Currently only support HS in the last level. It supports the following layouts:
 *      ::CNNL_ATTN_TENSOR_LAYOUT_PACKED,
 *      ::CNNL_ATTN_TENSOR_LAYOUT_B_S_HN_HS,
 *      ::CNNL_ATTN_TENSOR_LAYOUT_B_HN_S_HS,
 *      ::CNNL_ATTN_TENSOR_LAYOUT_S_B_HN_HS,
 *      ::CNNL_ATTN_TENSOR_LAYOUT_S_HN_B_HS,
 *      ::CNNL_ATTN_TENSOR_LAYOUT_HN_B_S_HS,
 *      and ::CNNL_ATTN_TENSOR_LAYOUT_HN_S_B_HS.
 *
 *  @par Scale Limitation
 *    - \p head_size must be in range of [1, 128].
 *    - \p head_size_v must be in range of [1, 256].
 *    - \p compute_dtype must be float.
 *    - \p act_perf must be \p CNNL_ACTIVATION_HIGH_PRECISION.
 *    - \p mask_mode must be \p CNNL_ATTN_MASK_NONE, \p CNNL_ATTN_MASK_CAUSAL or \p CNNL_ATTN_MASK_CAUSAL_TOP_LEFT.
 *    - \p is_pack_mode must be true.
 *    - \p is_out_zero must be false.
 *    - \p p_dropout must be in range of [0, 1).
 *    - \p qk_scale must be in range of (0, 1].
 */
CNNL_DEPRECATED_FOR(cnnlFlashAttentionForward_v2)
cnnlStatus_t CNNL_WIN_API
cnnlFlashAttentionForward(cnnlHandle_t handle,
                          const cnnlFlashAttentionDescriptor_t desc,
                          const cnnlTensorDescriptor_t query_desc,
                          const void *query,
                          const cnnlTensorDescriptor_t key_desc,
                          const void *key,
                          const cnnlTensorDescriptor_t value_desc,
                          const void *value,
                          const cnnlTensorDescriptor_t seqlens_q_desc,
                          const void *seqlens_q,
                          const cnnlTensorDescriptor_t seqlens_k_desc,
                          const void *seqlens_k,
                          const size_t rng_state[],
                          void *workspace,
                          const size_t workspace_size,
                          const cnnlTensorDescriptor_t dropout_mask_desc,
                          void *dropout_mask,
                          const cnnlTensorDescriptor_t softmax_lse_desc,
                          void *softmax_lse,
                          const cnnlTensorDescriptor_t output_desc,
                          void *output);

// Group:FlashAttentionForward
/*!
 *  @brief Executes flash attention in the transformer network.
 *
 *  Compared with ::cnnlFlashAttentionForward, the API supports the following extra functions:
 *  - \p seqused_k that is used to indicate the actual length of \p key when attention scores are being computed.
 *  - \p attn_mask that allows to deliver self-defined mask.
 *  - \p alibi_slopes that replaces positional encodings with biases added to attention scores.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
 *  @param[in] desc
 *    Input. Descriptor of the FlashAttentionForward operation.
 *  @param[in] query_desc
 *    Input. Descriptor of \p query tensor.
 *    Its shape is [total_q, head_num_q, head_size] when \p is_pack_mode is true.
 *    Its shape is [batch, seqlence_q, head_num_q, head_size] when \p is_pack_mode is false.
 *  @param[in] query
 *    Input. Pointer to the MLU memory that stores the query tensor.
 *  @param[in] key_desc
 *    Input. Descriptor of \p key tensor.
 *    Its shape is [total_k, head_num_k, head_size] when \p is_pack_mode is true.
 *    Its shape is [batch, seqlence_k, head_num_k, head_size] when \p is_pack_mode is false.
 *  @param[in] key
 *    Input. Pointer to the MLU memory that stores the key tensor.
 *  @param[in] value_desc
 *    Input. Descriptor of \p value tensor.
 *    Its shape is [total_k, head_num_k, head_size_v] when \p is_pack_mode is true.
 *    Its shape is [batch, seqlence_k, head_num_k, head_size_v] when \p is_pack_mode is false.
 *  @param[in] value
 *    Input. Pointer to the MLU memory that stores the value tensor.
 *  @param[in] seqlens_q_desc
 *    Input. Descriptor of \p seqlens_q tensor. Its shape is [batch + 1].
 *  @param[in] seqlens_q
 *    Input. Pointer to the MLU memory that stores the seqlens_q tensor.
 *  @param[in] seqlens_k_desc
 *    Input. Descriptor of \p seqlens_k tensor. Its shape is [batch + 1].
 *  @param[in] seqlens_k
 *    Input. Pointer to the MLU memory that stores the seqlens_k tensor.
 *  @param[in] seqused_k_desc
 *    Input. Descriptor of \p seqused_k tensor. Its shape is [batch]. It can be nullptr.
 *  @param[in] seqused_k
 *    Input. Pointer to the MLU memory that stores the \p seqused_k tensor. It can be nullptr.
 *  @param[in] attn_mask_desc
 *    Input. Descriptor of \p attn_mask tensor. Its shape can be chosen from [batch, head_num_q, max_seq_q, max_seq_k],
 *    [batch, 1, max_seq_q, max_seq_k], and [1, head_num_q, max_seq_q, max_seq_k]. It can be nullptr.
 *  @param[in] attn_mask
 *    Input. Pointer to the MLU memory that stores the \p attn_mask tensor. It can be nullptr.
 *  @param[in] alibi_slopes_desc
 *    Input. Descriptor of \p alibi_slopes tensor. Its shape can be [batch, head_num_q] or [head_num_q]. It can be nullptr.
 *  @param[in] alibi_slopes
 *    Input. Pointer to the MLU memory that stores the \p alibi_slopes tensor. It can be nullptr.
 *  @param[in] rng_state
 *    Input. Random seed used by dropout calculation, which is a pointer to the CPU memory that stores two uint64_t values.
 *    rng_state[0] represents keys and rng_state[1] represents offsets.
 *  @param[in] workspace
 *    Input. Pointer to the MLU memory that is used as an extra workspace.
 *  @param[in] workspace_size
 *    Input. Size of the workspace.
 *  @param[in] dropout_mask_desc
 *    Input. Descriptor of \p dropout_mask tensor. Its shape is [batch, head_num_q, max_seq_q, max_seq_kv].
 *  @param[in,out] dropout_mask
 *    Input. Pointer to the MLU memory that stores the dropout_mask tensor.
 *  @param[in] softmax_lse_desc
 *    Input. Descriptor of \p softmax_lse tensor. Its shape is [head_num_q, total_q].
 *  @param[out] softmax_lse
 *    Input. Pointer to the MLU memory that stores the softmax_lse tensor.
 *  @param[in] output_desc
 *    Input. Descriptor of \p output tensor.
 *    Its shape is [total_q, head_num_q, head_size_v] when \p is_pack_mode is true.
 *    Its shape is [batch, seqlence_q, head_num_q, head_size_v] when \p is_pack_mode is false.
 *  @param[out] output
 *    Output. Pointer to the MLU memory that stores the output tensor.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are encountered:
 *    - handle is NULL.
 *    - One or more required pointers are NULL.
 *    - The scale or data type limitation is not satisfied.
 *
 *  @par API Dependency
 *    - Before using this function, you need to call ::cnnlCreateFlashAttentionDescriptor
 *      to create a descriptor.
 *
 *  @par Data Type
 *    - Data types of \p query, \p key, \p value and \p output must be the same, which support half and bfloat16.
 *    - Data types of \p seqlens_q and \p seqlens_k must be the same, which support int32.
 *    - Data type of \p softmax_lse must be float.
 *    - If inputs contain \p attn_mask, its data type must be the same as that of \p query.
 *    - If inputs contain \p alibi_slopes, its data type must be float.
 *    - If inputs contain \p seqused_k, its data type must be int32.
 *
 *  @note
 *    - Only MLU300 and MLU500 series is supported.
 *    - Inputs and outputs cannot be homologous operand.
 *    - The content of all input tensors are not modified.
 *    - In pack mode, the memory layout of \p query must be [total_q, head_num_q, head_size],
 *      \p key must be [total_k, head_num_k, head_size], and \p value must be [total_k, head_num_k, head_size_v].
 *    - If \p window_size_left and \p window_size_right are not -1, sliding window local attention is implemented.
 *      \p query at position i will only attend to \p key between [i + seq_k - seq_q - window_size_left, i + seq_k - seq_q + window_size_right] inclusive.
 *    - If \p attn_mask tensor is not nullptr, the \p mask_mode must be set to \p CNNL_ATTN_MASK_NONE or \p CNNL_ATTN_MASK_CAUSAL_TOP_LEFT.
 *      The \p left_size and the \p right_size must be set to -1.
 *    - \p seq_offset_q and \p seq_offset_k are not required when \p is_pack_mode is false.
 *    - Currently only support HS in the last level. It supports the following layouts:
 *      ::CNNL_ATTN_TENSOR_LAYOUT_PACKED,
 *      ::CNNL_ATTN_TENSOR_LAYOUT_B_S_HN_HS,
 *      ::CNNL_ATTN_TENSOR_LAYOUT_B_HN_S_HS,
 *      ::CNNL_ATTN_TENSOR_LAYOUT_S_B_HN_HS,
 *      ::CNNL_ATTN_TENSOR_LAYOUT_S_HN_B_HS,
 *      ::CNNL_ATTN_TENSOR_LAYOUT_HN_B_S_HS,
 *      and ::CNNL_ATTN_TENSOR_LAYOUT_HN_S_B_HS.
 *
 *  @par Scale Limitation
 *    - \p head_size must be in range of [1, 512].
 *    - \p head_size_v must be in range of [1, 512].
 *    - \p compute_dtype must be float.
 *    - \p act_perf must be \p CNNL_ACTIVATION_HIGH_PRECISION.
 *    - \p mask_mode must be \p CNNL_ATTN_MASK_NONE, \p CNNL_ATTN_MASK_CAUSAL or \p CNNL_ATTN_MASK_CAUSAL_TOP_LEFT.
 *    - \p is_out_zero must be false.
 *    - \p p_dropout must be in range of [0, 1).
 *    - \p qk_scale must be in range of (0, 1].
 *    - \p head_num_q % \p head_num_k == 0.
 */
cnnlStatus_t CNNL_WIN_API
cnnlFlashAttentionForward_v2(cnnlHandle_t handle,
                             const cnnlFlashAttentionDescriptor_t desc,
                             const cnnlTensorDescriptor_t query_desc,
                             const void *query,
                             const cnnlTensorDescriptor_t key_desc,
                             const void *key,
                             const cnnlTensorDescriptor_t value_desc,
                             const void *value,
                             const cnnlTensorDescriptor_t seqlens_q_desc,
                             const void *seqlens_q,
                             const cnnlTensorDescriptor_t seqlens_k_desc,
                             const void *seqlens_k,
                             const cnnlTensorDescriptor_t seqused_k_desc,
                             const void *seqused_k,
                             const cnnlTensorDescriptor_t attn_mask_desc,
                             const void *attn_mask,
                             const cnnlTensorDescriptor_t alibi_slopes_desc,
                             const void *alibi_slopes,
                             const size_t rng_state[],
                             void *workspace,
                             const size_t workspace_size,
                             const cnnlTensorDescriptor_t dropout_mask_desc,
                             void *dropout_mask,
                             const cnnlTensorDescriptor_t softmax_lse_desc,
                             void *softmax_lse,
                             const cnnlTensorDescriptor_t output_desc,
                             void *output);

/******************************************************************************
 * Cambricon CNNL OP: FlashAttentionBackward
 ******************************************************************************/
// Group:FlashAttentionBackward
/*!
 *  @brief Sets the FlashAttentionBackward descriptor.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlSetFlashAttentionBackwardDescriptor_v2 instead.
 *
 *  @param[in,out] flash_attention_backward_desc
 *    Input/output. The FlashAttentionBackward descriptor.
 *  @param[in] compute_type
 *    Input. A cnnlDataType_t value used to indicate on-chip calculation type.
 *           Currently only supports CNNL_DTYPE_FLOAT.
 *  @param[in] act_pref
 *    Input. A cnnlActivationPreference_t value used to indicate on-chip activation
 *           accuracy preference. Currently only supports CNNL_ACTIVATION_HIGH_PRECISION.
 *  @param[in] mask_mode
 *    Input. A ::cnnlAttentionMaskMode_t value used to indicate on-chip masking algorithm.
 *           Currently it supports the following mask modes:
 *             ::CNNL_ATTN_MASK_NONE,
 *             ::CNNL_ATTN_MASK_CAUSAL,
 *             ::CNNL_ATTN_MASK_CAUSAL_TOP_LEFT,
 *             ::CNNL_ATTN_MASK_LOCAL_TOP_LEFT,
 *             and ::CNNL_ATTN_MASK_LOCAL_BOTTOM_RIGHT.
 *           See ::cnnlAttentionMaskMode_t for details.
 *  @param[in] is_pack_mode
 *    Input. A Boolean value used to indicate how different sequences are packed.
 *           If is_pack_mode is true, sequences are concatenated without any
 *           redundant space. Otherwise, all sequences are padded to max seq_len.
 *           Currently only supports true.
 *  @param[in] is_out_zero
 *    Input. A Boolean value used to indicate whether diff_q/k/v tensors are
 *           initialized with zero. Currently only supports false.
 *  @param[in] is_store_softmax_d
 *    Input. A Boolean value used to indicate whether to store softmax_d result
 *           for debugging. Currently only supports false.
 *  @param[in] packed_max_seq_q
 *    Input. An int32_t value used to indicate the largest sequence length among
 *           query-related sequences, including diff_out, fwd_out, query,
 *           and diff_q.
 *  @param[in] packed_max_seq_kv
 *    Input. An int32_t value used to indicate the largest sequence length among
 *           key/value-related sequences, including key, value, diff_k and diff_v.
 *  @param[in] p_dropout
 *    Input. A float32 value used to indicate the probability that a data will
 *           be set to zero during dropout process. p_dropout must be within the
 *           range of [0, 1].
 *  @param[in] qk_scale
 *    Input. A float32 value used to indicate the scaling factor of Q-K matmul.
 *           A typical value is 1 / sqrt(head_size_qk). qk_scale must be in the
 *           range of (0, 1].
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - \p flash_attention_backward_desc is NULL.
 *    - \p data_type is not CNNL_DTYPE_FLOAT.
 *    - \p act_pref is not CNNL_ACTIVATION_HIGH_PRECISION.
 *    - \p mask_mode is not CNNL_ATTN_MASK_NONE, CNNL_ATTN_MASK_CAUSAL,
 *         CNNL_ATTN_MASK_CAUSAL_TOP_LEFT, CNNL_ATTN_MASK_LOCAL_TOP_LEFT,
 *         or CNNL_ATTN_MASK_LOCAL_BOTTOM_RIGHT.
 *    - \p is_pack_mode is true but tensor_layout is not CNNL_ATTN_TENSOR_LAYOUT_PACKED.
 *    - \p is_pack_mode is false but tensor_layout is CNNL_ATTN_TENSOR_LAYOUT_PACKED.
 *    - \p is_out_zero is true.
 *    - \p is_store_softmax_d is true.
 *    - \p p_dropout is less than 0 or greater than 1.
 *    - \p qk_scale is less or equal to 0 or greater than 1.
 *
 *  @par API Dependency
 *    - Before using this function, you need to call ::cnnlCreateFlashAttentionDescriptor
 *    to create a descriptor.
 */
CNNL_DEPRECATED_FOR(cnnlSetFlashAttentionBackwardDescriptor_v2)
cnnlStatus_t CNNL_WIN_API cnnlSetFlashAttentionBackwardDescriptor(
    cnnlFlashAttentionDescriptor_t flash_attention_backward_desc,
    cnnlDataType_t compute_type,
    cnnlActivationPreference_t act_pref,
    cnnlAttentionMaskMode_t mask_mode,
    bool is_pack_mode,
    bool is_out_zero,
    bool is_store_softmax_d,
    int32_t packed_max_seq_q,
    int32_t packed_max_seq_kv,
    float p_dropout,
    float qk_scale);

// Group:FlashAttentionBackward
/*!
 *  @brief Sets the FlashAttentionBackward descriptor.
 *
 *  @param[in,out] flash_attention_backward_desc
 *    Input/output. The FlashAttentionBackward descriptor.
 *  @param[in] compute_type
 *    Input. A cnnlDataType_t value used to indicate on-chip calculation type.
 *           Currently only supports CNNL_DTYPE_FLOAT.
 *  @param[in] comp_pref
 *    Input. A cnnlComputationPreference_t value used to indicate on-chip activation
 *           accuracy preference. Currently only supports CNNL_COMPUTATION_HIGH_PRECISION.
 *  @param[in] mask_mode
 *    Input. A ::cnnlAttentionMaskMode_t value used to indicate on-chip masking algorithm.
 *           Currently, it supports the following mask modes:
 *             ::CNNL_ATTN_MASK_NONE,
 *             ::CNNL_ATTN_MASK_CAUSAL,
 *             ::CNNL_ATTN_MASK_CAUSAL_TOP_LEFT,
 *             ::CNNL_ATTN_MASK_LOCAL_TOP_LEFT,
 *             and ::CNNL_ATTN_MASK_LOCAL_BOTTOM_RIGHT.
 *           See ::cnnlAttentionMaskMode_t for details.
 *  @param[in] tensor_layout
 *    Input. The tensor layouts of input and output tensors.
 *           Currently only support HS in the last level. It supports the following layouts:
 *             ::CNNL_ATTN_TENSOR_LAYOUT_PACKED,
 *             ::CNNL_ATTN_TENSOR_LAYOUT_B_S_HN_HS,
 *             ::CNNL_ATTN_TENSOR_LAYOUT_B_HN_S_HS,
 *             ::CNNL_ATTN_TENSOR_LAYOUT_S_B_HN_HS,
 *             ::CNNL_ATTN_TENSOR_LAYOUT_S_HN_B_HS,
 *             ::CNNL_ATTN_TENSOR_LAYOUT_HN_B_S_HS,
 *             and ::CNNL_ATTN_TENSOR_LAYOUT_HN_S_B_HS.
 *           See ::cnnlAttentionTensorLayout_t for details.
 *  @param[in] is_pack_mode
 *    Input. A Boolean value used to indicate how different sequences are packed.
 *           If is_pack_mode is true, sequences are concatenated without any
 *           redundant space. Otherwise, all sequences are padded to max seq_len.
 *  @param[in] is_out_zero
 *    Input. A Boolean value used to indicate whether diff_q/k/v tensors are
 *           initialized with zero. Currently only supports false.
 *  @param[in] is_store_softmax_d
 *    Input. A Boolean value used to indicate whether to store softmax_d result
 *           for debugging. Currently only supports false.
 *  @param[in] packed_max_seq_q
 *    Input. An int32_t value used to indicate the largest sequence length among
 *           query-related sequences, including diff_out, fwd_out, query,
 *           and diff_q.
 *  @param[in] packed_max_seq_kv
 *    Input. An int32_t value used to indicate the largest sequence length among
 *           key/value-related sequences, including key, value, diff_k and diff_v.
 *  @param[in] p_dropout
 *    Input. A float32 value used to indicate the probability that a data will
 *           be set to zero during dropout process. p_dropout must be within the
 *           range of [0, 1].
 *  @param[in] qk_scale
 *    Input. A float32 value used to indicate the scaling factor of Q-K matmul.
 *           A typical value is 1 / sqrt(head_size_qk). qk_scale must be in the
 *           range of (0, 1].
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - \p flash_attention_backward_desc is NULL.
 *    - \p comp_pref is not CNNL_COMPUTATION_HIGH_PRECISION.
 *    - \p mask_mode is not CNNL_ATTN_MASK_NONE, CNNL_ATTN_MASK_CAUSAL,
 *         CNNL_ATTN_MASK_CAUSAL_TOP_LEFT, CNNL_ATTN_MASK_LOCAL_TOP_LEFT,
 *         or CNNL_ATTN_MASK_LOCAL_BOTTOM_RIGHT.
 *    - \p is_pack_mode is true but tensor_layout is not CNNL_ATTN_TENSOR_LAYOUT_PACKED.
 *    - \p is_pack_mode is false but tensor_layout is CNNL_ATTN_TENSOR_LAYOUT_PACKED.
 *    - \p is_out_zero is true.
 *    - \p is_store_softmax_d is true.
 *    - \p p_dropout is less than 0 or greater than 1.
 *    - \p qk_scale is less or equal to 0 or greater than 1.
 *
 *  @par API Dependency
 *    - Before using this function, you need to call ::cnnlCreateFlashAttentionDescriptor
 *    to create a descriptor.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetFlashAttentionBackwardDescriptor_v2(
    cnnlFlashAttentionDescriptor_t flash_attention_backward_desc,
    cnnlDataType_t compute_type,
    cnnlComputationPreference_t comp_pref,
    cnnlAttentionMaskMode_t mask_mode,
    cnnlAttentionTensorLayout_t tensor_layout,
    bool is_pack_mode,
    bool is_out_zero,
    bool is_store_softmax_d,
    int32_t packed_max_seq_q,
    int32_t packed_max_seq_kv,
    float p_dropout,
    float qk_scale);

// Group:FlashAttentionBackward
/*!
 *  @brief  Sets the sliding window size and dilation to
 *  the FlashAttention operation descriptor.
 *
 *  @param[in,out] flash_attention_desc
 *    Input/output. The FlashAttention operation descriptor.
 *  @param[in] left_size
 *    Input. An int32_t value used to indicate left window size of sliding window
 *           attention. Default value is -1.
 *  @param[in] right_size
 *    Input. An int32_t value used to indicate right window size of sliding window
 *           attention. Default value is -1.
 *  @param[in] dilation
 *    Input. An int32_t value used to indicate dilation size of sliding window
 *           attention. Currently not supported and must be 1.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - \p dilation is less than 1.
 *
 *  @par API Dependency
 *    - Before using this function, you need to call ::cnnlCreateFlashAttentionDescriptor
 *    to create a descriptor.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetFlashAttentionSlidingWindowSize(
    cnnlFlashAttentionDescriptor_t flash_attention_desc,
    int32_t left_size,
    int32_t right_size,
    int32_t dilation);

// Group:FlashAttentionBackward
/*!
 *  @brief  Set the determinism mode of flash_attention_backward operation.
 *
 *  @param[in,out] desc
 *    Input/output. The FlashAttention operation descriptor.
 *  @param[in] determinism
 *    Input. A cnnlDeterminism_t value used to indicate determinism mode.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    desc is NULL.
 *
 *  @par API Dependency
 *    - Before using this function, you need to call ::cnnlCreateFlashAttentionDescriptor
 *    to create a descriptor.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetFlashAttentionBackwardDeterminismMode(
    cnnlFlashAttentionDescriptor_t desc,
    cnnlDeterminism_t determinism);

// Group:FlashAttentionBackward
/*!
 *  @brief Retrieves extra space size needed in FlashAttentionBackward
 *  operation.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlGetFlashAttentionBackwardWorkspaceSize_v2 instead.
 *
 *  @param[in] handle
 *    Input. Handle to the Cambricon CNNL context that is used to manage MLU devices
 *    and queues in the FlashAttentionBackward operation.
 *  @param[in] desc
 *    Input. The descriptor of the FlashAttentionBackward. For detailed
 *    information, see ::cnnlSetFlashAttentionBackwardDescriptor.
 *  @param[in] query_desc
 *    Input. The descriptor of the \p query tensor, containing the dimension and
 *    layout of the tensor.
 *  @param[in] key_desc
 *    Input. The descriptor of the \p key tensor, containing the dimension and
 *    layout of the tensor.
 *  @param[in] value_desc
 *    Input. The descriptor of the \p value tensor, containing the dimension and
 *    layout of the tensor.
 *  @param[out] size
 *    Output. Pointer to the CPU memory that stores the extra space size (bytes)
 *    needed in FlashAttentionBackward operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - \p handle is NULL.
 *    - \p flash_attention_backward_desc is NULL.
 *    - \p query_desc is NULL.
 *    - \p key_desc is NULL.
 *    - \p size is NULL.
 */
CNNL_DEPRECATED_FOR(cnnlGetFlashAttentionBackwardWorkspaceSize_v2)
cnnlStatus_t CNNL_WIN_API
cnnlGetFlashAttentionBackwardWorkspaceSize(cnnlHandle_t handle,
                                    const cnnlFlashAttentionDescriptor_t desc,
                                    const cnnlTensorDescriptor_t query_desc,
                                    const cnnlTensorDescriptor_t key_desc,
                                    const cnnlTensorDescriptor_t value_desc,
                                    size_t *size);

/*!
 *  @brief Retrieves the extra space size needed in FlashAttentionBackward
 *  operation.
 *
 *  @param[in] handle
 *    Input. Handle to the Cambricon CNNL context that is used to manage MLU devices
 *    and queues in the FlashAttentionBackward operation.
 *  @param[in] desc
 *    Input. The descriptor of the FlashAttentionBackward operation. For detailed
 *    information, see ::cnnlSetFlashAttentionBackwardDescriptor.
 *  @param[in] query_desc
 *    Input. The descriptor of the \p query tensor, containing the dimension and
 *    layout of the tensor.
 *  @param[in] key_desc
 *    Input. The descriptor of the \p key tensor, containing the dimension and
 *    layout of the tensor.
 *  @param[in] value_desc
 *    Input. The descriptor of the \p value tensor, containing the dimension and
 *    layout of the tensor.
 *  @param[in] seq_offset_q_desc
 *    Input. The descriptor of the \p seq_offset_q tensor, containing the dimension and
 *    layout of the tensor. Currently only supports CNNL_DTYPE_INT32.
 *  @param[out] size
 *    Output. Pointer to the CPU memory that stores the extra space size in bytes
 *    needed in FlashAttentionBackward operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - \p handle is NULL.
 *    - \p flash_attention_backward_desc is NULL.
 *    - \p query_desc is NULL.
 *    - \p key_desc is NULL.
 *    - \p size is NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetFlashAttentionBackwardWorkspaceSize_v2(cnnlHandle_t handle,
                                    const cnnlFlashAttentionDescriptor_t desc,
                                    const cnnlTensorDescriptor_t query_desc,
                                    const cnnlTensorDescriptor_t key_desc,
                                    const cnnlTensorDescriptor_t value_desc,
                                    const cnnlTensorDescriptor_t seq_offset_q_desc,
                                    size_t *size);
// Group:FlashAttentionBackward
/*!
 *  @brief Performs the FlashAttentionBackward operation.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlFlashAttentionBackward_v2 instead.
 *
 *  @param[in] handle
 *    Input. Handle to the Cambricon CNNL context that is used to manage MLU devices
 *    and queues in the FlashAttentionBackward operation.
 *  @param[in] flash_attention_backward_desc
 *    Input. The descriptor of the FlashAttentionBackward operation. For detailed
 *    information, see ::cnnlSetFlashAttentionBackwardDescriptor.
 *  @param[in] diff_out_desc
 *    Input. The descriptor of the \p diff_out tensor, containing the dimension and
 *    layout of the tensor. Currently only supports CNNL_DTYPE_HALF and
 *    CNNL_DTYPE_BFLOAT16.
 *  @param[in] diff_out
 *    Input. Pointer to the MLU memory that stores the \p diff_out tensor. Its shape
 *    is [seq_q_total, head_num_q, head_size_v].
 *  @param[in] query_desc
 *    Input. The descriptor of the \p query tensor, containing the dimension and
 *    layout of the tensor. Currently only supports CNNL_DTYPE_HALF and
 *    CNNL_DTYPE_BFLOAT16.
 *  @param[in] query
 *    Input. Pointer to the MLU memory that stores the \p query tensor. Its shape
 *    is [seq_q_total, head_num_q, head_size_qk].
 *  @param[in] key_desc
 *    Input. The descriptor of the \p key tensor, containing the dimension and
 *    layout of the tensor. Currently only supports CNNL_DTYPE_HALF and
 *    CNNL_DTYPE_BFLOAT16.
 *  @param[in] key
 *    Input. Pointer to the MLU memory that stores the \p key tensor. Its shape
 *    is [seq_k_total, head_num_k, head_size_qk].
 *  @param[in] value_desc
 *    Input. The descriptor of the \p value tensor, containing the dimension and
 *    layout of the tensor. Currently only supports CNNL_DTYPE_HALF and
 *    CNNL_DTYPE_BFLOAT16.
 *  @param[in] value
 *    Input. Pointer to the MLU memory that stores the \p value tensor. Its shape
 *    is [seq_k_total, head_num_k, head_size_v].
 *  @param[in] fwd_out_desc
 *    Input. The descriptor of the \p fwd_out tensor, containing the dimension and
 *    layout of the tensor. Currently only supports CNNL_DTYPE_HALF and
 *    CNNL_DTYPE_BFLOAT16.
 *  @param[in] fwd_out
 *    Input. Pointer to the MLU memory that stores the \p fwd_out tensor. Its shape
 *    is [seq_q_total, head_num_q, head_size_v].
 *  @param[in] softmax_lse_desc
 *    Input. The descriptor of the \p softmax_lse tensor, containing the dimension and
 *    layout of the tensor. Currently only supports CNNL_DTYPE_FLOAT.
 *  @param[in] softmax_lse
 *    Input. Pointer to the MLU memory that stores the \p softmax_lse tensor. Its shape
 *    is [head_num_q, seq_q_total].
 *  @param[in] seq_offset_q_desc
 *    Input. The descriptor of the \p seq_offset_q tensor, containing the dimension and
 *    layout of the tensor. Currently only supports CNNL_DTYPE_INT32.
 *  @param[in] seq_offset_q
 *    Input. Pointer to the MLU memory that stores the \p seq_offset_q tensor. Its shape
 *    is [batch_num + 1]. The sequence length of batch i is calculated as:
 *    seq_q[i] = seq_offset_q[i + 1] - seq_offset_q[i].
 *  @param[in] seq_offset_k_desc
 *    Input. The descriptor of the \p seq_offset_k tensor, containing the dimension and
 *    layout of the tensor. Currently only supports CNNL_DTYPE_INT32.
 *  @param[in] seq_offset_k
 *    Input. Pointer to the MLU memory that stores the \p seq_offset_k tensor. Its shape
 *    is [batch_num + 1]. The sequence length of batch i is calculated as:
 *    seq_k[i] = seq_offset_k[i + 1] - seq_offset_k[i].
 *  @param[in] rng_state
 *    Input. Random seed used by dropout calculation, which is a pointer to the CPU
 *           memory that stores two uint64_t values. rng_state[0] represents keys
 *           and rng_state[1] represents offsets.
 *  @param[in] workspace
 *    Input. A pointer to an extra workspace required in the operation.
 *  @param[in] workspace_size
 *    Input. The size of the workspace.
 *  @param[in] diff_q_desc
 *    Input. The descriptor of the \p diff_q tensor, containing the dimension and
 *    layout of the tensor. Currently only supports CNNL_DTYPE_HALF and
 *    CNNL_DTYPE_BFLOAT16.
 *  @param[out] diff_q
 *    Output. Pointer to the MLU memory that stores the \p diff_q tensor. Its shape
 *    is [seq_q_total, head_num_q, head_size_qk].
 *  @param[in] diff_k_desc
 *    Input. The descriptor of the \p diff_k tensor, containing the dimension and
 *    layout of the tensor. Currently only supports CNNL_DTYPE_HALF and
 *    CNNL_DTYPE_BFLOAT16.
 *  @param[out] diff_k
 *    Output. Pointer to the MLU memory that stores the \p diff_k tensor. Its shape
 *    is [seq_k_total, head_num_k, head_size_qk].
 *  @param[in] diff_v_desc
 *    Input. The descriptor of the \p diff_v tensor, containing the dimension and
 *    layout of the tensor. Currently only supports CNNL_DTYPE_HALF and
 *    CNNL_DTYPE_BFLOAT16.
 *  @param[out] diff_v
 *    Output. Pointer to the MLU memory that stores the \p diff_v tensor. Its shape
 *    is [seq_k_total, head_num_k, head_size_v].
 *  @param[in] dropout_mask_desc
 *    Input. The descriptor of the \p dropout_mask tensor, containing the dimension and
 *    layout of the tensor. Currently only supports CNNL_DTYPE_FLOAT32. Used for debugging. Pass
 *    NULL to avoid FlashAttentionBackward operation to store dropout mask data.
 *  @param[out] dropout_mask
 *    Output. Pointer to the MLU memory that stores the \p dropout_mask tensor. Its shape
 *    is [batch_num, head_num_q, packed_max_seq_q, packed_max_seq_kv].
 *    If \p dropout_mask_desc is NULL, \p dropout_mask must also be NULL.
 *  @param[in] softmax_d_desc
 *    Input. The descriptor of the \p softmax_d tensor, containing the dimension and
 *    layout of the tensor. Used only for debugging.
 *    This tensor will store the result of reduce_sum(fwd_out * diff_out, dim = -1).
 *    Pass NULL to avoid FlashAttentionBackward operation to store softmax_d data.
 *  @param[out] softmax_d
 *    Output. Pointer to the MLU memory that stores the \p softmax_d tensor. Its shape
 *    is [head_num_q, seq_q_total]. If \p softmax_d_desc is NULL, \p softmax_d
 *    must also be NULL.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - \p handle is NULL.
 *    - \p flash_attention_backward_desc is NULL.
 *    - \p descs and data pointers are NULL.
 *    - \p head_num_q is not a multiple of head_num_k.
 *  @retval CNNL_STATUS_ARCH_MISMATCH
 *    - This function runs on the hardware platform that is not supported.
 *  @retval CNNL_STATUS_NOT_SUPPORTED
 *    - The requested functionality is not supported in this version.
 *  @par Data Type
 *    - Data types of \p diff_out, \p query, \p key, \p value,
 *      \p fwd_out, \p diff_q, \p diff_k, \p and diff_v must be same.
 *
 *  @note
 *    - Only MLU300 and MLU500 series are supported.
 *    - Inputs and outputs cannot be homologous operand.
 *    - The content of all input tensors is not modified.
 *    - Currently only support HS in the last level. It supports the following layouts:
 *      ::CNNL_ATTN_TENSOR_LAYOUT_PACKED,
 *      ::CNNL_ATTN_TENSOR_LAYOUT_B_S_HN_HS,
 *      ::CNNL_ATTN_TENSOR_LAYOUT_B_HN_S_HS,
 *      ::CNNL_ATTN_TENSOR_LAYOUT_S_B_HN_HS,
 *      ::CNNL_ATTN_TENSOR_LAYOUT_S_HN_B_HS,
 *      ::CNNL_ATTN_TENSOR_LAYOUT_HN_B_S_HS,
 *      and ::CNNL_ATTN_TENSOR_LAYOUT_HN_S_B_HS.
 *
 *  @par API Dependency
 *    - Before using this function, you need to call ::cnnlCreateFlashAttentionDescriptor
 *      function to create a descriptor, and call the ::cnnlSetFlashAttentionBackwardDescriptor
 *      function to set the attributes. Also, you need to destroy the descriptor at
 *      the end with the ::cnnlDestroyFlashAttentionDescriptor function.
 *
 *  @par Example
 *  @verbatim
    Dimension of diff_out: [seq_q_total, head_num, head_size_v]

    Dimension of query: [seq_q_total, head_num, head_size_qk]

    Dimension of key: [seq_k_total, head_num, head_size_qk]

    Dimension of value: [seq_k_total, head_num, head_size_v]

    Dimension of fwd_out: [seq_q_total, head_num, head_size_v]

    Dimension of softmax_lse: [head_num, seq_q_total]

    Dimension of seq_offset_q: [batch_num + 1],
    where seq_q_total = sum(seq_q[0], ..., seq_q[batch_num - 1])

    Dimension of seq_offset_k: [batch_num + 1],
    where seq_k_total = sum(seq_k[0], ..., seq_k[batch_num - 1])

    Then we will get the mask and output:

    Dimension of diff_q: [seq_q_total, head_num, head_size_qk]

    Dimension of diff_k: [seq_k_total, head_num, head_size_qk]

    Dimension of diff_v: [seq_k_total, head_num, head_size_v]
    @endverbatim
 */
CNNL_DEPRECATED_FOR(cnnlFlashAttentionBackward_v2)
cnnlStatus_t CNNL_WIN_API
cnnlFlashAttentionBackward(cnnlHandle_t handle,
                           cnnlFlashAttentionDescriptor_t flash_attention_backward_desc,
                           cnnlTensorDescriptor_t diff_out_desc,
                           const void * diff_out,
                           cnnlTensorDescriptor_t query_desc,
                           const void * query,
                           cnnlTensorDescriptor_t key_desc,
                           const void * key,
                           cnnlTensorDescriptor_t value_desc,
                           const void * value,
                           cnnlTensorDescriptor_t fwd_out_desc,
                           const void * fwd_out,
                           cnnlTensorDescriptor_t softmax_lse_desc,
                           const void * softmax_lse,
                           cnnlTensorDescriptor_t seq_offset_q_desc,
                           const void * seq_offset_q,
                           cnnlTensorDescriptor_t seq_offset_k_desc,
                           const void * seq_offset_k,
                           const size_t rng_state[],
                           void * workspace,
                           size_t workspace_size,
                           cnnlTensorDescriptor_t diff_q_desc,
                           void * diff_q,
                           cnnlTensorDescriptor_t diff_k_desc,
                           void * diff_k,
                           cnnlTensorDescriptor_t diff_v_desc,
                           void * diff_v,
                           cnnlTensorDescriptor_t dropout_mask_desc,
                           void * dropout_mask,
                           cnnlTensorDescriptor_t softmax_d_desc,
                           void * softmax_d);

// Group:FlashAttentionBackward
/*!
 *  @brief Performs the FlashAttentionBackward operation.
 *
 *  Compared with ::cnnlFlashAttentionBackward, four additional input parameters are added,
 *  including tensor descriptors and device pointers of additive attention mask and alibi slope.
 *  @param[in] handle
 *    Input. Handle to the Cambricon CNNL context that is used to manage MLU devices
 *    and queues in the FlashAttentionBackward operation.
 *  @param[in] flash_attention_backward_desc
 *    Input. The descriptor of the FlashAttentionBackward operation. For detailed
 *    information, see ::cnnlFlashAttentionDescriptor_t.
 *  @param[in] diff_out_desc
 *    Input. The descriptor of the \p diff_out tensor, containing the dimension and
 *    layout of the tensor. Currently only supports CNNL_DTYPE_HALF and
 *    CNNL_DTYPE_BFLOAT16.
 *  @param[in] diff_out
 *    Input. Pointer to the MLU memory that stores the \p diff_out tensor.
 *    According to \p tensor_layout, its shape can be [seq_q_total, head_num_q, head_size_v] or
 *    [batches, seq_q, head_num_q, head_size_v].
 *  @param[in] query_desc
 *    Input. The descriptor of the \p query tensor, containing the dimension and
 *    layout of the tensor. Currently only supports CNNL_DTYPE_HALF and
 *    CNNL_DTYPE_BFLOAT16.
 *  @param[in] query
 *    Input. Pointer to the MLU memory that stores the \p query tensor. Its shape
 *    According to \p tensor_layout, its shape can be [seq_q_total, head_num_q, head_size_qk] or
 *    [batches, seq_q, head_num_q, head_size_qk].
 *  @param[in] key_desc
 *    Input. The descriptor of the \p key tensor, containing the dimension and
 *    layout of the tensor. Currently only supports CNNL_DTYPE_HALF and
 *    CNNL_DTYPE_BFLOAT16.
 *  @param[in] key
 *    Input. Pointer to the MLU memory that stores the \p key tensor.
 *    According to \p tensor_layout, its shape can be [seq_kv_total, head_num_kv, head_size_qk] or
 *    [batches, seq_kv, head_num_kv, head_size_qk].
 *  @param[in] value_desc
 *    Input. The descriptor of the \p value tensor, containing the dimension and
 *    layout of the tensor. Currently only supports CNNL_DTYPE_HALF and
 *    CNNL_DTYPE_BFLOAT16.
 *  @param[in] value
 *    Input. Pointer to the MLU memory that stores the \p value tensor.
 *    According to \p tensor_layout, its shape can be [seq_kv_total, head_num_kv, head_size_v] or
 *    [batches, seq_kv, head_num_kv, head_size_v].
 *  @param[in] fwd_out_desc
 *    Input. The descriptor of the \p fwd_out tensor, containing the dimension and
 *    layout of the tensor. Currently only supports CNNL_DTYPE_HALF and
 *    CNNL_DTYPE_BFLOAT16.
 *  @param[in] fwd_out
 *    Input. Pointer to the MLU memory that stores the \p fwd_out tensor.
 *    According to \p tensor_layout, its shape can be [seq_q_total, head_num_q, head_size_v] or
 *    [batches, seq_kv, head_num_q, head_size_v].
 *  @param[in] softmax_lse_desc
 *    Input. The descriptor of the \p softmax_lse tensor, containing the dimension and
 *    layout of the tensor. Currently only supports CNNL_DTYPE_FLOAT.
 *  @param[in] softmax_lse
 *    Input. Pointer to the MLU memory that stores the \p softmax_lse tensor. Its shape
 *    is [head_num_q, seq_q_total].
 *  @param[in] seq_offset_q_desc
 *    Input. The descriptor of the \p seq_offset_q tensor, containing the dimension and
 *    layout of the tensor. Currently only supports CNNL_DTYPE_INT32.
 *  @param[in] seq_offset_q
 *    Input. Pointer to the MLU memory that stores the \p seq_offset_q tensor. Its shape
 *    is [batch_num + 1]. The sequence length of batch i is calculated as:
 *    seq_q[i] = seq_offset_q[i + 1] - seq_offset_q[i]. When \p is_pack_mode is false,
*     \p seq_offset_q_desc and \p seq_offset_q are not used.
 *  @param[in] seq_offset_k_desc
 *    Input. The descriptor of the \p seq_offset_k tensor, containing the dimension and
 *    layout of the tensor. Currently only supports CNNL_DTYPE_INT32.
 *  @param[in] seq_offset_k
 *    Input. Pointer to the MLU memory that stores the \p seq_offset_k tensor. Its shape
 *    is [batch_num + 1]. The sequence length of batch i is calculated as:
 *    seq_k[i] = seq_offset_k[i + 1] - seq_offset_k[i]. When \p is_pack_mode is false,
*     \p seq_offset_k_desc and \p seq_offset_k are not used.
*  @param[in] alibi_slope_desc
 *    Input. The descriptor of the \p alibi_slope tensor, containing the dimension and
 *    layout of the tensor. Currently it supports only CNNL_DTYPE_FLOAT and cannot be used
 *    together with \p additive_attention_mask_desc. It cannot be used when
 *    head_size_v != head_size_qk.
 *  @param[in] alibi_slope
 *    Input. Pointer to the MLU memory that stores the \p alibi_slope tensor. Its shape
 *    is [batch_size, head_num] or [head_num].
 *  @param[in] additive_attention_mask_desc
 *    Input. The descriptor of the \p additive_attention_mask tensor, containing the
 *    dimension and layout of the tensor. Currently it supports only CNNL_DTYPE_HALF and
 *    CNNL_DTYPE_BFLOAT16 and cannot be used together with \p alibi_slope_desc. It cannot
 *    be used when head_num_kv != head_num_q.
 *  @param[in] additive_attention_mask
 *    Input. Pointer to the MLU memory that stores the \p additive_attention_mask tensor.
 *    Its shape is [batch_size, head_num_q, packed_max_seq_q, packed_max_seq_kv].
 *    If \p additive_attention_mask_desc is NULL, \p additive_attention_mask must also be NULL.
 *  @param[in] rng_state
 *    Input. Random seed used by dropout calculation, which is a pointer to the CPU
 *           memory that stores two uint64_t values. rng_state[0] represents keys
 *           and rng_state[1] represents offsets.
 *  @param[in] workspace
 *    Input. A pointer to an extra workspace required in the operation.
 *  @param[in] workspace_size
 *    Input. The size of the workspace.
 *  @param[in] diff_q_desc
 *    Input. The descriptor of the \p diff_q tensor, containing the dimension and
 *    layout of the tensor. Currently only supports CNNL_DTYPE_HALF and
 *    CNNL_DTYPE_BFLOAT16.
 *  @param[out] diff_q
 *    Output. Pointer to the MLU memory that stores the \p diff_q tensor.
 *    According to \p tensor_layout, its shape can be [seq_q_total, head_num_q, head_size_qk] or
 *    [batches, seq_q, head_num_q, head_size_qk].
 *  @param[in] diff_k_desc
 *    Input. The descriptor of the \p diff_k tensor, containing the dimension and
 *    layout of the tensor. Currently only supports CNNL_DTYPE_HALF and
 *    CNNL_DTYPE_BFLOAT16.
 *  @param[out] diff_k
 *    Output. Pointer to the MLU memory that stores the \p diff_k tensor.
 *    According to \p tensor_layout, its shape can be [seq_k_total, head_num_k, head_size_qk] or
 *    [batches, seq_k, head_num_k, head_size_qk].
 *  @param[in] diff_v_desc
 *    Input. The descriptor of the \p diff_v tensor, containing the dimension and
 *    layout of the tensor. Currently only supports CNNL_DTYPE_HALF and
 *    CNNL_DTYPE_BFLOAT16.
 *  @param[out] diff_v
 *    Output. Pointer to the MLU memory that stores the \p diff_v tensor.
 *    According to \p tensor_layout, its shape can be [seq_k_total, head_num_k, head_size_v] or
 *    [batches, seq_k, head_num_k, head_size_v].
 *  @param[in] dropout_mask_desc
 *    Input. The descriptor of the \p dropout_mask tensor, containing the dimension and
 *    layout of the tensor. Currently only supports CNNL_DTYPE_FLOAT32. Used for debugging. Pass
 *    NULL to avoid FlashAttentionBackward operation to store or load dropout mask data.
 *  @param[in,out] dropout_mask
 *    Output. Pointer to the MLU memory that stores the \p dropout_mask tensor. Its shape
 *    is [batch_num, head_num_q, packed_max_seq_q, packed_max_seq_kv].
 *    If \p dropout_mask_desc is NULL, \p dropout_mask must also be NULL.
 *  @param[in] softmax_d_desc
 *    Input. The descriptor of the \p softmax_d tensor, containing the dimension and
 *    layout of the tensor. Used only for debugging.
 *    This tensor will store the result of reduce_sum(fwd_out * diff_out, dim = -1).
 *    Pass NULL to avoid FlashAttentionBackward operation to store softmax_d data.
 *  @param[out] softmax_d
 *    Output. Pointer to the MLU memory that stores the \p softmax_d tensor. Its shape
 *    is [head_num_q, seq_q_total]. If \p softmax_d_desc is NULL, \p softmax_d
 *    must also be NULL.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - \p handle is NULL.
 *    - \p flash_attention_backward_desc is NULL.
 *    - descs and data pointers are NULL.
 *    - \p head_num_q is not a multiple of \p head_num_k.
 *    - \p head_num_q != head_num_kv and head_size_qk != head_size are met at the same time.
 *  @retval CNNL_STATUS_ARCH_MISMATCH
 *    - This function runs on the hardware platform that is not supported.
 *  @retval CNNL_STATUS_NOT_SUPPORTED
 *    - The requested functionality is not supported in this version.
 *  @par Data Type
 *    - Data types of \p diff_out, \p query, \p key, \p value,
 *      \p fwd_out, \p additive_attention_mask, \p diff_q, \p diff_k, \p and diff_v must be same.
 *
 *  @note
 *    - Only MLU300 and MLU500 series are supported.
 *    - CNNL_DTYPE_BFLOAT16 is supported only on MLU500 series.
 *    - Inputs and outputs cannot be homologous operand.
 *    - The content of all input tensors is not modified.
 *    - \p seq_offset_q and \p seq_offset_k are not required when \p is_pack_mode is false.
 *    - Currently only support HS in the last level. It supports the following layouts:
 *      ::CNNL_ATTN_TENSOR_LAYOUT_PACKED,
 *      ::CNNL_ATTN_TENSOR_LAYOUT_B_S_HN_HS,
 *      ::CNNL_ATTN_TENSOR_LAYOUT_B_HN_S_HS,
 *      ::CNNL_ATTN_TENSOR_LAYOUT_S_B_HN_HS,
 *      ::CNNL_ATTN_TENSOR_LAYOUT_S_HN_B_HS,
 *      ::CNNL_ATTN_TENSOR_LAYOUT_HN_B_S_HS,
 *      and ::CNNL_ATTN_TENSOR_LAYOUT_HN_S_B_HS.
 *
 *  @par API Dependency
 *    - Before using this function, you need to call ::cnnlCreateFlashAttentionDescriptor
 *      to create a descriptor, call ::cnnlSetFlashAttentionBackwardDescriptor_v2
 *      to set the attributes, and call ::cnnlGetFlashAttentionBackwardWorkspaceSize_v2 to
 *      acquire the size of the additional DDR workspace. Also, at the end you need to destroy the descriptor
 *      with ::cnnlDestroyFlashAttentionDescriptor.
 *
 *  @par Example
 *  @verbatim
    Dimension of diff_out: [seq_q_total, head_num, head_size_v] or
                           [batches, seq_q, head_num, head_size_v] in different layout

    Dimension of query: [seq_q_total, head_num, head_size_qk] or
                        [batches, seq_q, head_num, head_size_qk] in different layout

    Dimension of key: [seq_k_total, head_num, head_size_qk] or
                      [batches, seq_k, head_num, head_size_qk] in different layout

    Dimension of value: [seq_k_total, head_num, head_size_v] or
                        [batches, seq_k, head_num, head_size_v] in different layout

    Dimension of fwd_out: [seq_q_total, head_num, head_size_v] or
                          [batches, seq_q, head_num, head_size_v] in different layout

    Dimension of softmax_lse: [head_num, seq_q_total]

    Dimension of seq_offset_q: [batch_num + 1],
    where seq_q_total = sum(seq_q[0], ..., seq_q[batch_num - 1])

    Dimension of seq_offset_k: [batch_num + 1],
    where seq_k_total = sum(seq_k[0], ..., seq_k[batch_num - 1])

    Dimension of additive_attention_mask:  [batch_size, head_num_q, packed_max_seq_q, packed_max_seq_kv]

    Then we will get the mask and output:

    Dimension of diff_q: [seq_q_total, head_num, head_size_qk] or
                         [batches, seq_q, head_num, head_size_qk] in different layout

    Dimension of diff_k: [seq_k_total, head_num, head_size_qk] or
                         [batches, seq_k, head_num, head_size_qk] in different layout

    Dimension of diff_v: [seq_k_total, head_num, head_size_v] or
                         [batches, seq_q, head_num, head_size_v] in different layout
    @endverbatim
 */
cnnlStatus_t CNNL_WIN_API
cnnlFlashAttentionBackward_v2(cnnlHandle_t handle,
                              cnnlFlashAttentionDescriptor_t flash_attention_backward_desc,
                              cnnlTensorDescriptor_t diff_out_desc,
                              const void *diff_out,
                              cnnlTensorDescriptor_t query_desc,
                              const void *query,
                              cnnlTensorDescriptor_t key_desc,
                              const void *key,
                              cnnlTensorDescriptor_t value_desc,
                              const void *value,
                              cnnlTensorDescriptor_t fwd_out_desc,
                              const void *fwd_out,
                              cnnlTensorDescriptor_t softmax_lse_desc,
                              const void *softmax_lse,
                              cnnlTensorDescriptor_t seq_offset_q_desc,
                              const void *seq_offset_q,
                              cnnlTensorDescriptor_t seq_offset_k_desc,
                              const void *seq_offset_k,
                              cnnlTensorDescriptor_t alibi_slope_desc,
                              const void *alibi_slope,
                              cnnlTensorDescriptor_t additive_attention_mask_desc,
                              void *additive_attention_mask,
                              const size_t rng_state[],
                              void *workspace,
                              size_t workspace_size,
                              cnnlTensorDescriptor_t diff_q_desc,
                              void *diff_q,
                              cnnlTensorDescriptor_t diff_k_desc,
                              void *diff_k,
                              cnnlTensorDescriptor_t diff_v_desc,
                              void *diff_v,
                              cnnlTensorDescriptor_t dropout_mask_desc,
                              void *dropout_mask,
                              cnnlTensorDescriptor_t softmax_d_desc,
                              void *softmax_d);
/******************************************************************************
 * Cambricon CNNL OP: RotaryEmbedding
 ******************************************************************************/
/*! The descriptor of the RotaryEmbedding operation.
 * You can use ::cnnlCreateRotaryEmbeddingDescriptor(), ::cnnlSetRotaryEmbeddingDescriptor_v2()
 * and ::cnnlDestroyRotaryEmbeddingDescriptor() to create, set and destroy the descriptor
 * respectively.
 */
typedef struct cnnlRotaryEmbeddingStruct *cnnlRotaryEmbeddingDescriptor_t;

// Group:RotaryEmbedding
/*!
 *  @brief Creates a descriptor pointed by \p desc
 *   for the RotaryEmbedding operation.
 *  @param[out] desc
 *    Output. Descriptor of the RotaryEmbedding operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_ALLOC_FAILED
 *    The function failed to allocate memory for the RotaryEmbedding descriptor.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCreateRotaryEmbeddingDescriptor(cnnlRotaryEmbeddingDescriptor_t *desc);

// Group:RotaryEmbedding
/*!
 *  @brief Destroys the descriptor of the RotaryEmbedding operation.
 *
 *  @param[in] desc
 *    Input. Descriptor of the RotaryEmbedding operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p desc is NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlDestroyRotaryEmbeddingDescriptor(cnnlRotaryEmbeddingDescriptor_t desc);

// Group:RotaryEmbedding
/*!
 *  @brief Sets the descriptor of the RotaryEmbedding operation.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlSetRotaryEmbeddingDescriptor_v2 instead.
 *
 *  @param[in,out] desc
 *    Input/output. Descriptor of the RotaryEmbedding operation.
 *  @param[in] conj
 *    Input. Controls whether forward or backward calculation is performed. The value True
 *    indicates backward calculation, and the value False indicates forward calculation.
 *  @param[in] seq_layout
 *    Input. The layout of input value in RotaryEmbedding.
 *    The value can only be \p CNNL_SEQDATA_NTBC or \p CNNL_SEQDATA_TNBC.
 *      - When \p seq_layout is \p CNNL_SEQDATA_NTBC, dim of batch_size should be ahead of dim of
 *        seq_len.
 *      - When \p seq_layout is \p CNNL_SEQDATA_TNBC, dim of seq_len should be ahead of dim of
 *        batch_size.
 *      - When \p seq_layout is not set, the default value is \p CNNL_SEQDATA_NTBC.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p desc is NULL, or one or more parameters are not supported.
 */
CNNL_DEPRECATED_FOR(cnnlSetRotaryEmbeddingDescriptor_v2)
cnnlStatus_t CNNL_WIN_API
cnnlSetRotaryEmbeddingDescriptor(cnnlRotaryEmbeddingDescriptor_t desc,
                                 bool conj,
                                 cnnlSeqDataLayout_t seq_layout);

// Group:RotaryEmbedding
/*!
 *  @brief Sets the descriptor of the RotaryEmbedding operation. Compared with
 *  ::cnnlSetRotaryEmbeddingDescriptor, a new interleaved option is added
 *  to perform interleaved calculations inside the operator. At the same time, in CNNL_SEQDATA_TNBC
 *  mode, two new options have been added to handle different modes of interleaved algorithms.
 *
 *  @param[in,out] desc
 *    Input/output. Descriptor of the RotaryEmbedding operation.
 *  @param[in] conj
 *    Input. Controls whether forward or backward calculation is performed. The value True
 *    indicates backward calculation, and the value False indicates forward calculation.
 *  @param[in] interleaved
 *    Input. Controls whether to stagger the numbers for calculation.
 *  @param[in] is_cross_input
 *    Input. Specifies whether the input data during the interleaved operation is crossed.
 *    The value True indicates that \p x1 data you entered is crossed, and the value
 *    False indicates that \p x1 data you entered is not crossed, and it needs to be processed
 *    as crossed inside the operator.
 *  @param[in] is_cross_output
 *    Input. Specifies whether the output data during the interleaved operation is crossed.
 *    The value True indicates that \p out1 data you need is crossed, and the value False
 *    indicates that \p out1 data you need is not crossed, and it needs to be processed as not
 *    crossed inside the operator.
 *  @param[in] seq_layout
 *    Input. The layout of input value in RotaryEmbedding.
 *    The value can only be \p CNNL_SEQDATA_NTBC or \p CNNL_SEQDATA_TNBC.
 *    - When \p seq_layout is \p CNNL_SEQDATA_NTBC, dim of batch_size should be ahead of dim of
 *      seq_len.
 *    - When \p seq_layout is \p CNNL_SEQDATA_TNBC, dim of seq_len should be ahead of dim of
 *      batch_size.
 *    - When \p seq_layout is not set, the default value is \p CNNL_SEQDATA_NTBC.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p desc is NULL, or one or more parameters are not supported.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetRotaryEmbeddingDescriptor_v2(cnnlRotaryEmbeddingDescriptor_t desc,
                                    bool conj,
                                    bool interleaved,
                                    bool is_cross_input,
                                    bool is_cross_output,
                                    cnnlSeqDataLayout_t seq_layout);

// Group:RotaryEmbedding
/*!
 *  @brief Executes rotary embedding in the transformer network with the following input methods:
 *  -  The first input method has four inputs: \p x1, \p x2, \p cos, \p sin. The layout of \p x1
 *     and \p x2 is [batch_size, seqlen_total, head_num, head_size], and the layout of \p cos and
 *     \p sin is [seqlen_total, 1, rotary_dim]. Among them, \p x1 and \p x2 together form the query
 *     or key in the transformer network.
 *  -  The second input method has three inputs: \p x1, \p cos, \p sin. The layout of \p x1 is
 *     [seqlen_total, batch_size, head_num, head_size], and the layout of \p cos and \p sin is
 *     [seqlen_total, rotary_dim]. Among them, \p cos and \p sin are angle values and need to be
 *     calculated internally, and their input type can only be float.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlRotaryEmbedding_v2 instead.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
 *  @param[in] rotary_embedding_desc
 *    Input. Descriptor of the RotaryEmbedding operation.
 *  @param[in] x1_desc
 *    Input. Descriptor of \p x1 tensor. The shape of the first input method is
 *    [batch_size, seqlen_total, head_num, head_size], and the shape of the second input method is
 *    [seqlen_total, batch_size, head_num, head_size].
 *  @param[in] x1
 *    Input. Pointer to the MLU memory that stores \p x1 tensor.
 *  @param[in] x2_desc
 *    Input. Descriptor of \p x2 tensor. The shape of the first input method is
 *    [batch_size, seqlen_total, head_num, head_size]. The second input method requires nullptr.
 *  @param[in] x2
 *    Input. Pointer to the MLU memory that stores \p x2 tensor. The second input method requires
 *    nullptr.
 *  @param[in] cos_desc
 *    Input. Descriptor of \p cos tensor. The shape of the first input method is
 *    [seqlen_total, 1, rotary_dim], and the shape of the second input method is
 *    [seqlen_total, rotary_dim]. If the second is used, the data type must be float.
 *  @param[in] cos
 *    Input. Pointer to the MLU memory that stores \p cos tensor.
 *  @param[in] sin_desc
 *    Input. Descriptor of \p sin tensor. The first input method shape is
 *    [seqlen_total, 1, rotary_dim], and the second input method is [seqlen_total, rotary_dim].
 *    If the second input method is used, the data type must be float.
 *  @param[in] sin
 *    Input. Pointer to the MLU memory that stores \p sin tensor.
 *  @param[in] workspace
 *    Input. Pointer to the MLU memory that is used as an extra workspace.
 *  @param[in] workspace_size
 *    Input. Size of the workspace.
 *  @param[in] out1_desc
 *    Input. Descriptor of \p out1 tensor. The shape of the first input method is
 *    [batch_size, seqlen_total, head_num, head_size], and the shape of the second input method is
 *    [seqlen_total, batch_size, head_num, head_size].
 *  @param[out] out1
 *    Output. Pointer to the MLU memory that stores \p out1 tensor.
 *  @param[in] out2_desc
 *    Input. Descriptor of \p out2 tensor. The shape of the first input method is
 *    [batch_size, seqlen_total, head_num, head_size]. The second input method requires nullptr.
 *  @param[out] out2
 *    Output. Pointer to the MLU memory that stores \p out2 tensor. The second input method
 *    requires nullptr.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are encountered:
 *    - \p handle is NULL.
 *    - One or more required pointers are NULL.
 *
 *  @par API Dependency
 *  - Before using this function, you need to call ::cnnlCreateRotaryEmbeddingDescriptor
 *    to create a descriptor.
 *
 *  @par Data Type
 *  - Data types of \p x1, \p x2, \p cos, \p sin, \p out1 and \p out2 in the first input method
 *    must be the same, which support half, bfloat16 and float.
 *  - Ddata types of \p x1 and \p out1 in the second input method must be the same, which support
 *    half, bfloat16 and float. The data types of \p cos and \p sin must be float.
 *
 *  @note
 *  - Only MLU300 and MLU500 series are supported.
 *  - Data type bfloat16 is only supported on MLU500 series.
 *  - The maximum value cannot exceed 1e4 and cannot be less than -1e4.
 *
 *  @par Scale Limitation
 *   - The shape of tensor \p cos and tensor \p sin in the first input method must be 3-D, and the
 *     second dimension is 1.
 *   - The shape of tensor \p cos and tensor \p sin in the second input method must be 2-D, and
 *     \p x2 and \p out2 must be nullptr.
 *   - Data type of tensor \p cos and tensor \p sin in the second input method must be float.
 *   - The last dimension of tensor \p cos and tensor \p sin in the second input method must be
 *     an even number.
 *   - CNNL_SEQDATA_NTBC mode only supports \p cos and \p sin that are exactly the same input.
 */
CNNL_DEPRECATED_FOR(cnnlRotaryEmbedding_v2)
cnnlStatus_t CNNL_WIN_API
cnnlRotaryEmbedding(cnnlHandle_t handle,
                    cnnlRotaryEmbeddingDescriptor_t rotary_embedding_desc,
                    const cnnlTensorDescriptor_t x1_desc,
                    const void *x1,
                    const cnnlTensorDescriptor_t x2_desc,
                    const void *x2,
                    const cnnlTensorDescriptor_t cos_desc,
                    const void *cos,
                    const cnnlTensorDescriptor_t sin_desc,
                    const void *sin,
                    void *workspace,
                    size_t workspace_size,
                    const cnnlTensorDescriptor_t out1_desc,
                    void *out1,
                    const cnnlTensorDescriptor_t out2_desc,
                    void *out2);

// Group:RotaryEmbedding
/*!
 *  @brief Executes rotary embedding in the transformer network with the following input methods:
 *  -  The first input method has four inputs: \p x1, \p x2, \p cos, \p sin. The layout of \p x1
 *     and \p x2 is [batch_size, seqlen_total, head_num, head_size], and the layout of \p cos and
 *     \p sin is [seqlen_total, 1, rotary_dim]. Among them, \p x1 and \p x2 together form the query
 *     or key in the transformer network.
 *  -  The second input method has three inputs: \p x1, \p cos, \p sin. The layout of \p x1 is
 *     [seqlen_total, batch_size, head_num, head_size], and the layout of \p cos and \p sin is
 *     [seqlen_total, rotary_dim]. Among them, \p cos and \p sin are cached values and do not
 *     require calculation.
 *  -  The third input method has two inputs: \p x1, \p freqs. The layout of \p x1 is
 *     [seqlen_total, batch_size, head_num, head_size], and the layout of \p freqs is
 *     [seqlen_total, rotary_dim]. Among them, \p freqs is angle values and needs to be calculated,
 *     and its input type can only be float.
 *  -  In addition, the above three input methods can also accept a variable-length input. In this
 *     case, a new input \p cu_seqlens is added, which is the cumulative sum of the sequence length
 *     of input \p x1. The batch_size of \p x1 must be 1, and the shape of \p cu_seqlens is
 *     [batch_size + 1].
 *
 *     When \p cu_seqlens is not nullptr, the layout of \p x1 and \p x2 in the first input method
 *     is [1, seqlen_total * batch_size, head_num, head_size], and the layout of \p x1 in the second
 *     and third input methods is [seqlen_total * batch_size, 1, head_num, head_size]. In the
 *     above layouts, 1 must be passed in as the batch_size position, and the actual batch_size is
 *     the size of cu_seqlens minus 1.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
 *  @param[in] rotary_embedding_desc
 *    Input. Descriptor of the RotaryEmbedding operation.
 *  @param[in] x1_desc
 *    Input. Descriptor of \p x1 tensor. The shape of the first input method is
 *    [batch_size, seqlen_total, head_num, head_size], and the shape of the second and third input
 *    methods is [seqlen_total, batch_size, head_num, head_size]. When \p cu_seqlens is not
 *    nullptr, batch_size is 1.
 *  @param[in] x1
 *    Input. Pointer to the MLU memory that stores \p x1 tensor.
 *  @param[in] x2_desc
 *    Input. Descriptor of \p x2 tensor. The shape of the first input method is
 *    [batch_size, seqlen_total, head_num, head_size]. The second and third input methods require
 *    nullptr. When \p cu_seqlens is not nullptr, batch_size is 1.
 *  @param[in] x2
 *    Input. Pointer to the MLU memory that stores \p x2 tensor. The second and third input methods
 *    require nullptr.
 *  @param[in] cos_desc
 *    Input. Descriptor of \p cos tensor. The shape of the first input method is
 *    [seqlen_total, 1, rotary_dim], and the shape of the second input method is
 *    [seqlen_total, rotary_dim]. The third input method requires nullptr.
 *  @param[in] cos
 *    Input. Pointer to the MLU memory that stores \p cos tensor. The third input method
 *    requires nullptr.
 *  @param[in] sin_desc
 *    Input. Descriptor of \p sin tensor. The first input method shape is
 *    [seqlen_total, 1, rotary_dim], the second input method is [seqlen_total, rotary_dim],
 *    and the third input method requires nullptr.
 *  @param[in] sin
 *    Input. Pointer to the MLU memory that stores \p sin tensor. The third input method
 *    requires nullptr.
 *  @param[in] freqs_desc
 *    Input. Descriptor of \p freqs tensor. The first and second input methods require nullptr.
 *    The shape of the third input method is [seqlen_total, rotary_dim], and its data type must be
 *    float. It needs to be calculated inside the operator.
 *  @param[in] freqs
 *    Input. Pointer to the MLU memory that stores \p freqs tensor. The first and second input
 *    methods require nullptr.
 *  @param[in] cu_seqlens_desc
 *    Input. Descriptor of \p cu_seqlens tensor. It is only used when input \p x1 is variable
 *    length, and its shape is [batch_size + 1].
 *  @param[in] cu_seqlens
 *    Input. Pointer to the MLU memory that stores \p cu_seqlens tensor. It is the cumulative
 *    sum of sequence lengths in a batch for \p x1.
 *    When the input \p x1 is not variable length, the input here needs to be nullptr.
 *  @param[in] workspace
 *    Input. Pointer to the MLU memory that is used as an extra workspace.
 *  @param[in] workspace_size
 *    Input. Size of the workspace.
 *  @param[in] out1_desc
 *    Input. Descriptor of \p out1 tensor. The shape of the first input method is
 *    [batch_size, seqlen_total, head_num, head_size], and the shape of the second and third input
 *    methods is [seqlen_total, batch_size, head_num, head_size].
 *  @param[out] out1
 *    Output. Pointer to the MLU memory that stores \p out1 tensor.
 *  @param[in] out2_desc
 *    Input. Descriptor of \p out2 tensor. The shape of the first input method is
 *    [batch_size, seqlen_total, head_num, head_size]. The second and third input methods require
 *    nullptr.
 *  @param[out] out2
 *    Output. Pointer to the MLU memory that stores \p out2 tensor. The second and third input
 *    methods require nullptr.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are encountered:
 *    - \p handle is NULL.
 *    - One or more required pointers are NULL.
 *
 *  @par API Dependency
 *  - Before using this function, you need to call ::cnnlCreateRotaryEmbeddingDescriptor
 *    to create a descriptor.
 *
 *  @par Data Type
 *  - Data types of \p x1, \p x2, \p cos, \p sin, \p out1 and \p out2 in the first input method
 *    must be the same, which support half, bfloat16 and float.
 *  - Data types of \p x1,  \p cos, \p sin and \p out1 in the second input method must be the
 *    same, which support half, bfloat16 and float.
 *  - Data types of \p x1 and \p out1 in the third input method must be the same, which support
 *    half, bfloat16 and float. The data type of \p freqs must be float.
 *  - When the input \p x1 is variable length, the data type of \p cu_seqlens must be int32.
 *
 *  @note
 *  - Only MLU300 and MLU500 series are supported.
 *  - Data type bfloat16 is only supported on MLU500 series.
 *  - The maximum value cannot exceed 1e4 and cannot be less than -1e4.
 *  - \p cu_seqlens is only used in the input \p x1 is of variable length, and is nullptr in
 *    other cases.
 *
 *  @par Scale Limitation
 *   - The shape of tensor \p cos and tensor \p sin in the first input method must be 3-D,
 *     and the second dimension is 1.
 *   - The shape of tensor \p cos and tensor \p sin in the second input method must be 2-D.
 *   - The shape of tensor \p freqs in the third input method must be 2-D.
 *   - The last dimension of tensor \p cos and tensor \p sin in the second and third input methods
 *     must be an even number.
 *   - The data type of tensor \p freqs in the third input method must be float.
 *   - In the first input method, \p freqs must be nullptr.
 *   - In the second input method, \p x2, \p freqs and \p out2 must be nullptr.
 *   - In the third input method, \p x2, \p cos, \p sin and \p out2 must be nullptr.
 *   - \p cu_seqlens is not nullptr only when the input is variable length.
 */
cnnlStatus_t CNNL_WIN_API
cnnlRotaryEmbedding_v2(cnnlHandle_t handle,
                       cnnlRotaryEmbeddingDescriptor_t rotary_embedding_desc,
                       const cnnlTensorDescriptor_t x1_desc,
                       const void *x1,
                       const cnnlTensorDescriptor_t x2_desc,
                       const void *x2,
                       const cnnlTensorDescriptor_t cos_desc,
                       const void *cos,
                       const cnnlTensorDescriptor_t sin_desc,
                       const void *sin,
                       const cnnlTensorDescriptor_t freqs_desc,
                       const void *freqs,
                       const cnnlTensorDescriptor_t cu_seqlens_desc,
                       const void *cu_seqlens,
                       void *workspace,
                       size_t workspace_size,
                       const cnnlTensorDescriptor_t out1_desc,
                       void *out1,
                       const cnnlTensorDescriptor_t out2_desc,
                       void *out2);
/******************************************************************************
 * Cambricon CNNL OP: BiasDropoutAddFusedTrainBackward
 ******************************************************************************/
/*! The descriptor of the ::cnnlBiasDropoutAddFusedTrainBackward operation.
 *
 *  You need to call the ::cnnlCreateBiasDropoutAddFusedTrainBackwardDescriptor function
 *  to create a descriptor, and call the ::cnnlSetBiasDropoutAddFusedTrainBackwardDescriptor
 *  function to set the attributes. Also, you need to destroy the descriptor at
 *  the end with the ::cnnlDestroyBiasDropoutAddFusedTrainBackwardDescriptor function.
 */
typedef struct cnnlBiasDropoutAddFusedTrainBackwardStruct
    *cnnlBiasDropoutAddFusedTrainBackwardDescriptor_t;

// Group:BiasDropoutAddFusedTrainBackward
/*!
 *  @brief Creates a descriptor pointed by
 *   \p bias_dropout_add_fused_train_backward_desc for the BiasDropoutAddFusedTrainBackward operation.
 *
 *  @param[out] bias_dropout_add_fused_train_backward_desc
 *    Output. A pointer to the BiasDropoutAddFusedTrainBackward descriptor.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p bias_dropout_add_fused_train_backward_desc is NULL.
 *  @retval CNNL_STATUS_ALLOC_FAILED
 *    The function fails to allocate memory for the BiasDropoutAddFusedTrainBackward descriptor.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCreateBiasDropoutAddFusedTrainBackwardDescriptor(
    cnnlBiasDropoutAddFusedTrainBackwardDescriptor_t *bias_dropout_add_fused_train_backward_desc);

// Group:BiasDropoutAddFusedTrainBackward
/*!
 *  @brief Destroys the BiasDropoutAddFusedTrainBackward descriptor.
 *
 *  @param[in] bias_dropout_add_fused_train_backward_desc
 *    Input. The BiasDropoutAddFusedTrainBackward descriptor.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p bias_dropout_add_fused_train_backward_desc is NULL.
 *
 *  @par API Dependency
 *  - This function is used after calling the ::cnnlBiasDropoutAddFusedTrainBackward function.
 */
cnnlStatus_t CNNL_WIN_API
cnnlDestroyBiasDropoutAddFusedTrainBackwardDescriptor(
    cnnlBiasDropoutAddFusedTrainBackwardDescriptor_t bias_dropout_add_fused_train_backward_desc);

// Group:BiasDropoutAddFusedTrainBackward
/*!
 *  @brief Sets the BiasDropoutAddFusedTrainBackward descriptor.
 *
 *  @param[in,out] bias_dropout_add_fused_train_backward_desc
 *    Input/output. The BiasDropoutAddFusedTrainBackward descriptor.
 *  @param[in] p
 *    Input. A float value used as the probability of an element to be zeroed.
 *  @param[in] math_pre
 *    Input. Onchip computation data type of the BiasDropoutAddFusedTrainBackward operation.
 *    Currently only CNNL_DTYPE_FLOAT is supported.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - \p bias_dropout_add_fused_train_backward_desc is NULL.
 *    - \p p is less than 0 or greater than 1.
 *    - \p math_pre is illegal.
 *
 *  @par API Dependency
 *    - Before using this function, you need to call ::cnnlCreateBiasDropoutAddFusedTrainBackwardDescriptor
 *    to create a descriptor.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetBiasDropoutAddFusedTrainBackwardDescriptor(
    cnnlBiasDropoutAddFusedTrainBackwardDescriptor_t bias_dropout_add_fused_train_backward_desc,
    const float p,
    const cnnlDataType_t math_pre);

// Group:BiasDropoutAddFusedTrainBackward
/*!
 *  @brief Retrieves extra space size needed in BiasDropoutAddFusedTrainBackward
 *  operation.
 *
 *  @param[in] handle
 *    Input. Handle to the Cambricon CNNL context that is used to manage MLU devices
 *    and queues in the BiasDropoutAddFusedTrainBackward operation.
 *  @param[in] bias_dropout_add_fused_train_backward_desc
 *    Input. The descriptor of the BiasDropoutAddFusedTrainBackward operation. For detailed
 *    information, see ::cnnlBiasDropoutAddFusedTrainBackwardDescriptor_t.
 *  @param[in] diff_y_desc
 *    Input. The descriptor of the \p diff_y tensor, containing the dimension and
 *    layout of the tensor.
 *  @param[in] diff_bias_desc
 *    Input. The descriptor of the \p diff_bias tensor, containing the dimension and
 *    layout of the tensor. It can be set to NULL.
 *  @param[out] workspace_size
 *    Output. Pointer to the MLU memory that stores the extra space size (bytes)
 *    needed in the BiasDropoutAddFusedTrainBackward operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - \p handle is NULL.
 *    - \p bias_dropout_add_fused_train_backward_desc is NULL.
 *    - \p diff_y_desc is NULL.
 *    - \p workspace_size is NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetBiasDropoutAddFusedTrainBackwardWorkspaceSize(
    cnnlHandle_t handle,
    const cnnlBiasDropoutAddFusedTrainBackwardDescriptor_t
    bias_dropout_add_fused_train_backward_desc,
    const cnnlTensorDescriptor_t diff_y_desc,
    const cnnlTensorDescriptor_t diff_bias_desc,
    size_t *workspace_size);

// Group:BiasDropoutAddFusedTrainBackward
/*!
 *  @brief Performs the BiasDropoutAddFusedTrainBackward operation.
 *
 *  This function performs with the following steps:
 *
 *  1. diff_input = (1 / (1 - p)) * diff_y * mask
 *
 *  2. diff_residual = diff_y
 *
 *  3. When \p diff_bias is not NULL,
 *
 *     diff_bias = reduce(diff_input, dims=[0, 1])
 *
 *  @param[in] handle
 *    Input. Handle to the Cambricon CNNL context that is used to manage MLU devices
 *    and queues in the BiasDropoutAddFusedTrainBackward operation.
 *  @param[in] bias_dropout_add_fused_train_backward_desc
 *    Input. The descriptor of the BiasDropoutAddFusedTrainBackward operation. For detailed
 *    information, see ::cnnlBiasDropoutAddFusedTrainBackwardDescriptor_t.
 *  @param[in] diff_y_desc
 *    Input. The descriptor of the \p diff_y tensor, containing the dimension and
 *    layout of the tensor.
 *  @param[in] diff_y
 *    Input. Pointer to the MLU memory that stores the \p diff_y tensor. Its shape
 *    is [s, b, h].
 *  @param[in] mask_desc
 *    Input. The descriptor of the \p mask tensor, containing the dimension and
 *    layout of the tensor.
 *  @param[in] mask
 *    Input. Pointer to the MLU memory that stores the \p mask tensor. Its shape
 *    is [s, b, h].
 *  @param[in] workspace
 *    Input. A pointer to an extra workspace required in the operation.
 *  @param[in] workspace_size
 *    Input. The size of the workspace.
 *  @param[in] diff_input_desc
 *    Input. The descriptor of the \p diff_input tensor, containing the dimension and
 *    layout of the tensor.
 *  @param[out] diff_input
 *    Output. Pointer to the MLU memory that stores the \p diff_input tensor. Its shape
 *    is [s, b, h].
 *  @param[in] diff_residual_desc
 *    Input. The descriptor of the \p diff_residual tensor, containing the dimension and
 *    layout of the tensor.
 *  @param[out] diff_residual
 *    Output. Pointer to the MLU memory that stores the \p diff_residual tensor. Its
 *    shape is [s, b, h].
 *  @param[in] diff_bias_desc
 *    Input. The descriptor of the \p diff_bias tensor, containing the dimension and
 *    layout of the tensor.
 *  @param[out] diff_bias
 *    Output. Pointer to the MLU memory that stores the \p diff_bias tensor. Its
 *    shape is [h].
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - \p handle is NULL.
 *    - \p bias_dropout_add_fused_train_backward_desc is NULL.
 *    - \p diff_y_desc is NULL.
 *    - \p diff_y is NULL.
 *    - \p mask_desc is NULL.
 *    - \p mask is NULL.
 *    - \p workspace is NULL.
 *    - \p diff_input_desc is NULL.
 *    - \p diff_input is NULL.
 *    - \p diff_residual_desc is NULL when \p diff_residual is not NULL.
 *    - \p diff_residual is NULL when \p diff_residual_desc is not NULL.
 *    - \p diff_bias_desc is NULL when \p diff_dias is not NULL.
 *    - \p diff_bias is NULL when \p diff_bias_desc is not NULL.
 *  @retval CNNL_STATUS_ARCH_MISMATCH
 *    - This function runs on unsupported hardware platforms.
 *  @retval CNNL_STATUS_NOT_SUPPORTED
 *    - The requested functionality is not supported in this version.
 *  @par Data Type
 *   - diff_y: float, half, bfloat16.
 *   - mask: uint8, bool.
 *   - diff_input: float, half, bfloat16.
 *   - diff_residual: float, half, bfloat16.
 *   - diff_bias: float, half, bfloat16.
 *   - Data types of \p diff_y, \p diff_input, \p diff_residual and \p diff_bias must be the same.
 *
 * @note
 * - This function is supported on MLU300 and MLU500 series.
 * - \p math_pre in ::cnnlBiasDropoutAddFusedTrainBackwardDescriptor_t should be CNNL_DTYPE_FLOAT.
 * - The value of \p mask must be 0 or 1.
 *
 *  @par Example
 *  @verbatim
    Dimension of diff_y: [s, b, h]

    Dimension of mask: [s, b, h]

    Then we will get the diff_input, diff_residual and diff_bias:

    Dimension of diff_input: [s, b, h]

    Dimension of diff_residual: [s, b, h]

    Dimension of diff_bias: [h]
    @endverbatim
 */
cnnlStatus_t CNNL_WIN_API
cnnlBiasDropoutAddFusedTrainBackward(cnnlHandle_t handle,
                                     const cnnlBiasDropoutAddFusedTrainBackwardDescriptor_t
                                     bias_dropout_add_fused_train_backward_desc,
                                     const cnnlTensorDescriptor_t diff_y_desc,
                                     const void *diff_y,
                                     const cnnlTensorDescriptor_t mask_desc,
                                     void *mask,
                                     void *workspace,
                                     const size_t workspace_size,
                                     const cnnlTensorDescriptor_t diff_input_desc,
                                     void *diff_input,
                                     const cnnlTensorDescriptor_t diff_residual_desc,
                                     void *diff_residual,
                                     const cnnlTensorDescriptor_t diff_bias_desc,
                                     void *diff_bias);

/******************************************************************************
 * Cambricon CNNL OP: BiasActivationGluForward and BiasActivationGlubackward
 ******************************************************************************/

/*!
 * @brief Enumeration variables describing the algorithms that are used in the
 * implementation of the BiasActivationGluForward operation.
 *
 * Implements with formula: ``input_1, input_2 = (input + bias).chunk(2, dim=-1)``
 * algo_v1: ``output = input_1 * activation(input_2)``
 * algo_v2: ``output = activation(input_1) * input_2``
 */
typedef enum {
  CNNL_BIAS_ACTIVATION_GLU_ALGO_V1 = 0,
  CNNL_BIAS_ACTIVATION_GLU_ALGO_V2 = 1
} cnnlBiasActivationGluAlgo_t;

/*! The descriptor of the BiasActivationGlu forward or backward operation.
 *
 *  You need to call the ::cnnlCreateBiasActivationGluDescriptor function
 *  to create a descriptor, and call the ::cnnlSetBiasActivationGluDescriptor
 *  function to set the tensor information to the descriptor.
 *  Also, you need to destroy the descriptor at the end with the
 *  ::cnnlDestroyBiasActivationGluDescriptor function.
 */
typedef struct cnnlBiasActivationGluStruct *cnnlBiasActivationGluDescriptor_t;

/*!
 *  @brief Creates a descriptor pointed by \p desc
 *   for the BiasActivationGlu forward or backward operation.
 *  @param[out] desc
 *    Output. Descriptor of the BiasActivationGlu forward or backward operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_ALLOC_FAILED
 *    The function failed to allocate memory for the BiasActivationGlu descriptor.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCreateBiasActivationGluDescriptor(cnnlBiasActivationGluDescriptor_t *desc);

/*!
 *  @brief Destroys the descriptor of the BiasActivationGlu forward or backward operation.
 *
 *  @param[in] desc
 *    Input. Descriptor of the BiasActivationGlu forward or backward operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p desc is NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlDestroyBiasActivationGluDescriptor(cnnlBiasActivationGluDescriptor_t desc);

/*!
 *  @brief Sets the descriptor of the BiasActivationGlu forward or backward operation with values.
 *
 *  @param[in,out] desc
 *    Input/output. Descriptor of the BiasActivationGlu forward or backward operation.
 *  @param[in] activation_desc
 *    Input. The descriptor of the activation in BiasActivationGlu forward or backward operation.
 *  @param[in] algo
 *    Input.  The algorithm of the BiasActivationGlu operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p desc is NULL, or one or more parameters are not supported.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetBiasActivationGluDescriptor(cnnlBiasActivationGluDescriptor_t desc,
                                   const cnnlActivationDescriptor_t activation_desc,
                                   const cnnlBiasActivationGluAlgo_t algo);

// Group: BiasActivationGluForward
/*!
*  @brief Performs the GEGLU or SwiGLU operation.
*
*  @deprecated
*    This function is deprecated and will be removed in future release.
*    Use ::cnnlBiasActivationGluForward_v2 instead.
*
*  @param[in] handle
*    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
*  @param[in] activation_desc
*    Input. The descriptor of activation.
*  @param[in] input_desc
*    Input. The descriptor of the input tensor. The shape is [..., 2d].
*  @param[in] input
*    Input. A pointer to the MLU memory that stores the input data.
*  @param[in] bias_desc
*    Input. The descriptor of the bias tensor. The shape is [2d].
*           \p bias_desc can be nullptr.
*  @param[in] bias
*    Input. A pointer to the MLU memory that stores the bias data.
*           \p bias can be nullptr.
*  @param[in] output_desc
*    Input. The descriptor of the output tensor. The shape is [..., d].
*  @param[out] output
*    Output. A pointer to the MLU memory that stores the output data.
*  @retval CNNL_STATUS_SUCCESS
*    The function ends normally.
*  @retval CNNL_STATUS_BAD_PARAM
*    One or more of the following conditions are encountered:
*    - \p handle, \p input_desc, \p output_desc or \p activation_desc is NULL.
*    - Tensor's dim, shape or data type is invalid.
*    - \p activation_desc is invalid.
*    - The quantitative relation of tensors' shape is invalid.
*  @retval CNNL_STATUS_NOT_SUPPORTED
*    - A tensor number is equal to or larger than 2G on MLU300 series.
*    - Data type bfloat16 is unsupported on MLU300 series.
*
*  @par API Dependency
*  - Before using this function, you need to call cnnlCreateActivationDescriptor
*    to create a descriptor.
*
*  @par Data Type
*  - Data types of \p input, \p bias and \p output must be the same,
*    which support half, bfloat16 and float.
*
*  @note
*  - Data type bfloat16 is only supported on MLU590.
*/
CNNL_DEPRECATED_FOR(cnnlBiasActivationGluForward_v2)
cnnlStatus_t CNNL_WIN_API
cnnlBiasActivationGluForward(cnnlHandle_t handle,
                             const cnnlActivationDescriptor_t activation_desc,
                             const cnnlTensorDescriptor_t input_desc,
                             const void *input,
                             const cnnlTensorDescriptor_t bias_desc,
                             const void *bias,
                             const cnnlTensorDescriptor_t output_desc,
                             void *output);

// Group: BiasActivationGluForward
/*!
*  @brief Performs the GEGLU or SwiGLU operation.
*
*  @param[in] handle
*    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
*  @param[in] compute_desc
*    Input. The descriptor of the BiasActivationGlu forward or backward operation.
*  @param[in] input_desc
*    Input. The descriptor of the input tensor. The shape is [..., 2d].
*  @param[in] input
*    Input. A pointer to the MLU memory that stores the input data.
*  @param[in] bias_desc
*    Input. The descriptor of the bias tensor. The shape is [2d].
*           \p bias_desc can be nullptr.
*  @param[in] bias
*    Input. A pointer to the MLU memory that stores the bias data.
*           \p bias can be nullptr.
*  @param[in] output_desc
*    Input. The descriptor of the output tensor. The shape is [..., d].
*  @param[out] output
*    Output. A pointer to the MLU memory that stores the output data.
*  @retval CNNL_STATUS_SUCCESS
*    The function ends normally.
*  @retval CNNL_STATUS_BAD_PARAM
*    One or more of the following conditions are encountered:
*    - \p handle, \p input_desc, \p output_desc or \p compute_desc is NULL.
*    - Tensor's dim, shape or data type is invalid.
*    - \p compute_desc is invalid.
*    - The quantitative relation of tensors' shape is invalid.
*  @retval CNNL_STATUS_NOT_SUPPORTED
*    - A tensor number is equal to or larger than 2G on MLU300 series.
*    - Data type bfloat16 is unsupported on MLU300 series.
*
*  @par API Dependency
*  - Before using this function, you need to call ::cnnlCreateBiasActivationGluDescriptor
*    to create a descriptor.
*
*  @par Data Type
*  - Data types of \p input, \p bias and \p output must be the same,
*    which support half, bfloat16 and float.
*
*  @note
*  - Data type bfloat16 is only supported on MLU590.
*/
cnnlStatus_t CNNL_WIN_API
cnnlBiasActivationGluForward_v2(cnnlHandle_t handle,
                                const cnnlBiasActivationGluDescriptor_t compute_desc,
                                const cnnlTensorDescriptor_t input_desc,
                                const void *input,
                                const cnnlTensorDescriptor_t bias_desc,
                                const void *bias,
                                const cnnlTensorDescriptor_t output_desc,
                                void *output);

/******************************************************************************
 * Cambricon CNNL OP: BiasActivationGluBackward
 ******************************************************************************/
// Group:BiasActivationGluBackward
/*!
 *  @brief Retrieves extra space size needed in the BiasActivationGluBackward
 *  operation.
 *
 *  @param[in] handle
 *    Input. Handle to the Cambricon CNNL context that is used to manage MLU devices
 *    and queues in the BiasActivationGluBackward operation.
 *  @param[in] input_desc
 *    Input. The descriptor of the \p input tensor, containing the dimension and
 *    layout of the tensor.
 *  @param[in] grad_output_desc
 *    Input. The descriptor of the \p grad_output tensor, containing the dimension and
 *    layout of the tensor.
 *  @param[in] bias_desc
 *    Input. The descriptor of the \p bias tensor, containing the dimension and
 *    layout of the tensor.
 *  @param[out] workspace_size
 *    Output. Pointer to the CPU memory that stores the extra space size (bytes)
 *    needed in BiasActivationGluBackward operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - \p handle is NULL.
 *    - \p input_desc is NULL.
 *    - \p grad_output_desc is NULL.
 *    - \p workspace_size is NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetBiasActivationGluBackwardWorkspaceSize(cnnlHandle_t handle,
                                              const cnnlTensorDescriptor_t input_desc,
                                              const cnnlTensorDescriptor_t grad_output_desc,
                                              const cnnlTensorDescriptor_t bias_desc,
                                              size_t *workspace_size);
// Group:BiasActivationGluBackward
/*!
 * @brief Performs the BiasActivationGluBackward operation.
 *
 * @deprecated
 *   This function is deprecated and will be removed in future release.
 *   Use ::cnnlBiasActivationGluBackward_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to the Cambricon CNNL context that is used to manage MLU devices
 *   and queues in the BiasActivationGluBackward operation.
 * @param[in] activation_desc
 *   Input. The descriptor of the activation in BiasActivationGluBackward operation.
 *   For detailed information, see cnnlActivationDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the \p input tensor. For detailed information, see cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. A pointer to the MLU memory that stores the \p input tensor.
 * @param[in] grad_output_desc
 *   Input. The descriptor of the \p grad_output tensor. For detailed information, see cnnlTensorDescriptor_t.
 * @param[in] grad_output
 *   Input. A pointer to the MLU memory that stores the \p grad_output tensor.
 * @param[in] bias_desc
 *   Input. The descriptor of the \p bias tensor. For detailed information, see cnnlTensorDescriptor_t.
 * @param[in] bias
 *   Input. A pointer to the MLU memory that stores the \p bias tensor.
 * @param[in] workspace
 *   Input. A pointer to an extra workspace required in the operation.
 * @param[in] workspace_size
 *   Input. The size of the workspace. You can get the size of the workspace with
 *   the ::cnnlGetBiasActivationGluBackwardWorkspaceSize function.
 * @param[in] grad_input_desc
 *   Input. The descriptor of the \p grad_input tensor. For detailed information, see cnnlTensorDescriptor_t.
 * @param[out] grad_input
 *   Output. A pointer to the MLU memory that stores the \p grad_input tensor.
 * @param[in] grad_bias_desc
 *   Input. The descriptor of the \p grad_bias tensor. For detailed information, see cnnlTensorDescriptor_t.
 * @param[out] grad_bias
 *   Output. A pointer to the MLU memory that stores the \p grad_bias tensor.
 * @retval CNNL_STATUS_SUCCESS
 *   The function ends normally.
 * @retval CNNL_STATUS_BAD_PARAM
 *  One or more of the following conditions are met:
 *  - \p handle is NULL.
 *  - \p activation_desc is NULL.
 *  - The descriptors or data pointers are NULL.
 * @retval CNNL_STATUS_NOT_SUPPORTED
 *  - The requested functionality is not supported in this version.
 *
 * @par Data Type
 *  - This function supports the following data types:
 *    - input tensor: float32, float16, bfloat16
 *    - grad_output tensor: float32, float16, bfloat16
 *    - bias tensor: float32, float16, bfloat16
 *    - grad_input tensor: float32, float16, bfloat16
 *    - grad_bias tensor: float32, float16, bfloat16
 *  - The data type of \p input, \p grad_output, \p bias, \p grad_input and \p grad_bias must be the same.
 *
 * @par Data Layout
 * - input tensor: CNNL_LAYOUT_ARRAY.
 * - grad_output tensor : CNNL_LAYOUT_ARRAY.
 * - bias tensor: CNNL_LAYOUT_ARRAY.
 * - grad_input tensor : CNNL_LAYOUT_ARRAY.
 * - grad_bias tensor : CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 *  - The shape of \p input and \p grad_input should be consistent.
 *  - The number of dimensions in \p input, \p grad_output and \p grad_input must be the same, and must be larger than 1.
 *  - The last dimension size of \p input and \p grad_input must be 2x larger than the last dimension size of \p grad_output.
 *  - When \p bias and \p grad_bias are not NULL, \p bias and \p grad_bias must be a 1-d tensor.
 *    The dimension size of \p bias and \p grad_bias must be the same, and must be equal to the last dimension size of \p input.
 *  - The activation mode only supports \p CNNL_ACTIVATION_GELU and \p CNNL_ACTIVATION_SILU currently.
 *  - The activation prefer mode only supports \p CNNL_COMPUTATION_HIGH_PRECISION currently.
 *  - When the activation mode is \p CNNL_ACTIVATION_GELU,
 *    the Boolean parameter \p approximate of \p activation_desc must be set to false currently.
 *
 * @note
 * - Only MLU300 and MLU500 series are supported.
 * - Data type bfloat16 is not supported on MLU300 series.
 * - The tensor stride feature is not supported currently.
 * - The tensor which data contains NaN/infinity is not supported currently.
 * - When \p bias is NULL, \p grad_bias must be NULL.
 * - When \p bias is not NULL, \p grad_bias must be not NULL.
 *
 * @par API Dependency
 *   - Before using this function, you need to call cnnlCreateActivationDescriptor
 *     function to create an activation descriptor, and call the cnnlSetActivationDescriptor_v6
 *     function to set the attributes. Also, you need to call the cnnlDestroyActivationDescriptor function
 *     to destroy the activation descriptor at the end.
 *   - You need to call the ::cnnlGetBiasActivationGluBackwardWorkspaceSize function before calling this function.
 *
 * @par Example
 * @verbatim
    Dimension of input: [M, 2 * N]

    Dimension of grad_output: [M, N]

    Dimension of bias: [2 * N]

    Then we will get the grad_input and grad_bias:

    Dimension of grad_input: [M, 2 * N]

    Dimension of grad_bias: [2 * N]
    @endverbatim
 */
CNNL_DEPRECATED_FOR(cnnlBiasActivationGluBackward_v2)
cnnlStatus_t CNNL_WIN_API
cnnlBiasActivationGluBackward(cnnlHandle_t handle,
                              cnnlActivationDescriptor_t activation_desc,
                              const cnnlTensorDescriptor_t input_desc,
                              const void *input,
                              const cnnlTensorDescriptor_t grad_output_desc,
                              const void *grad_output,
                              const cnnlTensorDescriptor_t bias_desc,
                              const void *bias,
                              void *workspace,
                              size_t workspace_size,
                              const cnnlTensorDescriptor_t grad_input_desc,
                              void *grad_input,
                              const cnnlTensorDescriptor_t grad_bias_desc,
                              void *grad_bias);

// Group:BiasActivationGluBackward
/*!
 * @brief Performs the BiasActivationGluBackward operation.
 *
 * Compared with ::cnnlBiasActivationGluBackward, this function allows you to choose different algorithm
 * by setting the parameter \p algo of \p bias_activation_glu_desc.
 *
 * @param[in] handle
 *   Input. Handle to the Cambricon CNNL context that is used to manage MLU devices
 *   and queues in the BiasActivationGluBackward operation.
 * @param[in] bias_activation_glu_desc
 *   Input. The descriptor of BiasActivationGlu operation.
 *   For detailed information, see cnnlBiasActivationGluDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the \p input tensor. For detailed information, see cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. A pointer to the MLU memory that stores the \p input tensor.
 * @param[in] grad_output_desc
 *   Input. The descriptor of the \p grad_output tensor. For detailed information, see cnnlTensorDescriptor_t.
 * @param[in] grad_output
 *   Input. A pointer to the MLU memory that stores the \p grad_output tensor.
 * @param[in] bias_desc
 *   Input. The descriptor of the \p bias tensor. For detailed information, see cnnlTensorDescriptor_t.
 * @param[in] bias
 *   Input. A pointer to the MLU memory that stores the \p bias tensor.
 * @param[in] workspace
 *   Input. A pointer to an extra workspace required in the operation.
 * @param[in] workspace_size
 *   Input. The size of the workspace. You can get the size of the workspace with
 *   the ::cnnlGetBiasActivationGluBackwardWorkspaceSize function.
 * @param[in] grad_input_desc
 *   Input. The descriptor of the \p grad_input tensor. For detailed information, see cnnlTensorDescriptor_t.
 * @param[out] grad_input
 *   Output. A pointer to the MLU memory that stores the \p grad_input tensor.
 * @param[in] grad_bias_desc
 *   Input. The descriptor of the \p grad_bias tensor. For detailed information, see cnnlTensorDescriptor_t.
 * @param[out] grad_bias
 *   Output. A pointer to the MLU memory that stores the \p grad_bias tensor.
 * @retval CNNL_STATUS_SUCCESS
 *   The function ends normally.
 * @retval CNNL_STATUS_BAD_PARAM
 *  One or more of the following conditions are met:
 *  - \p handle is NULL.
 *  - \p bias_activation_glu_desc is NULL.
 *  - The descriptors or data pointers are NULL.
 * @retval CNNL_STATUS_NOT_SUPPORTED
 *  - The requested functionality is not supported in this version.
 *
 * @par Data Type
 *  - This function supports the following data types:
 *    - input tensor: float32, float16, bfloat16
 *    - grad_output tensor: float32, float16, bfloat16
 *    - bias tensor: float32, float16, bfloat16
 *    - grad_input tensor: float32, float16, bfloat16
 *    - grad_bias tensor: float32, float16, bfloat16
 *  - The data type of \p input, \p grad_output, \p bias, \p grad_input and \p grad_bias must be the same.
 *
 * @par Data Layout
 * - input tensor: CNNL_LAYOUT_ARRAY.
 * - grad_output tensor: CNNL_LAYOUT_ARRAY.
 * - bias tensor: CNNL_LAYOUT_ARRAY.
 * - grad_input tensor: CNNL_LAYOUT_ARRAY.
 * - grad_bias tensor: CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 *  - The shape of \p input and \p grad_input should be consistent.
 *  - The number of dimensions in \p input, \p grad_output and \p grad_input must be the same, and must be larger than 1.
 *  - The last dimension size of \p input and \p grad_input must be 2x larger than the last dimension size of \p grad_output.
 *  - When \p bias and \p grad_bias are not NULL, \p bias and \p grad_bias must be a 1-d tensor.
 *    The dimension size of \p bias and \p grad_bias must be the same, and must be equal to the last dimension size of \p input.
 *  - The activation mode only supports \p CNNL_ACTIVATION_GELU and \p CNNL_ACTIVATION_SILU currently.
 *  - The activation prefer mode only supports \p CNNL_COMPUTATION_HIGH_PRECISION currently.
 *  - When the activation mode is \p CNNL_ACTIVATION_GELU,
 *    the Boolean parameter \p approximate of \p activation_desc in \p bias_activation_glu_desc must be set to false currently.
 *
 * @note
 * - Only MLU300 and MLU500 series are supported.
 * - Data type bfloat16 is not supported on MLU300 series.
 * - The tensor stride feature is not supported currently.
 * - Tensors whose data contains NaN or infinity are not supported currently.
 * - When \p bias is NULL, \p grad_bias must be NULL.
 * - When \p bias is not NULL, \p grad_bias must be not NULL.
 *
 * @par API Dependency
 *   - Before using this function, you need to call ::cnnlCreateBiasActivationGluDescriptor
 *     function to create a descriptor, and call the ::cnnlSetBiasActivationGluDescriptor
 *     function to set the attributes. Also, you need to call the ::cnnlDestroyBiasActivationGluDescriptor function
 *     to destroy the descriptor at the end.
 *   - You need to call the ::cnnlGetBiasActivationGluBackwardWorkspaceSize function before calling this function.
 *
 * @par Example
 * @verbatim
    Dimension of input: [M, 2 * N]

    Dimension of grad_output: [M, N]

    Dimension of bias: [2 * N]

    Then we will get the grad_input and grad_bias:

    Dimension of grad_input: [M, 2 * N]

    Dimension of grad_bias: [2 * N]
    @endverbatim
 */
cnnlStatus_t CNNL_WIN_API
cnnlBiasActivationGluBackward_v2(cnnlHandle_t handle,
                                 const cnnlBiasActivationGluDescriptor_t bias_activation_glu_desc,
                                 const cnnlTensorDescriptor_t input_desc,
                                 const void *input,
                                 const cnnlTensorDescriptor_t grad_output_desc,
                                 const void *grad_output,
                                 const cnnlTensorDescriptor_t bias_desc,
                                 const void *bias,
                                 void *workspace,
                                 size_t workspace_size,
                                 const cnnlTensorDescriptor_t grad_input_desc,
                                 void *grad_input,
                                 const cnnlTensorDescriptor_t grad_bias_desc,
                                 void *grad_bias);

// Group:FwFFMForward
/*!
 * @brief Performs the FwFFMForward operation.
 *
 * @param[in] handle
 *   Input. Handle to the Cambricon CNNL context that is used to manage MLU devices and queues.
 * @param[in] weight_desc
 *   Input. Descriptor of \p weight tensor. The shape of \p weight should be [S, K, C].
 * @param[in] weight
 *   Input. A pointer to the MLU memory that stores the \p weight tensor.
 * @param[in] fw_weight_desc
 *   Input. Descriptor of \p fw_weight tensor. The shape of \p fw_weight should be [F*(F+1)/2].
 * @param[in] fw_weight
 *   Input. A pointer to the MLU memory that stores the \p fw_weight tensor.
 * @param[in] field_desc
 *   Input. Descriptor of \p field tensor. The shape of \p field should be [S, 2].
 * @param[in] field
 *   Input. A pointer to the MLU memory that stores the \p field tensor.
 * @param[in] index_desc
 *   Input. Descriptor of \p index tensor. The shape of \p index should be [S+1].
 * @param[in] index
 *   Input. A pointer to the MLU memory that stores the \p index tensor.
 * @param[in] output_desc
 *   Input. Descriptor of \p output tensor. The shape of \p output should be [B, C].
 * @param[out] output
 *   Output. A pointer to the MLU memory that stores the \p output tensor.
 * @param[in] sum_field_desc
 *   Input. Descriptor of \p sum_field tensor. The shape of \p sum_field should be [B, K, F, C].
 * @param[out] sum_field
 *   Output. A pointer to the MLU memory that stores the \p sum_field tensor.
 * @param[in] ssum_field_desc
 *   Input. Descriptor of \p ssum_field tensor. The shape of \p ssum_field should be [B, K, F, C].
 * @param[out] ssum_field
 *   Output. A pointer to the MLU memory that stores the \p ssum_field tensor.
 * @param[in] field_map_desc
 *   Input. Descriptor of \p field_map tensor. The shape of \p field_map should be [B, F].
 * @param[out] field_map
 *   Output. A pointer to the MLU memory that stores the \p field_map tensor.
 *
 * @par Return
 *  - CNNL_STATUS_SUCCESS, CNNL_STATUS_BAD_PARAM, CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 *  - This function supports the following data types:
 *    - weight tensor: float32
 *    - field tensor: int32
 *    - fw_weight tensor: float32
 *    - index tensor: int32
 *    - output tensor: float32
 *    - sum_field tensor: float32
 *    - ssum_field tensor: float32
 *    - fw_field_map tensor: int32
 *
 * @par Data Layout
 * - All tensors: CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 *  - weight_desc->dims[1] should be in range of [1, 2].
 *  - weight_desc->dims[2] should be in range of [1, 192].
 *  - sum_field_desc->dims[2] should be in range of [1, 100].
 *  - output_desc->dims[0] should be in range of [1, 10000].
 *
 * @note
 * - Only MLU500 series are supported.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlFwFFMForward(cnnlHandle_t handle,
                                           const cnnlTensorDescriptor_t weight_desc,
                                           const void *weight,
                                           const cnnlTensorDescriptor_t fw_weight_desc,
                                           const void *fw_weight,
                                           const cnnlTensorDescriptor_t field_desc,
                                           const void *field,
                                           const cnnlTensorDescriptor_t index_desc,
                                           const void *index,
                                           const cnnlTensorDescriptor_t output_desc,
                                           void *output,
                                           const cnnlTensorDescriptor_t sum_field_desc,
                                           void *sum_field,
                                           const cnnlTensorDescriptor_t ssum_field_desc,
                                           void *ssum_field,
                                           const cnnlTensorDescriptor_t field_map_desc,
                                           void *field_map);

// Group:FwFFMBackward
/*!
 * @brief Retrieves the extra workspace size needed in ::cnnlFwFFMBackward operation.
 *
 * @param[in] handle
 *   Input. Handle to the Cambricon CNNL context that is used to manage MLU devices and queues.
 * @param[in] weight_desc
 *   Input. The descriptor of the \p weight tensor. For detailed information, see cnnlTensorDescriptor_t.
 * @param[in] fw_weight_desc
 *   Input. The descriptor of the \p fw_weight tensor. For detailed information, see cnnlTensorDescriptor_t.
 * @param[in] field_desc
 *   Input. The descriptor of the \p field tensor. For detailed information, see cnnlTensorDescriptor_t.
 * @param[in] index_desc
 *   Input. The descriptor of the \p index tensor. For detailed information, see cnnlTensorDescriptor_t.
 * @param[in] cross_mean_sum_desc
 *   Input. The descriptor of the \p cross_mean_sum tensor. For detailed information, see cnnlTensorDescriptor_t.
 * @param[in] cross_mean_square_sum_desc
 *   Input. The descriptor of the \p cross_mean_square_sum tensor. For detailed information, see cnnlTensorDescriptor_t.
 * @param[in] fw_field_map_desc
 *   Input. The descriptor of the \p fw_field_map tensor. For detailed information, see cnnlTensorDescriptor_t.
 * @param[in] grad_desc
 *   Input. The descriptor of the \p grad tensor. For detailed information, see cnnlTensorDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of the \p output tensor. For detailed information, see cnnlTensorDescriptor_t.
 * @param[in] fw_output_desc
 *   Input. The descriptor of the \p fw_output tensor. For detailed information, see cnnlTensorDescriptor_t.
 * @param[out] size_workspace
 *   Output. The size of the workspace.
 * @retval CNNL_STATUS_SUCCESS
 *   The function ends normally.
 * @retval CNNL_STATUS_BAD_PARAM
 *   One or more required parameters are NULL.
 *
 *   For other limitations, see the constraints of the ::cnnlFwFFMBackward operation.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetFwFFMBackwardWorkspaceSize(cnnlHandle_t handle,
                                  const cnnlTensorDescriptor_t weight_desc,
                                  const cnnlTensorDescriptor_t fw_weight_desc,
                                  const cnnlTensorDescriptor_t field_desc,
                                  const cnnlTensorDescriptor_t index_desc,
                                  const cnnlTensorDescriptor_t cross_mean_sum_desc,
                                  const cnnlTensorDescriptor_t cross_mean_square_sum_desc,
                                  const cnnlTensorDescriptor_t fw_field_map_desc,
                                  const cnnlTensorDescriptor_t grad_desc,
                                  const cnnlTensorDescriptor_t output_desc,
                                  const cnnlTensorDescriptor_t fw_output_desc,
                                  size_t *size_workspace);
// Group:FwFFMBackward
/*!
 * @brief Converts one channel of sparse weights to fwffm embedding output.
 *
 * @param[in] handle
 *   Input. Handle to the Cambricon CNNL context that is used to manage MLU devices and queues.
 * @param[in] weight_desc
 *   Input. The descriptor of the \p weight tensor. For detailed information, see cnnlTensorDescriptor_t.
 * @param[in] weight
 *   Input. A pointer to the MLU memory that stores the \p weight tensor.
 * @param[in] fw_weight_desc
 *   Input. The descriptor of the \p fw_weight tensor. For detailed information, see cnnlTensorDescriptor_t.
 * @param[in] fw_weight
 *   Input. A pointer to the MLU memory that stores the \p fw_weight tensor.
 * @param[in] field_desc
 *   Input. The descriptor of the \p field tensor. For detailed information, see cnnlTensorDescriptor_t.
 * @param[in] field
 *   Input. A pointer to the MLU memory that stores the \p field tensor.
 * @param[in] index_desc
 *   Input. The descriptor of the \p index tensor. For detailed information, see cnnlTensorDescriptor_t.
 * @param[in] index
 *   Input. A pointer to the MLU memory that stores the \p index tensor.
 * @param[in] cross_mean_sum_desc
 *   Input. The descriptor of the \p cross_mean_sum tensor. For detailed information, see cnnlTensorDescriptor_t.
 * @param[in] cross_mean_sum
 *   Input. A pointer to the MLU memory that stores the \p cross_mean_sum tensor.
 * @param[in] cross_mean_square_sum_desc
 *   Input. The descriptor of the \p cross_mean_square_sum tensor. For detailed information, see cnnlTensorDescriptor_t.
 * @param[in] cross_mean_square_sum
 *   Input. A pointer to the MLU memory that stores the \p cross_mean_square_sum tensor.
 * @param[in] fw_field_map_desc
 *   Input. The descriptor of the \p fw_field_map tensor. For detailed information, see cnnlTensorDescriptor_t.
 * @param[in] fw_field_map
 *   Input. A pointer to the MLU memory that stores the \p fw_field_map tensor.
 * @param[in] grad_desc
 *   Input. The descriptor of the \p grad tensor. For detailed information, see cnnlTensorDescriptor_t.
 * @param[in] grad
 *   Input. A pointer to the MLU memory that stores the \p grad tensor.
 * @param[in] workspace
 *   Input. A pointer to an extra workspace required in the operation.
 * @param[in] workspace_size
 *   Input. The size of the workspace. You can get the size of the workspace with
 *   the ::cnnlGetFwFFMBackwardWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the \p output tensor. For detailed information, see cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. A pointer to the MLU memory that stores the \p output tensor.
 * @param[in] fw_output_desc
 *   Input. The descriptor of the \p fw_output tensor. For detailed information, see cnnlTensorDescriptor_t.
 * @param[out] fw_output
 *   Output. A pointer to the MLU memory that stores the \p fw_output tensor.
 *
 * @par Return
 *  - CNNL_STATUS_SUCCESS, CNNL_STATUS_BAD_PARAM, CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 *  - This function supports the following data types:
 *    - weight tensor: float32
 *    - fw_weight tensor: float32
 *    - field tensor: int32
 *    - index tensor: int32
 *    - cross_mean_sum tensor: float32
 *    - cross_mean_square_sum tensor: float32
 *    - fw_field_map tensor: int32
 *    - grad tensor: float32
 *    - output tensor: float32
 *    - fw_output tensor: float32
 *
 * @par Data Layout
 * - all tensor: CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 *  - grad_desc->dim[0] should be in range of [1, 10K].
 *  - weight_desc->dim[0] should be in range of [1, 30M].
 *  - weight_desc->dim[1] should be in range of [1, 2].
 *  - weight_desc->dim[2] should be in range of [1, 192].
 *  - cross_mean_sum_desc->dim[2] should be in range of [1, 100].
 *
 * @note
 * - Only MLU500 series are supported.
 *
 * @par Example
 *  None.
 */
cnnlStatus_t CNNL_WIN_API cnnlFwFFMBackward(cnnlHandle_t handle,
                                            const cnnlTensorDescriptor_t weight_desc,
                                            const void *weight,
                                            const cnnlTensorDescriptor_t fw_weight_desc,
                                            const void *fw_weight,
                                            const cnnlTensorDescriptor_t field_desc,
                                            const void *field,
                                            const cnnlTensorDescriptor_t index_desc,
                                            const void *index,
                                            const cnnlTensorDescriptor_t cross_mean_sum_desc,
                                            const void *cross_mean_sum,
                                            const cnnlTensorDescriptor_t cross_mean_square_sum_desc,
                                            const void *cross_mean_square_sum,
                                            const cnnlTensorDescriptor_t fw_field_map_desc,
                                            const void *fw_field_map,
                                            const cnnlTensorDescriptor_t grad_desc,
                                            const void *grad,
                                            void *workspace,
                                            size_t workspace_size,
                                            const cnnlTensorDescriptor_t output_desc,
                                            void *output,
                                            const cnnlTensorDescriptor_t fw_output_desc,
                                            void *fw_output);

/******************************************************************************
 * Cambricon CNNL OP: ScaledDotProductAttn
 ******************************************************************************/
/*! The descriptor of ::cnnlScaledDotProductAttn operation. Reserved for future use.
 */
typedef struct cnnlScaledDotProductAttnStruct *cnnlScaledDotProductAttnDescriptor_t;

/*! The descriptor of ::cnnlScaledDotProductAttn operation that holds quantization information.
 * Reserved for future use.
 */
typedef struct cnnlScaledDotProductAttnQuantizeStruct *cnnlScaledDotProductAttnQuantizeDescriptor_t;

// Group:ScaledDotProductAttn
/*!
 *  @brief Retrieves extra memory needed in ::cnnlScaledDotProductAttn operation.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlGetScaledDotProductAttnWorkspaceSize_v3 instead.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
 *  @param[in]  desc
 *    Input. Descriptor of ::cnnlScaledDotProductAttn operation. Reserved for future use.
 *  @param[in]  quant_desc
 *    Input. Descriptor of ::cnnlScaledDotProductAttn operation including quantization related information. Reserved for future use.
 *  @param[in]  query_desc
 *    Input. Descriptor of \p query tensor.
 *  @param[in]  key_desc
 *    Input. Descriptor of \p key tensor.
 *  @param[in]  value_desc
 *    Input. Descriptor of \p value tensor.
 *  @param[in]  cu_seqlens_q_desc
 *    Input. Descriptor of \p cu_seqlens_q tensor.
 *  @param[in]  cu_seqlens_k_desc
 *    Input. Descriptor of \p cu_seqlens_k tensor.
 *  @param[in]  attn_bias_desc
 *    Input. Descriptor of \p attn_bias tensor. It can be nullptr.
 *  @param[in]  alibi_slopes_desc
 *    Input. Descriptor of \p alibi_slopes tensor. It can be nullptr.
 *  @param[in]  max_seq_q
 *    Input. The maximum of \p seq_len_q.
 *  @param[in]  max_seq_k
 *    Input. The maximum of \p seq_len_k.
 *  @param[in]  causal
 *    Input. A Boolean value indicating whether to apply causal attention.
 *  @param[in]  window_size_left
 *    Input. The left boundary of sliding window local attention. It should be used with \p window_size_right.
 *  @param[in]  window_size_right
 *    Input. The right boundary of sliding window local attention. It should be used with \p window_size_left.
 *  @param[in]  act_pref
 *    Input. The indicator of activation operation preference, including:
 *      - CNNL_ACTIVATION_FAST: Use high-performance for activation operations.
 *      - CNNL_ACTIVATION_HIGH_PRECISION: Use high-precision for activation operations.
 *  @param[in]  compute_dtype
 *    Input. Data type to be used during computation.
 *    - If data type of \p query is float, \p compute_dtype must be float.
 *    - If data type of \p query is half, \p compute_dtype can be half or float.
 *    - If data type of \p query is bfloat16, \p compute_dtype can be half, bfloat16 or float, but the operator will use float as the actual \p compute_dtype automatically.
 *  @param[out] size
 *    Output. Size of workspace.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more required parameters are NULL.
 */
CNNL_DEPRECATED_FOR(cnnlGetScaledDotProductAttnWorkspaceSize_v3)
cnnlStatus_t CNNL_WIN_API
cnnlGetScaledDotProductAttnWorkspaceSize(cnnlHandle_t handle,
                                         cnnlScaledDotProductAttnDescriptor_t desc,
                                         cnnlScaledDotProductAttnQuantizeDescriptor_t quant_desc,
                                         cnnlTensorDescriptor_t query_desc,
                                         cnnlTensorDescriptor_t key_desc,
                                         cnnlTensorDescriptor_t value_desc,
                                         cnnlTensorDescriptor_t cu_seqlens_q_desc,
                                         cnnlTensorDescriptor_t cu_seqlens_k_desc,
                                         cnnlTensorDescriptor_t attn_bias_desc,
                                         cnnlTensorDescriptor_t alibi_slopes_desc,
                                         const int max_seq_q,
                                         const int max_seq_k,
                                         const bool causal,
                                         const int window_size_left,
                                         const int window_size_right,
                                         cnnlActivationPreference_t act_pref,
                                         cnnlDataType_t compute_dtype,
                                         size_t *size);

// Group:ScaledDotProductAttn
/*!
 *  @brief Retrieves extra memory needed in ::cnnlScaledDotProductAttn_v3 operation. Compared with ::cnnlGetScaledDotProductAttnWorkspaceSize,
 *  this API adds supports for two new input parameters, which are \p block_tables_desc and \p return_softmax_lse.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlGetScaledDotProductAttnWorkspaceSize_v3 instead.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
 *  @param[in]  desc
 *    Input. Descriptor of ::cnnlScaledDotProductAttn operation. Reserved for future use.
 *  @param[in]  quant_desc
 *    Input. Descriptor of ::cnnlScaledDotProductAttn operation including quantization related information. Reserved for future use.
 *  @param[in]  query_desc
 *    Input. Descriptor of \p query tensor.
 *  @param[in]  key_desc
 *    Input. Descriptor of \p key tensor.
 *  @param[in]  value_desc
 *    Input. Descriptor of \p value tensor.
 *  @param[in]  cu_seqlens_q_desc
 *    Input. Descriptor of \p cu_seqlens_q tensor.
 *  @param[in]  cu_seqlens_k_desc
 *    Input. Descriptor of \p cu_seqlens_k tensor.
 *  @param[in]  attn_bias_desc
 *    Input. Descriptor of \p attn_bias tensor. It can be nullptr.
 *  @param[in]  alibi_slopes_desc
 *    Input. Descriptor of \p alibi_slopes tensor. It can be nullptr.
 *  @param[in]  block_tables_desc
 *    Input. Descriptor of \p block_tables tensor. It can be nullptr.
 *  @param[in]  max_seq_q
 *    Input. The maximum of \p seq_len_q.
 *  @param[in]  max_seq_k
 *    Input. The maximum of \p seq_len_k.
 *  @param[in]  causal
 *    Input. A Boolean value indicating whether to apply causal attention.
 *  @param[in]  window_size_left
 *    Input. The left boundary of sliding window local attention. It should be used with \p window_size_right.
 *  @param[in]  window_size_right
 *    Input. The right boundary of sliding window local attention. It should be used with \p window_size_left.
 *  @param[in]  return_softmax_lse
 *    Input. A Boolean value indicating whether to output the log of the softmax normalization factor.
 *  @param[in]  act_pref
 *    Input. The indicator of activation operation preference, including:
 *      - CNNL_ACTIVATION_FAST: Use high-performance for activation operations.
 *      - CNNL_ACTIVATION_HIGH_PRECISION: Use high-precision for activation operations.
 *  @param[in]  compute_dtype
 *    Input. Data type to be used during computation.
 *    - If data type of \p query is float, \p compute_dtype must be float.
 *    - If data type of \p query is half, \p compute_dtype can be half or float.
 *    - If data type of \p query is bfloat16, \p compute_dtype can be half, bfloat16 or float, but the operator will use float as the actual \p compute_dtype automatically.
 *  @param[out] size
 *    Output. Size of workspace.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more required parameters are NULL.
 */
CNNL_DEPRECATED_FOR(cnnlGetScaledDotProductAttnWorkspaceSize_v3)
cnnlStatus_t CNNL_WIN_API
cnnlGetScaledDotProductAttnWorkspaceSize_v2(cnnlHandle_t handle,
                                            cnnlScaledDotProductAttnDescriptor_t desc,
                                            cnnlScaledDotProductAttnQuantizeDescriptor_t quant_desc,
                                            cnnlTensorDescriptor_t query_desc,
                                            cnnlTensorDescriptor_t key_desc,
                                            cnnlTensorDescriptor_t value_desc,
                                            cnnlTensorDescriptor_t cu_seqlens_q_desc,
                                            cnnlTensorDescriptor_t cu_seqlens_k_desc,
                                            cnnlTensorDescriptor_t attn_bias_desc,
                                            cnnlTensorDescriptor_t alibi_slopes_desc,
                                            cnnlTensorDescriptor_t block_tables_desc,
                                            const int max_seq_q,
                                            const int max_seq_k,
                                            const bool causal,
                                            const int window_size_left,
                                            const int window_size_right,
                                            const bool return_softmax_lse,
                                            cnnlActivationPreference_t act_pref,
                                            cnnlDataType_t compute_dtype,
                                            size_t *size);

// Group:ScaledDotProductAttn
/*!
 *  @brief Retrieves extra memory needed in ::cnnlScaledDotProductAttn_v4 operation. Compared with ::cnnlGetScaledDotProductAttnWorkspaceSize_v2,
 *  this API updates cnnlActivationPreference_t to cnnlComputationPreference_t.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
 *  @param[in]  desc
 *    Input. Descriptor of ::cnnlScaledDotProductAttn operation. Reserved for future use.
 *  @param[in]  quant_desc
 *    Input. Descriptor of ::cnnlScaledDotProductAttn operation including quantization related information. Reserved for future use.
 *  @param[in]  query_desc
 *    Input. Descriptor of \p query tensor.
 *  @param[in]  key_desc
 *    Input. Descriptor of \p key tensor.
 *  @param[in]  value_desc
 *    Input. Descriptor of \p value tensor.
 *  @param[in]  cu_seqlens_q_desc
 *    Input. Descriptor of \p cu_seqlens_q tensor.
 *  @param[in]  cu_seqlens_k_desc
 *    Input. Descriptor of \p cu_seqlens_k tensor.
 *  @param[in]  attn_bias_desc
 *    Input. Descriptor of \p attn_bias tensor. It can be nullptr.
 *  @param[in]  alibi_slopes_desc
 *    Input. Descriptor of \p alibi_slopes tensor. It can be nullptr.
 *  @param[in]  block_tables_desc
 *    Input. Descriptor of \p block_tables tensor. It can be nullptr.
 *  @param[in]  max_seq_q
 *    Input. The maximum of \p seq_len_q.
 *  @param[in]  max_seq_k
 *    Input. The maximum of \p seq_len_k.
 *  @param[in]  causal
 *    Input. A Boolean value indicating whether to apply causal attention.
 *  @param[in]  window_size_left
 *    Input. The left boundary of sliding window local attention. It should be used with \p window_size_right.
 *  @param[in]  window_size_right
 *    Input. The right boundary of sliding window local attention. It should be used with \p window_size_left.
 *  @param[in]  return_softmax_lse
 *    Input. A Boolean value indicating whether to output the log of the softmax normalization factor.
 *  @param[in]  act_pref
 *    Input. The indicator of activation operation preference, including:
 *      - CNNL_COMPUTATION_FAST: Use high-performance for activation operations.
 *      - CNNL_COMPUTATION_HIGH_PRECISION: Use high-precision for activation operations.
 *  @param[in]  compute_dtype
 *    Input. Data type to be used during computation.
 *    - If data type of \p query is float, \p compute_dtype must be float.
 *    - If data type of \p query is half, \p compute_dtype can be half or float.
 *    - If data type of \p query is bfloat16, \p compute_dtype can be half, bfloat16 or float, but the operator will use float as the actual \p compute_dtype automatically.
 *  @param[out] size
 *    Output. Size of workspace.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more required parameters are NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetScaledDotProductAttnWorkspaceSize_v3(cnnlHandle_t handle,
                                            cnnlScaledDotProductAttnDescriptor_t desc,
                                            cnnlScaledDotProductAttnQuantizeDescriptor_t quant_desc,
                                            cnnlTensorDescriptor_t query_desc,
                                            cnnlTensorDescriptor_t key_desc,
                                            cnnlTensorDescriptor_t value_desc,
                                            cnnlTensorDescriptor_t cu_seqlens_q_desc,
                                            cnnlTensorDescriptor_t cu_seqlens_k_desc,
                                            cnnlTensorDescriptor_t attn_bias_desc,
                                            cnnlTensorDescriptor_t alibi_slopes_desc,
                                            cnnlTensorDescriptor_t block_tables_desc,
                                            const int max_seq_q,
                                            const int max_seq_k,
                                            const bool causal,
                                            const int window_size_left,
                                            const int window_size_right,
                                            const bool return_softmax_lse,
                                            cnnlComputationPreference_t act_pref,
                                            cnnlDataType_t compute_dtype,
                                            size_t *size);

// Group:ScaledDotProductAttn
/*!
 *  @brief Executes scaled dot production in transformer encoder network.
 *
 *  This function performs with the following steps:
 *
 *  scores = batch_matmul(query, key)
 *
 *  scores = scores * qk_scale
 *
 *  if attn_bias then
 *
 *    scores = scores + attn_bias
 *
 *  if alibi_slopes then
 *
 *    scores = scores + alibi_bias
 *
 *  if causal then
 *
 *    scores = scores + mask
 *
 *  attention = softmax(scores)
 *
 *  output = batch_matmul(attention, value)
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlScaledDotProductAttn_v4 instead.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
 *  @param[in]  desc
 *    Input. Descriptor of ::cnnlScaledDotProductAttn operation. Reserved for future use.
 *  @param[in]  quant_desc
 *    Input. Descriptor of ::cnnlScaledDotProductAttn operation including quantization related information. Reserved for future use.
 *  @param[in]  query_desc
 *    Input. Descriptor of \p query tensor.
 *    - In pack mode, the shape of \p query should be [total_seq_q, head_num_q, head_size_qk].
 *    - In pad mode, the shape of \p query should be [batch, seq_len_q, head_num_q, head_size_qk], and \p seq_len_q should be equal to \p max_seq_q.
 *  @param[in]  query
 *    Input. Pointer to the MLU memory that stores the \p query tensor.
 *  @param[in]  key_desc
 *    Input. Descriptor of \p key tensor.
 *    - In pack mode, the shape of \p key should be [total_seq_k, head_num_k, head_size_qk].
 *    - In pad mode, the shape of \p key should be [batch, seq_len_k, head_num_k, head_size_qk], and \p seq_len_k should be equal to \p max_seq_k.
 *  @param[in]  key
 *    Input. Pointer to the MLU memory that stores the \p key tensor.
 *  @param[in]  value_desc
 *    Input. Descriptor of \p value tensor.
 *    - In pack mode, the shape of \p value should be [total_seq_k, head_num_k, head_size_v].
 *    - In pad mode, the shape of \p value should be [batch, seq_len_k, head_num_k, head_size_v], and \p seq_len_k should be equal to \p max_seq_k.
 *  @param[in]  value
 *    Input. Pointer to the MLU memory that stores the \p value tensor.
 *  @param[in]  cu_seqlens_q_desc
 *    Input. Descriptor of \p cu_seqlens_q tensor.
 *    - In pack mode, the shape of \p cu_seqlens_q should be [batch + 1].
 *    - In pad mode, \p cu_seqlens_q_desc will not be used and can be nullptr.
 *  @param[in]  cu_seqlens_q
 *    Input. Pointer to the MLU memory that stores the \p cu_seqlens_q tensor. In pad mode, \p cu_seqlens_q will not be used and can be nullptr.
 *  @param[in]  cu_seqlens_k_desc
 *    Input. Descriptor of \p cu_seqlens_k tensor.
 *    - In pack mode, the shape of \p cu_seqlens_k should be [batch + 1].
 *    - In pad mode, \p cu_seqlens_k_desc will not be used and can be nullptr.
 *  @param[in]  cu_seqlens_k
 *    Input. Pointer to the MLU memory that stores the \p cu_seqlens_k tensor. In pad mode, \p cu_seqlens_k will not be used and can be nullptr.
 *  @param[in]  seqused_k_desc
 *    Input. Descriptor of \p seqused_k tensor. Reserved for future use.
 *  @param[in]  seqused_k
 *    Input. Pointer to the MLU memory that stores the \p seqused_k tensor. Reserved for future use.
 *  @param[in]  attn_bias_desc
 *    Input. Descriptor of \p attn_bias tensor. The shape of \p attn_bias should be [batch, head_num_q, bias_seq_q, bias_seq_k] or [batch, bias_seq_q, bias_seq_k].
 *    \p attn_bias_desc can be nullptr.
 *  @param[in]  attn_bias
 *    Input. Pointer to the MLU memory that stores the \p attn_bias tensor. \p attn_bias can be nullptr.
 *  @param[in] alibi_slopes_desc
 *    Input. Descriptor of \p alibi_slopes tensor. The shape of \p alibi_slopes should be [batch, head_num_q] or [head_num_q].
 *    \p alibi_slopes_desc can be nullptr.
 *  @param[in] alibi_slopes
 *    Input. Pointer to the MLU memory that stores the \p alibi_slopes tensor. \p alibi_slopes can be nullptr.
 *  @param[in]  max_seq_q
 *    Input. The maximum of \p seq_len_q.
 *  @param[in]  max_seq_k
 *    Input. The maximum of \p seq_len_k.
 *  @param[in]  causal
 *    Input. A Boolean value indicating whether to apply causal attention.
 *  @param[in]  window_size_left
 *    Input. The left boundary of sliding window local attention. It should be used with \p window_size_right.
 *  @param[in]  window_size_right
 *    Input. The right boundary of sliding window local attention. It should be used with \p window_size_left.
 *  @param[in]  qk_scale
 *    Input. The scale factor used after batchdot of \p query and \p key. It must be in range of (0.0, 1.0].
 *  @param[in]  act_pref
 *    Input. The indicator of activation operation preference, including:
 *      - CNNL_ACTIVATION_FAST: Use high-performance for activation operations.
 *      - CNNL_ACTIVATION_HIGH_PRECISION: Use high-precision for activation operations.
 *  @param[in]  compute_dtype
 *    Input. Data type to be used during computation.
 *    - If data type of \p query is float, \p compute_dtype must be float.
 *    - If data type of \p query is half, \p compute_dtype can be half or float.
 *    - If data type of \p query is bfloat16, \p compute_dtype can be half, bfloat16 or float, but the operator will use float as the actual \p compute_dtype automatically.
 *  @param[in]  workspace
 *    Input. Pointer to the MLU memory that is used as an extra workspace.
 *  @param[in]  workspace_size
 *    Input. Size of workspace.
 *  @param[in]  output_desc
 *    Input. Descriptor of \p output tensor.
 *  @param[out]  output
 *    Output. Pointer to the MLU memory that stores the \p output tensor.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are encountered:
 *    - \p handle is NULL.
 *    - One or more required pointers are NULL.
 *    - The scale or data type limitation is not satisfied.
 *
 *  @par Data Type
 *    - Data types of \p query, \p key, \p value and \p output must be the same, which support float, bfloat16, and half.
 *    - Data type bfloat16 is only supported on MLU500 series.
 *    - Data type of \p cu_seqlens_q and \p cu_seqlens_k must be int32 in pack mode.
 *    - If \p inputs contain \p attn_bias, then its data type must be the same as that of \p query.
 *    - If \p inputs contain \p alibi_slopes, then its data type must be float.
 *
 *  @par Scale Limitation
 *    - \p batch must be in range of (0, INT32_MAX].
 *    - \p max_seq_q and \p max_seq_k must be in range of (0, INT32_MAX].
 *    - \p bias_seq_q >= \p max_seq_q and \p bias_seq_k >= \p max_seq_k.
 *    - \p head_num_q and \p head_num_k must be in range of [1, 128], and \p head_num_q % \p head_num_k == 0.
 *    - \p head_size_qk and \p head_size_v must be in range of [16, 576].
 *
 *  @note
 *    - This function only works for inference and does not support training.
 *    - Only MLU300 and MLU500 series are supported.
 *    - \p Inputs and \p outputs cannot be homologous operand.
 *    - The content of all \p input tensors is not modified.
 *    - \p Query, \p key, \p value and \p output must be continuous along the last dimension.
 *    - All \p input tensors must be continuous except \p query, \p key and \p value.
 *    - If \p window_size_left and \p window_size_right are not -1, implements sliding window local attention.
 *      \p query at position i will only attend to \p key between [i + seq_k - seq_q - window_size_left, i + seq_k - seq_q + window_size_right] inclusive.
 */
CNNL_DEPRECATED_FOR(cnnlScaledDotProductAttn_v4)
cnnlStatus_t CNNL_WIN_API
cnnlScaledDotProductAttn(cnnlHandle_t handle,
                         cnnlScaledDotProductAttnDescriptor_t desc,
                         cnnlScaledDotProductAttnQuantizeDescriptor_t quant_desc,
                         cnnlTensorDescriptor_t query_desc,
                         const void *query,
                         cnnlTensorDescriptor_t key_desc,
                         const void *key,
                         cnnlTensorDescriptor_t value_desc,
                         const void *value,
                         cnnlTensorDescriptor_t cu_seqlens_q_desc,
                         const void *cu_seqlens_q,
                         cnnlTensorDescriptor_t cu_seqlens_k_desc,
                         const void *cu_seqlens_k,
                         cnnlTensorDescriptor_t seqused_k_desc,
                         const void *seqused_k,
                         cnnlTensorDescriptor_t attn_bias_desc,
                         const void *attn_bias,
                         cnnlTensorDescriptor_t alibi_slopes_desc,
                         const void *alibi_slopes,
                         const int max_seq_q,
                         const int max_seq_k,
                         const bool causal,
                         const int window_size_left,
                         const int window_size_right,
                         float qk_scale,
                         cnnlActivationPreference_t act_pref,
                         cnnlDataType_t compute_dtype,
                         void *workspace,
                         const size_t workspace_size,
                         cnnlTensorDescriptor_t output_desc,
                         void *output);

// Group:ScaledDotProductAttn
/*!
 *  @brief Executes scaled dot production in transformer encoder network.
 *  Compared with ::cnnlScaledDotProductAttn, this API
 *  supports one more output \p softmax_lse, according to the boolean parameter
 *  \p return_softmax_lse.
 *
 *  This function performs with the following steps:
 *
 *  scores = batch_matmul(query, key)
 *
 *  scores = scores * qk_scale
 *
 *  if attn_bias then
 *
 *    scores = scores + attn_bias
 *
 *  if alibi_slopes then
 *
 *    scores = scores + alibi_bias
 *
 *  if causal then
 *
 *    scores = scores + mask
 *
 *  if return_softmax_lse then
 *
 *    softmax_lse = lse(scores) = log(sum(exp(x - max))) + max
 *
 *  attention = softmax(scores)
 *
 *  output = batch_matmul(attention, value)
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlScaledDotProductAttn_v4 instead.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
 *  @param[in]  desc
 *    Input. Descriptor of ::cnnlScaledDotProductAttn_v2 operation. Reserved for future use.
 *  @param[in]  quant_desc
 *    Input. Descriptor of ::cnnlScaledDotProductAttn_v2 operation including quantization related information. Reserved for future use.
 *  @param[in]  query_desc
 *    Input. Descriptor of \p query tensor.
 *    - In pack mode, the shape of \p query should be [total_seq_q, head_num_q, head_size_qk].
 *    - In pad mode, the shape of \p query should be [batch, seq_len_q, head_num_q, head_size_qk], and \p seq_len_q should be equal to \p max_seq_q.
 *  @param[in]  query
 *    Input. Pointer to the MLU memory that stores the \p query tensor.
 *  @param[in]  key_desc
 *    Input. Descriptor of \p key tensor.
 *    - In pack mode, the shape of \p key should be [total_seq_k, head_num_k, head_size_qk].
 *    - In pad mode, the shape of \p key should be [batch, seq_len_k, head_num_k, head_size_qk], and \p seq_len_k should be equal to \p max_seq_k.
 *  @param[in]  key
 *    Input. Pointer to the MLU memory that stores the \p key tensor.
 *  @param[in]  value_desc
 *    Input. Descriptor of \p value tensor.
 *    - In pack mode, the shape of \p value should be [total_seq_k, head_num_k, head_size_v].
 *    - In pad mode, the shape of \p value should be [batch, seq_len_k, head_num_k, head_size_v], and \p seq_len_k should be equal to \p max_seq_k.
 *  @param[in]  value
 *    Input. Pointer to the MLU memory that stores the \p value tensor.
 *  @param[in]  cu_seqlens_q_desc
 *    Input. Descriptor of \p cu_seqlens_q tensor.
 *    - In pack mode, the shape of \p cu_seqlens_q should be [batch + 1].
 *    - In pad mode, \p cu_seqlens_q_desc will not be used and can be nullptr.
 *  @param[in]  cu_seqlens_q
 *    Input. Pointer to the MLU memory that stores the \p cu_seqlens_q tensor. In pad mode, \p cu_seqlens_q will not be used and can be nullptr.
 *  @param[in]  cu_seqlens_k_desc
 *    Input. Descriptor of \p cu_seqlens_k tensor.
 *    - In pack mode, the shape of \p cu_seqlens_k should be [batch + 1].
 *    - In pad mode, \p cu_seqlens_k_desc will not be used and can be nullptr.
 *  @param[in]  cu_seqlens_k
 *    Input. Pointer to the MLU memory that stores the \p cu_seqlens_k tensor. In pad mode, \p cu_seqlens_k will not be used and can be nullptr.
 *  @param[in]  seqused_k_desc
 *    Input. Descriptor of \p seqused_k tensor. Reserved for future use.
 *  @param[in]  seqused_k
 *    Input. Pointer to the MLU memory that stores the \p seqused_k tensor. Reserved for future use.
 *  @param[in]  attn_bias_desc
 *    Input. Descriptor of \p attn_bias tensor. The shape of \p attn_bias should be [batch, head_num_q, bias_seq_q, bias_seq_k] or [batch, bias_seq_q, bias_seq_k].
 *    \p attn_bias_desc can be nullptr.
 *  @param[in]  attn_bias
 *    Input. Pointer to the MLU memory that stores the \p attn_bias tensor. \p attn_bias can be nullptr.
 *  @param[in] alibi_slopes_desc
 *    Input. Descriptor of \p alibi_slopes tensor. The shape of \p alibi_slopes should be [batch, head_num_q] or [head_num_q].
 *    \p alibi_slopes_desc can be nullptr.
 *  @param[in] alibi_slopes
 *    Input. Pointer to the MLU memory that stores the \p alibi_slopes tensor. \p alibi_slopes can be nullptr.
 *  @param[in]  max_seq_q
 *    Input. The maximum of \p seq_len_q.
 *  @param[in]  max_seq_k
 *    Input. The maximum of \p seq_len_k.
 *  @param[in]  causal
 *    Input. A Boolean value indicating whether to apply causal attention.
 *  @param[in]  window_size_left
 *    Input. The left boundary of sliding window local attention. It should be used with \p window_size_right.
 *  @param[in]  window_size_right
 *    Input. The right boundary of sliding window local attention. It should be used with \p window_size_left.
 *  @param[in]  qk_scale
 *    Input. The scale factor used after batchdot of \p query and \p key. It must be in range of (0.0, 1.0].
 *  @param[in]  act_pref
 *    Input. The indicator of activation operation preference, including:
 *      - CNNL_ACTIVATION_FAST: Use high-performance for activation operations.
 *      - CNNL_ACTIVATION_HIGH_PRECISION: Use high-precision for activation operations.
 *  @param[in]  compute_dtype
 *    Input. Data type to be used during computation.
 *    - If data type of \p query is float, \p compute_dtype must be float.
 *    - If data type of \p query is half, \p compute_dtype can be half or float.
 *    - If data type of \p query is bfloat16, \p compute_dtype can be half, bfloat16 or float, but the operator will use float as the actual \p compute_dtype automatically.
 *  @param[in]  workspace
 *    Input. Pointer to the MLU memory that is used as an extra workspace.
 *  @param[in]  workspace_size
 *    Input. Size of workspace.
 *  @param[in]  return_softmax_lse
 *    Input. A Boolean value indicating whether to output the log of the softmax normalization factor.
 *  @param[in]  softmax_lse_desc
 *    Input. Descriptor of \p softmax_lse tensor. If \p return_softmax_lse is false, \p softmax_lse_desc will not
 *    be used and can be nullptr. In both pack and pad modes, the shape of \p softmax_lse should be [batch, head_num_q, max_seq_q].
 *  @param[out] softmax_lse
 *    Output. The logsumexp of each row of the matrix QK * scaling (for example, log of the softmax
 *    normalization factor). If \p return_softmax_lse is false, \p softmax_lse will not be used and can be nullptr.
 *  @param[in]  output_desc
 *    Input. Descriptor of \p output tensor.
 *  @param[out]  output
 *    Output. Pointer to the MLU memory that stores the \p output tensor.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are encountered:
 *    - \p handle is NULL.
 *    - One or more required pointers are NULL.
 *    - Scale or data type limitation is not satisfied.
 *
 *  @par Data Type
 *    - Data types of \p query, \p key, \p value and \p output must be the same, which support float, bfloat16, and half.
 *    - Data type bfloat16 is only supported on MLU500 series.
 *    - Data type of \p cu_seqlens_q and \p cu_seqlens_k must be int32 in pack mode.
 *    - If \p inputs contain \p attn_bias, then its data type must be the same as that of \p query.
 *    - If \p inputs contain \p alibi_slopes, then its data type must be float.
 *    - If \p return_softmax_lse is true, then data type of \p softmax_lse and \p compute_dtype must be float.
 *
 *  @par Scale Limitation
 *    - \p batch must be in range of (0, INT32_MAX].
 *    - \p max_seq_q and \p max_seq_k must be in range of (0, INT32_MAX].
 *    - \p bias_seq_q >= \p max_seq_q and \p bias_seq_k >= \p max_seq_k.
 *    - \p head_num_q and \p head_num_k must be in range of [1, 128], and \p head_num_q % \p head_num_k == 0.
 *    - \p head_size_qk and \p head_size_v must be in range of [16, 576].
 *
 *  @note
 *    - This function only works for inference and does not support training.
 *    - Only MLU300 and MLU500 series are supported.
 *    - \p Inputs and \p outputs cannot be homologous operand.
 *    - The content of all \p input tensors is not modified.
 *    - \p Query, \p key, \p value and \p output must be continuous along the last dimension.
 *    - All \p input tensors must be continuous except \p query, \p key and \p value.
 *    - If \p window_size_left and \p window_size_right are not -1, implements sliding window local attention.
 *      \p query at position i will only attend to \p key between [i + seq_k - seq_q - window_size_left, i + seq_k - seq_q + window_size_right] inclusive.
 */
CNNL_DEPRECATED_FOR(cnnlScaledDotProductAttn_v4)
cnnlStatus_t CNNL_WIN_API
cnnlScaledDotProductAttn_v2(cnnlHandle_t handle,
                            cnnlScaledDotProductAttnDescriptor_t desc,
                            cnnlScaledDotProductAttnQuantizeDescriptor_t quant_desc,
                            cnnlTensorDescriptor_t query_desc,
                            const void *query,
                            cnnlTensorDescriptor_t key_desc,
                            const void *key,
                            cnnlTensorDescriptor_t value_desc,
                            const void *value,
                            cnnlTensorDescriptor_t cu_seqlens_q_desc,
                            const void *cu_seqlens_q,
                            cnnlTensorDescriptor_t cu_seqlens_k_desc,
                            const void *cu_seqlens_k,
                            cnnlTensorDescriptor_t seqused_k_desc,
                            const void *seqused_k,
                            cnnlTensorDescriptor_t attn_bias_desc,
                            const void *attn_bias,
                            cnnlTensorDescriptor_t alibi_slopes_desc,
                            const void *alibi_slopes,
                            const int max_seq_q,
                            const int max_seq_k,
                            const bool causal,
                            const int window_size_left,
                            const int window_size_right,
                            float qk_scale,
                            cnnlActivationPreference_t act_pref,
                            cnnlDataType_t compute_dtype,
                            void *workspace,
                            const size_t workspace_size,
                            const bool return_softmax_lse,
                            cnnlTensorDescriptor_t softmax_lse_desc,
                            void *softmax_lse,
                            cnnlTensorDescriptor_t output_desc,
                            void *output);

// Group:ScaledDotProductAttn
/*!
 *  @brief Executes scaled dot production in transformer encoder network.
 *  Compared with ::cnnlScaledDotProductAttn_v2, this API adds some new features:
 *  - \p block_tables is supported, which would store the indexes for retrieving \p key and \p value from kv cache.
 *  - In addition to store the normal attention inputs, \p key and \p value can also be used to store kv cache if \p block_tables is not nullptr.
 *
 *  This function performs with the following steps:
 *
 *  scores = batch_matmul(query, key)
 *
 *  scores = scores * qk_scale
 *
 *  if attn_bias then
 *
 *    scores = scores + attn_bias
 *
 *  if alibi_slopes then
 *
 *    scores = scores + alibi_bias
 *
 *  if causal then
 *
 *    scores = scores + mask
 *
 *  if return_softmax_lse then
 *
 *    softmax_lse = lse(scores) = log(sum(exp(x - max))) + max
 *
 *  attention = softmax(scores)
 *
 *  output = batch_matmul(attention, value)
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlScaledDotProductAttn_v4 instead.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
 *  @param[in]  desc
 *    Input. Descriptor of ::cnnlScaledDotProductAttn_v3 operation. Reserved for future use.
 *  @param[in]  quant_desc
 *    Input. Descriptor of ::cnnlScaledDotProductAttn_v3 operation including quantization related information. Reserved for future use.
 *  @param[in]  query_desc
 *    Input. Descriptor of \p query tensor.
 *    - In pack mode, the shape of \p query should be [total_seq_q, head_num_q, head_size_qk].
 *    - In pad mode, the shape of \p query should be [batch, max_seq_q, head_num_q, head_size_qk].
 *  @param[in]  query
 *    Input. Pointer to the MLU memory that stores the \p query tensor.
 *  @param[in]  key_desc
 *    Input. Descriptor of \p key tensor.
 *    If \p block_tables is nullptr:
 *    - In pack mode, the shape of \p key should be [total_seq_k, head_num_k, head_size_qk].
 *    - In pad mode, the shape of \p key should be [batch, max_seq_k, head_num_k, head_size_qk].
 *    Otherwise, if \p block_tables is not nullptr, the shape of \p key should be [num_blocks, head_num_k, block_size, head_size_qk].
 *  @param[in]  key
 *    Input. Pointer to the MLU memory that stores the \p key tensor.
 *  @param[in]  value_desc
 *    Input. Descriptor of \p value tensor.
 *    If \p block_tables is nullptr:
 *    - In pack mode, the shape of \p value should be [total_seq_k, head_num_k, head_size_v].
 *    - In pad mode, the shape of \p value should be [batch, max_seq_k, head_num_k, head_size_v].
 *    Otherwise, if \p block_tables is not nullptr, the shape of \p value should be [num_blocks, head_num_k, block_size, head_size_v].
 *  @param[in]  value
 *    Input. Pointer to the MLU memory that stores the \p value tensor.
 *  @param[in]  cu_seqlens_q_desc
 *    Input. Descriptor of \p cu_seqlens_q tensor.
 *    - In pack mode, the shape of \p cu_seqlens_q should be [batch + 1].
 *    - In pad mode, \p cu_seqlens_q_desc will not be used and can be nullptr.
 *  @param[in]  cu_seqlens_q
 *    Input. Pointer to the MLU memory that stores the \p cu_seqlens_q tensor. In pad mode, \p cu_seqlens_q will not be used and can be nullptr.
 *  @param[in]  cu_seqlens_k_desc
 *    Input. Descriptor of \p cu_seqlens_k tensor. In pack mode or when \p block_tables is not nullptr, the shape of \p cu_seqlens_k should be [batch + 1].
 *    Otherwise, \p cu_seqlens_k_desc will not be used and can be nullptr.
 *  @param[in]  cu_seqlens_k
 *    Input. Pointer to the MLU memory that stores the \p cu_seqlens_k tensor. If \p block_tables is nullptr and in pad mode, \p cu_seqlens_k will not be used and can be nullptr.
 *  @param[in]  seqused_k_desc
 *    Input. Descriptor of \p seqused_k tensor. Reserved for future use.
 *  @param[in]  seqused_k
 *    Input. Pointer to the MLU memory that stores the \p seqused_k tensor. Reserved for future use.
 *  @param[in]  attn_bias_desc
 *    Input. Descriptor of \p attn_bias tensor. The shape of \p attn_bias should be [batch, head_num_q, bias_seq_q, bias_seq_k] or [batch, bias_seq_q, bias_seq_k].
 *    \p attn_bias_desc can be nullptr.
 *  @param[in]  attn_bias
 *    Input. Pointer to the MLU memory that stores the \p attn_bias tensor. \p attn_bias can be nullptr.
 *  @param[in] alibi_slopes_desc
 *    Input. Descriptor of \p alibi_slopes tensor. The shape of \p alibi_slopes should be [batch, head_num_q] or [head_num_q].
 *    \p alibi_slopes_desc can be nullptr.
 *  @param[in] alibi_slopes
 *    Input. Pointer to the MLU memory that stores the \p alibi_slopes tensor. \p alibi_slopes can be nullptr.
 *  @param[in] block_tables_desc
 *    Input. Descriptor of \p block_tables tensor. The shape of \p block_tables should be [batch, max_num_blocks_per_seq].
 *  @param[in] block_tables
 *    Input. Pointer to the MLU memory that stores the \p block_tables tensor. \p block_tables can be nullptr.
 *  @param[in]  max_seq_q
 *    Input. The maximum of \p seq_len_q.
 *  @param[in]  max_seq_k
 *    Input. The maximum of \p seq_len_k.
 *  @param[in]  causal
 *    Input. A Boolean value indicating whether to apply causal attention.
 *  @param[in]  window_size_left
 *    Input. The left boundary of sliding window local attention. It should be used with \p window_size_right.
 *  @param[in]  window_size_right
 *    Input. The right boundary of sliding window local attention. It should be used with \p window_size_left.
 *  @param[in]  qk_scale
 *    Input. The scale factor used after batchdot of \p query and \p key. It must be in range of (0.0, 1.0].
 *  @param[in]  act_pref
 *    Input. The indicator of activation operation preference, including:
 *      - CNNL_ACTIVATION_FAST: Use high-performance for activation operations.
 *      - CNNL_ACTIVATION_HIGH_PRECISION: Use high-precision for activation operations.
 *  @param[in]  compute_dtype
 *    Input. Data type to be used during computation.
 *    - If data type of \p query is float, \p compute_dtype must be float.
 *    - If data type of \p query is half, \p compute_dtype can be half or float.
 *    - If data type of \p query is bfloat16, \p compute_dtype can be half, bfloat16 or float, but the operator will use float as the actual \p compute_dtype automatically.
 *  @param[in]  workspace
 *    Input. Pointer to the MLU memory that is used as an extra workspace.
 *  @param[in]  workspace_size
 *    Input. Size of workspace.
 *  @param[in]  return_softmax_lse
 *    Input. A Boolean value indicating whether to output the log of the softmax normalization factor.
 *  @param[in]  softmax_lse_desc
 *    Input. Descriptor of \p softmax_lse tensor. If \p return_softmax_lse is false, \p softmax_lse_desc will not
 *    be used and can be nullptr. In both pack and pad modes, the shape of \p softmax_lse should be [batch, head_num_q, max_seq_q].
 *  @param[out] softmax_lse
 *    Output. The logsumexp of each row of the matrix QK * scaling (for example, log of the softmax
 *    normalization factor). If \p return_softmax_lse is false, \p softmax_lse will not be used and can be nullptr.
 *  @param[in]  output_desc
 *    Input. Descriptor of \p output tensor.
 *  @param[out]  output
 *    Output. Pointer to the MLU memory that stores the \p output tensor.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are encountered:
 *    - \p handle is NULL.
 *    - One or more required pointers are NULL.
 *    - Scale or data type limitation is not satisfied.
 *
 *  @par Data Type
 *    - Data types of \p query, \p key, \p value and \p output must be the same, which support float, bfloat16, and half.
 *    - Data type bfloat16 is only supported on MLU500 series.
 *    - Data type of \p cu_seqlens_q and \p cu_seqlens_k must be int32 in pack mode.
 *    - If \p inputs contain \p attn_bias, then its data type must be the same as that of \p query.
 *    - If \p inputs contain \p alibi_slopes, then its data type must be float.
 *    - If \p inputs contain \p block_tables, then its data type must be int32.
 *    - If \p return_softmax_lse is true, then data type of \p softmax_lse and \p compute_dtype must be float.
 *
 *  @par Scale Limitation
 *    - \p batch must be in range of (0, INT32_MAX].
 *    - \p max_seq_q and \p max_seq_k must be in range of (0, INT32_MAX].
 *    - \p bias_seq_q >= \p max_seq_q and \p bias_seq_k >= \p max_seq_k.
 *    - \p head_num_q and \p head_num_k must be in range of [1, 128], and \p head_num_q % \p head_num_k == 0.
 *    - \p head_size_qk and \p head_size_v must be in range of [16, 576].
 *    - \p num_blocks must be in range of (0, INT32_MAX].
 *    - \p max_num_blocks_per_seq >= 1.
 *    - When \p max_num_blocks_per_seq > 1, \p key and \p value are paged memory, and \p block_size only supports 16.
 *      Paged memory is only supported on MLU500 series.
 *    - When \p max_num_blocks_per_seq = 1, \p key and \p value are linear memory.
 *    - \p max_num_blocks_per_seq * \p block_size >= \p max_seq_k.
 *    - If \p block_tables is not nullptr, the total tensor bytes of \p key and \p value cannot exceed INT32_MAX, including strides.
 *
 *  @note
 *    - This function only works for inference and does not support training.
 *    - Only MLU300 and MLU500 series are supported.
 *    - \p Inputs and \p outputs cannot be homologous operand.
 *    - The content of all \p input tensors is not modified.
 *    - \p Query, \p key, \p value and \p output must be continuous along the last dimension.
 *    - If inputs contain \p block_tables, all \p input tensors, including \p key and \p value, must be continuous except \p query.
 *      Otherwise, all \p input tensors must be continuous except \p query, \p key and \p value.
 *    - If \p window_size_left and \p window_size_right are not -1, implements sliding window local attention.
 *      \p query at position i will only attend to \p key between [i + seq_k - seq_q - window_size_left, i + seq_k - seq_q + window_size_right] inclusive.
 */
CNNL_DEPRECATED_FOR(cnnlScaledDotProductAttn_v4)
cnnlStatus_t CNNL_WIN_API
cnnlScaledDotProductAttn_v3(cnnlHandle_t handle,
                            cnnlScaledDotProductAttnDescriptor_t desc,
                            cnnlScaledDotProductAttnQuantizeDescriptor_t quant_desc,
                            cnnlTensorDescriptor_t query_desc,
                            const void *query,
                            cnnlTensorDescriptor_t key_desc,
                            const void *key,
                            cnnlTensorDescriptor_t value_desc,
                            const void *value,
                            cnnlTensorDescriptor_t cu_seqlens_q_desc,
                            const void *cu_seqlens_q,
                            cnnlTensorDescriptor_t cu_seqlens_k_desc,
                            const void *cu_seqlens_k,
                            cnnlTensorDescriptor_t seqused_k_desc,
                            const void *seqused_k,
                            cnnlTensorDescriptor_t attn_bias_desc,
                            const void *attn_bias,
                            cnnlTensorDescriptor_t alibi_slopes_desc,
                            const void *alibi_slopes,
                            cnnlTensorDescriptor_t block_tables_desc,
                            const void *block_tables,
                            const int max_seq_q,
                            const int max_seq_k,
                            const bool causal,
                            const int window_size_left,
                            const int window_size_right,
                            float qk_scale,
                            cnnlActivationPreference_t act_pref,
                            cnnlDataType_t compute_dtype,
                            void *workspace,
                            const size_t workspace_size,
                            const bool return_softmax_lse,
                            cnnlTensorDescriptor_t softmax_lse_desc,
                            void *softmax_lse,
                            cnnlTensorDescriptor_t output_desc,
                            void *output);

// Group:ScaledDotProductAttn
/*!
 *  @brief Executes scaled dot production in transformer encoder network.
 *  Compared with ::cnnlScaledDotProductAttn_v3, this API adds some new features:
 *  - This API updates cnnlActivationPreference_t to cnnlComputationPreference_t.
 *
 *  This function performs with the following steps:
 *
 *  scores = batch_matmul(query, key)
 *
 *  scores = scores * qk_scale
 *
 *  if attn_bias then
 *
 *    scores = scores + attn_bias
 *
 *  if alibi_slopes then
 *
 *    scores = scores + alibi_bias
 *
 *  if causal then
 *
 *    scores = scores + mask
 *
 *  if return_softmax_lse then
 *
 *    softmax_lse = lse(scores) = log(sum(exp(x - max))) + max
 *
 *  attention = softmax(scores)
 *
 *  output = batch_matmul(attention, value)
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
 *  @param[in]  desc
 *    Input. Descriptor of ::cnnlScaledDotProductAttn_v3 operation. Reserved for future use.
 *  @param[in]  quant_desc
 *    Input. Descriptor of ::cnnlScaledDotProductAttn_v3 operation including quantization related information. Reserved for future use.
 *  @param[in]  query_desc
 *    Input. Descriptor of \p query tensor.
 *    - In pack mode, the shape of \p query should be [total_seq_q, head_num_q, head_size_qk].
 *    - In pad mode, the shape of \p query should be [batch, max_seq_q, head_num_q, head_size_qk].
 *  @param[in]  query
 *    Input. Pointer to the MLU memory that stores the \p query tensor.
 *  @param[in]  key_desc
 *    Input. Descriptor of \p key tensor.
 *    If \p block_tables is nullptr:
 *    - In pack mode, the shape of \p key should be [total_seq_k, head_num_k, head_size_qk].
 *    - In pad mode, the shape of \p key should be [batch, max_seq_k, head_num_k, head_size_qk].
 *    Otherwise, if \p block_tables is not nullptr, the shape of \p key should be [num_blocks, head_num_k, block_size, head_size_qk].
 *  @param[in]  key
 *    Input. Pointer to the MLU memory that stores the \p key tensor.
 *  @param[in]  value_desc
 *    Input. Descriptor of \p value tensor.
 *    If \p block_tables is nullptr:
 *    - In pack mode, the shape of \p value should be [total_seq_k, head_num_k, head_size_v].
 *    - In pad mode, the shape of \p value should be [batch, max_seq_k, head_num_k, head_size_v].
 *    Otherwise, if \p block_tables is not nullptr, the shape of \p value should be [num_blocks, head_num_k, block_size, head_size_v].
 *  @param[in]  value
 *    Input. Pointer to the MLU memory that stores the \p value tensor.
 *  @param[in]  cu_seqlens_q_desc
 *    Input. Descriptor of \p cu_seqlens_q tensor.
 *    - In pack mode, the shape of \p cu_seqlens_q should be [batch + 1].
 *    - In pad mode, \p cu_seqlens_q_desc will not be used and can be nullptr.
 *  @param[in]  cu_seqlens_q
 *    Input. Pointer to the MLU memory that stores the \p cu_seqlens_q tensor. In pad mode, \p cu_seqlens_q will not be used and can be nullptr.
 *  @param[in]  cu_seqlens_k_desc
 *    Input. Descriptor of \p cu_seqlens_k tensor. In pack mode or when \p block_tables is not nullptr, the shape of \p cu_seqlens_k should be [batch + 1].
 *    Otherwise, \p cu_seqlens_k_desc will not be used and can be nullptr.
 *  @param[in]  cu_seqlens_k
 *    Input. Pointer to the MLU memory that stores the \p cu_seqlens_k tensor. If \p block_tables is nullptr and in pad mode, \p cu_seqlens_k will not be used and can be nullptr.
 *  @param[in]  seqused_k_desc
 *    Input. Descriptor of \p seqused_k tensor. Reserved for future use.
 *  @param[in]  seqused_k
 *    Input. Pointer to the MLU memory that stores the \p seqused_k tensor. Reserved for future use.
 *  @param[in]  attn_bias_desc
 *    Input. Descriptor of \p attn_bias tensor. The shape of \p attn_bias should be [batch, head_num_q, bias_seq_q, bias_seq_k] or [batch, bias_seq_q, bias_seq_k].
 *    \p attn_bias_desc can be nullptr.
 *  @param[in]  attn_bias
 *    Input. Pointer to the MLU memory that stores the \p attn_bias tensor. \p attn_bias can be nullptr.
 *  @param[in] alibi_slopes_desc
 *    Input. Descriptor of \p alibi_slopes tensor. The shape of \p alibi_slopes should be [batch, head_num_q] or [head_num_q].
 *    \p alibi_slopes_desc can be nullptr.
 *  @param[in] alibi_slopes
 *    Input. Pointer to the MLU memory that stores the \p alibi_slopes tensor. \p alibi_slopes can be nullptr.
 *  @param[in] block_tables_desc
 *    Input. Descriptor of \p block_tables tensor. The shape of \p block_tables should be [batch, max_num_blocks_per_seq].
 *  @param[in] block_tables
 *    Input. Pointer to the MLU memory that stores the \p block_tables tensor. \p block_tables can be nullptr.
 *  @param[in]  max_seq_q
 *    Input. The maximum of \p seq_len_q.
 *  @param[in]  max_seq_k
 *    Input. The maximum of \p seq_len_k.
 *  @param[in]  causal
 *    Input. A Boolean value indicating whether to apply causal attention.
 *  @param[in]  window_size_left
 *    Input. The left boundary of sliding window local attention. It should be used with \p window_size_right.
 *  @param[in]  window_size_right
 *    Input. The right boundary of sliding window local attention. It should be used with \p window_size_left.
 *  @param[in]  qk_scale
 *    Input. The scale factor used after batchdot of \p query and \p key. It must be in range of (0.0, 1.0].
 *  @param[in]  act_pref
 *    Input. The indicator of activation operation preference, including:
 *      - CNNL_COMPUTATION_FAST: Use high-performance for activation operations.
 *      - CNNL_COMPUTATION_HIGH_PRECISION: Use high-precision for activation operations.
 *  @param[in]  compute_dtype
 *    Input. Data type to be used during computation.
 *    - If data type of \p query is float, \p compute_dtype must be float.
 *    - If data type of \p query is half, \p compute_dtype can be half or float.
 *    - If data type of \p query is bfloat16, \p compute_dtype can be half, bfloat16 or float, but the operator will use float as the actual \p compute_dtype automatically.
 *  @param[in]  workspace
 *    Input. Pointer to the MLU memory that is used as an extra workspace.
 *  @param[in]  workspace_size
 *    Input. Size of workspace.
 *  @param[in]  return_softmax_lse
 *    Input. A Boolean value indicating whether to output the log of the softmax normalization factor.
 *  @param[in]  softmax_lse_desc
 *    Input. Descriptor of \p softmax_lse tensor. If \p return_softmax_lse is false, \p softmax_lse_desc will not
 *    be used and can be nullptr. In both pack and pad modes, the shape of \p softmax_lse should be [batch, head_num_q, max_seq_q].
 *  @param[out] softmax_lse
 *    Output. The logsumexp of each row of the matrix QK * scaling (for example, log of the softmax
 *    normalization factor). If \p return_softmax_lse is false, \p softmax_lse will not be used and can be nullptr.
 *  @param[in]  output_desc
 *    Input. Descriptor of \p output tensor.
 *  @param[out]  output
 *    Output. Pointer to the MLU memory that stores the \p output tensor.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are encountered:
 *    - \p handle is NULL.
 *    - One or more required pointers are NULL.
 *    - Scale or data type limitation is not satisfied.
 *
 *  @par Data Type
 *    - Data types of \p query, \p key, \p value and \p output must be the same, which support float, bfloat16, and half.
 *    - Data type bfloat16 is only supported on MLU500 series.
 *    - Data type of \p cu_seqlens_q and \p cu_seqlens_k must be int32 in pack mode.
 *    - If \p inputs contain \p attn_bias, then its data type must be the same as that of \p query.
 *    - If \p inputs contain \p alibi_slopes, then its data type must be float.
 *    - If \p inputs contain \p block_tables, then its data type must be int32.
 *    - If \p return_softmax_lse is true, then data type of \p softmax_lse and \p compute_dtype must be float.
 *
 *  @par Scale Limitation
 *    - \p batch must be in range of (0, INT32_MAX].
 *    - \p max_seq_q and \p max_seq_k must be in range of (0, INT32_MAX].
 *    - \p bias_seq_q >= \p max_seq_q and \p bias_seq_k >= \p max_seq_k.
 *    - \p head_num_q and \p head_num_k must be in range of [1, 128], and \p head_num_q % \p head_num_k == 0.
 *    - \p head_size_qk and \p head_size_v must be in range of [16, 576].
 *    - \p num_blocks must be in range of (0, INT32_MAX].
 *    - \p max_num_blocks_per_seq >= 1.
 *    - When \p max_num_blocks_per_seq > 1, \p key and \p value are paged memory, and \p block_size only supports 16.
 *      Paged memory is only supported on MLU500 series.
 *    - When \p max_num_blocks_per_seq = 1, \p key and \p value are linear memory.
 *    - \p max_num_blocks_per_seq * \p block_size >= \p max_seq_k.
 *    - If \p block_tables is not nullptr, the total tensor bytes of \p key and \p value cannot exceed INT32_MAX, including strides.
 *
 *  @note
 *    - This function only works for inference and does not support training.
 *    - Only MLU300 and MLU500 series are supported.
 *    - \p Inputs and \p outputs cannot be homologous operand.
 *    - The content of all \p input tensors is not modified.
 *    - \p Query, \p key, \p value and \p output must be continuous along the last dimension.
 *    - If inputs contain \p block_tables, all \p input tensors, including \p key and \p value, must be continuous except \p query.
 *      Otherwise, all \p input tensors must be continuous except \p query, \p key and \p value.
 *    - If \p window_size_left and \p window_size_right are not -1, implements sliding window local attention.
 *      \p query at position i will only attend to \p key between [i + seq_k - seq_q - window_size_left, i + seq_k - seq_q + window_size_right] inclusive.
 *    - When \p causal is True and \p seq_len_q is greater than \p seq_len_k, the result may be undefined.
 */
cnnlStatus_t CNNL_WIN_API
cnnlScaledDotProductAttn_v4(cnnlHandle_t handle,
                            cnnlScaledDotProductAttnDescriptor_t desc,
                            cnnlScaledDotProductAttnQuantizeDescriptor_t quant_desc,
                            cnnlTensorDescriptor_t query_desc,
                            const void *query,
                            cnnlTensorDescriptor_t key_desc,
                            const void *key,
                            cnnlTensorDescriptor_t value_desc,
                            const void *value,
                            cnnlTensorDescriptor_t cu_seqlens_q_desc,
                            const void *cu_seqlens_q,
                            cnnlTensorDescriptor_t cu_seqlens_k_desc,
                            const void *cu_seqlens_k,
                            cnnlTensorDescriptor_t seqused_k_desc,
                            const void *seqused_k,
                            cnnlTensorDescriptor_t attn_bias_desc,
                            const void *attn_bias,
                            cnnlTensorDescriptor_t alibi_slopes_desc,
                            const void *alibi_slopes,
                            cnnlTensorDescriptor_t block_tables_desc,
                            const void *block_tables,
                            const int max_seq_q,
                            const int max_seq_k,
                            const bool causal,
                            const int window_size_left,
                            const int window_size_right,
                            float qk_scale,
                            cnnlComputationPreference_t act_pref,
                            cnnlDataType_t compute_dtype,
                            void *workspace,
                            const size_t workspace_size,
                            const bool return_softmax_lse,
                            cnnlTensorDescriptor_t softmax_lse_desc,
                            void *softmax_lse,
                            cnnlTensorDescriptor_t output_desc,
                            void *output);


// Group:cnnlCrossEntropyForward
/*!
 *  @brief Retrieves extra space size needed in cross entropy loss operation.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
 *  @param[in] split
 *    Input. A Boolean value, which is indicating whether the operator is executed in a tensor-parallel manner when run.
 *  @param[in] smoothing
 *    Input. A float value. Specifies the amount of smoothing when computing the loss,
 *    where 0.0 means no smoothing.
 *  @param[in] ignored_index
 *    Input. An integer value, which is ignored and does not contribute to the input gradient and stores one int64_t value.
 *  @param[in]  logits_desc
 *    Input. Descriptor of \p logits tensor, which is the input of ::cnnlCrossEntropyForward operation.
 *  @param[in]  labels_desc
 *    Input. Descriptor of \p labels tensor, which is the input of ::cnnlCrossEntropyForward operation.
 *  @param[out] workspace_size
 *    Input. Size of workspace.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more required parameters are NULL.
 *
 *    For other limitations, see the constraints of the ::cnnlCrossEntropyForward operation.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetCrossEntropyForwardWorkspaceSize(cnnlHandle_t handle,
                                        const bool split,
                                        const float smoothing,
                                        const int64_t ignored_index,
                                        const cnnlTensorDescriptor_t logits_desc,
                                        const cnnlTensorDescriptor_t labels_desc,
                                        size_t *workspace_size);

// Group:cnnlCrossEntropyForward
/*!
 *  @brief Computes the cross entropy loss between input and target.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
 *  @param[in] split
 *    Input. A Boolean value, which is indicating whether the operator is executed in a tensor-parallel manner when run.
 *  @param[in] smoothing
 *    Input. A float value. Specifies the amount of smoothing when computing the loss,
 *    where 0.0 means no smoothing.
 *  @param[in] lse_square_scale
 *    Input. A float value, which is referred to as "z-loss".
 *  @param[in]  logits_desc
 *    Input. Descriptor of \p logits tensor, which is the input of ::cnnlCrossEntropyForward operation.
 *  @param[in]  logits
 *    Input. Pointer to the MLU memory that stores the \p logits tensor.
 *    The shape of logits is [B, SEQ_LEN], where B represents the batch size, and SEQ_LEN represents the number
 *    of vocab_size in network.
 *  @param[in] ignored_index
 *    Input. An integer value, which is ignored and does not contribute to the input gradient and stores one int64_t value.
 *  @param[in] class_start_idx
 *    Input. An integer value, which is representing the column index of the current tensor in the origin \p logits tensor
 *    and stores one int64_t value.
 *  @param[in] total_classes
 *    Input. An integer value, which is the column of the origin \p logits tensor and stores one int64_t value.
 *  @param[in]  workspace
 *    Input. Pointer to the MLU memory that is used as an extra workspace.
 *  @param[in]  workspace_size
 *    Input. Size of workspace.
 *  @param[in]  labels_desc
 *    Input. Descriptor of \p labels tensor, which is the input of ::cnnlCrossEntropyForward operation.
 *  @param[in]  labels
 *    Input. Pointer to the MLU memory that stores the \p labels tensor.
 *    The shape of labels is [B], where B represents the batch size.
 *  @param[in]  losses_desc
 *    Output. Descriptor of \p losses tensor, which is the cross entropy value of ::cnnlCrossEntropyForward operation.
 *  @param[out]  losses
 *    Output. Pointer to the MLU memory that stores the \p losses tensor.
 *    The shape of losses is [B], where B represents the batch size.
 *  @param[in]  lse_desc
 *    Output. Descriptor of \p lse tensor, which is the output of ::cnnlCrossEntropyForward operation.
 *  @param[out]  lse
 *    Output. Pointer to the MLU memory that stores the \p lse tensor.
 *    The shape of lse is [B], which is representing intermediate results of the computation, intended
 *    for use by the backward operator.
 *
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are encountered:
 *    - Handle is NULL.
 *    - One or more required pointers are NULL.
 *    - Data type limitation is not satisfied.
 *    - Homologous operand is enabled.
 *    - The input has a count of zero elements.
 *    - The value of \p split is true.
 *    - The value of \p class_start_idx is not 0.
 *    - The value of \p total_classes is not equal to SEQ_LEN.
 *  @retval CNNL_STATUS_ARCH_MISMATCH
 *   This function is running on the hardware platform that is not supported.
 *
 *  @par Data Type
 *  - \p logits: half/float/bfloat16
 *  - \p labels: int32
 *  - \p losses: float32
 *  - \p lse: float32
 *  @par Data Layout
 *  - \p logits tensor: \p CNNL_LAYOUT_ARRAY.
 *  - \p labels tensor: \p CNNL_LAYOUT_ARRAY.
 *  - \p losses tensor: \p CNNL_LAYOUT_ARRAY.
 *  - \p lse tensor: \p CNNL_LAYOUT_ARRAY.
 *
 *  @par Scale Limitation
 *    - The value of \p split must be false.
 *    - The value of \p class_start_idx must be 0.
 *    - The value of \p total_classes must be equal to SEQ_LEN.
 *    - The value of \p smoothing must be 0 to 1.
 *
 *  @note
 *  - Only MLU300 and MLU500 series are supported.
 *  - Large tensor is only supported on MLU500 series, which means the input tensor number can be equal to or larger than 2G.
 *  - The data type of \p logits must be equal to that of \p dlogits, either float or half.
 *  - The dimension of \p logits tensor must be equal to 2, and the number of elements should be equal
 *  - in the same dimension.
 *  - The dimension of \p labels tensor, \p losses tensor and \p lse tensor must be same, and
 *    be equal to 1. They have an equal number of elements.
 *  - The element of \p grad_losses tensor, \p labels tensor and \p lse tensor is equal to the number of elements in
 *    the 0th dimension of \p logits tensor or \p dlogits tensor.
 *  - The operator does not support homologous operand.
 *  - The input \p ignored_index is int64 scalar, but must be in range of \f$-2^{31}\f$ and \f$2^{31}-1\f$.
 *  - The input \p class_start_idx is int64 scalar, but must be in range of \f$-2^{31}\f$ and \f$2^{31}-1\f$.
 *  - The input \p total_classes is int64 scalar, but must be in range of \f$-2^{31}\f$ and \f$2^{31}-1\f$.
 *
 *  @par Reference
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCrossEntropyForward(cnnlHandle_t handle,
                        const bool split,
                        const float smoothing,
                        const float lse_square_scale,
                        const cnnlTensorDescriptor_t logits_desc,
                        const void *logits,
                        const int64_t ignored_index,
                        const int64_t class_start_idx,
                        const int64_t total_classes,
                        void *workspace,
                        const size_t workspace_size,
                        const cnnlTensorDescriptor_t labels_desc,
                        const void *labels,
                        const cnnlTensorDescriptor_t losses_desc,
                        void *losses,
                        const cnnlTensorDescriptor_t lse_desc,
                        void *lse);

// Group:cnnlCrossEntropyBackward
/*!
 *  @brief Retrieves extra space size needed in ::cnnlCrossEntropyBackward operation.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
 *  @param[in] smoothing
 *    Input. A float value. Specifies the amount of smoothing when computing the loss,
 *    where 0.0 means no smoothing.
 *  @param[in] ignored_index
 *    Input. An integer value, which is ignored and does not contribute to the input gradient and stores one int64_t value.
 *  @param[in]  grad_losses_desc
 *    Input. Descriptor of \p logits tensor, which is the input of ::cnnlCrossEntropyBackward operation.
 *  @param[in]  logits_desc
 *    Input. Descriptor of \p logits tensor, which is the input of ::cnnlCrossEntropyBackward operation.
 *  @param[in]  labels_desc
 *    Input. Descriptor of \p labels tensor, which is the input of ::cnnlCrossEntropyBackward operation.
 *  @param[in]  lse_desc
 *    Input. Descriptor of \p lse tensor, which is the Input of ::cnnlCrossEntropyBackward operation.
 *  @param[out] workspace_size
 *    Input. Size of workspace.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more required parameters are NULL.
 *
 *    For other limitations, see the constraints of the ::cnnlCrossEntropyBackward operation.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetCrossEntropyBackwardWorkspaceSize(cnnlHandle_t handle,
                                         const float smoothing,
                                         const int64_t ignored_index,
                                         const cnnlTensorDescriptor_t grad_losses_desc,
                                         const cnnlTensorDescriptor_t logits_desc,
                                         const cnnlTensorDescriptor_t labels_desc,
                                         const cnnlTensorDescriptor_t lse_desc,
                                         size_t *workspace_size);

// Group:cnnlCrossEntropyBackward
/*!
 *  @brief Computes the backward gradient for ::cnnlCrossEntropyForward.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
 *  @param[in] smoothing
 *    Input. A float value. Specifies the amount of smoothing when computing the loss,
 *    where 0.0 means no smoothing.
 *  @param[in] lse_square_scale
 *    Input. A float value, which is referred to as "z-loss".
 *  @param[in]  grad_losses_desc
 *    Input. Descriptor of \p logits tensor, which is the input of ::cnnlCrossEntropyBackward operation.
 *  @param[in]  grad_losses
 *    Input. Pointer to the MLU memory that stores the \p grad_losses tensor.
 *    The shape of logits is [B], where B represents the batch size.
 *  @param[in]  logits_desc
 *    Input. Descriptor of \p logits tensor, which is the input of ::cnnlCrossEntropyBackward operation.
 *  @param[in]  logits
 *    Input. Pointer to the MLU memory that stores the \p logits tensor.
 *    The shape of \p logits is [B, SEQ_LEN], where B represents the batch size, and SEQ_LEN represents the number
 *    of vocab_size in network.
 *  @param[in] ignored_index
 *    Input. An integer value, which is ignored and does not contribute to the input gradient and stores one int64_t value.
 *  @param[in] class_start_idx
 *    Input. An integer value, which is representing the column index of the current tensor in the origin \p logits tensor
 *    and stores one int64_t value.
 *  @param[in] total_classes
 *    Input. An integer value, which is the column of the origin \p logits tensor and stores one int64_t value.
 *  @param[in]  workspace
 *    Input. Pointer to the MLU memory that is used as an extra workspace.
 *  @param[in]  workspace_size
 *    Input. Size of workspace.
 *  @param[in]  labels_desc
 *    Input. Descriptor of \p labels tensor, which is the input of ::cnnlCrossEntropyBackward operation.
 *  @param[in]  labels
 *    Input. Pointer to the MLU memory that stores the \p labels tensor.
 *    The shape of labels is [B], where B represents the batch size.
 *  @param[in]  lse_desc
 *    Input. Descriptor of \p lse tensor, which is the input of ::cnnlCrossEntropyBackward operation.
 *  @param[in]  lse
 *    Input. Pointer to the MLU memory that stores the \p lse tensor.
 *    The shape of lse is [B], which is representing intermediate results of the computation, intended
 *    for use by the backward operator.
 *  @param[in]  dlogits_desc
 *    Output. Descriptor of \p dlogits tensor, which is the cross entropy value of ::cnnlCrossEntropyBackward operation.
 *  @param[out]  dlogits
 *    Output. Pointer to the MLU memory that stores the \p dlogits tensor.
 *    The shape of \p dlogits is [B, SEQ_LEN], where B represents the batch size and SEQ_LEN represents the number
 *    of vocab_size in network.
 *
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are encountered:
 *    - Handle is NULL.
 *    - One or more required pointers are NULL.
 *    - Data type limitation is not satisfied.
 *    - Homologous operand is enabled.
 *    - The input has a count of zero elements.
 *    - Homologous operand is enabled, but the data types of input \p logits
 *      and output \p dlogits are different.
 *  @retval CNNL_STATUS_ARCH_MISMATCH
 *   This function is running on the hardware platform that is not supported.
 *
 *  @par Data Type
 *  - \p grad_losses: flaot32
 *  - \p logits: half/float/bfloat16
 *  - \p labels: int32
 *  - \p lse: float32
 *  - \p dlogits: half/float/bfloat16
 *  @par Data Layout
 *  - \p grad_losses tensor: \p CNNL_LAYOUT_ARRAY.
 *  - \p logits tensor: \p CNNL_LAYOUT_ARRAY.
 *  - \p labels tensor: \p CNNL_LAYOUT_ARRAY.
 *  - \p lse tensor: \p CNNL_LAYOUT_ARRAY.
 *  - \p dlogits tensor: \p CNNL_LAYOUT_ARRAY.
 *
 *  @par Scale Limitation
 *    - The value of \p class_start_idx must be 0.
 *    - The value of \p total_classes must be equal to SEQ_LEN.
 *    - The value of \p smoothing must be 0 to 1.
 *
 *  @note
 *  - Only MLU300 and MLU500 series are supported.
 *  - Large tensor is only supported on MLU500 series, which means the input tensor number can be equal to or larger than 2G.
 *  - The data type of \p logits must be equal to that of \p dlogits, either float, half or bfloat16.
 *  - The dimensions of \p logits tensor and \p dlogits tensor must be equal to 2. And the number of elements should equal
 *  - in the same dimension.
 *  - The dimensions of \p grad_losses tensor, \p labels tensor and \p lse tensor must be the same, and
 *    be equal to 1. They have an equal number of elements.
 *  - The element of \p grad_losses tensor, \p labels tensor and \p lse tensor is equal to the number of elements in
 *    the 0th dimension of \p logits tensor or \p dlogits tensor.
 *  - Homologous operand can be performed only when the data types of inputs \p logits and output \p dlogits are the same.
 *  - The input \p ignored_index is int64 scalar, but must be in range of \f$-2^{31}\f$ and \f$2^{31}-1\f$.
 *  - The input \p class_start_idx is int64 scalar, but must be in range of \f$-2^{31}\f$ and \f$2^{31}-1\f$.
 *  - The input \p total_classes is int64 scalar, but must be in range of \f$-2^{31}\f$ and \f$2^{31}-1\f$.
 *
 *  @par Reference
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCrossEntropyBackward(cnnlHandle_t handle,
                         const float smoothing,
                         const float lse_square_scale,
                         const cnnlTensorDescriptor_t grad_losses_desc,
                         const void *grad_losses,
                         const cnnlTensorDescriptor_t logits_desc,
                         const void *logits,
                         const int64_t ignored_index,
                         const int64_t class_start_idx,
                         const int64_t total_classes,
                         void *workspace,
                         const size_t workspace_size,
                         const cnnlTensorDescriptor_t labels_desc,
                         const void *labels,
                         const cnnlTensorDescriptor_t lse_desc,
                         const void *lse,
                         cnnlTensorDescriptor_t dlogits_desc,
                         void *dlogits);

/******************************************************************************
 * Cambricon CNNL OP: cnnlSingleQueryCachedKVAttn
 ******************************************************************************/
/*! The descriptor of ::cnnlSingleQueryCachedKVAttn operation. Reserved for future use.
 */
typedef struct cnnlSingleQueryCachedKVAttnStruct *cnnlSingleQueryCachedKVAttnDescriptor_t;

// Group: cnnlSingleQueryCachedKVAttn
/*!
 *  @brief Retrieves the workspace size for ::cnnlSingleQueryCachedKVAttn operation.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlGetSingleQueryCachedKVAttnWorkspaceSize_v2 instead.
 *
 *  @param[in] handle
 *    Input. Handle to the Cambricon CNNL context that is used to manage MLU devices
 *    and queues in the ::cnnlSingleQueryCachedKVAttn operation.
 *  @param[in] op_desc
 *    Input. The descriptor of the ::cnnlSingleQueryCachedKVAttn operation.
 *  @param[in] query_desc
 *    Input. The descriptor of the \p query tensor. The shape of \p query must be
 *    [bs, seq_q, q_heads, q_head_size]. The dimension of \p seq_q can be not contiguous,
 *    but other dimensions must be contiguous.
 *  @param[in] key_cache_desc
 *    Input. The descriptor of the \p key_cache tensor. The shape of \p key_cache must be
 *    [num_blocks, kv_heads, block_size, q_head_size]. All dimensions of \p key_cache must be contiguous.
 *  @param[in] max_context_len
 *    Input. The maximum context length of the current \p key_cache.
 *  @param[out] workspace_size
 *    Output. Pointer to the host memory that stores the workspace size for
 *    ::cnnlSingleQueryCachedKVAttn operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - \p handle is NULL.
 *    - \p query_desc is NULL.
 *    - \p key_cache_desc is NULL.
 *    - \p max_context_len is less than or equal to 0.
 *    - \p workspace_size is NULL.
 */
CNNL_DEPRECATED_FOR(cnnlGetSingleQueryCachedKVAttnWorkspaceSize_v2)
cnnlStatus_t CNNL_WIN_API
cnnlGetSingleQueryCachedKVAttnWorkspaceSize(cnnlHandle_t handle,
                                            const cnnlSingleQueryCachedKVAttnDescriptor_t op_desc,
                                            const cnnlTensorDescriptor_t query_desc,
                                            const cnnlTensorDescriptor_t key_cache_desc,
                                            int max_context_len,
                                            size_t* workspace_size);

// Group: cnnlSingleQueryCachedKVAttn
/*!
 *  @brief Retrieves the workspace size for ::cnnlSingleQueryCachedKVAttn_v3 operation.
 *
 *  @param[in] handle
 *    Input. Handle to the Cambricon CNNL context that is used to manage MLU devices
 *    and queues in the ::cnnlSingleQueryCachedKVAttn_v3 operation.
 *  @param[in] op_desc
 *    Input. The descriptor of the ::cnnlSingleQueryCachedKVAttn_v3 operation.
 *  @param[in] query_desc
 *    Input. The descriptor of the \p query tensor. The shape of \p query must be
 *    [bs, seq_q, q_heads, q_head_size]. The dimension of \p seq_q can be not contiguous,
 *    but other dimensions must be contiguous.
 *  @param[in] key_cache_desc
 *    Input. The descriptor of the \p key_cache tensor. The shape of \p key_cache must be
 *    [num_blocks, kv_heads, block_size, q_head_size]. All dimensions of \p key_cache must be contiguous.
 *  @param[in] value_cache_desc
 *    Input. The descriptor of the \p value_cache tensor. The shape of \p value_cache must be
 *    [num_blocks, kv_heads, block_size, v_head_size]. All dimensions of \p value_cache must be contiguous.
 *  @param[in] max_context_len
 *    Input. The maximum context length of the current \p key_cache and \p value_cache.
 *  @param[out] workspace_size
 *    Output. Pointer to the host memory that stores the workspace size for
 *    ::cnnlSingleQueryCachedKVAttn_v3 operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - \p handle is NULL.
 *    - \p query_desc is NULL.
 *    - \p key_cache_desc is NULL.
 *    - \p max_context_len is less than or equal to 0.
 *    - \p workspace_size is NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetSingleQueryCachedKVAttnWorkspaceSize_v2(cnnlHandle_t handle,
                                              const cnnlSingleQueryCachedKVAttnDescriptor_t op_desc,
                                              const cnnlTensorDescriptor_t query_desc,
                                              const cnnlTensorDescriptor_t key_cache_desc,
                                              const cnnlTensorDescriptor_t value_cache_desc,
                                              int max_context_len,
                                              size_t* workspace_size);

// Group: cnnlSingleQueryCachedKVAttn
/*!
 *  @brief Computes decoder attention with key/value cache in GPT networks.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlSingleQueryCachedKVAttn_v3 instead.
 *
 *  @param[in] handle
 *    Input. Handle to the Cambricon CNNL context that is used to manage MLU devices
 *    and queues in the ::cnnlSingleQueryCachedKVAttn operation.
 *  @param[in] op_desc
 *    Input. The descriptor of the ::cnnlSingleQueryCachedKVAttn operation.
 *  @param[in] query_desc
 *    Input. The descriptor of the \p query tensor. The shape of \p query must be
 *    [bs, seq_q, q_heads, q_head_size]. The dimension of \p bs and \p seq_q can be
 *    not contiguous, but other dimensions must be contiguous.
 *  @param[in] query
 *    Input. Pointer to the MLU memory that stores the \p query tensor.
 *  @param[in] key_cache_desc
 *    Input. The descriptor of the \p key_cache tensor. The shape of \p key_cache must be
 *    [num_blocks, kv_heads, block_size, q_head_size] if data type is not CNNL_DTYPE_INT4X2,
 *    otherwise the shape must be [num_blocks, kv_heads, block_size, q_head_size / 2].
 *    All dimensions of \p key_cache must be contiguous.
 *  @param[in] key_cache
 *    Input. Pointer to the MLU memory that stores the \p key_cache tensor.
 *  @param[in] value_cache_desc
 *    Input. The descriptor of the \p value_cache tensor. The shape of \p value_cache must be
 *    [num_blocks, kv_heads, block_size, v_head_size] if data type is not CNNL_DTYPE_INT4X2,
 *    otherwise the shape must be [num_blocks, kv_heads, block_size / 2, v_head_size].
 *    All dimensions of \p value_cache must be contiguous.
 *  @param[in] value_cache
 *    Input. Pointer to the MLU memory that stores the \p value_cache tensor.
 *  @param[in] key_cache_quant_scale_desc
 *    Input. The descriptor of the \p key_cache_quant_scale tensor. All dimensions of \p key_cache_quant_scale
 *    must be contiguous and the data type must be float.
 *    - If \p cache_quantize_layout is \p CNNL_QUANTIZE_PER_TOKEN, the shape of \p key_cache_quant_scale
 *      must be [num_blocks, kv_heads, block_size] or [num_blocks, kv_heads, block_size, 1].
 *    - If \p cache_quantize_layout is \p CNNL_QUANTIZE_PER_CHANNEL, the shape of \p key_cache_quant_scale
 *      must be [kv_heads, q_head_size].
 *  @param[in] key_cache_quant_scale
 *    Input. Pointer to the MLU memory that stores the \p key_cache_quant_scale tensor.
 *  @param[in] value_cache_quant_scale_desc
 *    Input. The descriptor of the \p value_cache_quant_scale tensor. All dimensions of \p value_cache_quant_scale
 *    must be contiguous and the data type must be float.
 *    - If \p cache_quantize_layout is \p CNNL_QUANTIZE_PER_TOKEN, the shape of \p value_cache_quant_scale
 *      must be [num_blocks, kv_heads, block_size] or [num_blocks, kv_heads, block_size, 1].
 *    - If \p cache_quantize_layout is \p CNNL_QUANTIZE_PER_CHANNEL, the shape of \p value_cache_quant_scale
 *      must be [kv_heads, v_head_size].
 *  @param[in] value_cache_quant_scale
 *    Input. Pointer to the MLU memory that stores the \p value_cache_quant_scale tensor.
 *  @param[in] context_lens_desc
 *    Input. The descriptor of the \p context_lens tensor. The shape of \p context_lens must be
 *    [bs], and the data type must be int32.
 *  @param[in] context_lens
 *    Input. Pointer to the MLU memory that stores the \p context_lens tensor.
 *  @param[in] block_tables_desc
 *    Input. The descriptor of the \p block_tables tensor. The shape of \p block_tables must be
 *    [bs, max_num_blocks_per_seq], and the data type must be int32.
 *  @param[in] block_tables
 *    Input. Pointer to the MLU memory that stores the \p block_tables tensor.
 *  @param[in] alibi_slope_desc
 *    Input. The descriptor of the \p alibi_slope tensor. The shape of \p alibi_slope must be
 *    [bs, q_heads] or [q_heads], and the data type must be float.
 *  @param[in] alibi_slope
 *    Input. Pointer to the MLU memory that stores the \p alibi_slope tensor.
 *  @param[in] cache_quantize_layout
 *    Input. Layout of quantized \p key_cache and \p value_cache.
 *    It supports CNNL_QUANTIZE_NONE, CNNL_QUANTIZE_PER_TOKEN and CNNL_QUANTIZE_PER_CHANNEL.
 *  @param[in] max_context_len
 *    Input. Negative value or zero indicates an uncertain maximum value of \p context_lens tensor.
 *           Positive value indicates the maximum value of \p context_lens tensor.
 *           If \p max_context_len is a positive value and less than the maximum value
 *           of \p context_lens tensor, the correctness is not guaranteed.
 *  @param[in] window_size_left
 *    Input. Left window size for sliding window attention. Positive value indicates the left sliding window size,
 *           and negative value or zero indicates no sliding window attention.
 *           If \p window_size_left is greater than actual sequence length, then the left sliding window size will be truncated.
 *  @param[in] window_size_right
 *    Input. Right window size for sliding window attention. Reserved for future use.
 *  @param[in] qk_scale
 *    Input. Scaling factor before softmax.
 *  @param[in] workspace
 *    Input. Pointer to the MLU memory that is used as an extra workspace.
 *  @param[in] workspace_size
 *    Input. Size of workspace in bytes.
 *  @param[in] output_desc
 *    Input. Descriptor of \p output tensor. The shape of \p output must be
 *    [bs, seq_q, q_heads, v_head_size]. The dimension of \p bs and \p seq_q can be
 *    not contiguous, but other dimensions must be contiguous.
 *  @param[out] output
 *    Output. Pointer to the MLU memory that stores the \p output tensor.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are encountered:
 *    - \p handle is NULL.
 *    - One or more required pointers are NULL.
 *    - The scale or data type limitation is not satisfied.
 *
 *  @par Data Type
 *    - Data types of \p query and \p output must be the same, which only supports
 *      float, bfloat16, and half.
 *    - Data types of \p key_cache and \p value_cache must be the same, which supports
 *      float, bfloat16, half, int8 and int4.
 *    - Bfloat16 data type is only supported on MLU500 series.
 *    - Data types of \p context_lens and \p block_tables must be int32.
 *    - When \p cache_quantize_layout is CNNL_QUANTIZE_PER_TOKEN or CNNL_QUANTIZE_PER_CHANNEL,
 *      the data type of \p key_cache and \p value_cache must be int8, and
 *      \p key_cache_quant_scale_desc, \p key_cache_quant_scale, \p value_cache_quant_scale_desc,
 *      and \p value_cache_quant_scale cannot be nullptr.
 *    - When \p cache_quantize_layout is CNNL_QUANTIZE_NONE, the data type of \p key_cache
 *      and \p value_cache must be the same as that of \p query, and \p key_cache_quant_scale_desc,
 *      \p key_cache_quant_scale, \p value_cache_quant_scale_desc, and \p value_cache_quant_scale
 *      must be nullptr.
 *
 *  @par Scale Limitation
 *    - \p bs must be greater than 0.
 *    - \p q_head_size and \p v_head_size must be in range of [16, 576].
 *    - \p q_head_size % 2 == 0 if int4 quantization is applied.
 *    - \p seq_q must be in range of [1, 16].
 *    - \p q_heads and \p kv_heads must be in range of [1, 128], and \p q_heads % \p kv_heads == 0.
 *    - \p q_heads, \p kv_heads and \p seq_q should meet the following requirement:
 *      \p q_heads / \p kv_heads * \p seq_q <= 128 if \p q_head_size and \p v_head_size are less than or equal to 256.
 *      \p q_heads / \p kv_heads * \p seq_q <= 48 if \p q_head_size or \p v_head_size is greater than 256.
 *    - When \p max_num_blocks_per_seq > 1, \p key_cache and \p value_cache are paged memory,
 *      \p block_size only supports 16, and \p max_num_blocks_per_seq * \p block_size must be greater than or
 *      equal to \p max_context_len. Paged memory is only supported on MLU500 series.
 *    - When \p max_num_blocks_per_seq = 1, \p key_cache and \p value_cache are linear memory,
 *      and \p block_size must be greater than or equal to \p max_context_len.
 *    - When \p seq_q > 1, casual mask is automatically applied.
 *    - The interface behavior is undefined when \p seq_q > \p seq_k, where \p seq_k is the minimum value of \p context_lens.
 *    - The total element number of \p input and \p output tensor cannot exceed INT32_MAX.
 *      Meanwhile, the product of \p bs and its stride cannot exceed INT32_MAX.
 *    - The total element number of \p key_cache and \p value_cache tensor cannot exceed INT32_MAX.
 *    - The total tensor bytes of \p key_cache and \p value_cache tensor cannot exceed UINT32_MAX.
 *
 *  @note
 *    - This function only works for inference and does not support training.
 *    - Only MLU300 series and MLU500 series are supported.
 *    - Inputs and outputs cannot be homologous operand.
 *    - The content of all input tensors cannot be modified.
 */
CNNL_DEPRECATED_FOR(cnnlSingleQueryCachedKVAttn_v3)
cnnlStatus_t CNNL_WIN_API
cnnlSingleQueryCachedKVAttn(cnnlHandle_t handle,
                            const cnnlSingleQueryCachedKVAttnDescriptor_t op_desc,
                            const cnnlTensorDescriptor_t query_desc,
                            const void *query,
                            const cnnlTensorDescriptor_t key_cache_desc,
                            const void *key_cache,
                            const cnnlTensorDescriptor_t value_cache_desc,
                            const void *value_cache,
                            const cnnlTensorDescriptor_t key_cache_quant_scale_desc,
                            const void *key_cache_quant_scale,
                            const cnnlTensorDescriptor_t value_cache_quant_scale_desc,
                            const void *value_cache_quant_scale,
                            const cnnlTensorDescriptor_t context_lens_desc,
                            const void *context_lens,
                            const cnnlTensorDescriptor_t block_tables_desc,
                            const void *block_tables,
                            const cnnlTensorDescriptor_t alibi_slope_desc,
                            const void* alibi_slope,
                            cnnlQuantizeLayout_t cache_quantize_layout,
                            int max_context_len,
                            int window_size_left,
                            int window_size_right,
                            float qk_scale,
                            void *workspace,
                            const size_t workspace_size,
                            const cnnlTensorDescriptor_t output_desc,
                            void *output);

/*!
 *  @brief Computes decoder attention with key/value cache in GPT networks.
 *  Compared with ::cnnlSingleQueryCachedKVAttn, ::cnnlSingleQueryCachedKVAttn_v2 uses quantization
 *  layout type \p cnnlQuantizeScheme_t, instead of \p cnnlQuantizeLayout_t.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlSingleQueryCachedKVAttn_v3 instead.
 *
 *  @param[in] handle
 *    Input. Handle to the Cambricon CNNL context that is used to manage MLU devices
 *    and queues in the ::cnnlSingleQueryCachedKVAttn_v2 operation.
 *  @param[in] op_desc
 *    Input. The descriptor of the ::cnnlSingleQueryCachedKVAttn_v2 operation.
 *  @param[in] query_desc
 *    Input. The descriptor of the \p query tensor. The shape of \p query must be
 *    [bs, seq_q, q_heads, q_head_size]. The dimension of \p bs and \p seq_q can be
 *    not contiguous, but other dimensions must be contiguous.
 *  @param[in] query
 *    Input. Pointer to the MLU memory that stores the \p query tensor.
 *  @param[in] key_cache_desc
 *    Input. The descriptor of the \p key_cache tensor. The shape of \p key_cache must be
 *    [num_blocks, kv_heads, block_size, q_head_size] if data type is not CNNL_DTYPE_INT4X2,
 *    otherwise the shape must be [num_blocks, kv_heads, block_size, q_head_size / 2].
 *    All dimensions of \p key_cache must be contiguous.
 *  @param[in] key_cache
 *    Input. Pointer to the MLU memory that stores the \p key_cache tensor.
 *  @param[in] value_cache_desc
 *    Input. The descriptor of the \p value_cache tensor. The shape of \p value_cache must be
 *    [num_blocks, kv_heads, block_size, v_head_size] if data type is not CNNL_DTYPE_INT4X2,
 *    otherwise the shape must be [num_blocks, kv_heads, block_size / 2, v_head_size].
 *    All dimensions of \p value_cache must be contiguous.
 *  @param[in] value_cache
 *    Input. Pointer to the MLU memory that stores the \p value_cache tensor.
 *  @param[in] key_cache_quant_scale_desc
 *    Input. The descriptor of the \p key_cache_quant_scale tensor. All dimensions of \p key_cache_quant_scale
 *    must be contiguous and the data type must be float.
 *    - If \p cache_quantize_layout is \p CNNL_QUANTIZE_PER_TOKEN, the shape of \p key_cache_quant_scale
 *      must be [num_blocks, kv_heads, block_size] or [num_blocks, kv_heads, block_size, 1].
 *    - If \p cache_quantize_layout is \p CNNL_QUANTIZE_PER_CHANNEL, the shape of \p key_cache_quant_scale
 *      must be [kv_heads, q_head_size].
 *  @param[in] key_cache_quant_scale
 *    Input. Pointer to the MLU memory that stores the \p key_cache_quant_scale tensor.
 *  @param[in] value_cache_quant_scale_desc
 *    Input. The descriptor of the \p value_cache_quant_scale tensor. All dimensions of \p value_cache_quant_scale
      must be contiguous and the data type must be float.
 *    - If \p cache_quantize_layout is \p CNNL_QUANTIZE_PER_TOKEN, the shape of \p value_cache_quant_scale
 *      must be [num_blocks, kv_heads, block_size] or [num_blocks, kv_heads, block_size, 1].
 *    - If \p cache_quantize_layout is \p CNNL_QUANTIZE_PER_CHANNEL, the shape of \p value_cache_quant_scale
 *      must be [kv_heads, v_head_size].
 *  @param[in] value_cache_quant_scale
 *    Input. Pointer to the MLU memory that stores the \p value_cache_quant_scale tensor.
 *  @param[in] context_lens_desc
 *    Input. The descriptor of the \p context_lens tensor. The shape of \p context_lens must be
 *    [bs], and the data type must be int32.
 *  @param[in] context_lens
 *    Input. Pointer to the MLU memory that stores the \p context_lens tensor.
 *  @param[in] block_tables_desc
 *    Input. The descriptor of the \p block_tables tensor. The shape of \p block_tables must be
 *    [bs, max_num_blocks_per_seq], and the data type must be int32.
 *  @param[in] block_tables
 *    Input. Pointer to the MLU memory that stores the \p block_tables tensor.
 *  @param[in] alibi_slope_desc
 *    Input. The descriptor of the \p alibi_slope tensor. The shape of \p alibi_slope must be
 *    [bs, q_heads] or [q_heads], and the data type must be float.
 *  @param[in] alibi_slope
 *    Input. Pointer to the MLU memory that stores the \p alibi_slope tensor.
 *  @param[in] cache_quantize_layout
 *    Input. Layout of quantized \p key_cache and \p value_cache.
 *    It supports CNNL_QUANTIZE_NONE, CNNL_QUANTIZE_PER_TOKEN and CNNL_QUANTIZE_PER_CHANNEL.
 *  @param[in] max_context_len
 *    Input. Negative value or zero indicates an uncertain maximum value of \p context_lens tensor.
 *           Positive value indicates the maximum value of \p context_lens tensor.
 *           If \p max_context_len is a positive value and less than the maximum value
 *           of \p context_lens tensor, the correctness is not guaranteed.
 *  @param[in] window_size_left
 *    Input. Left window size for sliding window attention. Positive value indicates the left sliding window size,
 *           and negative value or zero indicates no sliding window attention.
 *           If \p window_size_left is greater than actual sequence length, then the left sliding window size will be truncated.
 *  @param[in] window_size_right
 *    Input. Right window size for sliding window attention. Reserved for future use.
 *  @param[in] qk_scale
 *    Input. Scaling factor before softmax.
 *  @param[in] workspace
 *    Input. Pointer to the MLU memory that is used as an extra workspace.
 *  @param[in] workspace_size
 *    Input. Size of workspace in bytes.
 *  @param[in] output_desc
 *    Input. Descriptor of \p output tensor. The shape of \p output must be
 *    [bs, seq_q, q_heads, v_head_size]. The dimension of \p bs and \p seq_q can be
 *    not contiguous, but other dimensions must be contiguous.
 *  @param[out] output
 *    Output. Pointer to the MLU memory that stores the \p output tensor.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are encountered:
 *    - \p handle is NULL.
 *    - One or more required pointers are NULL.
 *    - The scale or data type limitation is not satisfied.
 *
 *  @par Data Type
 *    - Data types of \p query and \p output must be the same, which only supports
 *      float, bfloat16, and half.
 *    - Data types of \p key_cache and \p value_cache must be the same, which supports
 *      float, bfloat16, half, int8 and int4.
 *    - Bfloat16 data type is only supported on MLU500 series.
 *    - Data types of \p context_lens and \p block_tables must be int32.
 *    - When \p cache_quantize_layout is CNNL_QUANTIZE_PER_TOKEN or CNNL_QUANTIZE_PER_CHANNEL,
 *      the data type of \p key_cache and \p value_cache must be int8, and
 *      \p key_cache_quant_scale_desc, \p key_cache_quant_scale, \p value_cache_quant_scale_desc,
 *      and \p value_cache_quant_scale cannot be nullptr.
 *    - When \p cache_quantize_layout is CNNL_QUANTIZE_NONE, the data type of \p key_cache
 *      and \p value_cache must be the same as that of \p query, and \p key_cache_quant_scale_desc,
 *      \p key_cache_quant_scale, \p value_cache_quant_scale_desc, and \p value_cache_quant_scale
 *      must be nullptr.
 *
 *  @par Scale Limitation
 *    - \p bs must be greater than 0.
 *    - \p q_head_size and \p v_head_size must be in range of [16, 576].
 *    - \p q_head_size % 2 == 0 if int4 quantization is applied.
 *    - \p seq_q must be in range of [1, 16].
 *    - \p q_heads and \p kv_heads must be in range of [1, 128], and \p q_heads % \p kv_heads == 0.
 *    - \p q_heads, \p kv_heads and \p seq_q should meet the following requirement:
 *      \p q_heads / \p kv_heads * \p seq_q <= 128 if \p q_head_size and \p v_head_size are less than or equal to 256.
 *      \p q_heads / \p kv_heads * \p seq_q <= 48 if \p q_head_size or \p v_head_size is greater than 256.
 *    - When \p max_num_blocks_per_seq > 1, \p key_cache and \p value_cache are paged memory,
 *      \p block_size only supports 16, and \p max_num_blocks_per_seq * \p block_size must be greater than or
 *      equal to \p max_context_len. Paged memory is only supported on MLU500 series.
 *    - When \p max_num_blocks_per_seq = 1, \p key_cache and \p value_cache are linear memory,
 *      and \p block_size must be greater than or equal to \p max_context_len.
 *    - When \p seq_q > 1, casual mask is automatically applied.
 *    - The interface behavior is undefined when \p seq_q > \p seq_k, where \p seq_k is the minimum value of \p context_lens.
 *    - The total element number of \p input and \p output tensor cannot exceed INT32_MAX.
 *      Meanwhile, the product of \p bs and its stride cannot exceed INT32_MAX.
 *    - The total element number of \p key_cache and \p value_cache tensor cannot exceed INT32_MAX.
 *    - The total tensor bytes of \p key_cache and \p value_cache tensor cannot exceed UINT32_MAX.
 *
 *  @note
 *    - This function only works for inference and does not support training.
 *    - Only MLU300 series and MLU500 series are supported.
 *    - Inputs and outputs cannot be homologous operand.
 *    - The content of all input tensors cannot be modified.
 */
CNNL_DEPRECATED_FOR(cnnlSingleQueryCachedKVAttn_v3)
cnnlStatus_t CNNL_WIN_API
cnnlSingleQueryCachedKVAttn_v2(cnnlHandle_t handle,
                               const cnnlSingleQueryCachedKVAttnDescriptor_t op_desc,
                               const cnnlTensorDescriptor_t query_desc,
                               const void *query,
                               const cnnlTensorDescriptor_t key_cache_desc,
                               const void *key_cache,
                               const cnnlTensorDescriptor_t value_cache_desc,
                               const void *value_cache,
                               const cnnlTensorDescriptor_t key_cache_quant_scale_desc,
                               const void *key_cache_quant_scale,
                               const cnnlTensorDescriptor_t value_cache_quant_scale_desc,
                               const void *value_cache_quant_scale,
                               const cnnlTensorDescriptor_t context_lens_desc,
                               const void *context_lens,
                               const cnnlTensorDescriptor_t block_tables_desc,
                               const void *block_tables,
                               const cnnlTensorDescriptor_t alibi_slope_desc,
                               const void* alibi_slope,
                               cnnlQuantizeScheme_t cache_quantize_layout,
                               int max_context_len,
                               int window_size_left,
                               int window_size_right,
                               float qk_scale,
                               void *workspace,
                               const size_t workspace_size,
                               const cnnlTensorDescriptor_t output_desc,
                               void *output);

/*!
 *  @brief Computes decoder attention with key/value cache in GPT networks.
 *  Compared with ::cnnlSingleQueryCachedKVAttn_v2, ::cnnlSingleQueryCachedKVAttn_v3 supports returning logsumexp value.
 *
 *  @param[in] handle
 *    Input. Handle to the Cambricon CNNL context that is used to manage MLU devices
 *    and queues in the ::cnnlSingleQueryCachedKVAttn_v3 operation.
 *  @param[in] op_desc
 *    Input. The descriptor of the ::cnnlSingleQueryCachedKVAttn_v3 operation.
 *  @param[in] query_desc
 *    Input. The descriptor of the \p query tensor. The shape of \p query must be
 *    [bs, seq_q, q_heads, q_head_size]. The dimension of \p bs and \p seq_q can be
 *    not contiguous, but other dimensions must be contiguous.
 *  @param[in] query
 *    Input. Pointer to the MLU memory that stores the \p query tensor.
 *  @param[in] key_cache_desc
 *    Input. The descriptor of the \p key_cache tensor. The shape of \p key_cache must be
 *    [num_blocks, kv_heads, block_size, q_head_size] if data type is not CNNL_DTYPE_INT4X2,
 *    otherwise the shape must be [num_blocks, kv_heads, block_size, q_head_size / 2].
 *    All dimensions of \p key_cache must be contiguous.
 *  @param[in] key_cache
 *    Input. Pointer to the MLU memory that stores the \p key_cache tensor.
 *  @param[in] value_cache_desc
 *    Input. The descriptor of the \p value_cache tensor. The shape of \p value_cache must be
 *    [num_blocks, kv_heads, block_size, v_head_size] if data type is not CNNL_DTYPE_INT4X2,
 *    otherwise the shape must be [num_blocks, kv_heads, block_size / 2, v_head_size].
 *    All dimensions of \p value_cache must be contiguous.
 *  @param[in] value_cache
 *    Input. Pointer to the MLU memory that stores the \p value_cache tensor.
 *  @param[in] key_cache_quant_scale_desc
 *    Input. The descriptor of the \p key_cache_quant_scale tensor. All dimensions of \p key_cache_quant_scale
 *    must be contiguous and the data type must be float.
 *    - If \p cache_quantize_layout is \p CNNL_QUANTIZE_PER_TOKEN, the shape of \p key_cache_quant_scale
 *      must be [num_blocks, kv_heads, block_size] or [num_blocks, kv_heads, block_size, 1].
 *    - If \p cache_quantize_layout is \p CNNL_QUANTIZE_PER_CHANNEL, the shape of \p key_cache_quant_scale
 *      must be [kv_heads, q_head_size].
 *  @param[in] key_cache_quant_scale
 *    Input. Pointer to the MLU memory that stores the \p key_cache_quant_scale tensor.
 *  @param[in] value_cache_quant_scale_desc
 *    Input. The descriptor of the \p value_cache_quant_scale tensor. All dimensions of \p value_cache_quant_scale
      must be contiguous and the data type must be float.
 *    - If \p cache_quantize_layout is \p CNNL_QUANTIZE_PER_TOKEN, the shape of \p value_cache_quant_scale
 *      must be [num_blocks, kv_heads, block_size] or [num_blocks, kv_heads, block_size, 1].
 *    - If \p cache_quantize_layout is \p CNNL_QUANTIZE_PER_CHANNEL, the shape of \p value_cache_quant_scale
 *      must be [kv_heads, v_head_size].
 *  @param[in] value_cache_quant_scale
 *    Input. Pointer to the MLU memory that stores the \p value_cache_quant_scale tensor.
 *  @param[in] context_lens_desc
 *    Input. The descriptor of the \p context_lens tensor. The shape of \p context_lens must be [bs],
 *    and the data type must be int32.
 *  @param[in] context_lens
 *    Input. Pointer to the MLU memory that stores the \p context_lens tensor.
 *  @param[in] block_tables_desc
 *    Input. The descriptor of the \p block_tables tensor. The shape of \p block_tables must be
 *    [bs, max_num_blocks_per_seq], and the data type must be int32.
 *  @param[in] block_tables
 *    Input. Pointer to the MLU memory that stores the \p block_tables tensor.
 *  @param[in] alibi_slope_desc
 *    Input. The descriptor of the \p alibi_slope tensor. The shape of \p alibi_slope must be
 *    [bs, q_heads] or [q_heads], and the data type must be float.
 *  @param[in] alibi_slope
 *    Input. Pointer to the MLU memory that stores the \p alibi_slope tensor.
 *  @param[in] cache_quantize_layout
 *    Input. Layout of quantized \p key_cache and \p value_cache.
 *    It supports CNNL_QUANTIZE_NONE, CNNL_QUANTIZE_PER_TOKEN and CNNL_QUANTIZE_PER_CHANNEL.
 *  @param[in] max_context_len
 *    Input. Negative value or zero indicates an uncertain maximum value of \p context_lens tensor.
 *           Positive value indicates the maximum value of \p context_lens tensor.
 *           If \p max_context_len is a positive value and less than the maximum value
 *           of \p context_lens tensor, the correctness is not guaranteed.
 *  @param[in] window_size_left
 *    Input. Left window size for sliding window attention. Positive value indicates the left sliding window size,
 *           and negative value or zero indicates no sliding window attention.
 *           If \p window_size_left is greater than actual sequence length, then the left sliding window size will be truncated.
 *  @param[in] window_size_right
 *    Input. Right window size for sliding window attention. Reserved for future use.
 *  @param[in] qk_scale
 *    Input. Scaling factor before softmax.
 *  @param[in] return_lse
 *    Input. return \p lse tensor if true.
 *  @param[in] workspace
 *    Input. Pointer to the MLU memory that is used as an extra workspace.
 *  @param[in] workspace_size
 *    Input. Size of workspace in bytes.
 *  @param[in] output_desc
 *    Input. Descriptor of \p output tensor. The shape of \p output must be
 *    [bs, seq_q, q_heads, v_head_size]. The dimension of \p bs and \p seq_q can be
 *    not contiguous, but other dimensions must be contiguous.
 *  @param[out] output
 *    Output. Pointer to the MLU memory that stores the \p output tensor.
 *  @param[in] lse_desc
 *    Input. Descriptor of \p lse tensor. The shape of \p lse must be [bs, q_heads, seq_q].
 *    If \p return_lse is True, the \p seq_q must be equal to 1.
 *    All dimensions must be contiguous.
 *  @param[out] lse
 *    Output. Pointer to the MLU memory that stores the \p lse tensor.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are encountered:
 *    - \p handle is NULL.
 *    - One or more required pointers are NULL.
 *    - The scale or data type limitation is not satisfied.
 *
 *  @par Data Type
 *    - Data types of \p query and \p output must be the same, which only supports
 *      float, bfloat16, and half.
 *    - Data types of \p key_cache and \p value_cache must be the same, which supports
 *      float, bfloat16, half, int8 and int4.
 *    - Bfloat16 data type is only supported on MLU500 series.
 *    - Data types of \p context_lens and \p block_tables must be int32.
 *    - When \p cache_quantize_layout is CNNL_QUANTIZE_PER_TOKEN or CNNL_QUANTIZE_PER_CHANNEL,
 *      the data type of \p key_cache and \p value_cache must be int8 or int4, and
 *      \p key_cache_quant_scale_desc, \p key_cache_quant_scale, \p value_cache_quant_scale_desc,
 *      and \p value_cache_quant_scale cannot be nullptr.
 *    - When \p cache_quantize_layout is CNNL_QUANTIZE_NONE, the data type of \p key_cache
 *      and \p value_cache must be the same as that of \p query, and \p key_cache_quant_scale_desc,
 *      \p key_cache_quant_scale, \p value_cache_quant_scale_desc, and \p value_cache_quant_scale
 *      must be nullptr.
 *
 *  @par Scale Limitation
 *    - \p bs must be greater than 0.
 *    - \p q_head_size and \p v_head_size must be in range of [16, 576].
 *    - \p q_head_size % 2 == 0 if int4 quantization is applied.
 *    - \p seq_q must be in range of [1, 16].
 *    - \p q_heads and \p kv_heads must be in range of [1, 128], and \p q_heads % \p kv_heads == 0.
 *    - \p q_heads, \p kv_heads and \p seq_q should meet the following requirement:
 *      \p q_heads / \p kv_heads * \p seq_q <= 128 if \p q_head_size and \p v_head_size are less than or equal to 256.
 *      \p q_heads / \p kv_heads * \p seq_q <= 48 if \p q_head_size or \p v_head_size is greater than 256.
 *    - When \p max_num_blocks_per_seq > 1, \p key_cache and \p value_cache are paged memory,
 *      \p block_size only supports 16, and \p max_num_blocks_per_seq * \p block_size must be greater than or
 *      equal to \p max_context_len. Paged memory is only supported on MLU500 series.
 *    - When \p max_num_blocks_per_seq = 1, \p key_cache and \p value_cache are linear memory,
 *      and \p block_size must be greater than or equal to \p max_context_len.
 *    - When \p seq_q > 1, casual mask is automatically applied.
 *    - The interface behavior is undefined when \p seq_q > \p seq_k, where \p seq_k is the minimum value of \p context_lens.
 *    - The total element number of \p input and \p output tensor cannot exceed INT32_MAX.
 *      Meanwhile, the product of \p bs and its stride cannot exceed INT32_MAX.
 *    - The total element number of \p key_cache and \p value_cache tensor cannot exceed INT32_MAX.
 *    - The total tensor bytes of \p key_cache and \p value_cache tensor cannot exceed UINT32_MAX.
 *
 *  @note
 *    - This function only works for inference and does not support training.
 *    - Only MLU300 series and MLU500 series are supported.
 *    - Inputs and outputs cannot be homologous operand.
 *    - The content of all input tensors cannot be modified.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSingleQueryCachedKVAttn_v3(cnnlHandle_t handle,
                               const cnnlSingleQueryCachedKVAttnDescriptor_t op_desc,
                               const cnnlTensorDescriptor_t query_desc,
                               const void *query,
                               const cnnlTensorDescriptor_t key_cache_desc,
                               const void *key_cache,
                               const cnnlTensorDescriptor_t value_cache_desc,
                               const void *value_cache,
                               const cnnlTensorDescriptor_t key_cache_quant_scale_desc,
                               const void *key_cache_quant_scale,
                               const cnnlTensorDescriptor_t value_cache_quant_scale_desc,
                               const void *value_cache_quant_scale,
                               const cnnlTensorDescriptor_t context_lens_desc,
                               const void *context_lens,
                               const cnnlTensorDescriptor_t block_tables_desc,
                               const void *block_tables,
                               const cnnlTensorDescriptor_t alibi_slope_desc,
                               const void* alibi_slope,
                               const cnnlQuantizeScheme_t cache_quantize_layout,
                               const int max_context_len,
                               const int window_size_left,
                               const int window_size_right,
                               const float qk_scale,
                               const bool return_lse,
                               void *workspace,
                               const size_t workspace_size,
                               const cnnlTensorDescriptor_t output_desc,
                               void *output,
                               const cnnlTensorDescriptor_t lse_desc,
                               void* lse);

/******************************************************************************
 * Cambricon CNNL OP: LLMQuantMatmul
 ******************************************************************************/
/*! The descriptor of ::cnnlLLMQuantMatmul operation.
 * You can use ::cnnlCreateLLMQuantMatmulDescriptor, ::cnnlSetLLMQuantMatmulDescriptor and
 * ::cnnlDestroyLLMQuantMatmulDescriptor to create, set and destroy the descriptor respectively.
 */
typedef struct cnnlLLMQuantMatmulStruct *cnnlLLMQuantMatmulDescriptor_t;

// Group:LLMQuantMatmul
/*!
 *  @brief Creates a descriptor pointed by \p attn_proj_desc
 *  for the LLMQuantMatmul operation.
 *  @param[out]  op_desc
 *    Output. Descriptor of LLMQuantMatmul operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_ALLOC_FAILED
 *    The function failed to allocate memory space.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCreateLLMQuantMatmulDescriptor(cnnlLLMQuantMatmulDescriptor_t *op_desc);

// Group:LLMQuantMatmul
/*!
 *  @brief Destroys the descriptor of LLMQuantMatmul.
 *
 *  @param[in]  op_desc
 *    Input. Descriptor of LLMQuantMatmul operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p desc is NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlDestroyLLMQuantMatmulDescriptor(cnnlLLMQuantMatmulDescriptor_t op_desc);

// Group:LLMQuantMatmul
/*!
 *  @brief Sets a descriptor of LLMQuantMatmul with values.
 *
 *  @deprecated
 *    This function is deprecated and will be removed in future release.
 *    Use ::cnnlSetLLMQuantMatmulDescriptor_v2 instead.
 *
 *  @param[in] op_desc
 *    Input. Descriptor of LLMQuantMatmul operation.
 *  @param[in] a_quant_desc
 *    Input. The quantization descriptor that holds quantization information of tensor a.
 *  @param[in] b_quant_desc
 *    Input. The quantization descriptor that holds quantization information of tensor b.
 *  @param[in] c_quant_desc
 *    Input. The quantization descriptor that holds quantization information of tensor c.
 *  @param[in] d_quant_desc
 *    Input. The quantization descriptor that holds quantization information of tensor d.
 *  @param[in] act_desc
 *    Input. The descriptor of the activation in transformer_feed_forward operation.
 *    For detailed information, see cnnlActivationDescriptor_t.
 *  @param[in] compute_dtype
 *    Input. Onchip computation data type.
 *  @param[in] quant_algo
 *    Input. The algorithm of quantization. For detailed information, see ::cnnlLLMQuantAlgo_t.
 *  @param[in] trans_a
 *    Input. A Boolean value indicating whether matrix a is transposed. Reserved for future use.
 *  @param[in] trans_b
 *    Input. A Boolean value indicating whether matrix b is transposed. Reserved for future use.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p op_desc is NULL.
 */
CNNL_DEPRECATED_FOR(cnnlSetLLMQuantMatmulDescriptor_v2)
cnnlStatus_t CNNL_WIN_API
cnnlSetLLMQuantMatmulDescriptor(cnnlLLMQuantMatmulDescriptor_t op_desc,
                                const cnnlQuantizeDescriptor_t a_quant_desc,
                                const cnnlQuantizeDescriptor_t b_quant_desc,
                                const cnnlQuantizeDescriptor_t c_quant_desc,
                                const cnnlQuantizeDescriptor_t d_quant_desc,
                                const cnnlActivationDescriptor_t act_desc,
                                cnnlDataType_t compute_dtype,
                                cnnlLLMQuantAlgo_t quant_algo,
                                bool trans_a,
                                bool trans_b);

// Group:LLMQuantMatmul
/*!
 *  @brief Sets a descriptor of LLMQuantMatmul with values.
 *
 *  Compared with ::cnnlSetLLMQuantMatmulDescriptor, this API uses cnnlQuantizeExDescriptor_t to
 *  set quantization function and parameters, instead of ::cnnlQuantizeDescriptor_t.
 *
 *  @param[in] op_desc
 *    Input. Descriptor of LLMQuantMatmul operation.
 *  @param[in] a_quant_desc
 *    Input. The quantization descriptor that holds quantization information of tensor a.
 *  @param[in] b_quant_desc
 *    Input. The quantization descriptor that holds quantization information of tensor b.
 *  @param[in] c_quant_desc
 *    Input. The quantization descriptor that holds quantization information of tensor c.
 *  @param[in] d_quant_desc
 *    Input. The quantization descriptor that holds quantization information of tensor d.
 *  @param[in] act_desc
 *    Input. The descriptor of the activation in transformer_feed_forward operation.
 *    For detailed information, see cnnlActivationDescriptor_t.
 *  @param[in] compute_dtype
 *    Input. Onchip computation data type.
 *  @param[in] quant_algo
 *    Input. The algorithm of quantization. For detailed information, see ::cnnlLLMQuantAlgo_t.
 *  @param[in] trans_a
 *    Input. A Boolean value indicating whether matrix a is transposed. Reserved for future use.
 *  @param[in] trans_b
 *    Input. A Boolean value indicating whether matrix b is transposed. Reserved for future use.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    \p op_desc is NULL.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetLLMQuantMatmulDescriptor_v2(cnnlLLMQuantMatmulDescriptor_t op_desc,
                                   const cnnlQuantizeExDescriptor_t a_quant_desc,
                                   const cnnlQuantizeExDescriptor_t b_quant_desc,
                                   const cnnlQuantizeExDescriptor_t c_quant_desc,
                                   const cnnlQuantizeExDescriptor_t d_quant_desc,
                                   const cnnlActivationDescriptor_t act_desc,
                                   cnnlDataType_t compute_dtype,
                                   cnnlLLMQuantAlgo_t quant_algo,
                                   bool trans_a,
                                   bool trans_b);

// Group:LLMQuantMatmul
/*!
 *  @brief Retrieves extra space size needed in LLMQuantMatmul operation.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
 *  @param[in] op_desc
 *    Input. Descriptor of LLMQuantMatmul operation.
 *  @param[in] a_desc
 *    Input. Descriptor of a tensor.
 *  @param[in] b_desc
 *   Input. Descriptor of b tensor.
 *  @param[in] c_desc
 *   Input. Descriptor of c tensor.
 *  @param[in] d_desc
 *   Input. Descriptor of d tensor.
 *  @param[out] workspace_size
 *    Input. Size of workspace.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
*/
cnnlStatus_t CNNL_WIN_API
cnnlGetLLMQuantMatmulWorkspaceSize(cnnlHandle_t handle,
                                   cnnlLLMQuantMatmulDescriptor_t op_desc,
                                   cnnlTensorDescriptor_t a_desc,
                                   cnnlTensorDescriptor_t b_desc,
                                   cnnlTensorDescriptor_t c_desc,
                                   cnnlTensorDescriptor_t d_desc,
                                   size_t *workspace_size);

// Group:LLMQuantMatmul
/*!
 *  @brief Executes quant matmul in LLM networks.
 *
 *  This function performs with the following steps:
 *  - When the quantization algorithm is weight_only or smooth_quant per tensor:
 *
 *    d = active((matmul(a, b) * gemm_output_scale + bias) * alpha) + c * beta.
 *  - When the quantization algorithm is smooth_quant per token:
 *
 *    d = active((matmul(a, b) * a_scale * b_scale + bias) * alpha) + c * beta.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
 *  @param[in] op_desc
 *    Input. Descriptor of LLMQuantMatmul operation.
 *  @param[in] alpha_desc
 *    Input. Descriptor of alpha tensor. Reserved for future use.
 *  @param[in] alpha
 *    Input. Pointer to the host memory that stores the alpha tensor.
 *  @param[in] a_descs
 *    Input. An array that store the descriptors of \p a tensor, \p a_scale tensor and \p a_zero tensor.
 *  @param[in] a_tensors
 *    Input. An array that store the pointers of \p a tensor, \p a_scale tensor and \p a_zero tensor.
 *  @param[in] b_descs
 *    Input. An array that store the descriptors of \p b tensor, \p b_scale tensor and \p b_zero tensor.
 *  @param[in] b_tensors
 *    Input. An array that store the pointers of \p b tensor, \p b_scale tensor and \p b_zero tensor.
 *  @param[in] beta_desc
 *    Input. Input. Descriptor of \p beta tensor. Reserved for future use.
 *  @param[in] beta
 *    Input. Pointer to the host memory that stores the \p beta tensor.
 *  @param[in] c_descs
 *    Input. Input. An array that store the descriptors of \p c tensor, \p c_scale tensor and \p c_zero tensor.
 *    When c is not needed to add, \p c_descs must be nullptr.
 *  @param[in] c_tensors
 *    Input. An array that store the pointers of \p c tensor, \p c_scale tensor and \p c_zero tensor.
 *    When c is not needed to add, \p c_tensors must be nullptr.
 *  @param[in] bias_desc
 *    Input. Descriptor of bias tensor.
 *    When bias is not needed to add, \p bias_desc must be nullptr.
 *  @param[in] bias
 *    Input. Pointer to the MLU memory that stores the bias tensor.
 *    When bias is not needed to add, \p bias must be nullptr.
 *  @param[in] gemm_output_scale_desc
 *    Input. Descriptor of \p gemm_output_scale tensor.
 *  @param[in] gemm_output_scale
 *    Input. Pointer to the MLU memory that stores the \p gemm_output_scale tensor.
 *  @param[in] gemm_output_zero_desc
 *    Input. Descriptor of \p gemm_output_zero tensor.
 *  @param[in] gemm_output_zero
 *    Input. Pointer to the MLU memory that stores the \p gemm_output_zero tensor.
 *  @param[in] workspace
 *    Input. Pointer to the MLU memory that is used as an extra workspace.
 *  @param[in] workspace_size
 *    Input. The size of workspace.
 *  @param[out] d_descs
 *    Input. An array that store the descriptors of \p d tensor, \p d_scale tensor and \p d_zero tensor.
 *  @param[out] d_tensors
 *    Input. An array that store the pointers of \p d tensor, \p d_scale tensor and \p d_zero tensor.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are encountered:
 *    - \p handle is NULL.
 *    - One or more required pointers are NULL.
 *    - The scale or data type limitation is not satisfied.
 *  @par Data Type
 *  - \p a: half, bfloat16, int8, float8_e4m3fn.
 *  - \p b: int8, float8_e4m3fn.
 *  - \p c: half, bfloat16.
 *  - \p bias: half, bfloat16.
 *  - \p d: half, bfloat16.
 *  - The data type of \p c, \p bias and \p d must be the same.
 *  @par Quantization Layout
 *  - When the quantization layout of a tensor is CNNL_QUANTIZE_NONE, it means that the tensor will not be
 *    quantized.
 *  - The supported quantization layouts are as follows:
 *    - \p a: CNNL_QUANTIZE_NONE, CNNL_QUANTIZE_PER_TENSOR, CNNL_QUANTIZE_PER_TOKEN.
 *    - \p b: CNNL_QUANTIZE_PER_CHANNEL.
 *    - \p c: CNNL_QUANTIZE_NONE.
 *    - \p d: CNNL_QUANTIZE_NONE.
 *  @note
 *  - Currently, LLMQuantMatmul supports these quantization algorithms: weight_only per_channel,
 *     weight_only group_wise, smooth_quant per_tensor and smooth_quant per_token.
 *  - The shapes of all tensors must meet the following requirements:
 *     - The shape of \p a must be [M, K].
 *     - The shape of \p b must be [N, K]. When the quant_bit_size is 4, the shape must be [N, K / 2].
 *     - The shape of \p c must be [M, N].
 *     - The shape of \p bias must be [N] or [1, N].
 *     - The shape of \p d must be [M, N].
 *  - When the quantization algorithm is weight_only per_channel:
 *    - The quantization layout of \p a should be set to CNNL_QUANTIZE_NONE.
 *    - \p gemm_output_scale cannot be nullptr. The data type of gemm_output_scale must be CNNL_DTYPE_FLOAT.
 *    - The quantization bit width must be 4 or 8.
 *    - The shape of \p gemm_output_scale must be [N].
 *  - When the quantization algorithm is weight_only group_wise:
 *    - The quantization layout of \p a should be set to CNNL_QUANTIZE_NONE.
 *    - The group_size must be 64, 128, 256 or 512.
 *    - The \p b_scale cannot be nullptr. The data type of \p b_scale must be the same with \p a.
 *    - The quantization bit width must be 4 or 8.
 *    - The shape of \p b_scale must be [N, K / group_size].
 *    - The active mode must be CNNL_ACTIVATION_IDENTITY.
 *  - When the quantization algorithm is smooth_quant per_tensor:
 *    - The quantization layout of \p a should be set to CNNL_QUANTIZE_PER_TENSOR.
 *    - The \p gemm_output_scale cannot be nullptr. The data type of \p gemm_output_scale must be CNNL_DTYPE_FLOAT.
 *    - The quantization bit width must be 4 or 8.
 *    - The shape of \p gemm_output_scale must be [N].
 *  - When the quantization algorithm is smooth_quant per_token:
 *    - The quantization layout of \p a should be set to CNNL_QUANTIZE_PER_TOKEN.
 *    - The \p a_scale and \p b_scale cannot be nullptr. The data type of \p a_scale and \p b_scale must be CNNL_DTYPE_FLOAT.
 *    - The quantization bit width must be 4 or 8.
 *    - The shape of \p a_scale must be [M], and the shape of \p b_scale must be [N].
 *  - When active mode is not CNNL_ACTIVATION_IDENTITY, \p c tensor should be nullptr.
 *  - If \p weight_only per_tensor is needed, expand the \p per_tensor value to the shape of \p per_channel.
 *  - The all-zero tensor is not supported yet.
 *  @par Example
 *    None.
*/
cnnlStatus_t CNNL_WIN_API
cnnlLLMQuantMatmul(cnnlHandle_t handle,
                   const cnnlLLMQuantMatmulDescriptor_t op_desc,
                   const cnnlTensorDescriptor_t alpha_desc,
                   const void* alpha,
                   const cnnlTensorDescriptor_t* a_descs,
                   const void** a_tensors,
                   const cnnlTensorDescriptor_t* b_descs,
                   const void** b_tensors,
                   const cnnlTensorDescriptor_t beta_desc,
                   const void* beta,
                   const cnnlTensorDescriptor_t* c_descs,
                   const void** c_tensors,
                   const cnnlTensorDescriptor_t bias_desc,
                   const void* bias,
                   const cnnlTensorDescriptor_t gemm_output_scale_desc,
                   const void* gemm_output_scale,
                   const cnnlTensorDescriptor_t gemm_output_zero_desc,
                   const void* gemm_output_zero,
                   void* workspace,
                   size_t workspace_size,
                   const cnnlTensorDescriptor_t* d_descs,
                   void** d_tensors);

// Group:MultiTensorScale
/*!
 *  @brief Executes multi-tensor checking overflow and scaling.
 *
 *  This function performs with the following steps:
 *
 *  1. Check overflow value.
 *
 *     If overflow value exists in input tensor list, then noop_flag = 1;
 *     Otherwise, noop_flag = noop_flag.
 *
 *  2. Execute scaling to all elements in all tensors.
 *
 *     output_tensor_list = input_tensor_list * scale
 *
 *  @param[in]  handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
 *  @param[in]  scale
 *    Input. A float32 value used to indicate the scaling factor.
 *  @param[in]  input_tensor_list_desc
 *    Input. Array of descriptors of \p input_tensor_list tensors.
 *  @param[in]  input_tensor_list
 *    Input. Array of pointers to the MLU memory that stores the \p input_tensor_list tensors.
 *  @param[in]  tensor_list_size
 *    Input. An int32 value used to indicate tensor list length.
 *  @param[in]  noop_flag_desc
 *    Input. Descriptor of \p noop_flag tensor.
 *  @param[in,out]  noop_flag
 *    Input and Output. Pointer to the MLU memory that stores the \p noop_flag tensor.
 *  @param[out]  output_tensor_list_desc
 *    Output. Array of descriptors of \p output_tensor_list tensors.
 *  @param[out]  output_tensor_list
 *    Output. Array of pointers to the MLU memory that stores the \p output_tensor_list tensors.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are encountered:
 *    - \p handle is NULL.
 *    - One or more required pointers are NULL.
 *    - The scale or data type limitation is not satisfied.
 *
 *  @par Data Type
 *    - Data types of \p input_tensor_list and \p output_tensor_list must be the same, which support float and half.
 *    - Data type of \p noop_flag and \p tensor_list_size must be int.
 *    - Data type of \p scale must be float.
 *
 *  @note
 *    - Only MLU300 and MLU500 series are supported.
 */

cnnlStatus_t CNNL_WIN_API
cnnlMultiTensorScale(cnnlHandle_t handle,
                     const float scale,
                     const cnnlTensorDescriptor_t input_tensor_list_desc[],
                     const void *input_tensor_list[],
                     const int tensor_list_size,
                     const cnnlTensorDescriptor_t noop_flag_desc,
                     int *noop_flag,
                     const cnnlTensorDescriptor_t output_tensor_list_desc[],
                     const void *output_tensor_list[]);


/*! The descriptor of the ::cnnlSmoothQuantOnline operation. Reserved for future use.
 *
 */

typedef struct cnnlSmoothQuantOnlineStruct *cnnlSmoothQuantOnlineDescriptor_t;

// Group:smoothQuantOnline
/*!
 * @brief Performs the smoothQuantOnline operation.
 *
 * @deprecated
 *   This function is deprecated and will be removed in future release.
 *   Use ::cnnlSmoothQuantOnline_v4 instead.
 *
 * @param[in] handle
 *   Input. Handle to the Cambricon CNNL context that is used to manage MLU devices and queues.
 * @param[in] smooth_quant_online_desc
 *   Input. Descriptor of \p smooth_quant_online about the quantization methods. Reserved for future use. Now it must be nullptr.
 * @param[in] input_desc
 *   Input. Descriptor of \p input_desc tensor. The shape of \p input should be [N, C].
 * @param[in] input
 *   Input. A pointer to the MLU memory that stores the \p input tensor.
 * @param[in] input_smooth_quant_scale_desc
 *   Input. Descriptor of \p input_smooth_quant_scale tensor. The shape of \p input_smooth_quant_scale should be [C].
 * @param[in] input_smooth_quant_scale
 *   Input. A pointer to the MLU memory that stores the \p input_smooth_quant_scale tensor.
 * @param[in] input_smooth_quant_zero_desc
 *   Input. Descriptor of \p input_smooth_quant_zero tensor. The shape of \p index should be [C]. Reserved for future use. Now it must be nullptr.
 * @param[in] input_smooth_quant_zero
 *   Input. A pointer to the MLU memory that stores the \p input_smooth_quant_zero tensor. Reserved for future use. Now it must be nullptr.
 * @param[in] output_desc
 *   Input. Descriptor of \p output tensor. The shape of \p output should be [N, C].
 * @param[out] output
 *   Output. A pointer to the MLU memory that stores the \p output tensor.
 * @param[in] output_scale_desc
 *   Input. Descriptor of \p output_scale tensor. The shape of \p output_scale should be [N].
 * @param[out] output_scale
 *   Output. A pointer to the MLU memory that stores the \p output_scale tensor.
 * @param[in] output_zero_desc
 *   Input. Descriptor of \p output_zero tensor. The shape of \p output_zero should be [N]. Reserved for future use. Now it must be nullptr.
 * @param[out] output_zero
 *   Output. A pointer to the MLU memory that stores the \p output_zero tensor. Reserved for future use. Now it must be nullptr.
 *  @param[in] workspace
 *    Input. Pointer to the MLU memory that is used as an extra workspace.
 *  @param[in] workspace_size
 *    Input. Size of workspace in bytes.
 *
 *  @par Return
 *   *  - CNNL_STATUS_SUCCESS, CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 *  - This function supports the following data types:
 *    - input tensor: float32, float16, bfloat16
 *    - input_smooth_quant_scale tensor: float32
 *    - output tensor: int8
 *    - output_scale tensor: float32
 *
 *    Data type bfloat16 is only supported on MLU500 series.
 * @par Data Layout
 * - All tensors: CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 *  - input_desc->dims[1]                    should be in range of [1, 33280].
 *  - input_smooth_quant_scale_desc->dims[0] should be in range of [1, 33280].
 *  - output_desc->dims[1]                   should be in range of [1, 33280].
 *
 * @note
 * - This operator does not support the value of N larger than int32 limit.
 *
 * @par Example
 * - None.
 */

CNNL_DEPRECATED_FOR(cnnlGroupQuant)
cnnlStatus_t CNNL_WIN_API
cnnlSmoothQuantOnline(cnnlHandle_t handle,
                      cnnlSmoothQuantOnlineDescriptor_t smooth_quant_online_desc,
                      const cnnlTensorDescriptor_t input_desc,
                      void* input,
                      const cnnlTensorDescriptor_t input_smooth_quant_scale_desc,
                      void* input_smooth_quant_scale,
                      const cnnlTensorDescriptor_t input_smooth_quant_zero_desc,
                      void* input_smooth_quant_zero,
                      const cnnlTensorDescriptor_t output_desc,
                      void* output,
                      const cnnlTensorDescriptor_t output_scale_desc,
                      void* output_scale,
                      const cnnlTensorDescriptor_t output_zero_desc,
                      void* output_zero,
                      void* workspace,
                      size_t workspace_size);

// Group:smoothQuantOnline
/*!
 * @brief Performs the smoothQuantOnline operation.
 *        Compared with ::cnnlSmoothQuantOnline, this API supports the expert mode.
 *
 * @deprecated
 *   This function is deprecated and will be removed in future release.
 *   Use ::cnnlSmoothQuantOnline_v4 instead.
 *
 * @param[in] handle
 *   Input. Handle to the Cambricon CNNL context that is used to manage MLU devices and queues.
 * @param[in] smooth_quant_online_desc
 *   Input. Descriptor of \p smooth_quant_online about the quantization methods. Reserved for future use. Now it must be nullptr.
 * @param[in] input_desc
 *   Input. Descriptor of \p input_desc tensor. The shape of \p input should be [N, C].
 * @param[in] input
 *   Input. A pointer to the MLU memory that stores the \p input tensor.
 * @param[in] input_smooth_quant_scale_desc
 *   Input. Descriptor of \p input_smooth_quant_scale tensor. The shape of \p input_smooth_quant_scale should be [C] or [experts_num, C].
 * @param[in] input_smooth_quant_scale
 *   Input. A pointer to the MLU memory that stores the \p input_smooth_quant_scale tensor.
 * @param[in] input_smooth_quant_zero_desc
 *   Input. Descriptor of \p input_smooth_quant_zero tensor. Reserved for future use. Now it must be nullptr.
 * @param[in] input_smooth_quant_zero
 *   Input. A pointer to the MLU memory that stores the \p input_smooth_quant_zero tensor. Reserved for future use. Now it must be nullptr.
 * @param[in] token_count_desc
 *   Input. Descriptor of \p token_count_desc tensor. The shape of \p token_count should be [experts_num].
 * @param[in] token_count
 *   Input. A pointer to the MLU memory that stores the \p token_count tensor.
 * @param[in] output_desc
 *   Input. Descriptor of \p output tensor. The shape of \p output should be [N, C].
 * @param[out] output
 *   Output. A pointer to the MLU memory that stores the \p output tensor.
 * @param[in] output_scale_desc
 *   Input. Descriptor of \p output_scale tensor. The shape of \p output_scale should be [N].
 * @param[out] output_scale
 *   Output. A pointer to the MLU memory that stores the \p output_scale tensor.
 * @param[in] output_zero_desc
 *   Input. Descriptor of \p output_zero tensor. The shape of \p output_zero should be [N]. Reserved for future use. Now it must be nullptr.
 * @param[out] output_zero
 *   Output. A pointer to the MLU memory that stores the \p output_zero tensor. Reserved for future use. Now it must be nullptr.
 *  @param[in] workspace
 *    Input. Pointer to the MLU memory that is used as an extra workspace.
 *  @param[in] workspace_size
 *    Input. Size of workspace in bytes.
 *
 *  @par Return
 *   *  - CNNL_STATUS_SUCCESS, CNNL_STATUS_BAD_PARAM, CNNL_STATUS_ARCH_MISMATCH, CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Data Type
 *  - This function supports the following data types:
 *    - input tensor: float32, float16, bfloat16
 *    - input_smooth_quant_scale tensor: float32
 *    - token_count tensor: int32
 *    - output tensor: int8
 *    - output_scale tensor: float32
 *
 *    Data type bfloat16 is only supported on MLU500 series.
 * @par Data Layout
 * - All tensors: CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 *  - input_desc->dims[1]                    should be in range of [1, 33280].
 *  - input_smooth_quant_scale_desc->dims[0] should be in range of [1, 33280].
 *  - output_desc->dims[1]                   should be in range of [1, 33280].
 *  - token_count_desc->dims[0]              should be in range of [1,255].
 *
 * @note
 * - This operator does not support the value of N larger than int32 limit.
 *
 * @par Example
 * - None.
 */

CNNL_DEPRECATED_FOR(cnnlGroupQuant)
cnnlStatus_t CNNL_WIN_API
cnnlSmoothQuantOnline_v2(cnnlHandle_t handle,
                         cnnlSmoothQuantOnlineDescriptor_t smooth_quant_online_desc,
                         const cnnlTensorDescriptor_t input_desc,
                         void* input,
                         const cnnlTensorDescriptor_t input_smooth_quant_scale_desc,
                         void* input_smooth_quant_scale,
                         const cnnlTensorDescriptor_t input_smooth_quant_zero_desc,
                         void* input_smooth_quant_zero,
                         const cnnlTensorDescriptor_t token_count_desc,
                         void* token_count,
                         const cnnlTensorDescriptor_t output_desc,
                         void* output,
                         const cnnlTensorDescriptor_t output_scale_desc,
                         void* output_scale,
                         const cnnlTensorDescriptor_t output_zero_desc,
                         void* output_zero,
                         void* workspace,
                         size_t workspace_size);

// Group:smoothQuantOnline
/*!
 * @brief Performs the smoothQuantOnline operation.
 *        Compared with ::cnnlSmoothQuantOnline_v2, this API supports input expand function on MLU500 series or higher.
 * *
 * @deprecated
 *   This function is deprecated and will be removed in future release.
 *   Use ::cnnlSmoothQuantOnline_v4 instead.
 *
 * @param[in] handle
 *   Input. Handle to the Cambricon CNNL context that is used to manage MLU devices and queues.
 * @param[in] smooth_quant_online_desc
 *   Input. Descriptor of \p smooth_quant_online about the quantization methods. Reserved for future use. Now it must be nullptr.
 * @param[in] input_desc
 *   Input. Descriptor of \p input tensor. The shape of \p input should be [N, C].
 * @param[in] input
 *   Input. A pointer to the MLU memory that stores the \p input tensor.
 * @param[in] input_smooth_quant_scale_desc
 *   Input. Descriptor of \p input_smooth_quant_scale tensor. The shape of \p input_smooth_quant_scale should be [C] or [experts_num, C].
 * @param[in] input_smooth_quant_scale
 *   Input. A pointer to the MLU memory that stores the \p input_smooth_quant_scale tensor.
 * @param[in] input_smooth_quant_zero_desc
 *   Input. Descriptor of \p input_smooth_quant_zero tensor. Reserved for future use. Now it must be nullptr.
 * @param[in] input_smooth_quant_zero
 *   Input. A pointer to the MLU memory that stores the \p input_smooth_quant_zero tensor. Reserved for future use. Now it must be nullptr.
 * @param[in] token_count_desc
 *   Input. Descriptor of \p token_count tensor. The shape of \p token_count should be [experts_num].
 * @param[in] token_count
 *   Input. A pointer to the MLU memory that stores the \p token_count tensor.
 * @param[in] gather_index_desc
 *   Input. Descriptor of \p gather_index tensor. The shape of \p gather_index should be [N_expand].
 * @param[in] gather_index
 *   Input. A pointer to the MLU memory that stores the \p gather_index tensor.
 * @param[in] output_desc
 *   Input. Descriptor of \p output tensor. The shape of \p output should be [N_expand, C] or [N * C].
 * @param[out] output
 *   Output. A pointer to the MLU memory that stores the \p output tensor.
 * @param[in] output_scale_desc
 *   Input. Descriptor of \p output_scale tensor. The shape of \p output_scale should be [N_expand] or [N].
 * @param[out] output_scale
 *   Output. A pointer to the MLU memory that stores the \p output_scale tensor.
 * @param[in] output_zero_desc
 *   Input. Descriptor of \p output_zero tensor. The shape of \p output_zero should be [N]. Reserved for future use. Now it must be nullptr.
 * @param[out] output_zero
 *   Output. A pointer to the MLU memory that stores the \p output_zero tensor. Reserved for future use. Now it must be nullptr.
 *  @param[in] workspace
 *    Input. Pointer to the MLU memory that is used as an extra workspace.
 *  @param[in] workspace_size
 *    Input. Size of workspace in bytes.
 *
 *  @par Return
 *   *  - CNNL_STATUS_SUCCESS, CNNL_STATUS_BAD_PARAM, CNNL_STATUS_ARCH_MISMATCH, CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Data Type
 *  - This function supports the following data types:
 *    - input tensor: float32, float16, bfloat16
 *    - input_smooth_quant_scale tensor: float32
 *    - token_count tensor: int32
 *    - gather_index tensor: int32
 *    - output tensor: int8
 *    - output_scale tensor: float32
 *
 *    Data type bfloat16 is only supported on MLU500 series.
 * @par Data Layout
 * - All tensors: CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 *  - input_desc->dims[1]                    should be in range of [1, 33280].
 *  - input_smooth_quant_scale_desc->dims[0] should be in range of [1, 33280].
 *  - output_desc->dims[1]                   should be in range of [1, 33280].
 *  - token_count_desc->dims[0]              should be in range of [1,255].
 *
 * @note
 * - This operator does not support the value of N or N_expand larger than int32 limit.
 * - Expand input mode (gather_index != null) is only supported on MLU500 series.
 *
 * @par Example
 * - None.
 */

CNNL_DEPRECATED_FOR(cnnlGroupQuant)
cnnlStatus_t CNNL_WIN_API
cnnlSmoothQuantOnline_v3(cnnlHandle_t handle,
                         cnnlSmoothQuantOnlineDescriptor_t smooth_quant_online_desc,
                         const cnnlTensorDescriptor_t input_desc,
                         void* input,
                         const cnnlTensorDescriptor_t input_smooth_quant_scale_desc,
                         void* input_smooth_quant_scale,
                         const cnnlTensorDescriptor_t input_smooth_quant_zero_desc,
                         void* input_smooth_quant_zero,
                         const cnnlTensorDescriptor_t token_count_desc,
                         void* token_count,
                         const cnnlTensorDescriptor_t gather_index_desc,
                         void* gather_index,
                         const cnnlTensorDescriptor_t output_desc,
                         void* output,
                         const cnnlTensorDescriptor_t output_scale_desc,
                         void* output_scale,
                         const cnnlTensorDescriptor_t output_zero_desc,
                         void* output_zero,
                         void* workspace,
                         size_t workspace_size);

// Group:smoothQuantOnline
/*!
 * @brief Performs the smoothQuantOnline operation.
 *        Compared with ::cnnlSmoothQuantOnline_v3, this API adds \p gather_index_start_position_desc and \p gather_index_start_position in expert parallel mode.
 *
 * @param[in] handle
 *   Input. Handle to the Cambricon CNNL context that is used to manage MLU devices and queues.
 * @param[in] smooth_quant_online_desc
 *   Input. Descriptor of \p smooth_quant_online about the quantization methods. Reserved for future use. Now it must be nullptr.
 * @param[in] input_desc
 *   Input. Descriptor of \p input tensor. The shape of \p input should be [N, C].
 * @param[in] input
 *   Input. A pointer to the MLU memory that stores the \p input tensor.
 * @param[in] input_smooth_quant_scale_desc
 *   Input. Descriptor of \p input_smooth_quant_scale tensor. The shape of \p input_smooth_quant_scale should be [C] or [experts_num, C].
 * @param[in] input_smooth_quant_scale
 *   Input. A pointer to the MLU memory that stores the \p input_smooth_quant_scale tensor.
 * @param[in] input_smooth_quant_zero_desc
 *   Input. Descriptor of \p input_smooth_quant_zero tensor. Reserved for future use. Now it must be nullptr.
 * @param[in] input_smooth_quant_zero
 *   Input. A pointer to the MLU memory that stores the \p input_smooth_quant_zero tensor. Reserved for future use. Now it must be nullptr.
 * @param[in] token_count_desc
 *   Input. Descriptor of \p token_count tensor. The shape of \p token_count should be [experts_num].
 * @param[in] token_count
 *   Input. A pointer to the MLU memory that stores the \p token_count tensor.
 * @param[in] gather_index_desc
 *   Input. Descriptor of \p gather_index tensor. The shape of \p gather_index should be [N_expand].
 * @param[in] gather_index
 *   Input. A pointer to the MLU memory that stores the \p gather_index tensor.
 * @param[in] gather_index_start_position_desc
 *   Input. Descriptor of \p gather_index_start_position tensor. The shape of \p gather_index_start_position should be [1].
 * @param[in] gather_index_start_position
 *   Input. A pointer to the MLU memory that stores the \p gather_index_start_position tensor.
 * @param[in] output_desc
 *   Input. Descriptor of \p output tensor. The shape of \p output should be [N_expand, C] or [N * C].
 * @param[out] output
 *   Output. A pointer to the MLU memory that stores the \p output tensor.
 * @param[in] output_scale_desc
 *   Input. Descriptor of \p output_scale tensor. The shape of \p output_scale should be [N_expand] or [N].
 * @param[out] output_scale
 *   Output. A pointer to the MLU memory that stores the \p output_scale tensor.
 * @param[in] output_zero_desc
 *   Input. Descriptor of \p output_zero tensor. Reserved for future use. Now it must be nullptr.
 * @param[out] output_zero
 *   Output. A pointer to the MLU memory that stores the \p output_zero tensor. Reserved for future use. Now it must be nullptr.
 *  @param[in] workspace
 *    Input. Pointer to the MLU memory that is used as an extra workspace.
 *  @param[in] workspace_size
 *    Input. Size of workspace in bytes.
 *
 *  @par Return
 *   *  - CNNL_STATUS_SUCCESS, CNNL_STATUS_BAD_PARAM, CNNL_STATUS_ARCH_MISMATCH, CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Data Type
 *  - This function supports the following data types:
 *    - input tensor: float32, float16, bfloat16
 *    - input_smooth_quant_scale tensor: float32
 *    - token_count tensor: int32
 *    - gather_index tensor: int32
 *    - gather_index_start_position tensor: int32
 *    - output tensor: int8
 *    - output_scale tensor: float32
 *
 *    Data type bfloat16 is only supported on MLU500 series.
 * @par Data Layout
 * - All tensors: CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 *  - input_desc->dims[1]                    should be in range of [1, 33280].
 *  - input_smooth_quant_scale_desc->dims[0] should be in range of [1, 33280].
 *  - output_desc->dims[1]                   should be in range of [1, 33280].
 *  - token_count_desc->dims[0]              should be in range of [1,255].
 *
 * @note
 * - This operator does not support the value of N or N_expand larger than int32 limit.
 * - Expand input mode (gather_index != null) is only supported on MLU500 series.
 *
 * @par Example
 * - None.
 */

CNNL_DEPRECATED_FOR(cnnlGroupQuant)
cnnlStatus_t CNNL_WIN_API
cnnlSmoothQuantOnline_v4(cnnlHandle_t handle,
                         cnnlSmoothQuantOnlineDescriptor_t smooth_quant_online_desc,
                         const cnnlTensorDescriptor_t input_desc,
                         const void* input,
                         const cnnlTensorDescriptor_t input_smooth_quant_scale_desc,
                         const void* input_smooth_quant_scale,
                         const cnnlTensorDescriptor_t input_smooth_quant_zero_desc,
                         const void* input_smooth_quant_zero,
                         const cnnlTensorDescriptor_t token_count_desc,
                         const void* token_count,
                         const cnnlTensorDescriptor_t gather_index_desc,
                         const void* gather_index,
                         const cnnlTensorDescriptor_t gather_index_start_position_desc,
                         const void* gather_index_start_position,
                         const cnnlTensorDescriptor_t output_desc,
                         void* output,
                         const cnnlTensorDescriptor_t output_scale_desc,
                         void* output_scale,
                         const cnnlTensorDescriptor_t output_zero_desc,
                         void* output_zero,
                         void* workspace,
                         size_t workspace_size);


// Group:groupQuant
/*!
 * @brief Performs the groupQuant operation.
 *        Compared with ::cnnlSmoothQuantOnline_v4, this API support other input quant modes: PER_TENSOR, PER_TOKEN,
 *        And the max supported size of Input channel changes from 33280 to 18432.
 *
 * @param[in] handle
 *   Input. Handle to the Cambricon CNNL context that is used to manage MLU devices and queues.
 * @param[in] static_quant_layout
 *   Input. Descriptor of the quantization method of input tensor.
 *   - It can only be CNNL_QUANTIZE_PER_TENSOR, CNNL_QUANTIZE_PER_CHANNEL,
 *     CNNL_QUANTIZE_PER_TOKEN or CNNL_QUANTIZE_NONE.
 * @param[in] dynamic_quant_layout
 *   Input. Descriptor of the quantization method of output tensor.
            It can only be CNNL_QUANTIZE_PER_TOKEN or CNNL_QUANTIZE_NONE.
 * @param[in] input_desc
 *   Input. Descriptor of \p input tensor. The shape of \p input should be [N, C].
 * @param[in] input
 *   Input. A pointer to the MLU memory that stores the \p input tensor.
 * @param[in] input_scale_desc
 *   Input. Descriptor of \p input_scale tensor.
 *   If static_quant_layout != CNNL_QUANTIZE_NONE, the shape of \p input_scale should be:
 *   - [C] when static_quant_layout == CNNL_QUANTIZE_PER_CHANNEL && token_count == null.
 *   - [experts_num, C] when static_quant_layout == CNNL_QUANTIZE_PER_CHANNEL && token_count != null.
 *   - [N] when static_quant_layout == CNNL_QUANTIZE_PER_TOKEN.
 *   - [1] when static_quant_layout == CNNL_QUANTIZE_PER_TENSOR && token_count == null.
 *   - [experts_num] when static_quant_layout == CNNL_QUANTIZE_PER_TENSOR && token_count != null.
 * @param[in] input_scale
 *   Input. A pointer to the MLU memory that stores the \p input_scale tensor.
 * @param[in] input_zero_desc
 *   Input. Descriptor of \p input_zero tensor. Reserved for future use. Now it must be nullptr.
 * @param[in] input_zero
 *   Input. A pointer to the MLU memory that stores the \p input_zero tensor. Reserved for future use. Now it must be nullptr.
 * @param[in] token_count_desc
 *   Input. Descriptor of \p token_count tensor. The shape of \p token_count should be [experts_num].
 * @param[in] token_count
 *   Input. A pointer to the MLU memory that stores the \p token_count tensor.
 * @param[in] gather_index_desc
 *   Input. Descriptor of \p gather_index tensor. The shape of \p gather_index should be [N_expand].
 * @param[in] gather_index
 *   Input. A pointer to the MLU memory that stores the \p gather_index tensor.
 * @param[in] gather_index_start_position_desc
 *   Input. Descriptor of \p gather_index_start_position tensor. The shape of \p gather_index_start_position should be [1].
 * @param[in] gather_index_start_position
 *   Input. A pointer to the MLU memory that stores the \p gather_index_start_position tensor.
 * @param[in] scale_upper_bound_desc
 *   Input. Descriptor of \p scale_upper_bound tensor. The shape of \p scale_upper_bound should be [1].
 * @param[in] scale_upper_bound
 *   Input. A pointer to the MLU memory that stores the \p scale_upper_bound tensor.
 * @param[in] output_desc
 *   Input. Descriptor of \p output tensor. The shape of \p output should be [N_expand, C] or [N, C].
 * @param[out] output
 *   Output. A pointer to the MLU memory that stores the \p output tensor.
 * @param[in] output_scale_desc
 *   Input. Descriptor of \p output_scale tensor. The shape of \p output_scale should be [N_expand] or [N].
 * @param[out] output_scale
 *   Output. A pointer to the MLU memory that stores the \p output_scale tensor.
 * @param[in] output_zero_desc
 *   Input. Descriptor of \p output_zero tensor. Reserved for future use. Now it must be nullptr.
 * @param[out] output_zero
 *   Output. A pointer to the MLU memory that stores the \p output_zero tensor. Reserved for future use. Now it must be nullptr.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace.
 * @param[in] workspace_size
 *   Input. Size of workspace in bytes.
 *
 * @par Return
 *  - CNNL_STATUS_SUCCESS, CNNL_STATUS_BAD_PARAM, CNNL_STATUS_ARCH_MISMATCH, CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Data Type
 *  - This function supports the following data types:
 *    - input tensor:                       float32, float16, bfloat16
 *    - input_scale tensor:                 float32
 *    - token_count tensor:                 int32
 *    - gather_index tensor:                int32
 *    - gather_index_start_position tensor: int32
 *    - scale_upper_bound tensor:           float32
 *    - output tensor:                      int8, float8_e4m3fn, float8_e5m2
 *    - output_scale tensor:                float32
 *
 *    Data type bfloat16 is only supported on MLU500 series.
 * @par Data Layout
 *  - All tensors: CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 *  - input_desc->dims[1]                    should be in range of [1, 18432].
 *  - input_scale_desc->dims[0]              should be in range of [1, 18432].
 *  - output_desc->dims[1]                   should be in range of [1, 18432].
 *  - token_count_desc->dims[0]              should be in range of [1, 255].
 *
 * @note
 *  - This operator does not support the value of N or N_expand larger than int32 limit.
 *  - Expand input mode (gather_index != null) is only supported on MLU500 series or above.
 *
 * @par Example
 *  - None.
 */

cnnlStatus_t CNNL_WIN_API
cnnlGroupQuant(cnnlHandle_t handle,
               cnnlQuantizeScheme_t static_quant_layout,
               cnnlQuantizeScheme_t dynamic_quant_layout,
               const cnnlTensorDescriptor_t input_desc,
               const void* input,
               const cnnlTensorDescriptor_t input_scale_desc,
               const void* input_scale,
               const cnnlTensorDescriptor_t input_zero_desc,
               const void* input_zero,
               const cnnlTensorDescriptor_t token_count_desc,
               const void* token_count,
               const cnnlTensorDescriptor_t gather_index_desc,
               const void* gather_index,
               const cnnlTensorDescriptor_t gather_index_start_position_desc,
               const void* gather_index_start_position,
               const cnnlTensorDescriptor_t scale_upper_bound_desc,
               const void* scale_upper_bound,
               const cnnlTensorDescriptor_t output_desc,
               void* output,
               const cnnlTensorDescriptor_t output_scale_desc,
               void* output_scale,
               const cnnlTensorDescriptor_t output_zero_desc,
               void* output_zero,
               void* workspace,
               size_t workspace_size);

/*!
 *  @brief Gets extra space size required in the ::cnnlTreeEnsemble function.
 *
 *  @param[in] handle
 *    Input. Handle to the Cambricon CNNL context that is used to manage MLU devices
 *    and queues in the ::cnnlTreeEnsemble operation.
 *  @param[in] input_desc
 *    Input. The descriptor of the \p input tensor. The shape of \p input must be
 *    [batch_num, feature_num].
 *  @param[in] trees_info_desc
 *    Input. The descriptor of the \p trees_info tensor. The shape of \p trees_info
 *    must be [trees_num, 3].
 *  @param[in] nodes_info_desc
 *    Input. The descriptor of the \p nodes_info tensor. The shape of \p nodes_info
 *    must be [total_node_num, 7].
 *  @param[in] nodes_th_desc
 *    Input. The descriptor of the \p nodes_th tensor. The shape of \p nodes_th
 *    must be [total_node_num].
 *  @param[in] lass_weights_desc
 *    Input. The descriptor of the \p lass_weights tensor. The shape of \p lass_weights
 *    must be [total_leaf_node_num, class_num].
 *  @param[in] post_mode
 *    Input. Post progress mode.
 *  @param[in] max_tree_node_num
 *    Input. The max node number of a single tree.
 *  @param[in] has_categorical_mode
 *    Input. It indicates whether to share the same comparison mode for all nodes.
 *  @param[in] output_desc
 *    Input. Descriptor of \p output tensor.
 *    - When \p post_mode is CNNL_TE_SUM, the shape of \p output must be [batch_num, class_num].
 *    - When \p post_mode is CNNL_TE_NONE, the shape of \p output must be [batch_num, trees_num, class_num].
 *  @param[out] workspace_size
 *    Input. Size of workspace in bytes.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are encountered:
 *    - \p handle is NULL.
 *    - One or more required pointers are NULL.
 *
 *  @par Scale Limitation
 *    - \p post_mode only supports CNNL_TE_NONE or CNNL_TE_SUM.
 *    - \p max_tree_node_num must be greater than 0.
 *    - \p has_categorical_mode must be greater than 0.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetTreeEnsembleWorkspaceSize(cnnlHandle_t handle,
                                 const cnnlTensorDescriptor_t input_desc,
                                 const cnnlTensorDescriptor_t trees_info_desc,
                                 const cnnlTensorDescriptor_t nodes_info_desc,
                                 const cnnlTensorDescriptor_t nodes_th_desc,
                                 const cnnlTensorDescriptor_t lass_weights_desc,
                                 const cnnlTreeEnsemblePostMode_t post_mode,
                                 const int64_t max_tree_node_num,
                                 const cnnlTreeEnsembleNodeMode_t has_categorical_mode,
                                 const cnnlTensorDescriptor_t output_desc,
                                 size_t *workspace_size);

/*!
 *  @brief Computes TreeEnsemble operation.
 *
 *  @param[in] handle
 *    Input. Handle to the Cambricon CNNL context that is used to manage MLU devices
 *    and queues in the ::cnnlTreeEnsemble operation.
 *  @param[in] input_desc
 *    Input. The descriptor of the \p input tensor. The shape of \p input must be
 *    [batch_num, feature_num].
 *  @param[in] input
 *    Input. Pointer to the MLU memory that stores the \p input tensor.
 *  @param[in] trees_info_desc
 *    Input. The descriptor of the \p trees_info tensor. The shape of \p trees_info
 *    must be [trees_num, 3].
 *  @param[in] trees_info
 *    Input. Pointer to the MLU memory that stores the \p trees_info tensor.
 *  @param[in] nodes_info_desc
 *    Input. The descriptor of the \p nodes_info tensor. The shape of \p nodes_info
 *    must be [total_node_num, 7].
 *  @param[in] nodes_info
 *    Input. Pointer to the MLU memory that stores the \p nodes_info tensor.
 *  @param[in] nodes_th_desc
 *    Input. The descriptor of the \p nodes_th tensor. The shape of \p nodes_th
 *    must be [total_node_num].
 *  @param[in] nodes_th
 *    Input. Pointer to the MLU memory that stores the \p nodes_th tensor.
 *  @param[in] lass_weights_desc
 *    Input. The descriptor of the \p lass_weights tensor. The shape of \p lass_weights
 *    must be [total_leaf_node_num, class_num].
 *  @param[in] lass_weights
 *    Input. Pointer to the MLU memory that stores the \p lass_weights tensor.
 *  @param[in] post_mode
 *    Input. Post progress mode.
 *  @param[in] max_tree_node_num
 *    Input. The max node number of a single tree.
 *  @param[in] has_categorical_mode
 *    Input. It indicates whether to share the same comparison mode for all nodes.
 *  @param[in] workspace
 *    Input. Pointer to the MLU memory that is used as an extra workspace.
 *  @param[in] workspace_size
 *    Input. Size of \p workspace in bytes.
 *  @param[in] output_desc
 *    Input. Descriptor of \p output tensor. The shape of \p output must be [batch_num, class_num].
 *  @param[out] output
 *    Output. Pointer to the MLU memory that stores the \p output tensor.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ends normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are encountered:
 *    - \p handle is NULL.
 *    - One or more required pointers are NULL.
 *    - The scale or data type limitation is not satisfied.
 *
 *  @par Data Type
 *    - Data types of \p input, \p nodes_th, \p lass_weights and \p output must be the same,
 *      which support float and half on MLU300 series and float, bfloat16, and half on MLU500
 *      series or above.
 *    - Data types of \p trees_info and \p nodes_info must be the same, which only support
 *      int32.
 *
 *  @par Scale Limitation
 *    - \p post_mode only supports CNNL_TE_SUM.
 *    - \p max_tree_node_num must be greater than 0.
 *    - \p has_categorical_mode must be greater than or equal to 0.
 *
 *  @note
 *    - Inputs and outputs cannot be homologous operand.
 *    - The content of all input tensors cannot be modified.
 */
cnnlStatus_t CNNL_WIN_API
cnnlTreeEnsemble(cnnlHandle_t handle,
                 const cnnlTensorDescriptor_t input_desc,
                 const void *input,
                 const cnnlTensorDescriptor_t trees_info_desc,
                 const void *trees_info,
                 const cnnlTensorDescriptor_t nodes_info_desc,
                 const void *nodes_info,
                 const cnnlTensorDescriptor_t nodes_th_desc,
                 const void *nodes_th,
                 const cnnlTensorDescriptor_t lass_weights_desc,
                 const void *lass_weights,
                 const cnnlTreeEnsemblePostMode_t post_mode,
                 const int64_t max_tree_node_num,
                 const cnnlTreeEnsembleNodeMode_t has_categorical_mode,
                 void *workspace,
                 const size_t workspace_size,
                 const cnnlTensorDescriptor_t output_desc,
                 void *output);


#if defined(__cplusplus)
}
#endif

#endif  // CNNL_EXTRA_H_
