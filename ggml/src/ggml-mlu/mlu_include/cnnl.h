/*************************************************************************
 * Copyright (C) [2019-2022] by Cambricon, Inc.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
 * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
 * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
 * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 *************************************************************************/

#ifndef CNNL_H_
#define CNNL_H_

/******************************************************************************
 * CNNL: Cambricon Network Library
 ******************************************************************************/

#define CNNL_MAJOR 1
#define CNNL_MINOR 28
#define CNNL_PATCHLEVEL 6

/*********************************************************************************
 * deprecate CNNL_VERSION is not recommended to use, it is recommended to directly
 * use CNNL_MAJOR, CNNL_MINOR and CNNL_PATCHLEVEL to get cnnl version.
********************************************************************************/
#define CNNL_VERSION (CNNL_MAJOR * 1000 + CNNL_MINOR * 100 + CNNL_PATCHLEVEL)

#define CNNL_DIM_MAX 8

#include <stdbool.h>
#include <stddef.h>
#include <stdint.h>
#include "cn_api.h"
#include "cnrt.h"

#ifndef CNNL_WIN_API
#ifdef _WIN32
#define CNNL_WIN_API __stdcall
#else
#define CNNL_WIN_API
#endif
#endif

// Disable warning messages for deprecated API-s by default
#ifndef CNNL_WARN_DEPRECATED
#define CNNL_WARN_DEPRECATED 0
#endif

#ifndef CNNL_DEPRECATED_FOR

#if CNNL_WARN_DEPRECATED

#define CNNL_PP_EVAL_SECOND_(a1, a2, ...) a2
#define CNNL_PP_EVAL_SECOND(...) CNNL_PP_EVAL_SECOND_(__VA_ARGS__)
#define CNNL_DEPRECATED_MSG0_() deprecated("Dropped support")
#define CNNL_DEPRECATED_MSG1_(f...) deprecated("Use '" #f "' instead")
#define CNNL_DEFER_MSG0_() _, CNNL_DEPRECATED_MSG0_
#define CNNL_DEPRECATED_MSG(...) \
    CNNL_PP_EVAL_SECOND(CNNL_DEFER_MSG0_ __VA_ARGS__(), CNNL_DEPRECATED_MSG1_, _)(__VA_ARGS__)

#if defined(__cplusplus) && __cplusplus >= 201309L
// C++14 feature [[deprecated(...)]]
#define CNNL_DEPRECATED_FOR(...) [[CNNL_DEPRECATED_MSG(__VA_ARGS__)]]
#define CNNL_DEPRECATED_ENUM_FOR CNNL_DEPRECATED_FOR
#else
#if defined(__has_extension)
#if __has_extension(attribute_deprecated_with_message)
#define CNNL_DEPRECATED_SUPPORT_MSG 1
#endif
#endif  // defined(__has_extension)

#if defined(__GNUC__) && (__GNUC__ > 4 || (__GNUC__ == 4 && __GNUC_MINOR__ >= 5))
#define CNNL_DEPRECATED_SUPPORT_MSG 1
#endif

#if CNNL_DEPRECATED_SUPPORT_MSG
#define CNNL_DEPRECATED_FOR(...) __attribute__((CNNL_DEPRECATED_MSG(__VA_ARGS__)))
#undef CNNL_DEPRECATED_SUPPORT_MSG
#elif defined(_MSC_VER)
#define CNNL_DEPRECATED_FOR(...) __declspec(CNNL_DEPRECATED_MSG(__VA_ARGS__))
#else
#define CNNL_DEPRECATED_FOR(f) __attribute__((deprecated))
#endif  // CNNL_DEPRECATED_SUPPORT_MSG

#if !defined(__clang__) && (defined(__GNUC__) && __GNUC__ < 6)
#define CNNL_DEPRECATED_ENUM_FOR(...)
#else
#define CNNL_DEPRECATED_ENUM_FOR CNNL_DEPRECATED_FOR
#endif

#endif  // defined(__cplusplus) && __cplusplus >= 201309L

#else   // CNNL_WARN_DEPRECATED == 0
#define CNNL_DEPRECATED_FOR(...)
#define CNNL_DEPRECATED_ENUM_FOR(...)
#endif  // CNNL_WARN_DEPRECATED

#endif  // CNNL_DEPRECATED_FOR

#if defined(__cplusplus)
extern "C" {
#endif

/******************************************************************************
 * Cambricon CNNL Return Status
 ******************************************************************************/
/*! @brief Enumeration variables describing function return status.
 */
typedef enum {
  CNNL_STATUS_SUCCESS         = 0, /*!< The operation was successfully completed. */
  CNNL_STATUS_NOT_INITIALIZED = 1,
  /*!< Cambricon CNNL library was not initialized properly, which is usually caused by the
       failure of calling ::cnnlCreate, ::cnnlCreateTensorDescriptor or ::cnnlSetTensorDescriptor.
       Such error is usually due to incompatible MLU device or invalid driver environment.
       Notice that ::cnnlCreate should be called prior to any other functions.*/
  CNNL_STATUS_ALLOC_FAILED = 2,
  /*!< This error occurs when the resource allocation failed, usually caused by the failure
       of cnMallocHost, probably because of the exceeded memory usage. Make sure that
       the memory allocated previously is deallocated as much as possible.*/
  CNNL_STATUS_BAD_PARAM = 3,
  /*!< Invalid value or parameters are passed to the function, such as invalid data type, layout, and
       dimensions.*/
  CNNL_STATUS_INTERNAL_ERROR = 4,
  /*!< Error occurred inside of the function, which may indicate an internal error in
       the library. This error is usually due to the failure of cnrtMemcpyAsync.
       Check whether the memory passed to the function was deallocated before the completion
       of the routine.*/
  CNNL_STATUS_ARCH_MISMATCH = 5,
  /*!< Invalid MLU device which was not supported by current function.*/
  CNNL_STATUS_EXECUTION_FAILED = 6,
  /*!< Error occurred when the function failed to execute on MLU devices due to multiple reasons.
       You can check whether the hardware environment, driver version and other prerequisite
       libraries are correctly installed. For more information about prerequisite libraries,
       see "Cambricon CNNL User Guide".*/
  CNNL_STATUS_NOT_SUPPORTED = 7,
  /*!< Error occurred when the requested functionality was not supported in
       this version. */
  CNNL_STATUS_NUMERICAL_OVERFLOW = 8,
  /*!< Numerical overflow occurred when executing the function,
       which is usually due to large scale or inappropriate range of value of input tensor.*/
} cnnlStatus_t;

/******************************************************************************
 * Cambricon CNNL Tensor Layout
 ******************************************************************************/
/*!
 * @brief Enumeration variables describing the data layouts in CNNL.
 *
 * The data can be defined in three, four, or five dimensions.
 *
 * Take images for example, the format of the data layout can be NCHW:
 * - N: The number of images.
 * - C: The number of image channels.
 * - H: The height of images.
 * - W: The width of images.
 *
 * Take sequence for example, the format of the data layout can be TNC:
 * - T: The timing steps of sequence.
 * - N: The batch size of sequence.
 * - C: The alphabet size of sequence.
 */
typedef enum {
  CNNL_LAYOUT_NCHW = 0,
/*!< The data layout is in the following order: batch size, channel, height, and width. */
  CNNL_LAYOUT_NHWC = 1,
/*!< The data layout is in the following order: batch size, height, width, and channel. */
  CNNL_LAYOUT_HWCN = 2,
/*!< The data layout is in the following order: height, width, channel and batch size. */
  CNNL_LAYOUT_NDHWC = 3,
/*!< The data layout is in the following order: batch size, depth, height, width, and channel.*/
  CNNL_LAYOUT_ARRAY = 4,
/*!< The data is multi-dimensional tensor. */
  CNNL_LAYOUT_NCDHW = 5,
/*!< The data layout is in the following order: batch size, channel, depth, height, and width.*/
  CNNL_LAYOUT_TNC = 6,
/*!< The data layout is in the following order: timing steps, batch size, alphabet size.*/
  CNNL_LAYOUT_NTC = 7,
/*!< The data layout is in the following order: batch size, timing steps, alphabet size.*/
  CNNL_LAYOUT_NC = 8,
/*!< The data layout is in the following order: batch size, channel.*/
  CNNL_LAYOUT_NLC = 9,
/*!< The data layout is in the following order: batch size, length, channel.*/
  CNNL_LAYOUT_NCL = 10,
/*!< The data layout is in the following order: batch size, channel, length.*/
  CNNL_LAYOUT_GNC = 11,
/*!< The data layout is in the following order: gate size, batch size, channel.*/
  CNNL_LAYOUT_NGC = 12,
/*!< The data layout is in the following order: batch size, gate size, channel.*/
} cnnlTensorLayout_t;

/******************************************************************************
 * Cambricon CNNL sequence data Layout
 ******************************************************************************/
//! @brief
/*! Enumeration variables describing the sequence data (SeqData) layouts. */
/*! N is batch, B is beam, T is sequence length, C is embedding size. */
typedef enum {
  CNNL_SEQDATA_TNC = 0,  /*!< Sequence data layout order: TNC. */
  CNNL_SEQDATA_TNC_PACKED = 1,  /*!< Sequence data layout order: TNC_PACKED. */
  CNNL_SEQDATA_NTC = 2,  /*!< Sequence data layout order: NTC. */
  CNNL_SEQDATA_NC = 3,  /*!< Sequence data layout order:  NC. */
  CNNL_SEQDATA_TNBC = 4,  /*!< Sequence data layout order: TNBC. */
  CNNL_SEQDATA_TBNC = 5,  /*!< Sequence data layout order: TBNC. */
  CNNL_SEQDATA_NBTC = 6,  /*!< Sequence data layout order: NBTC. */
  CNNL_SEQDATA_NTBC = 7,  /*!< Sequence data layout order: NTBC. */
  CNNL_SEQDATA_BNTC = 8,  /*!< Sequence data layout order: BNTC. */
  CNNL_SEQDATA_BTNC = 9,  /*!< Sequence data layout order: BTNC. */
  CNNL_SEQDATA_TN = 10,  /*!< Sequence data layout order: TN. */
  CNNL_SEQDATA_NT = 11,  /*!< Sequence data layout order: NT. */
} cnnlSeqDataLayout_t;

/******************************************************************************
 * Cambricon CNNL Data Type
 ******************************************************************************/
/*! @brief Enumeration variables describing the data types in Cambricon CNNL. */
typedef enum {
  CNNL_DTYPE_INVALID       =  0,   /*!< The data is an invalid data type. */
  CNNL_DTYPE_HALF          =  1,
  /*!< The data is a 16-bit floating-point data type with one bit for sign,
   * 5 bits for exponent and 10 bits for fraction. */
  CNNL_DTYPE_BFLOAT16      =  17,
  /*!< The data is a 16-bit floating-point data type with one bit for sign,
   * 8 bits for exponent and 7 bits for fraction. */
  CNNL_DTYPE_FLOAT         =  2,   /*!< The data is a 32-bit floating-point data type. */
  CNNL_DTYPE_FLOAT8_E4M3FN =  20,
  /*!< The data is an 8-bit floating-point data type with one bit for sign,
   * 4 bits for exponent and 3 bits for fraction, which has only NaN values and no infinite values.
   *
   * The binary formats of some values are shown as below:
   * - NaN:        S.1111.111
   * - Infinity:   N/A
   * - Max normal: S.1111.110
   * */
  CNNL_DTYPE_FLOAT8_E5M2    = 21,
  /*!< The data is an 8-bit floating-point data type with one bit for sign,
   * 5 bits for exponent and 2 bits for fraction.
   *
   * The implementation of CNNL_DTYPE_FLOAT8_E4M3FN and CNNL_DTYPE_FLOAT8_E5M2 refers to
   * [FP8 Formats for Deep Learning](https://arxiv.org/abs/2209.05433).
   * */
  CNNL_DTYPE_DOUBLE        =  14,  /*!< The data is a 64-bit floating-point data type. */
  CNNL_DTYPE_INT8          =  3,   /*!< The data is an 8-bit signed integer data type. */
  CNNL_DTYPE_INT4X2        =  19,
  /*!< The data is a 4-bit signed integer data type that is stored as an 8-bit signed integer. */
  CNNL_DTYPE_INT16         =  4,   /*!< The data is a 16-bit signed integer data type. */
  CNNL_DTYPE_INT31         =  5,   /*!< The data is a 31-bit signed integer data type. */
  CNNL_DTYPE_INT32         =  6,   /*!< The data is a 32-bit signed integer data type. */
  CNNL_DTYPE_INT64         =  9,   /*!< The data is a 64-bit signed integer data type. */
  CNNL_DTYPE_UINT8         =  7,   /*!< The data is an 8-bit unsigned integer data type. */
  CNNL_DTYPE_UINT16        = 13,   /*!< The data is a 16-bit unsigned integer data type. */
  CNNL_DTYPE_UINT32        = 11,   /*!< The data is a 32-bit unsigned integer data type. */
  CNNL_DTYPE_UINT64        = 12,   /*!< The data is a 64-bit unsigned integer data type. */
  CNNL_DTYPE_BOOL          =  8,   /*!< The data is a Boolean data type. */
  CNNL_DTYPE_COMPLEX_HALF  =  15,  /*!< The data is a 32-bit complex number of two fp16. */
  CNNL_DTYPE_COMPLEX_FLOAT =  16,  /*!< The data is a 64-bit complex number of two fp32. */
  CNNL_DTYPE_COMPLEX_DOUBLE =  18,  /*!< The data is a 128-bit complex number of two fp64. */
} cnnlDataType_t;

/*!
 * @brief Enumeration variables describing whether to propagate NaN numbers.
 */
typedef enum {
  CNNL_NOT_PROPAGATE_NAN = 0,
  /*!< The NaN numbers are not propagated.*/
  CNNL_PROPAGATE_NAN = 1,
  /*!< The NaN numbers are propagated.*/
} cnnlNanPropagation_t;

/*!
 * @brief Enumeration variables describing whether the computed results are
 * deterministic.
 */
typedef enum {
  CNNL_NON_DETERMINISTIC = 0,
  /*!< Results are not guaranteed to be reproducible.*/
  CNNL_DETERMINISTIC = 1,
  /*!< Results are guaranteed to be reproducible.*/
} cnnlDeterminism_t;

/******************************************************************************
 * Cambricon CNNL Quantization Mode
 ******************************************************************************/
/*!
 * @brief Enumeration variables describing the quantization modes.
 */
typedef enum {
  CNNL_QUANTIZE_POSITION = 0,
  /*!< Quantization method with position factor and without scale factor.*/
  CNNL_QUANTIZE_POSITION_SCALE = 1,
  /*!< Quantization method with position and scale factors.*/
  CNNL_QUANTIZE_POSITION_SCALE_OFFSET = 2,
  /*!< Asymmetric quantization method with position, scale and offset factors.*/
  CNNL_QUANTIZE_SCALE = 3,
  /*!< Quantization method with only scale factor.*/
  CNNL_QUANTIZE_PARAM_NONE = 255,
  /*!< No quantization parameters are applied.*/
} cnnlQuantizeMode_t;

/******************************************************************************
 * Cambricon CNNL quantization scheme
 ******************************************************************************/
/*!
 * @brief Enumeration variables describing the schemes of quantization.
 */
typedef enum {
  CNNL_QUANTIZE_NONE = 0,  /*!< No quantization is applied.*/
  CNNL_QUANTIZE_PER_TENSOR = 1,  /*!< Quantization is applied by tensor.*/
  CNNL_QUANTIZE_PER_CHANNEL = 2,  /*!< Quantization is applied by channel.*/
  CNNL_QUANTIZE_PER_TOKEN = 3,  /*!< Quantization is applied by token.*/
  CNNL_QUANTIZE_GROUP_WISE = 4, /*!< Quantization is applied by group.*/
} cnnlQuantizeScheme_t;

typedef cnnlQuantizeScheme_t cnnlQuantizeLayout_t;

/*********************************************************************************
 * CNNL_QUANTIZE_LAYOUT_ENUM_DECLARED is a temporary macro. It is introduced because that
 * cnnl extra has a enum cnnlQuantizeLayout_t which has the same value with cnnlQuantizeSchemet_t.
 * If cnnlQuantizeScheme_t in cnnl and cnnlQuantizeLayout_t in cnnl_extra are declared at the
 * same time, it will occur redeclaration error. So using CNNL_QUANTIZE_LAYOUT_ENUM_DECLARED
 * to mark whether cnnlQuantizeScheme_t has been declared in cnnl, if it is declared, there is no
 * need to declare cnnlQuantizeLayout_t in cnnl extra. When cnnl extra adapts to the new
 * enum cnnlQutizeScheme_t, the macro can be deleted.
********************************************************************************/
#define CNNL_QUANTIZE_LAYOUT_ENUM_DECLARED

/******************************************************************************
 * Cambricon CNNL Data Structure: Customized Operation
 ******************************************************************************/

/*!
 * @brief Enumeration variables describing the operations that can be used to
 *        implement the ::cnnlOpTensor function.
 *
 */
typedef enum {
  CNNL_OP_TENSOR_ADD = 0, /*!< The addition operation is implemented. */
  CNNL_OP_TENSOR_SUB = 1, /*!< The subtraction operation is implemented. */
  CNNL_OP_TENSOR_MUL = 2, /*!< The multiplication operation is implemented. */
} cnnlOpTensorDesc_t;

/*!
 * @brief Enumeration variables describing the algorithms that are used in the
 * implementation of the softmax function.
 *
 */
typedef enum {
  CNNL_SOFTMAX_FAST = 0,
  /*!< The Softmax function is implemented in higher speed but in lower precision. */
  /*!< Note that the range of input should be in [-0.5, 0.5] using this algorithm. */
  CNNL_SOFTMAX_ACCURATE = 1,
  /*!< The Softmax function is implemented in higher precision but in lower speed. */
  CNNL_SOFTMAX_LOG = 2,
  /*!< The LogSoftmax function is implemented. */
} cnnlSoftmaxAlgorithm_t;

/*!
 * @brief Enumeration variables describing the dimension reduction methods that are used in the
 * implementation of the softmax function.
 *
 */
typedef enum {
  CNNL_SOFTMAX_MODE_HIGH_DIMENSION = 0,   /*!< The reduction is implemented in high dimension. */
  CNNL_SOFTMAX_MODE_MEDIUM_DIMENSION = 1, /*!< The reduction is implemented in medium dimension. */
  CNNL_SOFTMAX_MODE_LOW_DIMENSION = 2,    /*!< The reduction is implemented in low dimension. */
} cnnlSoftmaxMode_t;

/*!
 * @brief Enumeration variables describing the mode of implementing the reduction operation.
 *
 * It is deprecated and will be removed in future release.
 * Use ::cnnlEmbeddingBagReduceMode_t instead.
 */
typedef enum {
  CNNL_REDUCEMODE_SUM = 0,
  /*!< Computes the sum. */
  CNNL_REDUCEMODE_MEAN = 1,
  /*!< Computes the mean value. */
  CNNL_REDUCEMODE_MAX = 2,
  /*!< Computes the maximum value. */
} cnnlReduceMode_t;

/*!
 * @brief Enumeration variables describing the mode of implementing the reduction operation
 *        in ::cnnlEmbeddingBag_v3 and ::cnnlEmbeddingBagBackward_v2.
 */
typedef enum {
  CNNL_EMBEDDING_BAG_REDUCE_SUM = 0,
  /*!< Computes the sum. */
  CNNL_EMBEDDING_BAG_REDUCE_MEAN = 1,
  /*!< Computes the mean value. */
  CNNL_EMBEDDING_BAG_REDUCE_MAX = 2,
  /*!< Computes the maximum value. */
} cnnlEmbeddingBagReduceMode_t;

/*!
 * @brief Enumeration variables describing the operation methods that are used to specify the mode
 *        of normalization in ::cnnlBatchNormForwardInferenceV2 and
 *        ::cnnlBatchNormForwardTrainingV2 functions.
 */
typedef enum {
  CNNL_BATCHNORM_PER_ACTIVATION = 0,
  /*!< Performs normalization per-activation. */
  CNNL_BATCHNORM_SPATIAL = 1,
  /*!< Performs normalization over N+spatial dimensions. */
} cnnlBatchNormMode_t;

/*!
 * @brief Enumeration variables describing the operation modes used in the
 *        implementation of ::cnnlBatchNormForwardInferenceV2 and
 *        ::cnnlBatchNormForwardTrainingV2 functions.
 */
typedef enum {
  CNNL_BATCHNORM_OPS_BN = 0,
  /*!< Only performs batch normalization.*/
  CNNL_BATCHNORM_OPS_BN_ACTIVATION = 1,
  /*!< Performs the batch normalization first,
       and then the activation.*/
  CNNL_BATCHNORM_OPS_BN_ADD_ACTIVATION = 2,
  /*!< Performs the batch normalization first,
       and then element-wise addition, followed by the activation.*/
} cnnlBatchNormOps_t;

/*!
 * @brief Enumeration variables describing the logic operations in
 *        the ::cnnlLogicOp function.
 */
typedef enum {
  CNNL_LOGIC_OP_EQ = 0,
  /*!< The element-wise Equal To comparison is performed.*/
  CNNL_LOGIC_OP_NE = 1,
  /*!< The element-wise Not Equal To comparison is performed.*/
  CNNL_LOGIC_OP_GT = 2,
  /*!< The element-wise Greater Than comparison is performed.*/
  CNNL_LOGIC_OP_GE = 3,
  /*!< The element-wise Greater Than or Equal To comparison is performed.*/
  CNNL_LOGIC_OP_LT = 4,
  /*!< The element-wise Less Than comparison is performed.*/
  CNNL_LOGIC_OP_LE = 5,
  /*!< The element-wise Less Than or Equal To comparison is performed.*/
  CNNL_LOGIC_OP_AND = 6,
  /*!< The element-wise logical AND operation is performed.*/
  CNNL_LOGIC_OP_OR = 7,
  /*!< The element-wise logical OR operation is performed.*/
  CNNL_LOGIC_OP_XOR = 8,
  /*!< The element-wise logical XOR operation is performed.*/
  CNNL_LOGIC_OP_NOT CNNL_DEPRECATED_ENUM_FOR(cnnlLogicOpNot) = 9,
  /*!< The element-wise logical NOT operation is performed.
  *
  *   CNNL_LOGIC_OP_NOT is deprecated and will be removed in future release.
  *   Use ::cnnlLogicOpNot instead.*/
} cnnlLogicOp_t;

/*!
 * @brief Enumeration variables describing the modes that are used in the
 *        implementation of scatter_nd operation.
 */
typedef enum {
  CNNL_SCATTERND_ADD = 0,
  /*!< The ADD operation is implemented.*/
  CNNL_SCATTERND_SUB = 1,
  /*!< The SUB (subtraction) operation is implemented.
   * This mode is not supported currently.*/
  CNNL_SCATTERND_MUL = 2,
  /*!< The MUL (multiplication) operation is implemented.
   * This mode is not supported currently.*/
  CNNL_SCATTERND_UPDATE = 3,
  /*!< The replacement operation is implemented.*/
} cnnlScatterNdMode_t;

/*!
 * @brief Enumeration variables describing the modes that are used in the
 *        implementation of the ::cnnlForeachBinaryOp and ::cnnlForeachUnaryOp operations.
 */
typedef enum {
  CNNL_FOREACH_ADD = 0,
  /*!< The add operation is implemented.*/
  CNNL_FOREACH_SUB = 1,
  /*!< The sub operation is implemented.*/
  CNNL_FOREACH_ZERO = 2,
  /*!< The set zero operation is implemented.*/
  CNNL_FOREACH_MUL = 3,
  /*!< The mul operation is implemented.*/
  CNNL_FOREACH_DIV = 4,
  /*!< The div operation is implemented.*/
  CNNL_FOREACH_SQRT = 5,
  /*!< The sqrt operation is implemented.*/
  CNNL_FOREACH_ADDCMUL = 6,
  /*!< The addcmul operation is implemented.*/
  CNNL_FOREACH_ADDCDIV = 7,
  /*!< The addcdiv operation is implemented.*/
} cnnlForeachOpMode_t;

/*!
 * @brief Enumeration variables describing the modes that are used in the
 *        implementation of the ForeachBinaryOp operations.
 */
typedef enum {
  FOREACH_BINARY_TENSOR_LIST = 0,
  /*!< Computes two tensorlists in ForeachBinaryOp.*/
  FOREACH_BINARY_SCALAR_LIST = 1,
  /*!< Computes a tensorlist and a scalar_list in ForeachBinaryOp.*/
  FOREACH_BINARY_SCALAR = 2,
   /*!< Computes a tensorlist and a scalar in ForeachBinaryOp.*/
  FOREACH_BINARY_SCALAR_TENSOR = 3,
   /*!< Computes a tensorlist and a scalar tensor in ForeachBinaryOp.*/
} cnnlForeachBinaryMode_t;

/*!
 * @brief Enumeration variables describing the modes that are used in the
 *        implementation of the ::cnnlForeachLerp operation.
 */
typedef enum {
  FOREACH_LERP_SCALAR = 0,
  /*!< Computes two tensorlists and a scalar in the ::cnnlForeachLerp operation.*/
  FOREACH_LERP_TENSOR_LIST = 1,
  /*!< Computes three tensorlists in the ::cnnlForeachLerp operation.*/
} cnnlForeachLerpMode_t;

/*!
 * @brief Enumeration variables describing the SortedSegmentReduceMode that are used in the
 *        implementation of the SortedSegmentReduce operation.
 */
typedef enum {
  CNNL_SEGMENT_MAX = 0,
  /*!< The max operation is implemented.*/
  CNNL_SEGMENT_MIN = 1,
  /*!< The min operation is implemented.*/
  CNNL_SEGMENT_PROD = 2,
  /*!< The prod operation is implemented.*/
  CNNL_SEGMENT_SUM = 3,
  /*!< The sum operation is implemented.*/
  CNNL_SEGMENT_MEAN = 4,
  /*!< The mean operation is implemented.*/
} cnnlSortedSegmentReduceMode_t;

/*!
 * @brief Enumeration variables describing the modes that are used in the
 *        implementation of ::cnnlScatterRef function.
 */
typedef enum {
  CNNL_SCATTERREF_ADD = 0,
  /*!< The ADD operation is implemented.*/
  CNNL_SCATTERREF_SUB = 1,
  /*!< The SUB (subtraction) operation is implemented.*/
  CNNL_SCATTERREF_MUL = 2,
  /*!< The MUL (multiplication) operation is implemented.*/
  CNNL_SCATTERREF_DIV = 3,
  /*!< The DIV (division) operation is implemented.*/
  CNNL_SCATTERREF_MIN = 4,
  /*!< The MIN (minimum) operation is implemented.*/
  CNNL_SCATTERREF_MAX = 5,
  /*!< The MAX (maximum) operation is implemented.*/
  CNNL_SCATTERREF_UPDATE = 6,
  /*!< The replacement operation is implemented.*/
} cnnlScatterRefMode_t;

/*!
 * @brief Enumeration variables describing the bitwise operations that are used in the
 *        implementation of the bitcompute function.
 */
typedef enum {
  CNNL_CYCLE_BAND_OP = 0,     /*!< The bitwsie AND operation is implemented.*/
  CNNL_CYCLE_BOR_OP = 1,      /*!< The bitwsie OR operation is implemented.*/
  CNNL_CYCLE_BXOR_OP = 2,     /*!< The bitwsie XOR operation is implemented.*/
  CNNL_BNOT_OP = 3,           /*!< The bitwsie NOT operation is implemented.*/
  CNNL_BLEFT_SHIFT_OP = 4,
  /*!< The bitwise left-shift operation is implemented for TensorFlow 1.15.*/
  CNNL_BRIGHT_SHIFT_OP = 5,
  /*!< The bitwise right-shift operation is implemented for TensorFlow 1.15.*/
  CNNL_BLEFT_SHIFT_OP_V2 = 6,
  /*!< The bitwise left-shift operation is implemented for PyTorch 1.9.*/
  CNNL_BRIGHT_SHIFT_OP_V2 = 7,
  /*!< The bitwise right-shift operation is implemented for PyTorch 1.9.*/
} cnnlBitComputeOp_t;

/*!
 * @brief Enumeration variables describing the operation modes that are used in the
 *        ::cnnlCycleOp function.
 */
typedef enum {
  CNNL_CYCLE_ADD = 0,            /*!< The ADD operation is implemented.*/
  CNNL_CYCLE_SUB = 1,            /*!< The SUB (subtraction) operation is implemented.*/
  CNNL_CYCLE_MUL = 2,            /*!< The MUL (multiplication) operation is implemented.*/
  CNNL_CYCLE_MIN_EQUAL = 3,
  /*!< The MIN_EQUAL (selecting the minimum of two vectors) operation is implemented.*/
  CNNL_CYCLE_MAX_EQUAL = 4,
  /*!< The MAX_EQUAL (selecting the maximum of two vectors) operation is implemented.*/
  CNNL_CYCLE_LESS = 5,           /*!< The LESS (less than) operation is implemented.*/
  CNNL_CYCLE_LESS_EQUAL = 6,
  /*!< The LESS_EQUAL (less than or equal to) operation is implemented.*/
  CNNL_CYCLE_EQUAL = 7,          /*!< The EQUAL (equal to) operation is implemented.*/
  CNNL_CYCLE_NEQUAL = 8,         /*!< The NEQUAL (not equal) operation is implemented.*/
  CNNL_CYCLE_GREATER = 9,        /*!< The GREATER (greater than) operation is implemented.*/
  CNNL_CYCLE_GREATER_EQUAL = 10,
  /*!< The GREATER_EQUAL (greater than or equal to) operation is implemented.*/
  CNNL_CYCLE_OR = 11,            /*!< The OR operation is implemented.*/
  CNNL_CYCLE_AND = 12,           /*!< The AND operation is implemented.*/
  CNNL_CYCLE_XOR = 13,           /*!< The XOR operation is implemented.*/
} cnnlCycleOp_t;

/*!
 * @brief
 *
 * Enumeration variables describing the algorithms that are used in the
 * implementation of the activation function.
 *
 */
typedef enum {
  CNNL_ACTIVATION_FAST = 0,
  /*!< Applies the fastest algorithm regardless of precision.*/
  CNNL_ACTIVATION_HIGH_PRECISION = 1,
  /*!< Applies the high-precision algorithm regardless of the performance.*/
} cnnlActivationPreference_t;

/*!
 * @brief Enumeration variables describing the activation modes that are used in the
 *        implementation of the Activation operation.
 */
typedef enum {
  CNNL_ACTIVATION_SIGMOID = 0,      /*!< The Sigmoid function is implemented.*/
  CNNL_ACTIVATION_RELU = 1,         /*!< The ReLU function is implemented.*/
  CNNL_ACTIVATION_RELU6 = 2,        /*!< The ReLU6 function is implemented.*/
  CNNL_ACTIVATION_TANH = 3,         /*!< The Tanh function is implemented.*/
  CNNL_ACTIVATION_CLIPPED_RELU = 4, /*!< The Clipped Rectified Linear function is implemented.*/
  CNNL_ACTIVATION_ELU = 5,          /*!< The Exponential Linear function is implemented.*/
  CNNL_ACTIVATION_IDENTITY = 6,
  /*!< The Identity function is implemented, intended for bypassing the activation.*/
  CNNL_ACTIVATION_SELU = 7, /*!< The SELU function is implemented.*/
  CNNL_ACTIVATION_GELU = 8, /*!< The GELU function is implemented.*/
  CNNL_ACTIVATION_LEAKYRELU = 9, /*!< The LeakyReLU function is implemented.*/
  CNNL_ACTIVATION_TF_LEAKYRELU = 10, /*!< The TF_LeakyReLU function is implemented.*/
  CNNL_ACTIVATION_CAFFE_RELU6  = 11, /*!< The RELU6_CAFFE function is implemented.*/
  CNNL_ACTIVATION_GLU          = 12, /*!< The GLU function is implemented.*/
  CNNL_ACTIVATION_SWISH        = 13, /*!< Reserved.*/
  CNNL_ACTIVATION_SILU         = 14, /*!< The SILU function is implemented.*/
  CNNL_ACTIVATION_HARDSIGMOID  = 15, /*!< The HARDSIGMOID function is implemented.*/
  CNNL_ACTIVATION_HARDSWISH    = 16, /*!< The HARDSWISH function is implemented.*/
  CNNL_ACTIVATION_ELU_V2       = 17,
  /*!< The new ELU function is implemented.
   * The difference between CNNL_ACTIVATION_ELU and CNNL_ACTIVATION_ELU_V2 in cnnlActivationForward is that:
   * - x>0: ELU = x; ELU_V2 = scale * x.
   * - x<=0: ELU = coef *(exp(x)-1); ELU_V2 = scale * gamma * (exp(coef * x)-1).
   *
   * Users can utilize the ELU_V2 to implement ELU in PyTorch Framework in cnnlActivationForward by setting:
   * - coef = 1.0, scale = 1.0, gamma passed by the user.
   *
   * Users can utilize the ELU_V2 to implement SELU in PyTorch Framework in cnnlActivationForward by setting:
   * - coef = 1.0, scale = 1.05070102214813232421875, gamma = 1.67326319217681884765625.
   *
   * When mode is CNNL_ACTIVATION_ELU_V2 in cnnlActivationBackward, in addition to setting parameters
   * as in cnnlActivationForward, you also need to set \p is_result = False in PyTorch Framework.*/
  CNNL_ACTIVATION_LOGSIGMOID = 18, /*!< The LOGSIGMOID function is implemented.*/
  CNNL_ACTIVATION_HARDSHRINK = 19, /*!< The HARDSHRINK function is implemented.*/
  CNNL_ACTIVATION_SOFTSHRINK = 20, /*!< The SOFTSHRINK function is implemented.*/
  CNNL_ACTIVATION_ERFC       = 21, /*!< The ERFC function is implemented.*/
  CNNL_ACTIVATION_MISH       = 22, /*!<The MISH function is implemented.*/
} cnnlActivationMode_t;

/*!
 * @brief Enumeration variables describing the attributes of the activation computation.
 */
typedef enum {
  CNNL_ACTIVATION_MODE = 0,
  /*!< Specifies the type of activation function. */
  CNNL_ACTIVATION_PREFERENCE = 1,
  /*!< Specifies the type of calculation, which decides whether precision or performance
   *is preferred. */
  CNNL_ACTIVATION_NAN_PROP = 2,
  /*!< Specifies whether to propagate NaN numbers. */
  CNNL_ACTIVATION_COEF = 3,
  /*!< A scalar representing the coefficients. */
  CNNL_ACTIVATION_SLICED_DIM = 4,
  /*!< An integer value deciding which dimension of the input to be sliced. */
  CNNL_ACTIVATION_GAMMA = 5,
  /*!< The gamma hyper parameter. */
  CNNL_ACTIVATION_SCALE = 6,
  /*!< The scale hyper parameter. */
  CNNL_ACTIVATION_IS_RESULT = 7,
  /*!< A Boolean value describes the implementation logic of ::cnnlActivationBackward when
   *   the \p mode is \p CNNL_ACTIVATION_ELU. */
  CNNL_ACTIVATION_APPROXIMATE = 8,
  /*!< A Boolean value describes the implementation logic of different GELU approximation algorithms
   *   when the \p mode is \p CNNL_ACTIVATION_GELU. */
} cnnlActivationDescAttribute_t;

/*!
 * @brief Enumeration variables describing the options that can help choose
 *        the best suited algorithm used for implementation of the activation
 *        and accumulation operations.
 **/
typedef enum {
  CNNL_COMPUTATION_FAST = 0,
  /*!< Implementation with the fastest algorithm and lower precision.*/
  CNNL_COMPUTATION_HIGH_PRECISION = 1,
  /*!< Implementation with the high-precision algorithm regardless of the performance.*/
  CNNL_COMPUTATION_ULTRAHIGH_PRECISION = 2,
  /*!< Implementation with the ultrahigh-precision algorithm regardless of the performance.*/
} cnnlComputationPreference_t;

/*!
 * @brief Enumeration variables describing the attributes of the div computation.
 */
typedef enum {
  CNNL_DIV_MODE = 0,
  /*!< Specifies the type of div function. */
} cnnlDivDescAttribute_t;

/*!
 * @brief Enumeration variables describing the div modes that are used in the
 *        implementation of the division operation.
 */
typedef enum {
  CNNL_DIV_TRUE  = 0,  /*!< The True div is implemented.*/
  CNNL_DIV_FLOOR = 1,  /*!< The Floor div is implemented.*/
  CNNL_DIV_TRUNC = 2,  /*!< The Trunc div is implemented.*/
} cnnlDivMode_t;

/*!
 * @brief
 *
 * Enumeration variables describing the modes that are used in the
 * implementation of the Reduce function.
 *
 */
typedef enum {
  CNNL_REDUCE_ADD = 0,   /*!< The reduce addition operation is implemented.*/
  CNNL_REDUCE_AVG = 1,   /*!< The reduce average operation is implemented.*/
  CNNL_REDUCE_MUL = 2,   /*!< The reduce multiplication operation is implemented.*/
  CNNL_REDUCE_MAX = 3,   /*!< The reduce maximum operation is implemented.*/
  CNNL_REDUCE_MIN = 4,   /*!< The reduce minimum operation is implemented.*/
  CNNL_REDUCE_AND = 5,   /*!< The reduce and operation is implemented.*/
  CNNL_REDUCE_OR = 6,    /*!< The reduce or operation is implemented.*/
  CNNL_REDUCE_NORM1 = 7, /*!< The sum of absolute values operation is implemented.*/
  CNNL_REDUCE_NORM2 = 8, /*!< The square root of sum of squares operation is implemented.*/
  CNNL_REDUCE_MAX_LAST_INDEX = 9,
  /*!< The operation of returning the index of the last maximum value is implemented.*/
  CNNL_REDUCE_MIN_LAST_INDEX = 10,
  /*!< The operation of returning the index of the last minimum value is implemented.*/
  CNNL_REDUCE_NORMP = 11, /*!< The 1/p power of sum of p power operation is implemented.*/
  CNNL_REDUCE_ASUM = 12,
  /*!< The sum of absolute values operation adapted to Caffe framework is implemented.*/
  CNNL_REDUCE_SUMSQ = 13,
  /*!< The sum of the squared values operation adapted to Caffe framework is implemented.*/
  CNNL_REDUCE_NANSUM = 14,
  /*!< The sum of non-NaN values operation is implemented.*/
} cnnlReduceOp_t;

/*!
 * @brief
 *
 * Enumeration variables describing the modes that are used in the
 * implementation of the StdVarMean function.
 *
 */
typedef enum {
  CNNL_STD      = 0,   /*!< The standard deviation operation is implemented.*/
  CNNL_STD_MEAN = 1,   /*!< The standard deviation and mean operation is implemented.*/
  CNNL_VAR      = 2,   /*!< The variance operation is implemented.*/
  CNNL_VAR_MEAN = 3,   /*!< The variance and mean operation is implemented.*/
} cnnlStdVarMeanOp_t;


/*!
 * @brief Enumeration variables describing the mask modes that can be used to implement
 *        the Masked operation.
 */
typedef enum {
  CNNL_MASKED_FILL = 0,
  /*!< Fills the element of the input tensor with the specified value at the position where the mask value
   * is 1.*/
  CNNL_MASKED_SCATTER = 1,
  /*!< Fills the elements of the input tensor with the specified values of an array at the position where
   * the mask value is 1.*/
  CNNL_MASKED_SELECT = 2,
  /*!< Selects the elements from the input tensor at the position
   * where the corresponding mask values are not 0.*/
  CNNL_MASKED_SCALE = 3,
  /*!< Implements dot product of input tensor, masked tensor and scale (a scalar).*/
  CNNL_MASKED_FILL_HOST = 4,
  /*!< Fills the element of the input tensor with the specified host value at the position where the mask value
   * is 1.*/
} cnnlMaskedOp_t;

/*!
 * @brief Enumeration variables describing the unique modes that can be used to implement
 *        the unique operation.
 */
typedef enum {
  CNNL_UNSORT_FORWARD = 0,
  /*!< Returns the data in the same order as the input data after eliminating the
   * duplicated values.*/
  CNNL_SORT_ASCEND = 1,
  /*!< Returns the data sorted in ascending order by input value after eliminating
   * the duplicated values.*/
  CNNL_UNSORT_REVERSE = 2,
  /*!< Returns the data in the reversed order as the input data after eliminating
   * the duplicated values.*/
} cnnlUniqueSort_t;

/*!
 * @brief
 *
 * Enumeration variables describing the mode of scatter indices that are used in the
 * implementation of the Scatter function.
 *
 */
typedef enum {
  CNNL_SCATTER = 0,
  /*!< The replacement operation is implemented.*/
  CNNL_SCATTER_ADD = 1,
  /*!< The add operation is implemented.*/
  CNNL_SCATTER_MAX = 2,
  /*!< The max operation is implemented.*/
  CNNL_SCATTER_MIN = 3
  /*!< The min operation is implemented.*/
} cnnlScatterMode_t;

/*!
 * @brief Enumeration variables describing whether the indices are computed in the
 * implementation of the reduce function.
 *
 */
typedef enum {
  CNNL_REDUCE_NO_INDICES = 0,        /*!< The indices are not computed.*/
  CNNL_REDUCE_FLATTENED_INDICES = 1, /*!< The indices and the corresponding values are computed.*/
  CNNL_REDUCE_ONLY_INDICES = 2,      /*!< Only the indices are calculated.*/
} cnnlReduceIndices_t;

/*!
 * @brief Enumeration variables describing the data type of indices used in the reduce function.
 */
typedef enum {
  CNNL_32BIT_INDICES = 0, /*!< The data type of indices is unsigned int.*/
  CNNL_16BIT_INDICES = 1, /*!< The data type of indices is unsigned short.*/
  CNNL_64BIT_INDICES = 2, /*!< The data type of indices is unsigned long.*/
} cnnlIndicesType_t;

/*!
 * @brief Enumeration variables describing the options that can help
 * choose the best suited convolution algorithm used for implementation
 * of the convolution forward operation.
 *
 * This enum is used in the ::cnnlGetConvolutionForwardAlgo function.
 *
 */
typedef enum {
  CNNL_CONVOLUTION_FWD_FASTEST = 0,
  /*!< Implementation with the fastest convolution algorithm regardless of the workspace
   *   memory to be used. */
} cnnlConvolutionFwdPreference_t;

/*!
 * @brief Enumeration variables describing the convolution algorithms that
 *        can be used to implement the Convolution Forward operation.
 */
typedef enum {
  CNNL_CONVOLUTION_FWD_ALGO_DIRECT = 0,
  /*!<  This algorithm is the default selection.
   * Set this algo if ::cnnlFindConvolutionForwardAlgo is not invoked. */
  CNNL_CONVOLUTION_FWD_ALGO_GEMM = 1,
  /*!<  This algorithm expresses the convolution as a matrix product.
   *  This algo is returned by ::cnnlFindConvolutionForwardAlgo. */
  CNNL_CONVOLUTION_FWD_ALGO_0 = 2,
  CNNL_CONVOLUTION_FWD_ALGO_1 = 3,
  CNNL_CONVOLUTION_FWD_ALGO_2 = 4,
  CNNL_CONVOLUTION_FWD_ALGO_3 = 5,
  CNNL_CONVOLUTION_FWD_ALGO_4 = 6,
  CNNL_CONVOLUTION_FWD_ALGO_5 = 7,
  CNNL_CONVOLUTION_FWD_ALGO_6 = 8,
  CNNL_CONVOLUTION_FWD_ALGO_7 = 9,
  CNNL_CONVOLUTION_FWD_ALGO_8 = 10,
  CNNL_CONVOLUTION_FWD_ALGO_9 = 11,
  CNNL_CONVOLUTION_FWD_ALGO_10 = 12,
  CNNL_CONVOLUTION_FWD_ALGO_11 = 13,
  CNNL_CONVOLUTION_FWD_ALGO_12 = 14,
  CNNL_CONVOLUTION_FWD_ALGO_13 = 15,
  CNNL_CONVOLUTION_FWD_ALGO_14 = 16,
  CNNL_CONVOLUTION_FWD_ALGO_15 = 17,
  CNNL_CONVOLUTION_FWD_ALGO_16 = 18,
  CNNL_CONVOLUTION_FWD_ALGO_17 = 19,
  CNNL_CONVOLUTION_FWD_ALGO_18 = 20,
  CNNL_CONVOLUTION_FWD_ALGO_19 = 21,
  CNNL_CONVOLUTION_FWD_ALGO_20 = 22,
  /*!< ALGO 0~20 are returned by ::cnnlFindConvolutionForwardAlgo. */
} cnnlConvolutionForwardAlgo_t;

/*!
 * @brief Enumeration variables describing the algorithm search strategies
 *        for ::cnnlFindConvolutionForwardAlgo.
 */
typedef enum {
  CNNL_CONVOLUTION_FWD_ALGO_NO_SEARCH = 0,
  /*!< Disabled algorithm search. This is the default value. */
  CNNL_CONVOLUTION_FWD_ALGO_SEARCH_FAST = 1,
  /*!< Fast search mode. This mode does not guarantee to return global optimal
   * algorithm. */
  CNNL_CONVOLUTION_FWD_ALGO_SEARCH_EXHAUSTIVE = 2,
  /*!< Exhaustive search mode. This mode guarantees to return global optimal
   * algorithm, but might be time-consuming. */
} cnnlConvolutionFwdAlgoSearchMode_t;


/*!
 * @brief Enumeration variables describing the quantization modes used for * the convolution quantization.
 *
 * It is deprecated and will be removed in future release.
 * Use ::cnnlQuantizeMode_t instead.
 */
typedef enum {
    CNNL_OFFLINE_SYMMETRIC_QUANTIZE = 0,
    /*!< The offline symmetric quantization.*/
    CNNL_OFFLINE_ASYMMETRIC_QUANTIZE = 1,
    /*!< The offline asymmetric quantization.*/
    CNNL_PARTIAL_ONLINE_QUANTIZE = 2,
    /*!< The partial online quantization.*/
    CNNL_GLOBAL_ONLINE_QUANTIZE = 3,
    /*!< The global online quantization.*/
    CNNL_NO_QUANTIZE = 255,
    /*!< No quantization is applied. This mode is only used when the data type is half or float. */
} cnnlConvolutionCastMode_t;

/*!
 * @brief Enumeration variables describing the cnnlFusedOps descriptor pointer.
 */
typedef enum {
  CNNL_PTR_NULL = 0,
  /*!< The pointer to the tensor in the cparam_pack is NULL.*/
  CNNL_PTR_VALID = 1,
  /*!< The pointer to the tensor in the cparam_pack is valid.*/
} cnnlFusedOpsPointerPlaceHolder_t;

/*!
 * @brief Enumeration variables describing the reorder type used for the
 *        reorder filter data or bias data on host.
 *
 * This enumeration is used in the ::cnnlSetConvolutionDescriptorReorderType
 * and ::cnnlSetDeconvolutionDescriptorReorderType functions.
 *
 * It is deprecated and will be removed in future release.
 *
 */
typedef enum {
  CNNL_REORDER = 0,
  /*!< The reorder data on host will be used for filter data or bias data.*/
  CNNL_NO_REORDER = 1,
  /*!< The reorder data on host will not be used for filter data or bias data.*/
} cnnlReorderType_t;

/*!
 *
 * @brief Enumeration variables describing the MLU device used for the
 *        host reorder.
 *
 */
typedef enum {
  CNNL_UNKNOWN_DEV = 0, /*!< Unknown MLU device.*/
  CNNL_MLU_220 = 1, /*!< MLU 220.*/
  CNNL_MLU_270 = 2, /*!< MLU 270.*/
  CNNL_MLU_290 = 3, /*!< MLU 290.*/
  CNNL_MLU_370_S = 4, /*!< MLU 370 S4.*/
  CNNL_MLU_370_X = 5, /*!< MLU 370 X4.*/
  CNNL_CE_3226 = 6, /*!< CE3226.*/
  CNNL_MLU_365_D2 = 7, /*!< MLU 365 D2.*/
  CNNL_MLU_370_X8 = 8, /*!< MLU 370 X8.*/
  CNNL_MLU_370_M8 = 9, /*!< MLU 370 M8.*/
  CNNL_MLU_585 = 10, /*!< Reserved.*/
  CNNL_MLU_590_H8 = 11, /*!< MLU 590.*/
  CNNL_MLU_590_M9 = 12, /*!< MLU 590.*/
  CNNL_MLU_570 = 13, /*!< Reserved.*/
  CNNL_MLU_580 = 14, /*!< Reserved.*/
} cnnlDeviceType_t;

/*!
 * @brief Enumeration variables describing the options that can help choose
 *        the best suited algorithm used for implementation of the convolution
 *        backward data operation.
 *
 * This enumeration is used in the ::cnnlGetConvolutionBackwardDataAlgo function.
 */
typedef enum {
  CNNL_CONVOLUTION_BWD_DATA_FASTEST = 0,
  /*!< Implementation with the fastest convolution backward data algorithm
   *   regardless of the workspace memory to be occupied.*/
  CNNL_CONVOLUTION_BWD_DATA_LOW_MEMORY_OCCUPY = 1,
  /*!< Implementation with the convolution backward data algorithm that
   *   occupies the least workspace memory.*/
} cnnlConvolutionBwdDataPreference_t;

/*!
 * @brief Enumeration variables describing the options that can help choose the best suited
 *        deconvolution algorithms used for implementation of the deconvolution operation.
 *        Same as ::cnnlConvolutionBwdDataPreference_t.
 *
 * This enum is used in the ::cnnlGetDeconvolutionAlgorithm and ::cnnlGetDeconvolutionAlgorithm_v2
 * functions.
 */
typedef cnnlConvolutionBwdDataPreference_t cnnlDeconvolutionPreference_t;

/*!
 * @brief Enumeration variables describing the convolution backward data
 *        algorithms that can be used to implement the convolution backward
 *        data operation.
 */
typedef enum {
  CNNL_CONVOLUTION_BWD_DATA_ALGO_DIRECT = 0,
  /*!< The basic implementation of convolution backward data operation.*/
} cnnlConvolutionBwdDataAlgo_t;

/*!
 * @brief Enumeration variables describing the deconvolution algorithms that can be used
 *        to implement the deconvolution operation.
 *        Same as ::cnnlConvolutionBwdDataAlgo_t.
 */
typedef cnnlConvolutionBwdDataAlgo_t cnnlDeconvolutionAlgo_t;

/*!
 * @brief Enumeration variables describing the quantization modes used for
 *        the deconvolution operation.
 *        Same as ::cnnlConvolutionCastMode_t.
 */
typedef cnnlConvolutionCastMode_t cnnlDeconvolutionCastMode_t;

/*!
 * @brief Enumeration variables describing the pooling modes that can be used to
 * implement the pooling operation.
 */
typedef enum {
  CNNL_POOLING_MAX = 0, /*!< The max pooling mode is implemented.*/
  CNNL_POOLING_AVERAGE_COUNT_INCLUDE_PADDING = 1,
  /*!< The average pooling with padding mode is implemented.*/
  CNNL_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING = 2,
  /*!< The average pooling without padding mode is implemented.*/
  CNNL_POOLING_FIXED = 3,
  /*!< The fixed mode is implemented. This mode is used in the unpool operation.
   * In this mode, each input pixel will be put to the center of the pooling kernel
   * regardless of the index.*/
} cnnlPoolingMode_t;

/******************************************************************************
 * Cambricon CNNL Data Structure: Nllloss
 ******************************************************************************/
/*!
 * @brief Enumeration variables describing the reduction applied to the output in the
 * implementation of the Negative-Log-Likelihood Loss function.
 *
 */
typedef enum {
  CNNL_REDUCTION_NONE = 0,
  /*!< No reduction will be applied.*/
  CNNL_REDUCTION_SUM = 1,
  /*!< The reduction that calculates the sum of the output is applied.*/
  CNNL_REDUCTION_MEAN = 2,
  /*!< The reduction that calculates the mean of the sum of the output is applied.*/
} cnnlNlllossAlgorithm_t;

/******************************************************************************
 * Cambricon CNNL Data Structure: LossReduction
 ******************************************************************************/
/*!
 * @brief Enumeration variables describing the reduction applied to the output in the
 * implementation of the loss function.
 *
 */
typedef enum {
  CNNL_LOSS_REDUCTION_NONE = 0,
  /*!< No reduction is applied in the operation.*/
  CNNL_LOSS_REDUCTION_SUM = 1,
  /*!< The elements of output are summed in the operation.*/
  CNNL_LOSS_REDUCTION_MEAN = 2,
  /*!< The weighted mean of the output is applied in the operation.*/
} cnnlLossReduction_t;

/*!
 * @brief Enumeration variables describing the modes that are used in the implementation
 * of the determinant computing function.
 */
typedef enum {
  CNNL_DET_MODE_DET = 0,
  /*!< The DET mode in which the determinant is computed.*/
  CNNL_DET_MODE_LOGDET = 1,
  /*!< The LOGDET mode in which the logarithm of the determinant is computed.*/
  CNNL_DET_MODE_SLOGDET = 2,
  /*!< The SLOGDET mode in which the sign and natural logarithm of the determinant is computed.*/
} cnnlDetMode_t;

/******************************************************************************
 * Cambricon CNNL Data Structure: SmoothL1Loss
 ******************************************************************************/
/*!
 * @brief
 * Enumeration variables describing the reduction applied to the output in the SmoothL1 Loss function.
 *
 */
typedef enum {
  CNNL_SMOOTHL1LOSS_REDUCTION_NONE = 0,
  /*!< No reduction is applied in the operation.*/
  CNNL_SMOOTHL1LOSS_REDUCTION_SUM  = 1,
  /*!< The elements of output are summed in the operation.*/
  CNNL_SMOOTHL1LOSS_REDUCTION_MEAN = 2,
  /*!< The weighted mean of the output is applied in the operation.*/
} cnnlSmoothL1LossAlgorithm_t;

/*!
 * @brief Enumeration variables describing the options that can help to
 * choose the best suited algorithm used for implementation
 * of the convolution backward filter operation.
 * Currently only \p CNNL_CONVOLUTION_BWD_FILTER_FASTEST is supported.
 *
 */
typedef enum {
  CNNL_CONVOLUTION_BWD_FILTER_FASTEST = 0,
  /*!< Implementation with the fastest convolution backward filter algorithm
   *   regardless of the workspace memory to be occupied.*/
  CNNL_CONVOLUTION_BWD_FILTER_LOW_MEMORY_OCCUPY = 1,
  /*!< Implementation with the convolution backward filter algorithm that
   *   occupies the least workspace memory.*/
} cnnlConvolutionBwdFilterPreference_t;

/*!
 * @brief Enumeration variables describing the different algorithms that can be used to
 * implement the convolution backward filter operation.
 *
 */
typedef enum {
  CNNL_CONVOLUTION_BWD_FILTER_ALGO_DIRECT = 0,
  /*!< A direct convolution without doing matrix multiplication is applied.*/
  CNNL_CONVOLUTION_BWD_FILTER_ALGO_GEMM = 1,
  /*!< Convolution as an explicit matrix multiplication is applied.*/
} cnnlConvolutionBwdFilterAlgo_t;

/*!
 * @brief
 * Enumeration variables describing the supported data conversion modes of the Cast function that
 * converts data from one data type to another.
 */
typedef enum {
  CNNL_CAST_FLOAT_TO_HALF = 210,
  /*!< Converts data from float to half. The ``round-nearest-even`` mode is used. */
  CNNL_CAST_FLOAT_TO_INT32 = 260,
  /*!< Converts data from float to int32. The ``round-to-zero`` mode is used.*/
  CNNL_CAST_FLOAT_TO_INT16 = 240,
  /*!< Converts data from float to int16. The ``round-to-zero`` mode is used. */
  CNNL_CAST_FLOAT_TO_INT8 = 230,
  /*!< Converts data from float to int8. The ``round-to-zero`` mode is used. */
  CNNL_CAST_FLOAT_TO_UINT8 = 270,
  /*!< Converts data from float to uint8. The ``round-to-zero`` mode is used. */
  CNNL_CAST_FLOAT_TO_BOOL = 280,  /*!< Converts data from float to bool.  */
  CNNL_CAST_FLOAT_TO_COMPLEX_HALF = 215,
  /*!< Converts data from float to complex_half. The ``round-nearest-even`` mode is used.*/
  CNNL_CAST_FLOAT_TO_COMPLEX_FLOAT = 216, /*!< Converts data from float to complex_float.*/
  CNNL_CAST_HALF_TO_FLOAT = 120,  /*!< Converts data from half to float.  */
  CNNL_CAST_HALF_TO_INT32 = 160,
  /*!< Converts data from half to int32. The ``round-to-zero`` mode is used. */
  CNNL_CAST_HALF_TO_INT16 = 140,
  /*!< Converts data from half to int16. The ``round-to-zero`` mode is used. */
  CNNL_CAST_HALF_TO_INT8 = 130,
  /*!< Converts data from half to int8.  The ``round-to-zero`` mode is used. */
  CNNL_CAST_HALF_TO_UINT8 = 170,
  /*!< Converts data from half to uint8. The ``round-to-zero`` mode is used. */
  CNNL_CAST_HALF_TO_BOOL = 180,   /*!< Converts data from half to bool.      */
  CNNL_CAST_HALF_TO_COMPLEX_HALF = 115,
  /*!< Converts data from half to complex_half.  */
  CNNL_CAST_HALF_TO_COMPLEX_FLOAT = 116,
  /*!< Converts data from half to complex_float. */
  CNNL_CAST_INT32_TO_FLOAT = 620,
  /*!< Converts data from int32 to float. The ``round-to-zero`` mode is used. */
  CNNL_CAST_INT32_TO_HALF = 610,
  /*!< Converts data from int32 to half. The ``round-nearest-even`` mode is used. */
  CNNL_CAST_INT32_TO_COMPLEX_HALF = 615,
  /*!< Converts data from int32 to complex_half. The ``round-nearest-even`` mode is used. */
  CNNL_CAST_INT32_TO_COMPLEX_FLOAT = 616,
  /*!< Converts data from int32 to complex_float. The ``round-to-zero`` mode is used. */
  CNNL_CAST_INT32_TO_BFLOAT16 = 617,
  /*!< Converts data from int32 to bfloat16. The ``round-nearest-even`` mode is used. */
  CNNL_CAST_INT32_TO_INT8 = 630,  /*!< Converts data from int32 to int8.  */
  CNNL_CAST_INT32_TO_INT16 = 640, /*!< Converts data from int32 to int16. */
  CNNL_CAST_INT32_TO_UINT8 = 670, /*!< Converts data from int32 to uint8. */
  CNNL_CAST_INT16_TO_FLOAT = 420, /*!< Converts data from int16 to float. */
  CNNL_CAST_INT16_TO_HALF = 410,
  /*!< Converts data from int16 to half. The ``round-nearest-even`` mode is used.         */
  CNNL_CAST_INT16_TO_COMPLEX_HALF = 415,
  /*!< Converts data from int16 to complex_half. The ``round-nearest-even`` mode is used. */
  CNNL_CAST_INT16_TO_COMPLEX_FLOAT = 416,
  /*!< Converts data from int16 to complex_float. */
  CNNL_CAST_INT16_TO_BFLOAT16 = 417,
  /*!< Converts data from int16 to bfloat16. The ``round-nearest-even`` mode is used. */
  CNNL_CAST_INT16_TO_INT64 = 490, /*!< Converts data from int16 to int64. */
  CNNL_CAST_INT16_TO_INT32 = 460, /*!< Converts data from int16 to int32. */
  CNNL_CAST_INT16_TO_INT8 = 430,  /*!< Converts data from int16 to int8.  */
  CNNL_CAST_INT8_TO_FLOAT = 320,  /*!< Converts data from int8 to float.  */
  CNNL_CAST_INT8_TO_HALF = 310,   /*!< Converts data from int8 to half.   */
  CNNL_CAST_INT8_TO_COMPLEX_HALF = 315, /*!< Converts data from int8 to complex_half.    */
  CNNL_CAST_INT8_TO_COMPLEX_FLOAT = 316,  /*!< Converts data from int8 to complex_float. */
  CNNL_CAST_INT8_TO_BFLOAT16 = 317,   /*!< Converts data from int8 to bfloat16.*/
  CNNL_CAST_INT8_TO_INT32 = 360,  /*!< Converts data from int8 to int32.  */
  CNNL_CAST_UINT8_TO_FLOAT = 720, /*!< Converts data from uint8 to float. */
  CNNL_CAST_UINT8_TO_HALF = 710,  /*!< Converts data from uint8 to half.  */
  CNNL_CAST_UINT8_TO_COMPLEX_HALF = 715,  /*!< Converts data from uint8 to complex_half.  */
  CNNL_CAST_UINT8_TO_COMPLEX_FLOAT = 716, /*!< Converts data from uint8 to complex_float. */
  CNNL_CAST_UINT8_TO_BFLOAT16 = 717, /*!< Converts data from uint8 to bfloat16. */
  CNNL_CAST_BOOL_TO_FLOAT = 820,  /*!< Converts data from bool to float.  */
  CNNL_CAST_BOOL_TO_HALF = 810,   /*!< Converts data from bool to half.   */
  CNNL_CAST_BOOL_TO_COMPLEX_HALF = 815,   /*!< Converts data from bool to complex_half.  */
  CNNL_CAST_BOOL_TO_COMPLEX_FLOAT = 816,  /*!< Converts data from bool to complex_float. */
  CNNL_CAST_BOOL_TO_BFLOAT16 = 817, /*!< Converts data from bool to bfloat16.*/
  CNNL_CAST_BOOL_TO_INT32 = 860,  /*!< Converts data from bool to int32.  */
  CNNL_CAST_BOOL_TO_INT64 = 890,  /*!< Converts data from bool to int64.  */
  CNNL_CAST_UINT8_TO_INT32 = 760, /*!< Converts data from uint8 to int32. */
  CNNL_CAST_INT32_TO_INT64 = 690, /*!< Converts data from int32 to int64. */
  CNNL_CAST_INT64_TO_INT32 = 960, /*!< Converts data from int64 to int32. */
  CNNL_CAST_INT64_TO_INT16 = 940, /*!< Converts data from int64 to int16. */
  CNNL_CAST_INT64_TO_BOOL = 980,  /*!< Converts data from int64 to bool.  */
  CNNL_CAST_INT32_TO_BOOL = 680,  /*!< Converts data from int32 to bool.  */
  CNNL_CAST_UINT8_TO_INT64 = 790, /*!< Converts data from uint8 to int64. */
  CNNL_CAST_UINT64_TO_UINT32 = 213, /*!< Converts data from uint64 to uint32. */
  CNNL_CAST_INT64_TO_UINT32 = 912, /*!< Converts data from int64 to uint32.   */
  CNNL_CAST_UINT32_TO_INT64 = 191, /*!< Converts data from uint32 to int64.   */
  CNNL_CAST_UINT32_TO_UINT64 = 123, /*!< Converts data from uint32 to uint64. */
  CNNL_CAST_INT8_TO_INT16 = 340, /*!< Converts data from int8 to int16. */
  CNNL_CAST_HALF_TO_FLOAT_INF = 129,
  /*!< Converts data from half to float for auto mixed precision training. */
  CNNL_CAST_FLOAT_TO_HALF_IEEE754 CNNL_DEPRECATED_ENUM_FOR() = 219,
  /*!< Converts data from float to half according to IEEE 754.
  *    The ``round-nearest-even`` mode is used. */
  CNNL_CAST_FLOAT_TO_DOUBLE = 202,  /*!< Converts data from float to double. */
  CNNL_CAST_DOUBLE_TO_INT64 = 149,
  /*!< Converts data from double to int64. The ``round-to-zero`` mode is used. */
  CNNL_CAST_INT64_TO_DOUBLE = 914,
  /*!< Converts data from int64 to double. The ``round-nearest-even`` mode is used. */
  CNNL_CAST_DOUBLE_TO_FLOAT = 21,
  /*!< Converts data from double to float. The ``round-nearest-even`` mode is used. */
  CNNL_CAST_DOUBLE_TO_COMPLEX_FLOAT = 1416,
  /*!< Converts data from double to complex_float. The ``round-nearest-even`` mode is used. */
  CNNL_CAST_INT64_TO_FLOAT = 920,
  /*!< Converts data from int64 to float. The ``round-nearest-even`` mode is used. */
  CNNL_CAST_INT64_TO_HALF = 910, /*!< Converts data from int64 to half. */
  CNNL_CAST_INT64_TO_COMPLEX_HALF = 915, /*!< Converts data from int64 to complex_half.    */
  CNNL_CAST_INT64_TO_COMPLEX_FLOAT = 916,
  /*!< Converts data from int64 to complex_float. The ``round-nearest-even`` mode is used. */
  CNNL_CAST_INT64_TO_BFLOAT16 = 917,
  /*!< Converts data from int64 to bfloat16. The ``round-nearest-even`` mode is used. */
  CNNL_CAST_FLOAT_TO_INT64 = 290,
  /*!< Converts data from float to int64. The ``round-to-zero`` mode is used.*/
  CNNL_CAST_HALF_TO_INT64 = 190,
  /*!< Converts data from half to int64. The ``round-to-zero`` mode is used. */
  CNNL_CAST_FLOAT_TO_BFLOAT16 = 272,
  /*!< Converts data from float to bfloat16. The ``round-nearest-even`` mode is used. */
  CNNL_CAST_BFLOAT16_TO_INT8 = 173,
  /*!< Converts data from bfloat16 to int8. The ``round-to-zero`` mode is used.  */
  CNNL_CAST_BFLOAT16_TO_UINT8 = 177,
  /*!< Converts data from bfloat16 to uint8. The ``round-to-zero`` mode is used. */
  CNNL_CAST_BFLOAT16_TO_INT16 = 174,
  /*!< Converts data from bfloat16 to int16. The ``round-to-zero`` mode is used. */
  CNNL_CAST_BFLOAT16_TO_INT32 = 176,
  /*!< Converts data from bfloat16 to int32. The ``round-to-zero`` mode is used. */
  CNNL_CAST_BFLOAT16_TO_INT64 = 179,
  /*!< Converts data from bfloat16 to int64. The ``round-to-zero`` mode is used. */
  CNNL_CAST_BFLOAT16_TO_FLOAT = 721,
  /*!< Converts data from bfloat16 to float. */
  CNNL_CAST_BFLOAT16_TO_BOOL = 178,
  /*!< Converts data from bfloat16 to bool.  */
  CNNL_CAST_BFLOAT16_TO_COMPLEX_HALF = 1715,
  /*!< Converts data from bfloat16 to complex_half. The ``round-nearest-even`` mode is used. */
  CNNL_CAST_BFLOAT16_TO_COMPLEX_FLOAT = 1716,
  /*!< Converts data from bfloat16 to complex_float.*/
  CNNL_CAST_COMPLEX_HALF_TO_INT8 = 153,
  /*!< Converts data from complex_half to int8.  The ``round-to-zero`` mode is used. */
  CNNL_CAST_COMPLEX_HALF_TO_UINT8 = 157,
  /*!< Converts data from complex_half to uint8. The ``round-to-zero`` mode is used. */
  CNNL_CAST_COMPLEX_HALF_TO_INT16 = 154,
  /*!< Converts data from complex_half to int16. The ``round-to-zero`` mode is used. */
  CNNL_CAST_COMPLEX_HALF_TO_INT32 = 156,
  /*!< Converts data from complex_half to int32. The ``round-to-zero`` mode is used. */
  CNNL_CAST_COMPLEX_HALF_TO_INT64 = 159,
  /*!< Converts data from complex_half to int64. The ``round-to-zero`` mode is used. */
  CNNL_CAST_COMPLEX_HALF_TO_BOOL = 158,
  /*!< Converts data from complex_half to bool.  */
  CNNL_CAST_COMPLEX_HALF_TO_HALF = 151,
  /*!< Converts data from complex_half to half. */
  CNNL_CAST_COMPLEX_HALF_TO_FLOAT = 152,
  /*!< Converts data from complex_half to float.*/
  CNNL_CAST_COMPLEX_HALF_TO_BFLOAT16 = 1517,
  /*!< Converts data from complex_half to bfloat16. The ``round-nearest-even`` mode is used.  */
  CNNL_CAST_COMPLEX_HALF_TO_COMPLEX_FLOAT = 1516,
  /*!< Converts data from complex_half to complex_float.*/
  CNNL_CAST_COMPLEX_FLOAT_TO_INT8 = 163,
  /*!< Converts data from complex_float to int8. The ``round-to-zero`` mode is used.  */
  CNNL_CAST_COMPLEX_FLOAT_TO_UINT8 = 167,
  /*!< Converts data from complex_float to uint8. The ``round-to-zero`` mode is used. */
  CNNL_CAST_COMPLEX_FLOAT_TO_INT16 = 164,
  /*!< Converts data from complex_float to int16. The ``round-to-zero`` mode is used. */
  CNNL_CAST_COMPLEX_FLOAT_TO_INT32 = 166,
  /*!< Converts data from complex_float to int32. The ``round-to-zero`` mode is used. */
  CNNL_CAST_COMPLEX_FLOAT_TO_INT64 = 169,
  /*!< Converts data from complex_float to int64. The ``round-to-zero`` mode is used. */
  CNNL_CAST_COMPLEX_FLOAT_TO_BOOL = 168, /*!< Converts data from complex_float to bool.  */
  CNNL_CAST_COMPLEX_FLOAT_TO_FLOAT = 162, /*!< Converts data from complex_float to float.*/
  CNNL_CAST_COMPLEX_FLOAT_TO_HALF = 161,
  /*!< Converts data from complex_float to half. The ``round-nearest-even`` mode is used.*/
  CNNL_CAST_COMPLEX_FLOAT_TO_COMPLEX_DOUBLE = 1618,
  /*!< Converts data from complex_float to complex_double. */
  CNNL_CAST_COMPLEX_FLOAT_TO_COMPLEX_HALF = 1615,
  /*!< Converts data from complex_float to complex_half. The ``round-nearest-even`` mode is used. */
  CNNL_CAST_COMPLEX_FLOAT_TO_BFLOAT16 = 1617,
  /*!< Converts data from complex_float to bfloat16. The ``round-nearest-even`` mode is used.     */
  CNNL_CAST_COMPLEX_DOUBLE_TO_COMPLEX_FLOAT = 1816,
  /*!< Converts data from complex_double to complex_float. The ``round-nearest-even`` mode
     is used. */
} cnnlCastDataType_t;

/******************************************************************************
 * Cambricon CNNL Data Structure: BceWithLogitsReduction
 ******************************************************************************/
/*!
 * @brief  Enumeration variables describing the reduction applied to the output in
 *  the implementation of the BceWithLogits and BceWithLogitsBackward functions.
 *
 */
typedef enum {
  CNNL_BCE_WITH_LOGITS_NONE = 0,
  /*!< No reduction will be applied to the output. */
  CNNL_BCE_WITH_LOGITS_MEAN = 1,
  /*!< The reduction that calculates the mean of the sum of the output is applied. */
  CNNL_BCE_WITH_LOGITS_SUM = 2
  /*!<  The reduction that calculates the sum of the output is applied. */
} cnnlBceWithLogitsReduction_t;

/******************************************************************************
 * Cambricon CNNL Data Structure: BceLossReduction
 ******************************************************************************/
/*!
 * @brief Enumeration variables describing the reduction applied to the output in
 * the BceLoss and BceLossBackward operations.
 *
 */
typedef enum {
  CNNL_BCE_LOSS_NONE = 0,
  /*!< No reduction will be applied to the output. */
  CNNL_BCE_LOSS_MEAN = 1,
  /*!< The reduction that calculates the mean of the sum of the output is applied.  */
  CNNL_BCE_LOSS_SUM = 2
  /*!< The reduction that calculates the sum of the output is applied.*/
} cnnlBceLossReduction_t;

/******************************************************************************
 * Cambricon CNNL Data Structure: Lrn
 ******************************************************************************/
/*!
 * @brief Enumeration variables describing the computing modes that are used in the
 * implementation of the LRN function.
 *
 */
typedef enum {
  CNNL_LRN_LOCAL_SIZE = 0,
  /*!< Applies local response normalization over an input signal composed of
   * several input planes. */
  CNNL_LRN_LOCAL_SIZE_ORIGINAL = 1,
  /*!< Same as CNNL_LRN_LOCAL_SIZE. */
  CNNL_LRN_CROSS_CHANNEL = 2,
  /*!< Applies normalization across channels, while each channel component is
   * divided by the weighted, squared sum of inputs within adjacent channels. */
  CNNL_LRN_WITHIN_CHANNEL = 3
  /*!< Applies normalization over nearby spatial locations. Each input is divided
   * by the weighted, squared sum of inputs within local spatial regions */
} cnnlLrnMode_t;

/******************************************************************************
 * Cambricon CNNL Data Structure: Gru
 ******************************************************************************/
/*!
 * @brief Enumeration variables describing the algorithms that are used in the
 * implementation of the GRU operation. For detailed information
 * about algorithm, see "GRU Operator" section of "Cambricon CNNL User Guide".
 */
typedef enum {
  CNNL_GRU_ALGO_V1 = 0, /*!< Implements with formula: ``h_new = (1 - z) * h + z * n``. */
  CNNL_GRU_ALGO_V2 = 1, /*!< Implements with formula: ``h_new = (1 - z) * n + z * h``. */
  /*!< z means the update gate;
       n means the candidate hidden state;
       h means the hidden state in the current time step;
       h_new means the hidden state in the next time step. */
} cnnlGruAlgo_t;

/*!
 * @brief Enumeration variables describing the processing direction of input sequence data
 * in the implementation of the GRU operation.
 */
typedef enum {
  CNNL_GRU_FORWARD = 0,
  /*!< The GRU network iteratively processes the input sequence data from front to back.*/
  CNNL_GRU_BACKWARD = 1,
  /*!< The GRU network iteratively processes the input sequence data from back to front.*/
  CNNL_GRU_BIDIRECTIONAL = 2,
  /*!< Two GRU networks iteratively process the input sequence data from two directions,
       first from front to back, and then from back to front. The results of the two GRU
       networks are concatenated at each iteration as the output.
       Note that two GRU networks have different model parameters.*/
} cnnlGruDir_t;

/*!
 * @brief Enumeration variables describing the order of filter parameters in the implementation
 * of the GRU operation.
 */
typedef enum {
  CNNL_GRU_RZN = 0,
  /*!< The filter parameter order of GRU gate and state:
       ``W_r_i, W_r_h, W_z_i, W_z_h, W_n_iW_n_h``.
       The order of bias parameters is consistent with the filter parameters, which is
       ``B_r_i, B_r_h, B_z_i, B_z_h, B_n_iB_n_h``.*/
  CNNL_GRU_ZRN = 1,
  /*!< The filter parameter order of GRU gate and state:
       ``W_z_i, W_z_h, W_r_i, W_r_h, W_n_i, W_n_h``.
       The order of bias parameters is consistent with the filter parameters, which is
       ``B_z_i, B_z_h, B_r_i, B_r_h, B_n_iB_n_h``.*/
  CNNL_GRU_IH_RZN = 2,
  /*!< The filter parameter order of GRU gate and state:
       ``W_r_i, W_z_i, W_n_i, W_r_h, W_z_h, W_n_h``.
       The order of bias parameters is consistent with the filter parameters, which is
       ``B_r_i, B_z_i, B_n_i, B_r_h, B_z_hB_n_h``.*/
  CNNL_GRU_IH_ZRN = 3,
  /*!< The filter parameter order of GRU gate and state:
       ``W_z_i, W_r_i, W_n_i, W_z_h, W_r_h, W_n_h``.
       The order of bias parameters is consistent with the filter parameters, which is
       ``B_z_i, B_r_i, B_n_i, B_z_h, B_r_hB_n_h``.*/
  /*!< r means for the reset gate;
       z means for the update gate;
       n means for the candidate hidden state;
       i means for the input;
       h means for the hidden state. */
} cnnlGruWeightOrder_t;

/*!
 * @brief Enumeration variables describing the mode that is used in the
 * implementation of the multi-layer bidirectional GRU operation. For detailed information
 * about mode, you can see "GRU Operator" section in "Cambricon CNNL User Guide".
 */
typedef enum {
  CNNL_GRU_MODE_V1 = 0,
  /*!< Implements multi-layer bidirectional GRU with the same mode as TensorFlow. */
  CNNL_GRU_MODE_V2 = 1,
  /*!< Implements multi-layer bidirectional GRU with the same mode as PyTorch. */
} cnnlGruMode_t;

/*!
 * @brief Enumeration variables describing the data layouts of state and state_output in ::cnnlGru_v2.
 *
 * The data can be defined in four dimensions:
 * - L: The number of layers of GRU.
 * - D: The number of directions of GRU.
 * - N: The batch size of sequence.
 * - C: The hidden size of sequence.
 */
typedef enum {
  CNNL_GRU_LDNC = 0,
  /*!< The data layout is LDNC.
  */
  CNNL_GRU_DLNC = 1,
  /*!< This enum is not supported currently. */
  CNNL_GRU_LNDC = 2,
  /*!< The data layout is LNDC.
  */
} cnnlGruLayout_t;

/******************************************************************************
 * Cambricon CNNL Data Structure: Attention
 ******************************************************************************/
/*!
 * @brief
 *
 * Enumeration variables describing the algorithms that are used in the
 * implementation of the ATTENTION function.
 *
 */
typedef enum {
  CNNL_LOCATION_SENSITIVE_ATTENTION = 0, /*!< Location-Sensitive-Attention.*/
} cnnlAttentionAlgo_t;

/******************************************************************************
 * Cambricon CNNL Data Structure: Nms
 ******************************************************************************/
/*!
 * @brief Enumeration variables describing the input box modes that can be used
 *        in the implementation of the NMS function.
 */
typedef enum {
  CNNL_NMS_BOX_DIAGONAL = 0,
  /*!< The box mode is [x1, y1, x2, y2]. */
  CNNL_NMS_BOX_CENTER = 1,
  /*!< The box mode is
   * [x_center, y_center, width, height] where width > 0 and
   * height > 0. */
} cnnlNmsBoxPointMode_t;

/*!
 * @brief Enumeration variables describing the output modes that can be used
 *        in the implementation of the NMS function.
 */
typedef enum {
  CNNL_NMS_OUTPUT_TARGET_INDICES = 0,
  /*!< Returns target indices, sorted in decreasing order of confidences. */
  CNNL_NMS_OUTPUT_TARGET_CONFIDENCE_AND_POS_1 = 1,
  /*!< Returns target confidences and positions with the order of:
   * confidence_0, x_01, y_01, x_02, y_02, confidence_1, x_11, y_11, x_12, y_12,
   * ... ,confidence_n, x_n1, y_n1, x_n2, y_n2. The (x_01, y_01) and (x_02, y_02)
   * represent the top left corner and bottom right corner coordinates of first box,
   * respectively. */
  CNNL_NMS_OUTPUT_TARGET_CONFIDENCE_AND_POS_2 = 2,
  /*!< Returns target confidences and positions with the order of:
   * confidence_0, confidence_1, ... , confidence_n, x_01, x_11, ... , x_n1,
   * y_01, y_11, ... , y_n1, x_02, x_12, ... , x_n2, y_02, y_12, ... , y_n2.
   * The (x_01, y_01) and (x_02, y_02) represent the top left corner and bottom right
   * corner coordinates of first box, respectively. */
  CNNL_NMS_OUTPUT_TARGET_BATCH_AND_CLASS = 3,
  /*!< Returns batch indices, class indices and positions with the order of:
   * batch_0, class_0, box_0, ... ,batch_0, class_0, box_m, batch_0, class_1, box_0, ... ,
   * batch_0, class_1, box_m, ... , ... , batch_s, class_n, box_m. */
} cnnlNmsOutputMode_t;

/*!
 * @brief Enumeration variables describing the algorithms that can be used
 *        in the update of confidence in NMS function.
 */
typedef enum {
  CNNL_NMS_HARD_NMS = 0,
  /*!< A type of algorithm which updates confidence using hard nms, i.e.
   * confidence = IOU < IOU_threshold ? confidence : 0 */
  CNNL_NMS_SOFT_NMS_LINEAR = 1,
  /*!< A type of algorithm which updates confidence using linear method, i.e.
   * confidence = IOU < IOU_threshold ? confidence : confidence * (1 - IOU) */
  CNNL_NMS_SOFT_NMS_GAUSSIAN = 2,
  /*!< A type of algorithm which updates confidence using Gaussian method, i.e.
   * confidence = confidence * exp{- \f$IOU^2\f$ / (2 * sigma)} */
} cnnlNmsMethodMode_t;

/*!
 * @brief Enumeration variables describing the algorithms that can be used in
 *        in the implementation of the NMS function.
 */
typedef enum {
  CNNL_NMS_ALGO_EXCLUDE_BOUNDARY = 0,
  /*!< Implements NMS with boundary excluded. In this mode,
   * the height or width of boxes is ``(x2 - x1)``. */
  CNNL_NMS_ALGO_INCLUDE_BOUNDARY = 1,
  /*!< Implements NMS with boundary included. In this mode,
   * the height or width of boxes is ``(x2 - x1 + offset)``. */
} cnnlNmsAlgo_t;

/*!
 * @brief Enumeration variables describing the attributes of the NMS function.
 */
typedef enum {
  CNNL_NMS_DESC_IOU_THRESHOLD = 0,
  /*!< Specifies the intersection over union (iou) threshold used in nms computation.
       Default value is 0. */
  CNNL_NMS_DESC_INPUT_LAYOUT = 1,
  /*!< Specifies the input data layout. Supported values are 0 and 1. 0 represents
       [boxes_num, 4], [boxes_num, 7] or [batches_num, boxes_num, 4]. 1 represents
       [4, boxes_num], [7, boxes_num] or [batches_num, 4, boxes_num].
       Default value is 0. */
  CNNL_NMS_DESC_MAX_OUTPUT_SIZE = 2,
  /*!< Specifies the maximum number of output boxes. If the dimension of input box is 3,
       this parameter indicates the maximum number of output boxes per class.
       Default value is 0. */
  CNNL_NMS_DESC_CONFIDENCE_THRESHOLD = 3,
  /*!< Specifies the confidence threshold used in NMS computation.
       Boxes would be filtered out directly if the confidence of boxes is no more than this
       threshold.
       Default value is -INFINITY. */
  CNNL_NMS_DESC_OUTPUT_MODE = 4,
  /*!< Specifies the output shape mode. For detailed information, see ::cnnlNmsOutputMode_t.
       Default value is ::CNNL_NMS_OUTPUT_TARGET_INDICES. */
  CNNL_NMS_DESC_OFFSET = 5,
  /*!< Specifies the offset size of boundary. Supported values are 0 and 1.
       Default value is 0. */
  CNNL_NMS_DESC_BOX_MODE = 6,
  /*!< Specifies box data structure. For detailed information, see ::cnnlNmsBoxPointMode_t.
       Default value is ::CNNL_NMS_BOX_DIAGONAL. */
  CNNL_NMS_DESC_PAD_TO_MAX_OUTPUT_SIZE = 7
  /*!< Specifies whether the output will be padded to max_output_size with zero.
       Default value is false. */
} cnnlNmsDescAttribute_t;

/*!
 * @brief Enumeration variables describing the algorithms that are used to
 *        distinguish different frameworks and versions.
 */
typedef enum {
  CNNL_ANGLE_ALGO_V1 = 0, /*!< For PyTorch <1.8 and TensorFlow 1.15. */
  CNNL_ANGLE_ALGO_V2 = 1, /*!< For PyTorch >=1.8 and TensorFlow >=2.5. */
} cnnlAngleAlgo_t;

/*!
 * @brief Enumeration variables describing the algorithms that are used in the
 * implementation of the ::cnnlTrigonForward function.
 */
typedef enum {
  CNNL_TRIGON_SIN = 0, /*!< Implements: ``y = sin(x)``. */
  CNNL_TRIGON_COS = 1, /*!< Implements: ``y = cos(x)``. */
  CNNL_TRIGON_TAN = 2, /*!< Implements: ``y = tan(x)``. */
  CNNL_TRIGON_ASIN = 3, /*!< Implements: ``y = asin(x)``. */
  CNNL_TRIGON_ACOS = 4, /*!< Implements: ``y = acos(x)``. */
  CNNL_TRIGON_ATAN = 5, /*!< Implements: ``y = atan(x)``. */
  CNNL_TRIGON_SINH = 6, /*!< Implements: ``y = sinh(x)``. */
  CNNL_TRIGON_COSH = 7, /*!< Implements: ``y = cosh(x)``. */
  CNNL_TRIGON_TANH = 8, /*!< Implements: ``y = tanh(x)``. */
  CNNL_TRIGON_ASINH = 9, /*!< Implements: ``y = asinh(x)``. */
  CNNL_TRIGON_ACOSH = 10, /*!< Implements: ``y = acosh(x)``. */
  CNNL_TRIGON_ATANH = 11, /*!< Implements: ``y = atanh(x)``. */
} cnnlTrigonFunctionMode_t;

/******************************************************************************
 * Cambricon CNNL Data Structure: MatMul
 ******************************************************************************/
/*!
 * @brief Enumeration variables describing the attributes of the matrix multiplication computation.
 */
typedef enum {
  CNNL_MATMUL_DESC_COMPUTE_TYPE = 0,
  /*!< Defines data type used for multiplication and accumulation operations, and the
       accumulator for implementing matrix multiplication. It must be set before
       doing matrix multiplication. */
  CNNL_MATMUL_DESC_SCALE_TYPE = 1,
  /*!< Defines data type of the scaling factors \p alpha and \p beta. Default value
       is the same as ::CNNL_MATMUL_DESC_COMPUTE_TYPE. It is not supported now. */
  CNNL_MATMUL_DESC_POINTER_MODE CNNL_DEPRECATED_ENUM_FOR() = 2,
  /*!< Specifies whether \p alpha and \p beta are stored on the host or on the device.
       It is deprecated and will be removed in future release. */
  CNNL_MATMUL_DESC_TRANSA = 3,
  /*!< Specifies whether transpose should be performed on matrix A. Default
       value is 0 (false). */
  CNNL_MATMUL_DESC_TRANSB = 4,
  /*!< Specifies whether transpose should be performed on matrix B. Default
       value is 0 (false). */
  CNNL_MATMUL_DESC_TRANSC = 5,
  /*!< Specifies whether transpose should be performed on matrix C. Default
       value is 0 (false). It is not supported now. */
  CNNL_MATMUL_DESC_EPILOGUE CNNL_DEPRECATED_ENUM_FOR() = 6,
  /*!< Specifies epilogue function. It is not supported now.
       It is deprecated and will be removed in future release. */
  CNNL_MATMUL_DESC_BIAS_POINTER CNNL_DEPRECATED_ENUM_FOR() = 7,
  /*!< Pointer to bias vector on MLU device memory. Currently it is only supported to set
       the attribute \p matmul_desc in ::cnnlMatMulInference.
       It is deprecated and will be removed in future release. */
  CNNL_MATMUL_DESC_EPILOGUE_TYPE CNNL_DEPRECATED_ENUM_FOR() = 8,
  /*!< Specifies matmul multiplication epilogue fusion type.
       It is deprecated and will be removed in future release. */
  CNNL_MATMUL_DESC_EPILOGUE_OPERAND CNNL_DEPRECATED_ENUM_FOR() = 9,
  /*!< Specifies matmul multiplication epilogue fusion operand.
       It is deprecated and will be removed in future release. */
  CNNL_MATMUL_ALLOW_TF32 = 10,
  /*!< Determines whether to enable TensorFloat-32 mode.
       TensorFloat-32 is enabled by default. */
  CNNL_MATMUL_USE_BETA = 11,
  /*!< Specifies whether to use \p beta on matrix C. */
  CNNL_MATMUL_CAST_MODE CNNL_DEPRECATED_ENUM_FOR()= 12,
  /*!< Specifies the quantization mode used for the matrix multiplication quantization.
       It is deprecated and will be removed in future release. */
  CNNL_MATMUL_USE_STRIDE = 13,
  /*!< Specifies whether stride should be performed on tensor. */
  CNNL_MATMUL_DESC_LDA = 14,
  /*!< Leading dimension of matrix A that is stored in row-major format. */
  CNNL_MATMUL_DESC_LDB = 15,
  /*!< Leading dimension of matrix B that is stored in row-major format. */
  CNNL_MATMUL_DESC_LDC = 16,
  /*!< Leading dimension of matrix C that is stored in row-major format. */
  CNNL_MATMUL_DESC_A_SCALE_POINTER = 17,
  /*!< Scaling factor value that is used to convert data of matrix A to compute type range. */
  CNNL_MATMUL_DESC_B_SCALE_POINTER = 18,
  /*!< Scaling factor value that is used to convert data of matrix B to compute type range. */
  CNNL_MATMUL_DESC_C_SCALE_POINTER = 19,
  /*!< Scaling factor value that is used to convert data of matrix C to compute type range. */
  CNNL_MATMUL_DESC_D_SCALE_POINTER = 20,
  /*!< Scaling factor value that is used to convert data of matrix D to compute type range. */
  CNNL_MATMUL_DESC_GROUP_M_MAX = 96,
  /*!< Maximum of m in group matmul. */
  CNNL_MATMUL_DESC_GROUP_N_MAX = 97,
  /*!< Maximum of n in group matmul. */
  CNNL_MATMUL_DESC_GROUP_K_MAX = 98,
  /*!< Maximum of k in group matmul. */
  CNNL_MATMUL_A_QUANT = 21,
  /*!< Specifies quantization information of input matrix A. */
  CNNL_MATMUL_B_QUANT = 22,
  /*!< Specifies quantization information of input matrix B. */
  CNNL_MATMUL_D_QUANT = 23,
  /*!< Specifies quantization information of input matrix D. */
  CNNL_MATMUL_DESC_B_QUANT_FLAG_POINTER = 24,
  /*!< Specifies quantization type of input matrix B. */
} cnnlMatMulDescAttribute_t;

/******************************************************************************
 * Cambricon CNNL Data Structure: MatMulInferenceV2
 ******************************************************************************/
/*!
 * @brief Enumeration variables describing the transformation operation
 * performed on matrix multiplication.
 */
typedef enum {
  CNNL_MATMUL_OP_N = 0,
  /*!< The transformation operation is not performed when doing the matrix multiplication. */
  CNNL_MATMUL_OP_T = 1,
  /*!< The transformation operation is performed when doing the matrix multiplication. */
} cnnlMatrixOperation_t;

/*!
 * @brief Enumeration variables describing the fused operation
 * performed after matrix multiplication.
 */
typedef enum {
  CNNL_MATMUL_EPI_NONE = 0,
  /*!< No fused operation is applied. */
  CNNL_MATMUL_EPI_BIAS = 1,
  /*!< Applies the fused operation only with bias.*/
  CNNL_MATMUL_EPI_BIAS_SCALE_BN_ACTIVATION = 2,
  /*!< Applies the fused operation with bias, scale, bn and activation.*/
  CNNL_MATMUL_EPI_PER_ROW_COL_SCALE_BIAS_ACT = 3,
  /*!< Applies the column scale, row scale, bias and activation. */
} cnnlMatMulEpilogueType_t;

/*! The descriptor of the matrix multiplication inference algorithm. It is reserved for future use and
 *  should be set to NULL When called by ::cnnlMatMulInference_v2.
 *
 *  It is deprecated and will be removed in future release.
 */
typedef struct cnnlMatMulInferenceAlgo *cnnlMatMulInferenceAlgo_t;

/******************************************************************************
 * Cambricon CNNL Data Structure: MatMulEx
 ******************************************************************************/
/*!
 * @brief Enumeration variables describing the quantization modes used for the
 *        matrix multiplication quantization. The quantization mode is not necessary for matrix multiplication.
 *        The default value is CNNL_MATMUL_BYPASS_QUANTIZE when the quantization mode is not set, and the position, scale
 *        and offset value of each tensor descriptor will be used when onchip data type is fix-point.
 *
 * It is deprecated and will be removed in future release.
 * Use ::cnnlQuantizeMode_t instead.
 */
typedef enum {
    CNNL_MATMUL_OFFLINE_SYMMETRIC_QUANTIZE = 0,
    /*!< The offline symmetric quantization is applied. */
    CNNL_MATMUL_OFFLINE_ASYMMETRIC_QUANTIZE = 1,
    /*!< The offline asymmetric quantization is applied. */
    CNNL_MATMUL_PARTIAL_ONLINE_QUANTIZE = 2,
    /*!< The partial online quantization is applied. */
    CNNL_MATMUL_GLOBAL_ONLINE_QUANTIZE = 3,
    /*!< The global online quantization is applied. */
    CNNL_MATMUL_NO_QUANTIZE = 4,
    /*!< No quantization is applied. This mode is only used when the data type is half or float. */
    CNNL_MATMUL_BYPASS_QUANTIZE = 255,
    /*!< The bypass operation is applied, when user not set the cast mode. */
} cnnlMatMulCastMode_t;

/*!
 * @brief Enumeration variables describing the attributes of the matrix multiplication extend computation.
 */
typedef enum {
  CNNL_MATMUL_EX_DESC_COMPUTE_TYPE = 0,
  /*!< Specifies the data type used for multiplication and accumulation operations, and the
       accumulator for implementing matrix multiplication. */
  CNNL_MATMUL_EX_DESC_SCALE_TYPE = 1,
  /*!< Specifies the data type of the scaling factors \p alpha and \p beta. Default value
       is the same as ::CNNL_MATMUL_DESC_COMPUTE_TYPE. It is not supported now. */
  CNNL_MATMUL_EX_DESC_POINTER_MODE CNNL_DEPRECATED_ENUM_FOR() = 2,
  /*!< Specifies whether \p alpha and \p beta are stored on the host or on the device.
       It is deprecated and will be removed in future release. */
  CNNL_MATMUL_EX_DESC_TRANSA = 3,
  /*!< Specifies whether transpose should be performed on matrix A. Default
       value is 0 (false). */
  CNNL_MATMUL_EX_DESC_TRANSB = 4,
  /*!< Specifies whether transpose should be performed on matrix B. Default
       value is 0 (false). */
  CNNL_MATMUL_EX_DESC_TRANSC = 5,
  /*!< Specifies whether transpose should be performed on matrix C. Default
       value is 0 (false). It is not supported now. */
  CNNL_MATMUL_EX_DESC_EPILOGUE CNNL_DEPRECATED_ENUM_FOR() = 6,
  /*!< Specifies epilogue function. It is deprecated and will be removed in future release. */
  CNNL_MATMUL_EX_DESC_BIAS_POINTER CNNL_DEPRECATED_ENUM_FOR() = 7,
  /*!< Pointer to bias vector on MLU device memory. It is deprecated and will be removed
       in future release. */
  CNNL_MATMUL_EX_DESC_EPILOGUE_TYPE = 8,
  /*!< Specifies matmul multiplication epilogue fusion type. */
  CNNL_MATMUL_EX_DESC_EPILOGUE_OPERAND CNNL_DEPRECATED_ENUM_FOR() = 9,
  /*!< Specifies matmul multiplication epilogue fusion operand. It is deprecated and will
       be removed in future release. */
  CNNL_MATMUL_EX_ALLOW_TF32 = 10,
  /*!< Determines enabling TensorFloat-32 mode. TensorFloat-32 is enabled by default. */
  CNNL_MATMUL_EX_USE_BETA = 11,
  /*!< Specifies whether to use \p beta on matrix C. */
  CNNL_MATMUL_EX_CAST_MODE CNNL_DEPRECATED_ENUM_FOR() = 12,
  /*!< Specifies the quantization mode used for the matrix multiplication quantization.
       It is deprecated and will be removed in future release.*/
  CNNL_MATMUL_EX_PREF_MAX_WORKSPACE_BYTES = 13,
  /*!< Specifies the maximum allowed workspace memory. */
  CNNL_MATMUL_EX_DESC_LDA = 14,
  /*!< Leading dimension of matrix A that is stored in row-major format. */
  CNNL_MATMUL_EX_DESC_LDB = 15,
  /*!< Leading dimension of matrix B that is stored in row-major format. */
  CNNL_MATMUL_EX_DESC_LDC = 16,
  /*!< Leading dimension of matrix C that is stored in row-major format. */
  CNNL_MATMUL_EX_A_QUANT = 17,
  /*!< Specifies quantization information of input matrix A. */
  CNNL_MATMUL_EX_B_QUANT = 18,
  /*!< Specifies quantization information of input matrix B. */
  CNNL_MATMUL_EX_D_QUANT = 19,
  /*!< Specifies quantization information of output matrix. */
} cnnlMatMulExDescAttribute_t;

/******************************************************************************
 * Cambricon CNNL Data Structure: BatchMatMul
 ******************************************************************************/
/*!
 * @brief Enumeration variables describing the attribute names of the batch matrix multiplication computation.
 */
typedef enum {
  CNNL_BMM_DESC_COMPUTE_TYPE = 0,
  /*!< Specifies the data type used for multiplication and accumulation operations, and the
       accumulator for implementing batch matrix multiplication. It must be set before
       doing matrix multiplication. */
  CNNL_BMM_DESC_SCALE_TYPE = 1,
  /*!< Specifies the data type of the scaling factors \p alpha and \p beta. Default value
       is the same as ::CNNL_BMM_DESC_COMPUTE_TYPE. It is not supported now. */
  CNNL_BMM_DESC_POINTER_MODE = 2,
  /*!< Specifies whether \p alpha and \p beta are stored on the host or on the device.
       It is not supported now. */
  CNNL_BMM_DESC_TRANSA = 3,
  /*!< Specifies whether transpose should be performed on matrix A. Default
       value is 0 (false). */
  CNNL_BMM_DESC_TRANSB = 4,
  /*!< Specifies whether transpose should be performed on matrix B. Default
       value is 0 (false). */
  CNNL_BMM_DESC_TRANSC = 5,
  /*!< Specifies whether transpose should be performed on matrix C. Default
       value is 0 (false). It is not supported now. */
} cnnlBatchMatMulDescAttribute_t;

/*!
 * @brief Enumeration variables describing the attribute names of the stride batch matrix
 * multiplication computation.
 */
typedef enum {
  CNNL_STRIDE_BMM_MAX_BATCH_DIM = 0,
  /*!< Specifies the batch_size dimensions. */
  CNNL_STRIDE_BMM_ALLOW_TF32 = 1,
  /*!< Determines enabling TensorFloat-32 mode.
       TensorFloat-32 is enabled by default. */
  CNNL_STRIDE_BMM_A_QUANT_PARAM = 2,
  /*!< Specifies quantization information of input matrix A. */
  CNNL_STRIDE_BMM_B_QUANT_PARAM = 3,
  /*!< Specifies quantization information of input matrix B. */
  CNNL_STRIDE_BMM_D_QUANT_PARAM = 4,
  /*!< Specifies quantization information of output matrix D. */
  CNNL_STRIDE_BMM_COMPUTE_TYPE  = 5,
  /*!< Defines data type used for multiplication and accumulation operations, and the
       accumulator for implementing matrix multiplication. It must be set before
       doing matrix multiplication. */
} cnnlStrideBatchMatMulDescAttribute_t;

/******************************************************************************
 * Cambricon CNNL Data Structure: Normalize
 ******************************************************************************/
/**
 * @brief Enumeration variables describing the algorithms that are used in the
 * implementation of the normalize function.
 *
 */

typedef enum {
  CNNL_NORMALIZE_EUCLIDEAN = 0, /*!<The square root of sum of squares operation is implemented.*/
} cnnlNormalizeMode_t;

/******************************************************************************
 * Cambricon CNNL Data Structure: BatchMatMulBCast
 ******************************************************************************/
/**
 * @brief Enumeration variables describing the attribute names of the batch matrix multiplication computation with broadcasting.
 *
 */
typedef enum {
  CNNL_BMM_BCAST_DESC_COMPUTE_TYPE = 0,
  /*!< Specifies the data type used for multiplication and accumulation operations, and the accumulator
       for implementing batch matrix multiplication with broadcasting. It must be set before doing
       batch matrix multiplication with broadcasting. */
  CNNL_BMM_BCAST_DESC_SCALE_TYPE = 1,
  /*!< Specifies the data type of the scaling factors \p alpha and \p beta. Default value is the same
       as ::CNNL_BMM_BCAST_DESC_COMPUTE_TYPE. It is not supported now. */
  CNNL_BMM_BCAST_DESC_POINTER_MODE = 2,
  /*!< Specifies whether \p alpha and \p beta are stored on the host or on the device.
       It is not supported now. */
  CNNL_BMM_BCAST_DESC_TRANSA = 3,
  /*!< Specifies whether transpose should be performed on matrix A.
       Default value is 0 (false). */
  CNNL_BMM_BCAST_DESC_TRANSB = 4,
  /*!< Specifies whether transpose should be performed on matrix B.
       Default value is 0 (false). */
  CNNL_BMM_BCAST_DESC_TRANSC = 5,
  /*!< Specifies whether transpose should be performed on matrix C.
       Default value is 0 (false). It is not supported now. */
} cnnlBatchMatMulBCastDescAttribute_t;

/*!
 * @brief Enumeration variables describing the preference of matrix multiplication algorithm.
 */
typedef enum {
  CNNL_MATMUL_FASTEST = 0,
  /*!< The high-speed preference is used. */
  CNNL_MATMUL_LOW_MEMORY_OCCUPY = 1,
  /*!< The low-memory preference is used. This is not supported now. */
} cnnlMatMulPreference_t;

/*!
 * @brief Enumeration variables describing the preference of matrix multiplication algorithm
 * initialization.
 */
typedef enum {
  CNNL_MATMUL_ALGO_INIT_LIMITED_BY_ALGO_ID = 0,
  /*!< Only algorithm configurations specified by the algoId are allowed. */
  CNNL_MATMUL_ALGO_INIT_ALTER_BEST_FIT = 1,
  /*!< The algorithm configurations will alter to the best one when the algorithm initialization
       failed by the specified algoId. */
  CNNL_MATMUL_ALGO_INIT_ALTER_DEFAULT = 2,
  /*!< The algorithm configurations will alter to the default one when the algorithm initialization
       failed by the specified algoId. */
} cnnlMatMulAlgoInitPrefer_t;

/*!
 * @brief Enumeration variables describing the preference of batch matrix multiplication algorithm.
 */
typedef enum {
  CNNL_BMM_FASTEST = 0,
  /*!< The high-speed preference is used. */
  CNNL_BMM_LOW_MEMORY_OCCUPY = 1,
  /*!< The low-memory preference is used. This is not supported now. */
} cnnlBatchMatMulPreference_t;

/*!
 * @brief Enumeration variables describing the preference of batch matrix multiplication with broadcasting algorithm.
 */
typedef enum {
  CNNL_BMM_BCAST_FASTEST = 0,
  /*!< The high-speed preference is used. */
  CNNL_BMM_BCAST_LOW_MEMORY_OCCUPY = 1,
  /*!< The low-memory preference is used. It is not supported now. */
} cnnlBatchMatMulBCastPreference_t;


/*!
 * @brief Enumeration variables describing the pointer modes.
 */
typedef enum {
  CNNL_POINTER_MODE_HOST = 0,
  /*!< A host pointer, which means that the values passed by reference are on the host. */
  CNNL_POINTER_MODE_DEVICE = 1,
  /*!< A device pointer, which means that the values passed by reference are on the device. */
} cnnlPointerMode_t;

/*!
 * @brief Enumeration variables describing the atomics mode in CNNL.
 */
typedef enum {
  CNNL_ATOMICS_NOT_ALLOWED = 1,
  /*!< The atomics is not allowed to cumulate results. */
  CNNL_ATOMICS_ALLOWED = 2,
  /*!< The atomics is allowed to cumulate results. */
} cnnlAtomicsMode_t;

/*!
 * @brief Enumeration variables describing the rounding mode of quantization conversion.
 */
typedef enum {
  CNNL_ROUND_HALF_TO_EVEN = 0,
  /*!< The rounding mode to round towards the nearest even neighbor
   *   is applied.*/
  CNNL_ROUND_HALF_UP = 1,
  /*!< The rounding mode to round up towards the nearest neighbor is
   *   applied.*/
  CNNL_ROUND_HALF_OFF_ZERO = 2,
  /*!< The rounding mode to round half away from zero is
   *   applied.*/
} cnnlQuantizeRoundMode_t;

/*!
 * @brief Enumeration variables describing the detection and output modes of the ::cnnlIsInf operation.
 */
typedef enum {
  CNNL_NEG_INF = 0,
  /*!< Detects the negative infinity in the input and outputs true
       in corresponding position of input. */
  CNNL_POS_INF = 1,
  /*!< Detects the positive infinity in the input and outputs true
       in corresponding position of input. */
  CNNL_INF = 2,
  /*!< Detects the negative infinity or positive infinity in the input and outputs true
       at the position of negative infinity or positive infinity in output. */
  CNNL_INF_INDICATOR = 3
  /*!< Detects the type of negative infinity and positive infinity and outputs the corresponding indicators.
       If the value is a negative infinity, outputs 1 in corresponding position of input;
       if the value is a positive infinity, outputs 2 in corresponding position of input;
       if the value is neither a positive infinity nor a negative infinity,
       outputs 0 in corresponding position of input. */
} cnnlIsInfMode_t;

/*!
 * @brief Enumeration variables describing the sort type that are used in the
 *        implementation of the SortPairs operation.
 */
typedef enum {
  CNNL_ASCENDING = 0,
  /*!< The Ascending is implemented.*/
  CNNL_DESCENDING = 1
  /*!< The Descending is implemented.*/
} cnnlSortType_t;

/******************************************************************************
 * Cambricon CNNL Data Structure: SparseDenseMatMul
 ******************************************************************************/
/*!
 * @brief Enumeration variables describing the formats of sparse tensor.
 */
typedef enum {
    CNNL_SPARSE_FORMAT_COO = 0,
    /*!< The sparse tensor is stored in Coordinate (COO) format. */
    CNNL_SPARSE_FORMAT_CSR = 1,
    /*!< The sparse tensor is stored in Compressed Sparse Row (CSR) format. */
    CNNL_SPARSE_FORMAT_CSC = 2,
    /*!< The sparse tensor is stored in Compressed Sparse Column (CSC) format. */
} cnnlSparseFormat_t;

/*!
 * @brief Enumeration variables describing the algorithms of sparse dense matmul.
 *
 * It is deprecated and will be removed in future release.
 */
typedef enum {
    CNNL_SPMM_ALGO_0 = 0,
    /*!< Algorithm 0 (default) for sparse dense matmul. */
    /*!< CNNL_SPMM_ALGO_1  = 1, */
    /*!< Algorithm 1 for sparse dense matmul. */
    /*!< CNNL_SPMM_ALGO_2  = 2, */
    /*!< Algorithm 2 for sparse dense matmul. */
} cnnlSparseDenseMatMulAlgo_t;

/*!
 * @brief Enumeration variables describing the algorithms of sparse dense matmul.
 *
 * It is deprecated and will be removed in future release.
 */

typedef cnnlSparseDenseMatMulAlgo_t cnnlSparseDenseMatmulAlgo_t;

/*!
 * @brief Enumeration variables describing the attributes of sparse tensor descriptor.
 *        Each attribute has a data pointer and its tensor descriptor.
 */
typedef enum {
    CNNL_SPARSE_TENSOR_DESC_ROW_INDEX = 0,
    /*!< The row index in sparse tensor descriptor for CNNL_SPARSE_FORMAT_COO
         and CNNL_SPARSE_FORMAT_CSC formats. */
    CNNL_SPARSE_TENSOR_DESC_COL_INDEX = 1,
    /*!< The column index in sparse tensor descriptor for CNNL_SPARSE_FORMAT_COO
         and CNNL_SPARSE_FORMAT_CSR formats. */
    CNNL_SPARSE_TENSOR_DESC_VALUES = 2,
    /*!< The values in sparse tensor descriptor for all sparse formats. */
    CNNL_SPARSE_TENSOR_DESC_ROW_OFFSETS = 3,
    /*!< The compressed row offsets in sparse tensor descriptor for
         CNNL_SPARSE_FORMAT_CSR format. */
    CNNL_SPARSE_TENSOR_DESC_COL_OFFSETS = 4,
    /*!< The compressed column offsets in sparse tensor descriptor for
         CNNL_SPARSE_FORMAT_CSC format. */
} cnnlSparseTensorDescAttribute_t;

/*!
 * @brief Enumeration variables describing the attributes of sparse dense matmul.
 */
typedef enum {
    CNNL_SPARSE_DENSE_MATMUL_DESC_COMPUTE_TYPE = 0,
    /*!< The compute type of sparse dense matmul operation. */
    CNNL_SPARSE_DENSE_MATMUL_DESC_TRANSA = 1,
    /*!< Specifies whether transpose should be performed on tensor A.
         Default value is 0 (false). */
    CNNL_SPARSE_DENSE_MATMUL_DESC_TRANSB = 2,
    /*!< Specifies whether transpose should be performed on tensor B.
         Default value is 0 (false). */
} cnnlSparseDenseMatMulDescAttribute_t;

/*!
 * @brief Enumeration variables describing the attributes of sparse dense matmul.
 *
 * It is deprecated and will be removed in future release.
 */

typedef cnnlSparseDenseMatMulDescAttribute_t cnnlSparseDenseMatmulDescAttribute_t;

/******************************************************************************
 * Cambricon CNNL Data Structure: Norm
 ******************************************************************************/

/*!
 * @brief Enumeration variables describing the attributes of the norm computation.
 */
typedef enum {
  CNNL_NORM_DESC_ALLOW_REDUCED_PRECISION = 0,
  /*!< Precision-reduced calculations. */
  CNNL_NORM_DESC_OUTPUT_TYPE_SAME_AS_WEIGHT = 1,
  /*!< Output type calculations. */
  CNNL_NORM_DESC_MEMORY_EFFICIENT = 2,
  /*!< Memory-efficient calculations.*/
  CNNL_NORM_DESC_BIAS_DESC = 3,
  /*!< When memory efficiency is turned on, bias_desc is used. */
  CNNL_NORM_DESC_BIAS_PTR = 4,
  /*!< When memory efficiency is turned on, bias calculation is used. */
  CNNL_NORM_DESC_EPS = 5,
  /*!< When memory efficiency is turned on, eps is used. */
} cnnlNormDescAttribute_t;

/******************************************************************************
 * CNNL Runtime Management
 ******************************************************************************/

/*!
 * @struct cnnlContext
 * @brief The cnnlContext is a structure describing the Cambricon CNNLcontext.
 *
 *
 */
struct cnnlContext;
/*!
 * A pointer to ::cnnlContext struct that holds the Cambricon CNNL context.
 *
 * MLU device resources cannot be accessed directly, so Cambricon CNNL uses
 * handle to manage Cambricon CNNL context including MLU device information
 * and queues.
 *
 * The Cambricon CNNLcontext is created with ::cnnlCreate and the returned
 * handle should be passed to all the subsequent function calls.
 * You need to destroy the Cambricon CNNL context at the end with ::cnnlDestroy.
 * For more information, see "Cambricon CNNL User Guide".
 *
 */
typedef struct cnnlContext *cnnlHandle_t;

// Group:Common Interface
// Subgroup:Runtime Management
/*!
 *  @brief Initializes the Cambricon CNNL library and creates a \p handle to a structure
 *  that holds the Cambricon CNNL library context. It allocates hardware resources on the host
 *  and device. You need to call this function before any other Cambricon CNNL functions.
 *
 *  You need to call the ::cnnlDestroy function to release the resources later.
 *
 *  @param[out] handle
 *    Output. Pointer to the Cambricon CNNL context that is used to manage MLU devices and
 *    queues. For detailed information, see ::cnnlHandle_t.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlCreate(cnnlHandle_t *handle);

// Group:Common Interface
// Subgroup:Runtime Management
/*!
 *  @brief Updates the Cambricon CNNL context information that holds by the \p handle. This function
 *  should be called if you call Cambricon Driver API cnSetCtxConfigParam to set the context
 *  information. The related context information will be synchronized to Cambricon CNNL with this function.
 *  For detailed information, see "Cambricon Driver API Developer Guide".
 *
 *  @param[in] handle
 *    Input. Pointer to the Cambricon CNNL context that is used to manage MLU devices and
 *    queues. For detailed information, see ::cnnlHandle_t.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlUpdateContextInformation(cnnlHandle_t handle);

// Group:Common Interface
// Subgroup:Runtime Management
/*!
 *  @brief Releases the resources of the specified Cambricon CNNL \p handle that was
 *  created by the ::cnnlCreate function.
 *  It is usually the last call to destroy the handle to the Cambricon CNNL handle.
 *
 *  @param[in] handle
 *    Input. Pointer to the MLU devices that holds information to be destroyed.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroy(cnnlHandle_t handle);

// Group:Common Interface
// Subgroup:Runtime Management
/*!
 *  @brief Sets the runtime \p queue in the \p handle. The queue is used to
 *  launch kernels or to synchronize to this queue.
 *
 *  Before setting a \p queue, you need to call the ::cnnlCreate function to initialize
 *  Cambricon CNNL library.
 *
 *  If the \p queue is set to NULL, the default queue is used.
 *  Otherwise, you need to call `cnrtQueueCreate` to create a \p queue.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *    queues. For detailed information, see ::cnnlHandle_t.
 *  @param[in] queue
 *    Input. The runtime queue to be set to the Cambricon CNNL handle.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - When the \p queue is set to NULL and you are in multi-threaded programming,
 *    remember that the default queue in different threads differs from each other, and
 *    CNRT function calls involving the \p queue (e.g. `cnrtQueueSync`)
 *    only affect Cambricon CNNL function calls in the same thread.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetQueue(cnnlHandle_t handle, cnrtQueue_t queue);

// Group:Common Interface
// Subgroup:Runtime Management
/*!
 *  @brief Retrieves the \p queue that was previously set to the \p handle.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *    queues. For detailed information, see ::cnnlHandle_t.
 *  @param[out] queue
 *    Output. Pointer to the queue that was previously set to the specified handle.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetQueue(cnnlHandle_t handle, cnrtQueue_t *queue);

// Group:Common Interface
// Subgroup:Runtime Management
/*!
 *  @brief Retrieves the device ID in the handle created by ::cnnlCreate.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *    queues. For detailed information, see ::cnnlHandle_t.
 *  @param[out] device
 *    Output. Pointer to the MLU device ID that was previously created by ::cnnlCreate.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetDevice(cnnlHandle_t handle, CNdev *device);

// Group:Common Interface
// Subgroup:QuantizeRoundMode
/*!
 *  @brief Updates the specific rounding mode of Cambricon CNNL context information that holds by the \p handle. This function
 *  should be called if you want to change the cnnl rounding mode that used to cumulate the results.
 *  For detailed information, see Cambricon Driver API Developer Guide.
 *
 *  @param[in] handle
 *    Input. Pointer to the Cambricon CNNL context that is used to manage MLU devices and
 *    queues. For detailed information, see ::cnnlHandle_t.
 *  @param[in] round_mode
 *    Input. The rounding mode of quantization conversion to be set to the CNNL handle.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - On MLU200 series:
 *    You can't set CNNL_ROUND_HALF_TO_EVEN for the rounding mode because the hardware does not support it.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetQuantizeRoundMode(cnnlHandle_t handle,
                                                   cnnlQuantizeRoundMode_t round_mode);

// Group:Common Interface
// Subgroup:QuantizeRoundMode
/*!
 *  @brief Retrieves the rounding mode of a specific Cambricon CNNL context.
 *
 *  @param[in] handle
 *    Input. Pointer to the Cambricon CNNL context that is used to manage MLU devices and
 *    queues. For detailed information, see ::cnnlHandle_t.
 *
 *  @param[out] round_mode
 *    Output. The rounding mode of quantization conversion that was previously set to the specified handle.
 *
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - The default round mode of default initialized cnnlHandle_t is CNNL_ROUND_TO_EVEN.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetQuantizeRoundMode(cnnlHandle_t handle,
                                                   cnnlQuantizeRoundMode_t *round_mode);

// Group:Common Interface
// Subgroup:Runtime Management
/*!
 *  @brief Updates the specific atomics mode of Cambricon CNNL context information that holds by the \p handle. This function
 *  should be called if you want to change the atomics mode that is used to cumulate the results.
 *  For detailed information, see Cambricon Driver API Developer Guide.
 *
 *  @param[in] handle
 *    Input. Pointer to the Cambricon CNNL context that is used to manage MLU devices and
 *    queues. For detailed information, see ::cnnlHandle_t.
 *
 *  @param[in] atomics_mode
 *    Input. The atomics mode.
 *
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetAtomicsMode(cnnlHandle_t handle, cnnlAtomicsMode_t atomics_mode);

// Group:Common Interface
// Subgroup:Runtime Management
/*!
 *  @brief Retrieves the atomics mode of a specific Cambricon CNNL context.
 *
 *  @param[in] handle
 *    Input. Pointer to the Cambricon CNNL context that is used to manage MLU devices and
 *    queues. For detailed information, see ::cnnlHandle_t.
 *
 *  @param[out] atomics_mode
 *    Output. The atomics mode.
 *
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - The default atomics mode of default initialized cnnlHandle_t is ::CNNL_ATOMICS_NOT_ALLOWED.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetAtomicsMode(cnnlHandle_t handle, cnnlAtomicsMode_t *atomics_mode);

// Group:Common Interface
// Subgroup:Runtime Management
/**
 *  @brief Retrieves the configuration parameters in the handle created by ::cnnlCreate.
 *
 *  @param[in] handle
 *     Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *     queues. For detailed information, see ::cnnlHandle_t.
 *  @param[in] type
 *     Input. The configuration parameter type. The parameter type includes
 *     CN_CTX_CONFIG_UNION_LIMIT and CN_CTX_CONFIG_VISIBLE_CLUSTER_NUM.
 *     For more information, see "Cambricon Driver API User Guide".
 *  @param[out] param
 *     Output. Pointer to the host memory that stores the value of configuration parameter.
 *     For more information, see "Cambricon Driver API User Guide".
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @par Scale Limitation
 *  - The value of \p type can only be CN_CTX_CONFIG_UNION_LIMIT or
 *    CN_CTX_CONFIG_VISIBLE_CLUSTER_NUM.

 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetContextParam(cnnlHandle_t handle,
                                              CNctxConfigParamType type,
                                              CNctxConfigParam *param);

// Group:Common Interface
// Subgroup:Runtime Management
/*!
 *  @brief Gets the size of device memory reserved for CNNL.
 *
 *  @param[out] mem_size
 *    Output. The memory size reserved for CNNL.
 *  @return
 *  - CNNL_STATUS_SUCCESS, CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - This parameter \p mem_size can be configured by the environment variable CNNL_MEM_POOL_SIZE.
 *    If CNNL_MEM_POOL_SIZE not configured, this function will return a built-in hardcode value.
 *    For more information about setting environment variable, see "Cambricon CNNL User Guide".
 */
cnnlStatus_t CNNL_WIN_API cnnlGetReservedMemSize(uint64_t* mem_size);

// Group:Common Interface
// Subgroup:Runtime Management
/*!
 *  @brief Converts the enumerated status code to ASCIIZ static string and returns
 *  a pointer to host memory that holds information about ASCIIZ static string with the status name.
 *  For example, when the input argument is
 *  ::CNNL_STATUS_SUCCESS, the returned string is CNNL_STATUS_SUCCESS. When an invalid status value is passed
 *  to the function, the returned string is CNNL_STATUS_BAD_PARAM.
 *
 *  @param[in] status
 *    Input. The enumerated status code.
 *  @return
 *  - CNNL_STATUS_SUCCESS, CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 *
 */
const char CNNL_WIN_API *cnnlGetErrorString(cnnlStatus_t status);

/******************************************************************************
 * Cambricon CNNL Data Structure: Descriptor
 * The struct represent input, filter and the AI network layer
 ******************************************************************************/
/*! The descriptor of a tensor that holds the information including tensor
 *  layout, data type, the number of dimensions, shape and strides.
 *
 *  You need to call the ::cnnlCreateTensorDescriptor function to create a descriptor,
 *  and call the ::cnnlSetTensorDescriptor function or the ::cnnlSetTensorDescriptorEx
 *  function to set the tensor information to the descriptor. Also, you need to destroy
 *  the Cambricon CNNL context at the end with the ::cnnlDestroyTensorDescriptor function.
 */
typedef struct cnnlTensorStruct *cnnlTensorDescriptor_t;
/*! The descriptor of the sparse tensor that holds the information including the format, layout,
 *  dimentions, nonzero elemets and values of a sparse tensor.
 *
 *  You need to call the ::cnnlCreateSparseTensorDescriptor function to create a
 *  desriptor, and call ::cnnlSetSparseTensorDescAttr to set the row, column and values
 *  to the descriptor. Also, you need to destroy the Cambricon CNNL context at the
 *  end with the ::cnnlDestroySparseTensorDescriptor function.
 */
typedef struct cnnlSparseTensorStruct *cnnlSparseTensorDescriptor_t;
/*! The descriptor of Sequence Data that holds the dimensions,
 * layout, data type, sequence length, padding fill, position, and scale.
 * The total size of the tensor descriptor only support less than 2 Giga-elements.
 * You need to call the ::cnnlCreateSeqDataDescriptor function to create a descriptor, and
 * call ::cnnlSetSeqDataDescriptor to set the sequence data information to the descriptor.
 * If the sequence data is in fixed-point data type, call ::cnnlSetSeqDataDescriptorPositionAndScale
 * to set the position and scale of the sequence data.
 * At last, you need to destroy the descriptor at the end with the ::cnnlDestroySeqDataDescriptor
 * function.
 */
typedef struct cnnlSeqDataStruct *cnnlSeqDataDescriptor_t;

/*! The descriptor of the quantization that holds the information including \p position_desc,
 *  \p position_ptr, \p scale_desc, \p scale_ptr, \p offset_desc, \p offset_ptr, \p pointer_mode,
 *  \p quantize_mode, \p onchip_dtype and \p channel_group_size.
 *
 *  You need to call the ::cnnlCreateQuantizeExDescriptor function to create a descriptor,
 *  and call the ::cnnlSetQuantizeExDescriptorScalarQuant, ::cnnlSetQuantizeExDescriptor
 *  or ::cnnlSetQuantizeExDescriptorQuantSchemeAndDtype function to set the tensor information
 *  to the descriptor. Also, you need to destroy the Cambricon CNNL context at the end with the
 *  ::cnnlDestroyQuantizeExDescriptor function.
 */
typedef struct cnnlQuantizeExStruct *cnnlQuantizeExDescriptor_t;
/*! The descriptor of the ::cnnlSpace2batchNd_v2 and ::cnnlBatch2spaceNd_v2
 * operations that holds operation information including the number of block
 * dimensions and padding dimensions.
 *
 *  You need to call the ::cnnlCreateSpaceBatchNdDescriptor function
 *  to create a descriptor, and call the ::cnnlSetSpaceBatchNdDescriptor
 *  function to set the information of the operation to the descriptor.
 *  Also, you need to destroy the Cambricon CNNL context at the end with
 *  the ::cnnlDestroySpaceBatchNdDescriptor function.
 */
typedef struct cnnlSpaceBatchNdStruct *cnnlSpaceBatchNdDescriptor_t;
/*! The descriptor of the convolution operation that holds convolution information
 *  including the number of input dimensions, padding, stride, dilation, and group_count.
 *
 *  You need to call the ::cnnlCreateConvolutionDescriptor function
 *  to create a descriptor, and call the ::cnnlSetConvolutionDescriptor function to set
 *  the information of the convolution operation to the descriptor. Also, you need to
 *  destroy the Cambricon CNNL context at the end with the ::cnnlDestroyConvolutionDescriptor function.
 */
typedef struct cnnlConvolutionStruct *cnnlConvolutionDescriptor_t;

/*! The struct of the convolution algorithm information, including the algorithm ID,
 *  returned status, hardware time of execution, workspace required and the determinism
 *  of computation result. This struct is returned by ::cnnlFindConvolutionForwardAlgo.
 */
typedef struct cnnlConvolutionFwdAlgoPerf_s {
  cnnlConvolutionForwardAlgo_t algo;
  /*!< Algorithm ID.*/
  cnnlStatus_t status;
  /*!< Returned status.*/
  float time;
  /*!< Hardware time.*/
  size_t memory;
  /*!< Workspace size required for this algorithm.*/
  cnnlDeterminism_t determinism;
  /*!< Whether the results are guaranteed to be reproducible.*/
} cnnlConvolutionFwdAlgoPerf_t;

/*! The descriptor of the deconvolution operation that holds deconvolution information
 *  including the number of input dimensions, padding, stride, dilation, group_count
 *  and compute_type.
 *
 *  You need to call the ::cnnlCreateDeconvolutionDescriptor function
 *  to create a descriptor, and call the ::cnnlSetDeconvolutionDescriptor function to set
 *  the information of the deconvolution operation to the descriptor. Also, you need to
 *  destroy the Cambricon CNNL context at the end with the ::cnnlDestroyDeconvolutionDescriptor function.
 */
typedef struct cnnlDeconvolutionStruct *cnnlDeconvolutionDescriptor_t;
/*! The descriptor of the matrix multiplication function that holds compute type, bias type, transpose flag, and
 *  other attributes defined in ::cnnlMatMulDescAttribute_t.
 *
 *  You need to call the ::cnnlCreateMatMulDescriptor function to create a descriptor, and call the ::cnnlSetMatMulDescAttr
 *  function to set the information of the matrix multiplication to the descriptor. Also, you need to destroy
 *  the Cambricon CNNL context at the end with the ::cnnlDestroyMatMulDescriptor function.
 */
typedef struct cnnlMatMulStruct *cnnlMatMulDescriptor_t;
/*! The descriptor of the sparse dense matrix multiplication function that holds the compute type
 *  and transpose flag defined in ::cnnlSparseDenseMatmulDescAttribute_t.
 *
 *  You need to call the ::cnnlSparseDenseMatmulDescCreate function to create a descriptor, and call
 *  the ::cnnlSetSparseDenseMatmulDescAttr function to set the information of the matrix multiplication to the descriptor.
 *  Also, you need to destroy the Cambricon CNNL context at the end with the ::cnnlSparseDenseMatmulDescDestroy function.
 *
 *  It is deprecated and will be removed in future release.
 */

typedef struct cnnlSparseDenseMatmulStruct *cnnlSparseDenseMatmulDescriptor_t;
/*! The descriptor of the sparse dense matrix multiplication function that holds the compute type
 *  and transpose flag defined in ::cnnlSparseDenseMatMulDescAttribute_t.
 *
 *  You need to call the ::cnnlCreateSparseTensorDescriptor function to create a descriptor, and call
 *  the ::cnnlSetSparseDenseMatMulDescAttr function to set the information of the matrix multiplication to the descriptor.
 *  Also, you need to destroy the Cambricon CNNL context at the end with the ::cnnlDestroySparseTensorDescriptor function.
 */
typedef struct cnnlSparseDenseMatMulStruct *cnnlSparseDenseMatMulDescriptor_t;
/*! The descriptor of the matrix multiplication that holds the configured matrix multiplication
 *  algorithm descriptor and its runtime properties.
 *
 *  You need to call the ::cnnlCreateMatMulHeuristicResult function to create a descriptor.
 *  Also, you need to destroy the Cambricon CNNL context at the end with the ::cnnlDestroyMatMulHeuristicResult function.
 */
typedef struct cnnlMatMulHeuristicResult *cnnlMatMulHeuristicResult_t;
/*! The descriptor of the matrix multiplication that holds the preferences for cnnlMatMulHeuristicResult_t
 *  configuration.
 */
typedef struct cnnlMatMulPrefer *cnnlMatMulPrefer_t;
/*! The descriptor of the batch matrix multiplication function that holds compute type, bias type, transpose flag,
 *  and other attributes defined in ::cnnlBatchMatMulDescAttribute_t.
 *
 *  You need to call the ::cnnlBatchMatMulDescCreate function to create a descriptor, and call the ::cnnlSetBatchMatMulDescAttr
 *  function to set the information of the batch matrix multiplication to the descriptor. Also, you need to destroy
 *  the Cambricon CNNL context at the end with the ::cnnlBatchMatMulDescDestroy function.
 */
typedef struct cnnlBatchMatMulStruct *cnnlBatchMatMulDescriptor_t;
/*! The descriptor of the matrix multiplication computation algorithm.
 *
 *  You need to call the ::cnnlCreateMatMulAlgo function to create a descriptor.
 *  Also, you need to destroy the Cambricon CNNL context at the end with the ::cnnlDestroyMatMulAlgo function.
 */
typedef struct cnnlMatMulAlgoStruct *cnnlMatMulAlgo_t;
/*! The descriptor of the batch matrix multiplication computation algorithm.
 *
 *  You need to call the ::cnnlBatchMatMulAlgoCreate function to create a descriptor, and call the ::cnnlGetQuantizeBatchMatMulAlgorithm
 *  function to set the information to the descriptor. Also, you need to destroy the Cambricon CNNL context at the end with the
 *  ::cnnlBatchMatMulAlgoDestroy function.
 */
typedef struct cnnlBatchMatMulAlgoStruct *cnnlBatchMatMulAlgo_t;
/*! The descriptor of the batch matrix multiplication with broadcasting function that holds compute type, bias type,
 *  transpose flag, and other attributes defined in ::cnnlBatchMatMulBCastDescAttribute_t.
 *
 *  You need to call the ::cnnlBatchMatMulBCastDescCreate function to create a descriptor, and call the
 *  ::cnnlSetBatchMatMulBCastDescAttr function to set the information of the matrix multiplication to the descriptor.
 *  Also, you need to destroy the Cambricon CNNL context at the end with the ::cnnlBatchMatMulBCastDescDestroy function.
 */
typedef struct cnnlBatchMatMulBCastStruct *cnnlBatchMatMulBCastDescriptor_t;
/*! The descriptor of the batch matrix multiplication with broadcasting computation algorithm.
 *
 *  You need to call the ::cnnlBatchMatMulBCastAlgoCreate function to create a descriptor, and call the
 *  ::cnnlGetQuantizeBatchMatMulBCastAlgorithm function to set the information to the descriptor. Also, you need to
 *  destroy the Cambricon CNNL context at the end with the ::cnnlBatchMatMulBCastAlgoDestroy function.
 */
typedef struct cnnlBatchMatMulBCastAlgoStruct *cnnlBatchMatMulBCastAlgo_t;
/*! The descriptor of the pooling operation that holds pooling information
 *  including the pooling mode, the NaN propagation mode, the number of input dimensions, padding, window and stride.
 *
 *  You need to call the ::cnnlCreatePoolingDescriptor function
 *  to create a descriptor, and call the ::cnnlSetPooling2dDescriptor, ::cnnlSetPooling2dDescriptor_v2,
 *  ::cnnlSetPoolingNdDescriptor or ::cnnlSetPoolingNdDescriptor_v2 function to set the information of the pooling operation to the descriptor.
 *  Also, you need to destroy the Cambricon CNNL context at the end with the ::cnnlDestroyPoolingDescriptor function.
 */
typedef struct cnnlPoolingStruct *cnnlPoolingDescriptor_t;
/*! The descriptor of the activation operation that holds activation information including
 *  the \p activation_desc, \p mode, \p nan_prop, and \p coef.
 *
 *  You need to call the ::cnnlCreateActivationDescriptor function to create a descriptor,
 *  and call the ::cnnlSetActivationDescAttr function to set the information of the descriptor.
 *  Also, you need to destroy the Cambricon CNNL context at the end with the ::cnnlDestroyActivationDescriptor
 *  function.
 */
typedef struct cnnlActivationStruct *cnnlActivationDescriptor_t;
/*! The descriptor of customized Activation function that holds the activation modes.*/
/*! The descriptor of the customized activation operation that holds customized activation information
 *  including the number of input range, output range, and segment.
 *
 *  You need to call the ::cnnlCreateCustomizedActiveDescriptor function
 *  to create a descriptor, and call the ::cnnlSetCustomizedActiveDescriptor function to set
 *  the information of the customized activation operation to the descriptor. Also, you need to
 *  destroy the Cambricon CNNL context at the end with the ::cnnlDestroyCustomizedActiveDescriptor function.
 */
typedef struct cnnlCustomizedActiveStruct *cnnlCustomizedActiveDescriptor_t;
/*! The descriptor of OpTensor operation that holds OpTensor information including
 *  ::cnnlOpTensorDesc_t, ::cnnlDataType_t and ::cnnlNanPropagation_t.
 *
 *  You need to call the ::cnnlCreateOpTensorDescriptor function to create a descriptor,
 *  and call the ::cnnlSetOpTensorDescriptor function to set the information of the
 *  OpTensor operation to the descriptor. Also, you need to destroy the descriptor at the
 *  end with the ::cnnlDestroyOpTensorDescriptor function.
 */
typedef struct cnnlOpTensorStruct *cnnlOpTensorDescriptor_t;
/*! The descriptor of Reduce function that holds  information including ::cnnlReduceOp_t,
 * ::cnnlDataType_t, ::cnnlNanPropagation_t, ::cnnlReduceIndices_t, and ::cnnlIndicesType_t.*/
typedef struct cnnlReduceStruct *cnnlReduceDescriptor_t;
/*! The descriptor of Trigon function that holds trigon function mode and computation preference
 *  mode. You need to call the ::cnnlCreateTrigonDescriptor function to create a descriptor, and
 *  call ::cnnlSetTrigonDescriptor or ::cnnlSetTrigonDescriptor_v2 to set the trigon function
 *  mode and computation preference to the descriptor. At last, you need to destroy the descriptor
 *  at the end with the ::cnnlDestroyTrigonDescriptor function. */
typedef struct cnnlTrigonStruct *cnnlTrigonDescriptor_t;
/*! The descriptor of the GRU operation that holds GRU information
 *  including the algorithm defined in ::cnnlGruAlgo_t, flag to determine whether the operation
 *  is bidirectional, and the number of layer.
 *
 *  You need to call the ::cnnlCreateGruDescriptor function
 *  to create a descriptor, and call the ::cnnlSetGruDescriptor function to set
 *  the information of the GRU operation to the descriptor. Also, you need to
 *  destroy the Cambricon CNNL context at the end with the ::cnnlDestroyGruDescriptor function.
 */
typedef struct cnnlGruStruct *cnnlGruDescriptor_t;
/*! The descriptor of Attention function that holds cnnlAttentionAlgo_t. */
typedef struct cnnlAttentionStruct *cnnlAttentionDescriptor_t;
/*! The descriptor of multi-head attention function that holds the information required in the
 * multi-head attention operation. You need to call ::cnnlCreateMultiHeadAttnDescriptor to
 * create a descriptor, and call ::cnnlSetMultiHeadAttnDescriptor to set the information
 * of the multi-head attention operation to the descriptor. If the filters are in fixed-point data
 * type, you also need to call ::cnnlSetMultiHeadAttnWeightsQuantifyInfo to set the position and
 * scale of all filters. At last, you need to destroy the descriptor at the end with the
 * ::cnnlDestroyMultiHeadAttnDescriptor function.*/
typedef struct cnnlMultiHeadAttnStruct *cnnlMultiHeadAttnDescriptor_t;
/*! The descriptor of the grep operation that holds grep information
 *  including \p begin, \p size, \p space_number, \p mlu_input_dim, \p mlu_begin,
 *  \p mlu_size, \p mlu_mlutiplier, and \p mlu_divisor.
 *
 *  You need to call ::cnnlSetGrepDescriptor to set the information of the grep operation
 *  to the descriptor. At last, you need to destroy the descriptor at the end with the
 *  ::cnnlDestroyGrepDescriptor function. */
typedef struct cnnlGrepStruct *cnnlGrepDescriptor_t;
/*! The descriptor of the reorg operation that holds reorg information
 *  including the number of reorg height dimension, reorg width dimension and whether to
 *  forward or not.
 *
 *  You need to call the ::cnnlCreateReorgDescriptor function
 *  to create a descriptor, and call the ::cnnlSetReorgDescriptor function to set
 *  the information of the reorg operation to the descriptor. Also, you need to
 *  destroy the Cambricon CNNL context at the end with the ::cnnlDestroyReorgDescriptor function.
 */
typedef struct cnnlReorgStruct *cnnlReorgDescriptor_t;
/*! The descriptor of Unique function that holds cnnlUniqueSort_t, dim, return_inverse,
 *  and return_counts.
 *
 *  You need to call ::cnnlCreateUniqueDescriptor to create a descriptor,
 *  and call ::cnnlSetUniqueDescriptor to set the information of the unique operator to
 *  the descriptor. At last, you need to destroy the descriptor at the end with the
 *  ::cnnlDestroyUniqueDescriptor function.*/
typedef struct cnnlUniqueStruct *cnnlUniqueDescriptor_t;
/*! The descriptor of UniqueConsecutive function that holds dim, return_inverse, and return_counts.
 *
 *  You need to call ::cnnlCreateUniqueConsecutiveDescriptor to create a descriptor,
 *  and call ::cnnlSetUniqueConsecutiveDescriptor to set the information of
 *  the unique_consecutive operator to the descriptor.
 *  At last, you need to destroy the descriptor at the end with the
 *  ::cnnlDestroyUniqueConsecutiveDescriptor function.*/
typedef struct cnnlUniqueConsecutiveStruct *cnnlUniqueConsecutiveDescriptor_t;
/*! The descriptor of the NMS function that holds information including ::cnnlNmsBoxPointMode_t, ::cnnlNmsOutputMode_t,
 *  ::cnnlNmsAlgo_t, ::cnnlNmsMethodMode_t, iou_threshold, soft_nms_sigma, max_output_size,
 *  confidence_threshold, offset, input_layout and pad_to_max_output_size.
 *
 *  You need to call ::cnnlCreateNmsDescriptor to create a descriptor, and call
 *  ::cnnlSetNmsDescAttr to set the information of the NMS operation separately to the descriptor.
 *  At last, you need to destroy the descriptor at the end with the ::cnnlDestroyNmsDescriptor
 *  function.*/
typedef struct cnnlNmsStruct *cnnlNmsDescriptor_t;
/*! The function pointer of CustomizedActive function that holds activation function.
 *  In function pointer, input data type and return data type must be double. For example:
 *  ``double relu(double input) {input < 0 ? 0 : input}`` */
typedef double (*active_func_type)(double);
/*! The descriptor of Normalize function that holds the information required in the
 *  normalize operation.
 *
 *  You need to call ::cnnlCreateNormalizeDescriptor to create
 *  a descriptor, and call ::cnnlSetNormalizeDescriptor to set the information of
 *  the normalize operation to the descriptor. At last, you need to destroy the descriptor
 *  at the end with the ::cnnlDestroyNormalizeDescriptor function.*/
typedef struct cnnlNormalizeStruct *cnnlNormalizeDescriptor_t;

/*! The descriptor of RNN function that holds the information required in the
 * RNN operation. You need to call ::cnnlCreateRNNDescriptor to create a descriptor,
 * and call ::cnnlSetRNNDescriptor to set the basic information of the RNN operation to the descriptor.
 * In addition to basic information, you can optionally call the following functions to set additional
 * information: ::cnnlSetRNNProjectionLayers, ::cnnlSetRNNPeepholeMode, ::cnnlSetRNNBiasMode,
 * ::cnnlSetRNNMaskMode, ::cnnlSetRNNClip, ::cnnlSetRNNPaddingMode.
 * Also, you need to destroy the descriptor at the end with the
 * ::cnnlDestroyRNNDescriptor function.*/
typedef struct cnnlRNNParam *cnnlRNNDescriptor_t;

/*! The descriptor of the collection of tensor which is used in the RNN operation, such as filter and bias.
 *  You need to call the ::cnnlCreateTensorSetDescriptor function to create a descriptor, and
 *  call ::cnnlInitTensorSetMemberDescriptor to set the information about each tensor in
 *  the tensor set. If the tensor in the tensor set is in fixed-point data type,
 *  call ::cnnlInitTensorSetMemberDescriptorPositionAndScale function to set quantization parameters.
 *  At last, you need to destroy the descriptor at the end with the ::cnnlDestroyTensorSetDescriptor
 *  function. */
typedef struct cnnlTensorSetStruct *cnnlTensorSetDescriptor_t;

/*! The descriptor of CTC_Loss function that holds information including ::cnnlCTCLossNormalizationMode_t,
 *  ::cnnlCTCLossReduceMode_t, ::cnnlCTCLossZeroInfinityMode_t, blank, maximum input length,
 *  and maximum label length.
 *
 *  You need to call ::cnnlCreateCTCLossDescriptor to create a descriptor,
 *  and call ::cnnlSetCTCLossDescriptor to set the information of the CTC_Loss operator to
 *  the descriptor. At last, you need to destroy the descriptor at the end with the
 *  ::cnnlDestroyCTCLossDescriptor function.*/
typedef struct cnnlCTCLossStruct *cnnlCTCLossDescriptor_t;

/*! The descriptor of RNNT_Loss operation that holds  information including ::cnnlLossReduction_t, blank, clamp,
 *  fused_log_softmax, maximum logit length, and maximum target length information.
 *
 *  You need to call ::cnnlCreateRNNTLossDescriptor to create a descriptor,
 *  and call ::cnnlSetRNNTLossDescriptor to set the information of the RNNT_Loss operator to
 *  the descriptor. At last, you need to destroy the descriptor at the end with the
 *  ::cnnlDestroyRNNTLossDescriptor function.*/
typedef struct cnnlRNNTLossStruct *cnnlRNNTLossDescriptor_t;

/*! The descriptor of interpolation function that holds  information including ::cnnlInterpMode_t,
 *  ::cnnlInterpCoordinateTransformationMode_t, ::cnnlInterpRoundMode_t, scale_factors, pad,
 *  cubic_coeff_a and exclude_outside.
 *
 *  You need to call ::cnnlCreateInterpDescriptor to create a descriptor,
 *  and call ::cnnlSetInterpDescriptor to set the information of the interpolation operation to
 *  the descriptor. When the extra parameters are used, you need to call ::cnnlSetInterpDescriptorEx.
 *  When the extra parameters are not used, you don't need to call ::cnnlSetInterpDescriptorEx.
 *  At last, you need to destroy the descriptor at the end with the
 *  ::cnnlDestroyInterpDescriptor function.*/

typedef struct cnnlInterpStruct *cnnlInterpDescriptor_t;

/*! The descriptor of deformable convolution function that holds the deformable convolution
 * information including the number of input dimensions, padding, stride, dilation,
 * deformable group, convolution group, and img2col_step.
 *
 * You need to call the ::cnnlCreateDCNDescriptor function to create a descriptor, and call the
 * ::cnnlSetDCNDescriptor function to set the information of the deformable convolution operation
 * to the descriptor. Also, you need to destroy the Cambricon CNNL context at the end with the
 * ::cnnlDestroyDCNDescriptor function.
 */
typedef struct cnnlDCNStruct *cnnlDCNDescriptor_t;

/*! The descriptor of CARAFE (Content-Aware ReAssembly of FEatures) operator that holds
 * CARAFE information including the number of input dimensions, kernel size, group size,
 * and scale factor.
 *
 * You need to call the ::cnnlCreateCarafeDescriptor function to create a descriptor,
 * and call the ::cnnlSetCarafeDescriptor function to set the information of the CARAFE operator
 * to the descriptor. Also, you need to destroy the Cambricon CNNL context at the end with the
 * ::cnnlDestroyCarafeDescriptor function.
 */
typedef struct cnnlCarafeStruct *cnnlCarafeDescriptor_t;

/*! The descriptor of FFT (Fast Fourier Transform) operation that holds FFT information including
 * the tensor descriptor of input tensor and output tensor, the rank of FFT, the FFT size on each
 * dimension, the size of reserved space and the size of workspace.
 *
 * You need to call the ::cnnlCreateFFTPlan function to create a descriptor for the FFT operation, and call
 * the ::cnnlMakeFFTPlanMany function to set the information of the FFT operation to the descriptor.
 * Then, you need to allocate the reserved space and set the space to the FFT descriptor by ::cnnlSetFFTReserveArea.
 * Also, you need to destroy the Cambricon CNNL context at the end with ::cnnlDestroyFFTPlan.
 */
typedef struct cnnlFFTStruct *cnnlFFTPlan_t;

/*! The descriptor of grid_sample operation that holds the information
 *  including the interpolation mode, padding mode and align_corners mode.
 *
 * You need to call the ::cnnlCreateGridSampleDescriptor function to create a descriptor,
 * and call the ::cnnlSetGridSampleDescriptor function to set the information of the grid_sample
 * operation to the descriptor. Also, you need to destroy the Cambricon CNNL context at the end with the
 * ::cnnlDestroyGridSampleDescriptor function.
 */
typedef struct cnnlGridSampleStruct *cnnlGridSampleDescriptor_t;

/*! The descriptor of the norm forward function.
 *
 *  You need to call the ::cnnlCreateNormDesc function to create a descriptor, and call
 *  the ::cnnlSetNormDescAttr function to set the information of the norm forward to the descriptor.
 *  Also, you need to destroy the Cambricon CNNL context at the end with the ::cnnlDestroyNormDesc function.
 */
typedef struct cnnlNormStruct *cnnlNormDescriptor_t;

/*! The descriptor of the division operation that holds div information including
 *  the \p mode.
 *
 *  You need to call the ::cnnlCreateDivDescriptor function to create a descriptor,
 *  and call the ::cnnlSetDivDescAttr function to set the information of the descriptor.
 *  Also, you need to destroy the Cambricon CNNL context at the end with the ::cnnlDestroyDivDescriptor
 *  function.
 */
typedef struct cnnlDivStruct *cnnlDivDescriptor_t;

// Group:Common Interface
// Subgroup:Tensor
/*!
 *  @brief Gets the size of a data type in ::cnnlDataType_t.
 *
 *  @param[in] data_type
 *    Input. The data type. For detailed information, see ::cnnlDataType_t.
 *  @param[out] size
 *    Output. Host pointer to the size of the data type.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetSizeOfDataType(cnnlDataType_t data_type, size_t *size);

// Group:Common Interface
// Subgroup:Tensor
/*!
 *  @brief Creates a tensor descriptor pointed by \p desc that holds the dimensions, data type,
 *  and layout of input tensor. If the input tensor is in fixed-point data type,
 *  the ::cnnlSetTensorDescriptorPositionAndScale function or the ::cnnlSetTensorDescriptorPosition
 *  function needs to be called to set quantization parameters.
 *
 *  The ::cnnlDestroyTensorDescriptor function needs to be called to destroy the
 *  tensor descriptor later.
 *
 *  @param[out] desc
 *    Output. Pointer to the struct that holds information about the tensor descriptor.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateTensorDescriptor(cnnlTensorDescriptor_t *desc);

// Group:Common Interface
// Subgroup:Tensor
/*!
 *  @brief Creates a group of tensor descriptors stored by \p group_desc that holds the dimensions,
 *  data type and layout of input tensors. If the input tensor is in fixed-point data type,
 *  the ::cnnlSetTensorDescriptorPositionAndScale function or the ::cnnlSetTensorDescriptorPosition
 *  function need to be called to set quantization parameters.
 *
 *  @param[out] group_desc
 *    Output. An array of pointers to the structs that hold information about the tensor descriptors.
 *  @param[in] desc_num
 *    Input. The length of the input array \p group_desc.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The ::cnnlDestroyTensorDescriptor function needs to be called for each descriptor to destroy all tensors
 *   in group_desc or ::cnnlDestroyGroupTensorDescriptors needs to be called to destroy the all tensor
 *   descriptors in group_desc later.
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateGroupTensorDescriptors(cnnlTensorDescriptor_t *group_desc[],
                                                           const int desc_num);

// Group:SparseDenseMatMul
/*!
 * @brief Creates a sparse tensor descriptor pointed by \p sp_desc that holds the information including
 *        the format, layout, dimentions, nonzero elemets and values of a sparse tensor.
 *
 * To destroy the sparse tensor descriptor later, call the ::cnnlDestroySparseTensorDescriptor function.
 *
 * @param[out] sp_desc
 *   Output. The pointer to the struct that holds information about the sparse tensor
 *   descriptor.
 * @param[in] format
 *   Input. The format of sparse tensor. For detailed information, see ::cnnlSparseFormat_t.
 * @param[in] rows
 *   Input. The number of rows of sparse tensor.
 * @param[in] cols
 *   Input. The number of columns of sparse tensor.
 * @param[in] nnz
 *   Input. The number of non-zero elements of sparse tensor.
 * @param[in] block_size
 *   Input. The pointer to the block size of sparse tensor for block sparse format. For non-block sparse
 *   formats such as CNNL_SPARSE_FORMAT_COO, CNNL_SPARSE_FORMAT_CSR and CNNL_SPARSE_FORMAT_CSC,
 *   set it to nullptr.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCreateSparseTensorDescriptor(cnnlSparseTensorDescriptor_t *sp_desc,
                                 const cnnlSparseFormat_t format,
                                 const int64_t rows,
                                 const int64_t cols,
                                 const int64_t nnz,
                                 const int64_t block_size[]);

// Group:Common Interface
// Subgroup:Tensor
/*!
 *  @brief Initializes the tensor descriptor pointed by \p desc that was previously created
 *  with the ::cnnlCreateTensorDescriptor function, and sets the information about
 *  the dimensions, data type and layout of the input tensor.
 *
 *  If ::cnnlSetTensorDescriptor is called, you do not need to specify the strides of all
 *  dimensions. The strides are inferred by parameters passed to this function. Also, the data
 *  will be treated as contiguous in memory with no padding between dimensions. To specify the
 *  strides of all dimensions, you can call ::cnnlSetTensorDescriptorEx. But the data might not
 *  be treated as contiguous in memory.
 *
 *  @param[in,out] desc
 *    Input/output. The descriptor of the input tensor. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[in] layout
 *    Input. The layout of the input tensor. For detailed information, see ::cnnlTensorLayout_t.
 *  @param[in] dtype
 *    Input. The data type of the input tensor. For detailed information, see ::cnnlDataType_t.
 *  @param[in] dimNb
 *    Input. The number of dimensions in the input tensor of the initialized operation.
 *  @param[in] dimSize
 *    Input. An array that contains the size of the tensor for each dimension.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - dimSize[0] represents the highest dimension, dimSize[DIM_MAX - 1] represents
 *    the lowest dimension, and DIM_MAX represents the number of dimensions in the input tensor.
 *  - This function cannot be called continuously. You need to call ::cnnlResetTensorDescriptor
 *    before calling another ::cnnlSetTensorDescriptor to avoid memory leaks.
 *  - This function does not support negative dimension for tensor dimensions.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetTensorDescriptor(cnnlTensorDescriptor_t desc,
                                                  cnnlTensorLayout_t layout,
                                                  cnnlDataType_t dtype,
                                                  int dimNb,
                                                  const int dimSize[]);

// Group:Common Interface
// Subgroup:Tensor
/*!
 *  @brief Initializes the tensor descriptor pointed by \p desc that was previously created
 *  with the ::cnnlCreateTensorDescriptor function, and sets the information about
 *  the dimensions, data type and layout of the input tensor.
 *
 *  If ::cnnlSetTensorDescriptor_v2 is called, you do not need to specify the strides of all
 *  dimensions. The strides are inferred by parameters passed to this function. Also, the data
 *  will be treated as contiguous in memory with no padding between dimensions. To specify the
 *  strides of all dimensions, you can call ::cnnlSetTensorDescriptorEx_v2. But the data might not
 *  be treated as contiguous in memory.
 *
 *  @param[in,out] desc
 *    Input/output. The descriptor of the input tensor. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[in] layout
 *    Input. The layout of the input tensor. For detailed information, see ::cnnlTensorLayout_t.
 *  @param[in] dtype
 *    Input. The data type of the input tensor. For detailed information, see ::cnnlDataType_t.
 *  @param[in] dimNb
 *    Input. The number of dimensions in the input tensor of the initialized operation.
 *  @param[in] dimSize
 *    Input. An array that contains the size of the tensor for each dimension.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - dimSize[0] represents the highest dimension, dimSize[DIM_MAX - 1] represents
 *    the lowest dimension, and DIM_MAX represents the number of dimensions in the input tensor.
 *  - This function cannot be called continuously. You need to call ::cnnlResetTensorDescriptor
 *    before calling another ::cnnlSetTensorDescriptor to avoid memory leaks.
 *  - This function does not support negative dimension for tensor dimensions.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetTensorDescriptor_v2(cnnlTensorDescriptor_t desc,
                                                     cnnlTensorLayout_t layout,
                                                     cnnlDataType_t dtype,
                                                     int dimNb,
                                                     const int64_t dimSize[]);

// Group:SparseDenseMatMul
/*!
 * @brief Sets the attribute of sparse tensor descriptor. Each attribute has its data
 *   and corresponding tensor descritptor.
 *
 * @param[in] sp_desc
 *   Input. The pointer to the struct that holds information about the sparse tensor
 *   descriptor. For detailed information, see ::cnnlSparseTensorDescriptor_t.
 * @param[in] attr
 *   Input. Attribute of sparse tensor descriptor to be set. For detailed information, see
 *   ::cnnlSparseTensorDescAttribute_t. Each attribute has its data and corresponding
 *   tensor descritptor.
 * @param[in] data
 *   Input. The device pointer to the data of sparse tensor, which is described by \p data_desc.
 * @param[in] data_desc
 *   Input. The descriptor of data of sparse tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - Different sparse tensor format have different attributes. For detailed information,
 *   see ::cnnlSparseTensorDescAttribute_t.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetSparseTensorDescAttr(cnnlSparseTensorDescriptor_t sp_desc,
                            const cnnlSparseTensorDescAttribute_t attr,
                            void *data,
                            cnnlTensorDescriptor_t data_desc);

// Group:Common Interface
// Subgroup:Tensor
/*!
 *  @brief Initializes the group of tensor descriptors stored by \p group_desc that was previously
 *  created with the ::cnnlCreateTensorDescriptor function or ::cnnlCreateGroupTensorDescriptors
 *  function, and sets the information about the dimensions, data type and layout of all the
 *  input tensors.
 *
 *  If ::cnnlSetTensorDescriptor or ::cnnlSetGroupTensorDescriptors is called, you do not need
 *  to specify the strides of all dimensions. The strides are inferred by parameters passed to
 *  this function. Also, the data will be treated as contiguous in memory with no padding between
 *  dimensions. To specify the strides of all dimensions, you can call ::cnnlSetTensorDescriptorEx.
 *  But the data might not be treated as contiguous in memory.
 *
 *  @param[in,out] group_desc
 *    Input/output. An array of pointers to the struct that hold information about the tensor descriptor.
 *  @param[in] group_layout
 *    Input. An array that stores the layouts of all input tensors. For detailed information, see
 *    ::cnnlTensorLayout_t.
 *  @param[in] group_dtype
 *    Input. An array that stores the data types of all input tensors. For detailed information, see
 *    ::cnnlDataType_t.
 *  @param[in] group_dimNb
 *    Input. An array that stores the dimensions of all input tensors.
 *  @param[in] group_dimSize
 *    Input. An array that stores the size of each dimension of all tensors.
 *  @param[in] desc_num
 *    Input. The length of the input array \p group_desc.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - The group_dimSize includes dimensions of all tensors. You need to store the dimension of each
 *  tensor one by one in order. For example, If we have three tensors, the first tensor dimension is
 *  [3,4,5,6], the second tensor dimension is [9,7,8], and the third tensor dimension is [4,7], the
 *  group_dimSize should be [3,4,5,6,9,7,8,4,7].
 *  - For better performance, there is no overflow check in this function. Make sure that the
 *  size of each tensor is in range of [0, \f$2^{31}\f$]. Otherwise, you will get wrong result.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetGroupTensorDescriptors(cnnlTensorDescriptor_t *group_desc[],
                                                        const cnnlTensorLayout_t group_layout[],
                                                        const cnnlDataType_t group_dtype[],
                                                        const int group_dimNb[],
                                                        const int group_dimSize[],
                                                        const int desc_num);

// Group:Common Interface
// Subgroup:Tensor
/*!
 *  @brief Initializes the group of tensor descriptors stored by \p group_desc that was previously
 *  created with the ::cnnlCreateTensorDescriptor function or ::cnnlCreateGroupTensorDescriptors
 *  function, and sets the information about the dimensions, data type and layout of all the
 *  input tensors.
 *
 *  If ::cnnlSetTensorDescriptor or ::cnnlSetGroupTensorDescriptors_v2 is called, you do not need
 *  to specify the strides of all dimensions. The strides are inferred by parameters passed to
 *  this function. Also, the data will be treated as contiguous in memory with no padding between
 *  dimensions. To specify the strides of all dimensions, you can call ::cnnlSetTensorDescriptorEx_v2.
 *  But the data might not be treated as contiguous in memory.
 *
 *  @param[in,out] group_desc
 *    Input/output. An array of pointers to the struct that hold information about the tensor descriptor.
 *  @param[in] group_layout
 *    Input. An array that stores the layouts of all input tensors. For detailed information, see
 *    ::cnnlTensorLayout_t.
 *  @param[in] group_dtype
 *    Input. An array that stores the data types of all input tensors. For detailed information, see
 *    ::cnnlDataType_t.
 *  @param[in] group_dimNb
 *    Input. An array that stores the dimensions of all input tensors.
 *  @param[in] group_dimSize
 *    Input. An array that stores the size of each dimension of all tensors.
 *  @param[in] desc_num
 *    Input. The length of the input array \p group_desc.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - The group_dimSize includes dimensions of all tensors. You need to store the dimension of each
 *  tensor one by one in order. For example, If we have three tensors, the first tensor dimension is
 *  [3,4,5,6], the second tensor dimension is [9,7,8], and the third tensor dimension is [4,7], the
 *  group_dimSize should be [3,4,5,6,9,7,8,4,7].
 *  - For better performance, there is no overflow check in this function. Make sure that the
 *  size of each tensor is in range of [0, \f$2^{31}\f$]. Otherwise, you will get wrong result.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetGroupTensorDescriptors_v2(cnnlTensorDescriptor_t *group_desc[],
                                                           const cnnlTensorLayout_t group_layout[],
                                                           const cnnlDataType_t group_dtype[],
                                                           const int group_dimNb[],
                                                           const int64_t group_dimSize[],
                                                           const int desc_num);

// Group:Common Interface
// Subgroup:Tensor
/*!
 *  @brief Resets the tensor descriptor pointed by \p desc that was previously created
 *  with the ::cnnlCreateTensorDescriptor function.
 *
 *  If ::cnnlResetTensorDescriptor is called, all the information about the tensor will be reset to
 *  initial value, which means layout is CNNL_LAYOUT_ARRAY, dtype is CNNL_DTYPE_FLOAT, dimsNb is 0,
 *  and dimSize points to an \p CNNL_DIM_MAX-dimension array.
 *
 *  @param[in] desc
 *    Input. The descriptor of the tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - This function is used to avoid memory leaks when more than one ::cnnlSetTensorDescriptor
 *    function is called. You should call this function before calling another
 *    ::cnnlSetTensorDescriptor
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlResetTensorDescriptor(cnnlTensorDescriptor_t desc);

// Group:Common Interface
// Subgroup:Tensor
/*!
 *  @brief Initializes the tensor descriptor pointed by \p desc that was previously created
 *  with the ::cnnlCreateTensorDescriptor function, and sets the information about
 *  the dimensions, strides, data type and layout of the input tensor.
 *
 *  Compare with ::cnnlSetTensorDescriptor, you can specify the strides of all dimensions with
 *  this function. If ::cnnlSetTensorDescriptor is called, you do not need to specify the
 *  strides of all dimensions and the strides are inferred by parameters passed to this function.
 *
 *  This function does not support all the operations in this version. You can check
 *  if an operation supports this function in the "note" section of the operation description.
 *
 *  @param[in,out] desc
 *    Input/output. The descriptor of the input tensor. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[in] layout
 *    Input. The layout of the input tensor. For detailed information, see ::cnnlTensorLayout_t.
 *  @param[in] dtype
 *    Input. The data type of the input tensor. For detailed information, see ::cnnlDataType_t.
 *  @param[in] dimNb
 *    Input. The number of dimensions in the input tensor of the initialized operation.
 *  @param[in] dimSize
 *    Input. An array that contains the size of the tensor for each dimension.
 *  @param[in] dimStride
 *    Input. An array that contains the stride of the tensor for each dimension.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - dimSize[0] represents the highest dimension, and dimSize[DIM_MAX - 1] represents
 *    the lowest dimension.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlSetTensorDescriptorEx(cnnlTensorDescriptor_t desc,
                                                    cnnlTensorLayout_t layout,
                                                    cnnlDataType_t dtype,
                                                    int dimNb,
                                                    const int dimSize[],
                                                    const int dimStride[]);

// Group:Common Interface
// Subgroup:Tensor
/*!
 *  @brief Initializes the tensor descriptor pointed by \p desc that was previously created
 *  with the ::cnnlCreateTensorDescriptor function, and sets the information about
 *  the dimensions, strides, data type and layout of the input tensor.
 *
 *  Compare with ::cnnlSetTensorDescriptor_v2, you can specify the strides of all dimensions with
 *  this function. If ::cnnlSetTensorDescriptor_v2 is called, you do not need to specify the
 *  strides of all dimensions and the strides are inferred by parameters passed to this function.
 *
 *  This function does not support all the operations in this version. You can check
 *  if an operation supports this function in the "note" section of the operation description.
 *
 *  @param[in,out] desc
 *    Input/output. The descriptor of the input tensor. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[in] layout
 *    Input. The layout of the input tensor. For detailed information, see ::cnnlTensorLayout_t.
 *  @param[in] dtype
 *    Input. The data type of the input tensor. For detailed information, see ::cnnlDataType_t.
 *  @param[in] dimNb
 *    Input. The number of dimensions in the input tensor of the initialized operation.
 *  @param[in] dimSize
 *    Input. An array that contains the size of the tensor for each dimension.
 *  @param[in] dimStride
 *    Input. An array that contains the stride of the tensor for each dimension.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - dimSize[0] represents the highest dimension, and dimSize[DIM_MAX - 1] represents
 *    the lowest dimension.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlSetTensorDescriptorEx_v2(cnnlTensorDescriptor_t desc,
                                                       cnnlTensorLayout_t layout,
                                                       cnnlDataType_t dtype,
                                                       int dimNb,
                                                       const int64_t dimSize[],
                                                       const int64_t dimStride[]);


// Group:Common Interface
// Subgroup:Tensor
/*!
 * @brief Sets the \p dimNb and \p dimSize factors to the input tensor descriptor.
 *
 * If ::cnnlSetTensorDescriptorDim is called, you do not need to specify the strides of all
 * dimensions. The strides are inferred by parameters passed to this function. Also, the data
 * will be treated as contiguous in memory with no padding between dimensions. To specify the
 * strides of all dimensions, you can call ::cnnlSetTensorDescriptorEx. But the data might not
 * be treated as contiguous in memory.
 *
 * @param[in] desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] dimNb
 *   Input. The number of dimensions in the input tensor of the initialized operation.
 * @param[in] dimSize
 *   Input. An array that contains the size of the tensor for each dimension.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - dimSize[0] represents the highest dimension, dimSize[DIM_MAX - 1] represents
 *   the lowest dimension, and DIM_MAX represents the number of dimensions in the input tensor.
 * - This function does not support negative dimension for tensor dimensions.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t cnnlSetTensorDescriptorDim(cnnlTensorDescriptor_t desc,
                                        int dimNb,
                                        const int *dimSize);

// Group:Common Interface
// Subgroup:Tensor
/*!
 * @brief Sets the \p dimNb and \p dimSize factors to the input tensor descriptor.
 *
 * If ::cnnlSetTensorDescriptorDim_v2 is called, you do not need to specify the strides of all
 * dimensions. The strides are inferred by parameters passed to this function. Also, the data
 * will be treated as contiguous in memory with no padding between dimensions. To specify the
 * strides of all dimensions, you can call ::cnnlSetTensorDescriptorEx_v2. But the data might not
 * be treated as contiguous in memory.
 *
 * @param[in] desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] dimNb
 *   Input. The number of dimensions in the input tensor of the initialized operation.
 * @param[in] dimSize
 *   Input. An array that contains the size of the tensor for each dimension.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - dimSize[0] represents the highest dimension, dimSize[DIM_MAX - 1] represents
 *   the lowest dimension, and DIM_MAX represents the number of dimensions in the input tensor.
 * - This function does not support negative dimension for tensor dimensions.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t cnnlSetTensorDescriptorDim_v2(cnnlTensorDescriptor_t desc,
                                           int dimNb,
                                           const int64_t *dimSize);

// Group:Common Interface
// Subgroup:Tensor
/*!
 *  @deprecated
 *  This function is deprecated and will be removed in future release.
 *  Use ::cnnlSetQuantizeExDescriptorScalarQuant instead.
 *
 *  @brief Sets the onchip data type to the descriptor of a tensor \p desc.
 *  The onchip data type \p onchip_dtype can be different from the offchip data type of the tensor.
 *  This function is optional. If the onchip data type is not set with this function, the
 *  ::CNNL_STATUS_BAD_PARAM data type is used by default.
 *
 *  @param[in] desc
 *    Input. The descriptor of input tensor. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[in] onchip_dtype
 *    Input. The onchip data type of the tensor used in the operations that support fixed-point computing.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - The onchip data type is only used on the operations that support fixed-point computing. It
 *    has no effect on other operations. If you call this function to get onchip data type for an
 *    operation that does not support fixed-point computing, ::CNNL_STATUS_BAD_PARAM is returned. To check
 *    if an operation supports fixed-point computing, see the detailed description of the operation.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
CNNL_DEPRECATED_FOR(cnnlSetQuantizeExDescriptorScalarQuant)
cnnlStatus_t CNNL_WIN_API cnnlSetTensorDescriptorOnchipDataType(cnnlTensorDescriptor_t desc,
                                                                cnnlDataType_t onchip_dtype);
// Group:Common Interface
// Subgroup:Tensor
/*!
 * @deprecated
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlSetQuantizeExDescriptorScalarQuant instead.
 *
 * @brief Sets the \p position factor to the descriptor \p desc of fixed-point data in
 * fixed-point quantization. It is used in ::CNNL_QUANTIZE_POSITION mode. For more information
 * about quantization, see "Cambricon CNNL User Guide".
 *
 * @param[in] desc
 *   Input. The descriptor of the tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] position
 *   Input. A scalar of fixed position factor that is used for quantization.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - The \p position should be limited in [-128, 127], otherwise the result is undefined.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
CNNL_DEPRECATED_FOR(cnnlSetQuantizeExDescriptorScalarQuant)
cnnlStatus_t CNNL_WIN_API cnnlSetTensorDescriptorPosition(cnnlTensorDescriptor_t desc,
                                                          int position);
// Group:Common Interface
// Subgroup:Tensor
/*!
 * @deprecated
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlSetQuantizeExDescriptorScalarQuant instead.
 *
 * @brief Sets the \p position and \p scale factors to the descriptor of fixed-point data in
 * fixed-point quantization. It is used in ::CNNL_QUANTIZE_POSITION_SCALE mode. For more information
 * about quantization, see "Cambricon CNNL User Guide".
 *
 * @param[in] desc
 *   Input. The descriptor of the tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] position
 *   Input. A scalar of fixed position factor that is used for quantization.
 * @param[in] scale
 *   Input. A scalar of scale factor that is used for quantization.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - The \p position should be limited in [-128, 127], otherwise the result is undefined.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
CNNL_DEPRECATED_FOR(cnnlSetQuantizeExDescriptorScalarQuant)
cnnlStatus_t CNNL_WIN_API cnnlSetTensorDescriptorPositionAndScale(cnnlTensorDescriptor_t desc,
                                                                  int position,
                                                                  float scale);
// Group:Common Interface
// Subgroup:Tensor
/*!
 * @deprecated
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlSetQuantizeExDescriptorScalarQuant instead.
 *
 * @brief Sets the \p position, \p scale and \p offset factors to the descriptor of fixed-point
 * data in fixed-point quantization. It is used in ::CNNL_QUANTIZE_POSITION_SCALE_OFFSET mode.
 * For more information about quantization, see "Cambricon CNNL User Guide".
 *
 * @param[in] desc
 *   Input. The descriptor of the tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] position
 *   Input. A scalar of fixed position factor that is used for quantization.
 * @param[in] scale
 *   Input. A scalar of scale factor that is used for quantization.
 * @param[in] offset
 *   Input. A scalar of offset factor that is used for quantization.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - The \p position should be limited in [-128, 127], otherwise the result is undefined.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
CNNL_DEPRECATED_FOR(cnnlSetQuantizeExDescriptorScalarQuant)
cnnlStatus_t CNNL_WIN_API
cnnlSetTensorDescriptorPositionScaleAndOffset(cnnlTensorDescriptor_t desc,
                                              int position,
                                              float scale,
                                              int offset);

// Group:Common Interface
// Subgroup:Tensor
/*!
 * @brief Sets the pointer mode \p pointer_mode factor to the input tensor descriptor \p desc.
 *
 * @param[in,out] desc
 *   Input. The descriptor of the tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] pointer_mode
 *   Input. The pointer mode of the input tensor. For detailed information, seee ::cnnlPointerMode_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - Currently. the \p pointer_mode setting to CNNL_POINTER_MODE_HOST is only supported when the number
 *   of dimensions of \p desc is 0.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetTensorDescriptorPointerMode(cnnlTensorDescriptor_t desc,
                                   cnnlPointerMode_t pointer_mode);

// Group:Common Interface
// Subgroup:Tensor
/*!
 *  @brief Retrieves a tensor descriptor \p desc that was previously created with the
 *  ::cnnlCreateTensorDescriptor function, and sets the information about the dimensions,
 *  data type and layout of input tensor.
 *
 *  @param[in] desc
 *    Input. The descriptor of the input tensor. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[out] layout
 *    Output. Pointer to the host memory that holds information about the layout of the input tensor.
 *  For detailed information, see ::cnnlTensorLayout_t.
 *  @param[out] dtype
 *    Output. Pointer to the host memory that holds information about the data type of the input tensor.
 *  For detailed information, see ::cnnlDataType_t.
 *  @param[out] dimNb
 *    Output. Pointer to the host memory that holds information about the dimension of input tensor.
 *  @param[out] dimSize
 *    Output. An array that contains the size of the tensor for each dimension.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - dimSize[0] represents the highest dimension, and dimSize[DIM_MAX - 1] represents the lowest
 *    dimension.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetTensorDescriptor(const cnnlTensorDescriptor_t desc,
                                                  cnnlTensorLayout_t *layout,
                                                  cnnlDataType_t *dtype,
                                                  int *dimNb,
                                                  int dimSize[]);

// Group:Common Interface
// Subgroup:Tensor
/*!
 *  @brief Retrieves a tensor descriptor \p desc that was previously created with the
 *  ::cnnlCreateTensorDescriptor function, and sets the information about the dimensions,
 *  data type and layout of input tensor.
 *
 *  @param[in] desc
 *    Input. The descriptor of the input tensor. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[out] layout
 *    Output. Pointer to the host memory that holds information about the layout of the input tensor.
 *  For detailed information, see ::cnnlTensorLayout_t.
 *  @param[out] dtype
 *    Output. Pointer to the host memory that holds information about the data type of the input tensor.
 *  For detailed information, see ::cnnlDataType_t.
 *  @param[out] dimNb
 *    Output. Pointer to the host memory that holds information about the dimension of input tensor.
 *  @param[out] dimSize
 *    Output. An array that contains the size of the tensor for each dimension.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - dimSize[0] represents the highest dimension, and dimSize[DIM_MAX - 1] represents the lowest
 *    dimension.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetTensorDescriptor_v2(const cnnlTensorDescriptor_t desc,
                                                     cnnlTensorLayout_t *layout,
                                                     cnnlDataType_t *dtype,
                                                     int *dimNb,
                                                     int64_t dimSize[]);

// Group:Common Interface
// Subgroup:Tensor
/*!
 *  @brief Retrieves a tensor descriptor \p desc that was previously created with the
 *  ::cnnlCreateTensorDescriptor and sets the information about the dimensions, data type,
 *  stride and layout of input tensor with ::cnnlSetTensorDescriptorEx.
 *
 *  @param[in] desc
 *    Input. The descriptor of the input tensor. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[out] layout
 *    Output. Pointer to the host memory that holds information about the layout of the input tensor.
 *  For detailed information, see ::cnnlTensorLayout_t.
 *  @param[out] dtype
 *    Output. Pointer to the host memory that holds information about the data type of the input tensor.
 *  For detailed information, see ::cnnlDataType_t.
 *  @param[out] dimNb
 *    Output. Pointer to the host memory that holds information about the dimension of input tensor.
 *  @param[out] dimSize
 *    Output. An array that contains the size of the tensor for each dimension.
 *  @param[out] dimStride
 *    Output. An array that contains the stride of the tensor for each dimension.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - dimSize[0] represents the highest dimension, and dimSize[DIM_MAX - 1] represents the lowest
 *    dimension.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetTensorDescriptorEx(const cnnlTensorDescriptor_t desc,
                                                    cnnlTensorLayout_t *layout,
                                                    cnnlDataType_t *dtype,
                                                    int *dimNb,
                                                    int dimSize[],
                                                    int dimStride[]);

// Group:Common Interface
// Subgroup:Tensor
/*!
 *  @brief Retrieves a tensor descriptor \p desc that was previously created with the
 *  ::cnnlCreateTensorDescriptor and sets the information about the dimensions, data type,
 *  stride and layout of input tensor with ::cnnlSetTensorDescriptorEx_v2.
 *
 *  @param[in] desc
 *    Input. The descriptor of the input tensor. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[out] layout
 *    Output. Pointer to the host memory that holds information about the layout of the input tensor.
 *  For detailed information, see ::cnnlTensorLayout_t.
 *  @param[out] dtype
 *    Output. Pointer to the host memory that holds information about the data type of the input tensor.
 *  For detailed information, see ::cnnlDataType_t.
 *  @param[out] dimNb
 *    Output. Pointer to the host memory that holds information about the dimension of input tensor.
 *  @param[out] dimSize
 *    Output. An array that contains the size of the tensor for each dimension.
 *  @param[out] dimStride
 *    Output. An array that contains the stride of the tensor for each dimension.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - dimSize[0] represents the highest dimension, and dimSize[DIM_MAX - 1] represents the lowest
 *    dimension.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetTensorDescriptorEx_v2(const cnnlTensorDescriptor_t desc,
                                                       cnnlTensorLayout_t *layout,
                                                       cnnlDataType_t *dtype,
                                                       int *dimNb,
                                                       int64_t dimSize[],
                                                       int64_t dimStride[]);

// Group:SparseDenseMatMul
/*!
 * @brief Gets the desctriptor information of sparse tensor descriptor.
 *
 * @param[in] sp_desc
 *   Input. The pointer to the struct that holds information about the sparse tensor
 *   descriptor. For detailed information, see ::cnnlSparseTensorDescriptor_t.
 * @param[out] format
 *   Output. The pointer to the format of sparse tensor. For detailed information, see
 *   ::cnnlSparseFormat_t.
 * @param[out] rows
 *   Output. The pointer to the number of rows of sparse tensor.
 * @param[out] cols
 *   Output. The pointer to the number of columns of sparse tensor.
 * @param[out] nnz
 *   Output. The pointer to the number of non-zero elements of sparse tensor.
 * @param[out] block_size
 *   Output. The pointer to the block size of sparse tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetSparseTensorDescriptor(const cnnlSparseTensorDescriptor_t sp_desc,
                              cnnlSparseFormat_t *format,
                              int64_t *rows,
                              int64_t *cols,
                              int64_t *nnz,
                              int64_t *block_size[]);

// Group:SparseDenseMatMul
/*!
 * @brief Gets the attribute of sparse tensor descriptor. Each attribute has its data and corresponding
 *   tensor descritptor.
 *
 * @param[in] sp_desc
 *   Input. The pointer to the struct that holds information about the sparse tensor
 *   descriptor. For detailed information, see ::cnnlSparseTensorDescriptor_t.
 * @param[in] attr
 *   Input. Attribute of sparse tensor descriptor to be get. For detailed information, see
 *   ::cnnlSparseTensorDescAttribute_t. Each attribute has its data and corresponding
 *   tensor descritptor.
 * @param[out] data
 *   Output. The pointer of the device pointer to the data, which is described
 *   by \p data_desc.
 * @param[out] data_desc
 *   Output. The pointer to the descriptor of data. For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - Different sparse tensor format have different attributes. For detailed information,
 *   see ::cnnlSparseTensorDescAttribute_t.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetSparseTensorDescAttr(const cnnlSparseTensorDescriptor_t sp_desc,
                            const cnnlSparseTensorDescAttribute_t attr,
                            void **data,
                            cnnlTensorDescriptor_t *data_desc);

// Group:Common Interface
// Subgroup:Tensor
/*!
 *  @brief Retrieves the number of elements according to the input descriptor \p desc. You
 *  need to call the ::cnnlSetTensorDescriptor function first to create a tensor descriptor
 *  before calling this function.
 *
 *  @param[in] desc
 *    Input. The descriptor of input tensor. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @return
 *  - ::CNNL_STATUS_SUCCESS
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
     @verbatim
      cnnlTensorDescriptor_t input_desc;
      cnnlCreateTensorDescriptor(&input_desc);
      cnnlSetTensorDescriptor(input_desc, CNNL_LAYOUT_ARRAY,CNNL_DTYPE_FLOAT, 2,{2, 3});
      size_t nums=cnnlGetTensorElementNum(input_desc);  // nums = 6

      input one array by 2 * 3
      input: [[1,2,3],[4,5,6]]
      output: 6
     @endverbatim
 */
size_t CNNL_WIN_API cnnlGetTensorElementNum(const cnnlTensorDescriptor_t desc);

// Group:Common Interface
// Subgroup:Tensor
/*!
 *  @deprecated
 *  This function is deprecated and will be removed in future release.
 *  Use ::cnnlGetQuantizeExDescriptorScalarQuant instead.
 *
 *  @brief Retrieves the onchip data type of a tensor descriptor \p desc set by
 *  ::cnnlSetTensorDescriptorOnchipDataType.
 *  If the onchip data type is not set with the ::cnnlSetTensorDescriptorOnchipDataType function,
 *  ::CNNL_STATUS_BAD_PARAM is returned.
 *
 *  @param[in] desc
 *    Input. The descriptor of input tensor. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[in] onchip_dtype
 *    Input. Pointer to the MLU memory that holds information about the onchip data type of the tensor.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - The onchip data type is only used on the operations that support fixed-point computing. It
 *    has no effect on other operations. If you call this function to get onchip data type for an
 *    operation that does support fixed-point computing, ::CNNL_STATUS_BAD_PARAM is returned. To check
 *    if an operation supports fixed-point computing, see the detailed description of the operation.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 *
 */
CNNL_DEPRECATED_FOR(cnnlGetQuantizeExDescriptorScalarQuant)
cnnlStatus_t CNNL_WIN_API cnnlGetTensorDescriptorOnchipDataType(const cnnlTensorDescriptor_t desc,
                                                                cnnlDataType_t *onchip_dtype);
// Group:Common Interface
// Subgroup:Tensor
/*!
 * @deprecated
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlGetQuantizeExDescriptorScalarQuant instead.
 *
 * @brief Gets the \p position factor to the descriptor \p desc of fixed-point data in
 * fixed-point quantization. For more information about quantization, see "Cambricon CNNL User Guide".
 *
 * @param[in] desc
 *   Input. The descriptor of the tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] position
 *   Output. A host pointer of fixed position factor that is used for quantization.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
CNNL_DEPRECATED_FOR(cnnlGetQuantizeExDescriptorScalarQuant)
cnnlStatus_t CNNL_WIN_API cnnlGetTensorDescriptorPosition(const cnnlTensorDescriptor_t desc,
                                                          int *position);
// Group:Common Interface
// Subgroup:Tensor
/*!
 *  @deprecated
 *  This function is deprecated and will be removed in future release.
 *  Use ::cnnlGetQuantizeExDescriptorScalarQuant instead.
 *
 *  @brief Gets the position and scale factors of a tensor descriptor \p desc used in
 *  fixed-point quantization. For more information about quantization, see "Cambricon CNNL User Guide".
 *
 *  @param[in] desc
 *    Input. The descriptor of the input tensor. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[out] position
 *    Output. Pointer to the MLU memory that holds information about fixed position
 *    used for quantization.
 *  @param[out] scale
 *    Output. Pointer to the MLU memory that holds information about scale factor
 *    used for quantization.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 *
 */
CNNL_DEPRECATED_FOR(cnnlGetQuantizeExDescriptorScalarQuant)
cnnlStatus_t CNNL_WIN_API
cnnlGetTensorDescriptorPositionAndScale(const cnnlTensorDescriptor_t desc,
                                        int *position,
                                        float *scale);
// Group:Common Interface
// Subgroup:Tensor
/*!
 * @brief Retrieves the pointer mode of the input tensor descriptor \p desc set by ::cnnlSetTensorDescriptorPointerMode.
 *
 * @param[in] desc
 *   Input. The descriptor of the tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] pointer_mode
 *   Input. Pointer to the host memory that holds information about the pointer mode of the input tensor.
 *   For detailed information, seee ::cnnlPointerMode_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetTensorDescriptorPointerMode(cnnlTensorDescriptor_t desc,
                                   cnnlPointerMode_t *pointer_mode);

// Group:Common Interface
// Subgroup:Tensor
/*!
 * @deprecated
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlGetQuantizeExDescriptorScalarQuant instead.
 *
 * @brief Gets the \p position, \p scale and \p offset factors to the descriptor \p desc of
 * fixed-point data in fixed-point quantization. For more information about quantization,
 * see "Cambricon CNNL User Guide".
 *
 * @param[in] desc
 *   Input. The descriptor of the tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] position
 *   Output. A host pointer of fixed position factor that is used for quantization.
 * @param[out] scale
 *   Output. A host pointer of scale factor that is used for quantization.
 * @param[in] offset
 *   Output. A host pointer of offset factor that is used for quantization.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
CNNL_DEPRECATED_FOR(cnnlGetQuantizeExDescriptorScalarQuant)
cnnlStatus_t CNNL_WIN_API
cnnlGetTensorDescriptorPositionScaleAndOffset(const cnnlTensorDescriptor_t desc,
                                              int *position,
                                              float *scale,
                                              int *offset);
// Group:Common Interface
// Subgroup:Tensor
/*!
 *  @brief Destroies a tensor descriptor that was created by
 *         ::cnnlCreateTensorDescriptor.
 *
 *  @param[in] desc
 *    Input. A tensor descriptor created by ::cnnlCreateTensorDescriptor.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyTensorDescriptor(cnnlTensorDescriptor_t desc);

// Group:SparseDenseMatMul
/*!
 * @brief Destroys the sparse tensor descriptor.
 *
 * @param[in] sp_desc
 *   Input. The pointer to the struct that holds information about the sparse tensor
 *   descriptor. For detailed information, see ::cnnlSparseTensorDescriptor_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlDestroySparseTensorDescriptor(cnnlSparseTensorDescriptor_t sp_desc);

// Group:Common Interface
// Subgroup:Tensor
/*!
 *  @brief Destroys a group of tensor descriptors that was created by
 *         ::cnnlCreateTensorDescriptor or ::cnnlCreateGroupTensorDescriptors.
 *
 *  @param[in] group_desc
 *    Input. An array of pointers to the struct that hold information about the tensor descriptor.
 *  @param[in] desc_num
 *    Input. The length of the input array \p group_desc.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyGroupTensorDescriptors(cnnlTensorDescriptor_t *group_desc[],
                                                            const int desc_num);

// Group:Common Interface
// Subgroup:SeqData
/*!
 *  @brief Creates a sequence data instance \p seq_data_desc that holds the dimensions, data type,
 *  sequence lengths, padding fill and layout of sequence data on the host memory.
 *
 *  Use ::cnnlSetSeqDataDescriptor to configure the descriptor and ::cnnlDestroySeqDataDescriptor
 *  function to destroy the sequence data descriptor.
 *
 *  @param[out] seq_data_desc
 *  Output. Pointer to the host memory that holds information about
 *  the struct of the sequence data descriptor.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateSeqDataDescriptor(cnnlSeqDataDescriptor_t *seq_data_desc);

// Group:Common Interface
// Subgroup:SeqData
/*!
 * @brief Sets the sequence data descriptor \p seq_data_desc that holds the dimensions,
 * data type, sequence lengths, padding fill and layout of the sequence data.
 *
 * The number of dimensions in the \p dimSize[] is defined by \p dimNb. For example,
 * if the layout of the sequence data is set to ::CNNL_SEQDATA_NC, the \p dimNb is 2, with \p dimSize={batch, embedding}.
 *
 * The ::cnnlSeqDataDescriptor_t container is a collection of fixed-length sequential
 * vectors, similar to the words constructing sentences. The T dimension described in
 * ::cnnlSeqDataLayout_t is the time dimension. Different sequences are bundled together to a
 * batch. The beam dimension described in the ::cnnlSeqDataLayout_t is
 * different candidates presenting a similar meaning in a typical translation task. The original
 * sentence can be translated to several versions before picking the optimal one, and the number
 * of candidates is beam.
 *
 * Note that different sentences have different sequence lengths, even inside a beam.
 * \p seqLengthArray is to record the real sequence lengths before padding to the maximum sequence
 * length. The value of \p seqLengthArray should follow a batch-beam order, in despite of
 * sequence data layout. Take a sequence of batch=3, beam=2 for example, the \p seqLengthArray
 * should be as follows:
   @verbatim
   {batch_idx = 0, beam_idx = 0}
   {batch_idx = 0, beam_idx = 1}
   {batch_idx = 1, beam_idx = 0}
   {batch_idx = 1, beam_idx = 1}
   {batch_idx = 2, beam_idx = 0}
   {batch_idx = 2, beam_idx = 1}
   @endverbatim
 * If the real sequence lengths are not requested, pass NULL to \p seqLengthArray in this function.
 *
 * The \p seqLengthArraySize should be batch * beam, which is 6 in the example above.
 *
 * The \p PaddingFill describes whether the sequence data needs to be padded using a
 * specified value. In the multi-head attention operation, the padding part should be zero before
 * entering the attention part to ensure the result validity. If the sequence data is padding
 * zero in advance, pass NULL to \p PaddingFill in this function. Otherwise, pass a pointer to padding
 * value (e.g. float a = 0, &a) to \p PaddingFill to indicate this function that extra padding are
 * needed.
 *
 * @param[in,out] seq_data_desc
 *   Input/output. The descriptor of the sequence data. For detailed information,
 *   see ::cnnlSeqDataDescriptor_t.
 * @param[in] layout
 *   Input. The layout of the sequence data. See ::cnnlSeqDataLayout_t for the description of the
 *   enumeration type.
 * @param[in] dtype
 *   Input. The data type of the sequence data. See ::cnnlDataType_t for the description of the
 *   enumeration type.
 * @param[in] dimNb
 *   Input. The number of dimensions of the sequence data.
 * @param[in] dimSize
 *   Input. An array that contains the size of the sequence data for each dimension.
 * @param[in] seqLengthArraySize
 *   Input. Number of elements in sequence length array, \p seqLengthArray[]. It should be
 *   batch * beam. The batch and beam are described in ::cnnlSeqDataLayout_t.
 * @param[in] seqLengthArray
 *   Input. An integer array recording the length of all sequences. Note that the array should be
 *   set in the batch-beam order, in despite of sequence data layout. Set this parameter to NULL
 *   when sequence length array is not requested.
 * @param[in] paddingFill
 *   Input. A host pointer to the data type \p dtype to fill up the padding vectors within
 *   the valid length of each sequence. Use NULL when extra padding is not requested.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, ::cnnlCreateSeqDataDescriptor should be called.
 *
 * @note
 * - dimSize[0] represents the highest dimension, and dimSize[dimNb - 1] represents
 *   the lowest dimension.
 *   The \p seqLengthArray and \p PaddingFill of the descriptors of
 *   queries, keys, values, and outputs can only be NULL in the
 *   ::cnnlMultiHeadAttnForward function.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetSeqDataDescriptor(cnnlSeqDataDescriptor_t seq_data_desc,
                                                   cnnlSeqDataLayout_t layout,
                                                   cnnlDataType_t dtype,
                                                   int dimNb,
                                                   const int dimSize[],
                                                   int seqLengthArraySize,
                                                   const int seqLengthArray[],
                                                   void *paddingFill);

// Group:Common Interface
// Subgroup:SeqData
/*!
 * @brief Sets the sequence data descriptor \p seq_data_desc that holds the dimensions,
 * data type, sequence lengths, padding fill and layout of the sequence data.
 *
 * The number of dimensions in the \p dimSize[] is defined by \p dimNb. For example,
 * if the layout of the sequence data is set to ::CNNL_SEQDATA_NC, the \p dimNb is 2, with \p dimSize={batch, embedding}.
 *
 * The ::cnnlSeqDataDescriptor_t container is a collection of fixed-length sequential
 * vectors, similar to the words constructing sentences. The T dimension described in the
 * ::cnnlSeqDataLayout_t is the time dimension. Different sequences are bundled together to a
 * batch. The beam dimension described in ::cnnlSeqDataLayout_t is
 * different candidates presenting a similar meaning in a typical translation task. The original
 * sentence can be translated to several versions before picking the optimal one, and the number
 * of candidates is beam.
 *
 * Note that different sentences have different sequence lengths, even inside a beam.
 * \p seqLengthArray is to record the real sequence lengths before padding to the maximum sequence
 * length. The value of \p seqLengthArray should follow a batch-beam order, in despite of
 * sequence data layout. Take a sequence of batch=3, beam=2 for example, the \p seqLengthArray
 * should be as follows:
   @verbatim
   {batch_idx = 0, beam_idx = 0}
   {batch_idx = 0, beam_idx = 1}
   {batch_idx = 1, beam_idx = 0}
   {batch_idx = 1, beam_idx = 1}
   {batch_idx = 2, beam_idx = 0}
   {batch_idx = 2, beam_idx = 1}
   @endverbatim
 * If the real sequence lengths are not requested, pass NULL to \p seqLengthArray in this function.
 *
 * The \p seqLengthArraySize should be batch * beam, which is 6 in the example above.
 *
 * The \p PaddingFill describes whether the sequence data needs to be padded using a
 * specified value. In the multi-head attention operation, the padding part should be zero before
 * entering the attention part to ensure the result validity. If the sequence data is padding
 * zero in advance, pass NULL to \p PaddingFill in this function. Otherwise, pass a pointer to padding
 * value (e.g. float a = 0, &a) to \p PaddingFill to indicate this function that extra padding are
 * needed.
 *
 * @param[in,out] seq_data_desc
 *   Input/output. The descriptor of the sequence data. For detailed information,
 *   see ::cnnlSeqDataDescriptor_t.
 * @param[in] layout
 *   Input. The layout of the sequence data. See ::cnnlSeqDataLayout_t for the description of the
 *   enumeration type.
 * @param[in] dtype
 *   Input. The data type of the sequence data. See ::cnnlDataType_t for the description of the
 *   enumeration type.
 * @param[in] dimNb
 *   Input. The number of dimensions of the sequence data.
 * @param[in] dimSize
 *   Input. An array that contains the size of the sequence data for each dimension.
 * @param[in] seqLengthArraySize
 *   Input. Number of elements in sequence length array, \p seqLengthArray[]. It should be
 *   batch * beam. The batch and beam are described in ::cnnlSeqDataLayout_t.
 * @param[in] seqLengthArray
 *   Input. An integer array recording the length of all sequences. Note that the array should be
 *   set in the batch-beam order, in despite of sequence data layout. Set this parameter to NULL
 *   when sequence length array is not requested.
 * @param[in] paddingFill
 *   Input. A host pointer to the data type \p dtype to fill up the padding vectors within
 *   the valid length of each sequence. Use NULL when extra padding is not requested.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, ::cnnlCreateSeqDataDescriptor should be called.
 *
 * @note
 * - dimSize[0] represents the highest dimension, and dimSize[dimNb - 1] represents
 *   the lowest dimension.
 *   The \p seqLengthArray and \p PaddingFill of the descriptors of
 *   queries, keys, values, and outputs can only be NULL in the
 *   ::cnnlMultiHeadAttnForward function.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetSeqDataDescriptor_v2(cnnlSeqDataDescriptor_t seq_data_desc,
                                                      cnnlSeqDataLayout_t layout,
                                                      cnnlDataType_t dtype,
                                                      int dimNb,
                                                      const int64_t dimSize[],
                                                      int seqLengthArraySize,
                                                      const int seqLengthArray[],
                                                      void *paddingFill);

// Group:Common Interface
// Subgroup:SeqData
/*!
 * @brief Sets the position \p position and scale \p scale factors used in fixed-point quantization.
 * It is only used if you have quantized the input data with the symmetric fixed-point
 * quantization with scale factor quantization method. For more information about quantization,
 * see "Cambricon CNNL User Guide".
 *
 * @param[in] seq_data_desc
 *   Input. The descriptor of the sequence data. For detailed information,
 *   see ::cnnlSeqDataDescriptor_t.
 * @param[in] position
 *   Input. An integer of fixed position factor that is used for quantization.
 * @param[in] scale
 *   Input. A scalar of scale factor that is used for quantization.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, ::cnnlCreateSeqDataDescriptor, ::cnnlSetSeqDataDescriptor
 *   should be called.
 *
 * @note
 * - If the sequence data is in fixed-point data type, you need to call this function.
 *   This function is only used in the inference mode.
 * - The \p position should be limited in [-128, 127], otherwise the result is undefined.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetSeqDataDescriptorPositionAndScale(cnnlSeqDataDescriptor_t seq_data_desc,
                                         int position,
                                         float scale);

// Group:Common Interface
// Subgroup:SeqData
/*!
 * @brief Gets the \p position and \p scale factors used in fixed-point quantization.
 * It is only used if you have quantized the input data with the symmetric fixed-point
 * quantization with scale factor quantization method. For more information about quantization,
 * see "Cambricon CNNL User Guide".
 *
 * @param[in] seq_data_desc
 *   Input. The descriptor of the sequence data. For detailed information,
 *   see ::cnnlSeqDataDescriptor_t.
 * @param[in] position
 *   Input. Pointer to the host memory that holds position factor used for quantization.
 * @param[in] scale
 *   Input. Pointer to the host memory that holds scale factor used for quantization.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, ::cnnlCreateSeqDataDescriptor, ::cnnlSetSeqDataDescriptor
     and ::cnnlSetSeqDataDescriptorPositionAndScale should be called.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetSeqDataDescriptorPositionAndScale(const cnnlSeqDataDescriptor_t seq_data_desc,
                                         int *position,
                                         float *scale);

// Group:Common Interface
// Subgroup:SeqData
/*!
 * @brief Retrieves the sequence data descriptor \p seq_data_desc that holds the dimensions,
 * data type, layout, padding fill and the sequence lengths of the input sequence data.
 *
 * @param[in] seq_data_desc
 *   Input. The descriptor of the sequence data. For detailed information,
 *   see ::cnnlSeqDataDescriptor_t.
 * @param[out] layout
 *   Output. The layout of the sequence data. See ::cnnlSeqDataLayout_t.
 * @param[out] dtype
 *   Output. The data type of the sequence data. See ::cnnlDataType_t.
 * @param[out] dimNb
 *   Output. The number of dimensions of the sequence data.
 * @param[out] dimSize
 *   Output. An array that contains the size of the sequence data for each dimension.
 * @param[out] seqLengthArraySize
 *   Output. Number of elements in sequence length array, \p seqLengthArray[]. It is equal to
 *   batch * beam (N and B described in ::cnnlSeqDataLayout_t).
 * @param[out] seqLengthArray
 *   Output. An integer array recording the length of all sequences. Note that the array is
 *   ordered in a batch-beam order, in despite of sequence data layout. Return NULL when sequence
 *   length array is not set.
 * @param[out] paddingFill
 *   Output. A host pointer to the data type \p dtype to fill up the padding vectors within
 *   the valid length of each sequence. Use NULL when extra padding is not requested.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, ::cnnlCreateSeqDataDescriptor, ::cnnlSetSeqDataDescriptor
 *   should be called.
 *
 * @note
 * - dimSize[0] represents the highest dimension, and dimSize[dimNb - 1] represents
 *   the lowest dimension.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetSeqDataDescriptor(const cnnlSeqDataDescriptor_t seq_data_desc,
                                                   cnnlSeqDataLayout_t* layout,
                                                   cnnlDataType_t* dtype,
                                                   int* dimNb,
                                                   int dimSize[],
                                                   int* seqLengthArraySize,
                                                   int seqLengthArray[],
                                                   void* paddingFill);

// Group:Common Interface
// Subgroup:SeqData
/*!
 * @brief Retrieves the sequence data descriptor \p seq_data_desc that holds the dimensions,
 * data type, layout, padding fill and the sequence lengths of the input sequence data.
 *
 * @param[in] seq_data_desc
 *   Input. The descriptor of the sequence data. For detailed information,
 *   see ::cnnlSeqDataDescriptor_t.
 * @param[out] layout
 *   Output. The layout of the sequence data. See ::cnnlSeqDataLayout_t.
 * @param[out] dtype
 *   Output. The data type of the sequence data. See ::cnnlDataType_t.
 * @param[out] dimNb
 *   Output. The number of dimensions of the sequence data.
 * @param[out] dimSize
 *   Output. An array that contains the size of the sequence data for each dimension.
 * @param[out] seqLengthArraySize
 *   Output. Number of elements in sequence length array, \p seqLengthArray[]. It is equal to
 *   batch * beam (N and B described in ::cnnlSeqDataLayout_t).
 * @param[out] seqLengthArray
 *   Output. An integer array recording the length of all sequences. Note that the array is
 *   ordered in a batch-beam order, in despite of sequence data layout. Return NULL when sequence
 *   length array is not set.
 * @param[out] paddingFill
 *   Output. A host pointer to the data type \p dtype to fill up the padding vectors within
 *   the valid length of each sequence. Use NULL when extra padding is not requested.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, ::cnnlCreateSeqDataDescriptor, ::cnnlSetSeqDataDescriptor_v2
 *   should be called.
 *
 * @note
 * - dimSize[0] represents the highest dimension, and dimSize[dimNb - 1] represents
 *   the lowest dimension.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetSeqDataDescriptor_v2(const cnnlSeqDataDescriptor_t seq_data_desc,
                                                      cnnlSeqDataLayout_t* layout,
                                                      cnnlDataType_t* dtype,
                                                      int* dimNb,
                                                      int64_t dimSize[],
                                                      int64_t* seqLengthArraySize,
                                                      int64_t seqLengthArray[],
                                                      void* paddingFill);

// Group:Common Interface
// Subgroup:SeqData
/*!
 * @brief Destroys a sequence data descriptor \p seq_data_desc that was created by
 * ::cnnlCreateSeqDataDescriptor.
 *
 * @param[in] seq_data_desc
 *   Input. A sequence data descriptor created by ::cnnlCreateSeqDataDescriptor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroySeqDataDescriptor(cnnlSeqDataDescriptor_t seq_data_desc);

// Group:Common Interface
// Subgroup:SeqData
/*!
 *  @brief Sets the onchip data type to the descriptor of a data sequence \p desc.
 *  The onchip data type \p onchip_dtype can be different from the offchip data type of the data.
 *
 *  @param[in] desc
 *    Input. The descriptor of sequence data. For detailed information,
 *    see ::cnnlSeqDataDescriptor_t.
 *  @param[in] onchip_dtype
 *    Input. The onchip data type of the sequence data used in the operations that supports
 *    fixed-point computation.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - This function is optional, the onchip data type is only used on the operations that supports
 *    fixed-point computation, and it has no effect on other operations.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetSeqDataDescriptorOnchipDataType(cnnlSeqDataDescriptor_t desc,
                                       cnnlDataType_t onchip_dtype);

// Group:Common Interface
// Subgroup:SeqData
/*!
 *  @brief Retrieves the onchip data type of sequence data set in the descriptor \p desc
 *         with ::cnnlSetSeqDataDescriptorOnchipDataType.
 *
 *  @param[in] desc
 *    Input. The descriptor of sequence data. For detailed information,
 *    see ::cnnlSeqDataDescriptor_t.
 *  @param[out] onchip_dtype
 *    Output. Pointer to host memory where the onchip data type saved.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - The onchip data type is only used on the operations that support fixed-point computation,
 *    has no effect on other operations. If you call this function to get onchip data type for an
 *    operation that does support fixed-point computation, ::CNNL_STATUS_BAD_PARAM is returned.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetSeqDataDescriptorOnchipDataType(cnnlSeqDataDescriptor_t desc,
                                       cnnlDataType_t *onchip_dtype);

// Group:Common Interface
// Subgroup:TensorSet
/*!
 *  @brief Creates a descriptor \p tensorSetDesc of tensor set that holds a series of tensors.
 *  The number of tensors of tensor set is jointly determined by \p setDimNb and \p setDimSize.
 *  Use ::cnnlInitTensorSetMemberDescriptor to set information for descriptor
 *  and ::cnnlDestroySeqDataDescriptor function to destroy the tensor set descriptor.
 *
 *  @param[out] tensorSetDesc
 *    Input. Pointer to the memory that holds information about the descriptor of tensor set.
 *  @param[in] setDimNb
 *    Input. The number of dimensions of the tensor set.
 *  @param[in] setDimSize
 *    Input. An array that contains the number of the tensors for each dimension of the tensor set.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 * - After calling this function, you can call the ::cnnlInitTensorSetMemberDescriptor function to initialize
 *   and set the information to the tensor set descriptor.
 * - You need to call the ::cnnlDestroyTensorSetDescriptor function to destroy the descriptor.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCreateTensorSetDescriptor(cnnlTensorSetDescriptor_t *tensorSetDesc,
                              const int setDimNb,
                              const int setDimSize[]);

// Group:Common Interface
// Subgroup:TensorSet
/*!
 *  @brief Retrieves a tensor set descriptor \p tensorSetDesc that was previously created with the
 *  ::cnnlCreateTensorSetDescriptor function.
 *
 *  @param[in] tensorSetDesc
 *    Input. The descriptor of the tensor set. For detailed information,
 *    see ::cnnlSeqDataDescriptor_t.
 *  @param[out] setDimNb
 *    Output. The number of dimensions of the tensor set.
 *  @param[out] setDimSize
 *    Output. An array that contains the number of the tensor for each dimension of the tensor set.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @par API Dependency
 *  - Before calling this function, ::cnnlCreateTensorSetDescriptor should be called.
 *
 *  @note
 *  - setDimSize[0] represents the highest dimension, and dimSize[dimNb - 1] represents
 *    the lowest dimension.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetTensorSetDescriptor(cnnlTensorSetDescriptor_t tensorSetDesc,
                           int *setDimNb,
                           int setDimSize[]);

// Group:Common Interface
// Subgroup:TensorSet
/*!
 *  @brief Destroys a tensor set descriptor \p tensorSetDesc that was previously created by
 *  ::cnnlCreateTensorSetDescriptor.
 *
 *  @param[in] tensorSetDesc
 *    Input. A tensor descriptor created by ::cnnlCreateTensorSetDescriptor.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - This function should be called to destroy the tensor set descriptor.
 *    Otherwise, the memory leak may occur.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlDestroyTensorSetDescriptor(cnnlTensorSetDescriptor_t tensorSetDesc);

// Group:Common Interface
// Subgroup:TensorSet
/*!
 *  @brief Initializes a member tensor in the tensor set descriptors pointed by \p desc that was previously created
 *  with the ::cnnlCreateTensorSetDescriptor function, and sets the information about
 *  the dimensions, data type and layout.
 *
 *  @param[in] tensorSetDesc
 *    Input. The descriptor of the tensor set. For detailed information,
 *    see ::cnnlTensorSetDescriptor_t.
 *  @param[in] setDimNb
 *    Input. The number of dimensions of the tensor set.
 *  @param[in] tensorIndex
 *    Input. An array that contains the index of each dimension of a member tensor to be initialized in the tensor set.
 *  @param[in] layout
 *    Input. The layout of the member tensor. For detailed information, see ::cnnlTensorLayout_t.
 *  @param[in] dtype
 *    Input. The data type of the member tensor. For detailed information, see ::cnnlDataType_t.
 *  @param[in] dimNb
 *    Input. The number of dimensions in the member tensor.
 *  @param[in] dimSize
 *    Input. An array that contains the size of the member tensor for each dimension.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - Before calling this function,
 *   You need to call the ::cnnlCreateTensorSetDescriptor function to create the tensor descriptors
 *   \p tensorSetDesc.
 *  - All member tensors in the tensor set need to call this function to initialize related properties.
 *  - dimSize[0] and dimSize[DIM_MAX - 1] represent the highest and lowest dimension respectively, where
 *     DIM_MAX is the number of dimensions in the input tensor.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlInitTensorSetMemberDescriptor(cnnlTensorSetDescriptor_t tensorSetDesc,
                                  const int setDimNb,
                                  const int tensorIndex[],
                                  cnnlTensorLayout_t layout,
                                  cnnlDataType_t dtype,
                                  const int dimNb,
                                  const int dimSize[]);

// Group:Common Interface
// Subgroup:TensorSet
/*!
 *  @brief Initializes a member tensor in the tensor set descriptors pointed by \p desc that was previously created
 *  with the ::cnnlCreateTensorSetDescriptor function, and sets the information about
 *  the dimensions, data type and layout.
 *
 *  @param[in] tensorSetDesc
 *    Input. The descriptor of the tensor set. For detailed information,
 *    see ::cnnlTensorSetDescriptor_t.
 *  @param[in] setDimNb
 *    Input. The number of dimensions of the tensor set.
 *  @param[in] tensorIndex
 *    Input. An array that contains the index of each dimension of a member tensor to be initialized in the tensor set.
 *  @param[in] layout
 *    Input. The layout of the member tensor. For detailed information, see ::cnnlTensorLayout_t.
 *  @param[in] dtype
 *    Input. The data type of the member tensor. For detailed information, see ::cnnlDataType_t.
 *  @param[in] dimNb
 *    Input. The number of dimensions in the member tensor.
 *  @param[in] dimSize
 *    Input. An array that contains the size of the member tensor for each dimension.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - Before calling this function,
 *   You need to call the ::cnnlCreateTensorSetDescriptor functions to create the tensor descriptors
 *   \p tensorSetDesc.
 *  - All member tensors in the tensor set need to call this function to initialize related properties.
 *  - dimSize[0] and dimSize[DIM_MAX - 1] represent the highest and lowest dimension respectively, where
 *     DIM_MAX is the number of dimensions in the input tensor.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlInitTensorSetMemberDescriptor_v2(cnnlTensorSetDescriptor_t tensorSetDesc,
                                     const int setDimNb,
                                     const int tensorIndex[],
                                     cnnlTensorLayout_t layout,
                                     cnnlDataType_t dtype,
                                     const int dimNb,
                                     const int64_t dimSize[]);

// Group:Common Interface
// Subgroup:TensorSet
/*!
 *  @brief Sets the position and scale factors used in fixed-point quantization.
 *  It is only used if you have quantized the input data with the symmetric fixed-point
 *  quantization with scale factor quantization method. For more information about quantization,
 *  see "Cambricon CNNL User Guide".
 *
 * @deprecated
 * This function is deprecated and will be removed
 * in future release. Use ::cnnlInitRNNWeightPositionAndScale instead, which obtains position
 * and scale from \p weight_position_ptr and \p weight_scale_ptr.
 *
 *  @param[in] tensorSetDesc
 *    Input. The descriptor of tensor set. For detailed information,
 *    see ::cnnlTensorSetDescriptor_t.
 *  @param[in] setDimNb
 *    Input. The number of dimensions of the tensor set.
 *  @param[in] tensorIndex
 *    Input. An array that contains the position index information of the member tensor in the tensor set.
 *  @param[in] position
 *    Input. A position of fixed position factor that is used for quantification.
 *  @param[in] scale
 *    Input. A scalar of scale factor that is used for quantification.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - If the member tensor is in floating-point data type, you need to call this function.
 *  - If the member tensor is in fixed-point data type, you need to call this function.
 *  - Before calling this function, you need to call ::cnnlCreateTensorSetDescriptor
 *    to create the tensor set descriptor \p tensorSetDesc.
 *  - The \p position should be in range of [-128, 127], otherwise the result is undefined.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
CNNL_DEPRECATED_FOR(cnnlInitRNNWeightPositionAndScale)
cnnlStatus_t CNNL_WIN_API
cnnlInitTensorSetMemberDescriptorPositionAndScale(cnnlTensorSetDescriptor_t tensorSetDesc,
                                                  const int setDimNb,
                                                  const int tensorIndex[],
                                                  const int position,
                                                  const float scale);

// Group:Common Interface
// Subgroup:TensorSet
/*!
 *  @brief Retrieves the size of tensor set according to the input descriptor \p tensorSetDesc. You
 *  need to call the ::cnnlInitTensorSetMemberDescriptor function first to create a tensor set descriptor
 *  before calling this function.
 *
 *  @param[in] tensorSetDesc
 *    Input. The descriptor of tensor set. For detailed information,
 *    see ::cnnlTensorSetDescriptor_t.
 *  @param[out] sizeInBytes
 *    Output. Size in bytes of tensor set.
 *    You can allocate MLU memory for the tensor set with this value.
 *
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetTensorSetDescriptorSize(cnnlTensorSetDescriptor_t tensorSetDesc,
                               int *sizeInBytes);

// Group:Common Interface
// Subgroup:TensorSet
/*!
 *  @brief Retrieves the tensor descriptor in the tensor set and the corresponding offset address
 *         based on the entire block of MLU memory through the index \p tensorIndex.
 *
 *  @param[in] tensorSetDesc
 *    Input. The descriptor of tensor set. For detailed information,
 *    see ::cnnlTensorSetDescriptor_t.
 *  @param[in] setDimNb
 *    Input. The number of dimensions of the tensor set.
 *  @param[in] tensorIndex
 *    Input. An array that contains the position information of the member tensor in the tensor set.
 *  @param[in] data
 *    Input. Pointer to the MLU memory that is described by \p tensorSetDesc.
 *  @param[out] tensorDesc
 *    Output. Pointer to the host member. It is member tensor descriptor that indexed by \p tensorIndex in the tensor set.
 *    \p *tensorDesc contains tensor member information about dimensions, layout, data type, position and scale.
 *  @param[out] dataAddrInDevice
 *    Output. Pointer to the MLU memory that indexed by \p tensorIndex in the whole block of data
 *    \p dataAddrInDevice.
 *
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetTensorAndDataFromTensorSet(cnnlTensorSetDescriptor_t tensorSetDesc,
                                  const int setDimNb,
                                  const int tensorIndex[],
                                  void *data,
                                  cnnlTensorDescriptor_t *tensorDesc,
                                  void **dataAddrInDevice);
// Group:Common Interface
// Subgroup:QuantizeEx
/*!
 *  @brief Creates a tensor descriptor pointed by \p quant_desc that holds the parameters of quantization
 *  operation.
 *
 *  The ::cnnlDestroyQuantizeExDescriptor function needs to be called to destroy the
 *  tensor descriptor later.
 *
 *  @param[out] quant_desc
 *    Output. Pointer to the struct that holds information about the quantization operation.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateQuantizeExDescriptor(cnnlQuantizeExDescriptor_t *quant_desc);

// Group:Common Interface
// Subgroup:QuantizeEx
/*!
 *  @brief Initializes the quantization descriptor \p quant_desc that was previously created
 *  with ::cnnlCreateQuantizeExDescriptor function, and sets the information to the
 *  quantization descriptor \p quant_desc.
 *
 *  @param[in] quant_desc
 *    Input. The descriptor of the quantization. For detailed information,
 *    see ::cnnlQuantizeExDescriptor_t.
 *  @param[in] position_ptr
 *    Input. Pointer to an integer value of position parameter that is used for quantization.
 *  @param[in] scale_ptr
 *    Input. Pointer to a float value of scale parameter that is used for quantization.
 *  @param[in] offset_ptr
 *    Input. Pointer to an integer value of offset parameter that is used for quantization.
 *  @param[in] pointer_mode
 *    Input. The pointer mode of the quantization parameters \p position_ptr \p scale_ptr and
 *    \p offset_ptr. For detailed information, see ::cnnlPointerMode_t.
 *    - If \p pointer_mode is \p CNNL_POINTER_MODE_DEVICE, \p position_ptr, \p scale_ptr or \p offset_ptr
 *    should be a device pointer.
 *    - If \p pointer_mode is \p CNNL_POINTER_MODE_HOST, \p position_ptr, \p scale_ptr or \p offset_ptr
 *    should be a host pointer.
 *  @param[in] quant_scheme
 *    Input. An enum with information of quantization scheme. For more information, see ::cnnlQuantizeScheme_t.
 *  @param[in] quant_mode
 *    Input. An enum with information of quantization parameters. For more information, see ::cnnlQuantizeMode_t.
 *  @param[in] onchip_dtype
 *    Input. The fixed-point data type after quantization.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - If the data that \p position_ptr, \p scale_ptr or \p offset_ptr points to is a scalar, it is needed to
 *    call ::cnnlSetQuantizeExDescriptorScalarQuant. If the data is a tensor, it is
 *    needed to call ::cnnlSetQuantizeExDescriptor.
 *  - When \p pointer_mode is CNNL_POINTER_MODE_HOST, it means that the quantization parameters \p position_ptr,
 *    \p scale_ptr and \p offset_ptr are scalar values on the host. In this case,
 *    it makes a copy of data that \p position_ptr, \p scale_ptr and \p offset_ptr point to, which makes them
 *    independent of user operation.
 *  - When \p pointer_mode is CNNL_POINTER_MODE_DEVICE, it means that the quantization parameters \p position_ptr,
 *    \p scale_ptr and \p offset_ptr are scalar values on the device. In this case,
 *    it simply copies \p position_ptr, \p scale_ptr and \p offset_ptr, instead of copying the data that
 *    these pointers point to. So users need to ensure that the pointer life cycle is longer than the
 *    use of \p quant_desc. If users modify the data that is pointed to, it will affect the operator behaviors.
 *
 *  @par Requirements
 *  - The \p quant_desc and \p pointer_mode cannot be NULL.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t
CNNL_WIN_API cnnlSetQuantizeExDescriptorScalarQuant(cnnlQuantizeExDescriptor_t quant_desc,
                                                    const void *position_ptr,
                                                    const void *scale_ptr,
                                                    const void *offset_ptr,
                                                    cnnlPointerMode_t pointer_mode,
                                                    cnnlQuantizeScheme_t quant_scheme,
                                                    cnnlQuantizeMode_t quant_mode,
                                                    cnnlDataType_t onchip_dtype);
// Group:Common Interface
// Subgroup:QuantizeEx
/*!
 *  @brief Initializes the quantization descriptor \p quant_desc that was previously created
 *  with the ::cnnlCreateQuantizeExDescriptor function, and sets the information to the
 *  quantization descriptor \p quant_desc.
 *
 *  @param[in] quant_desc
 *    Input. The descriptor of the quantization. For detailed information,
 *    see ::cnnlQuantizeExDescriptor_t.
 *  @param[in] position_desc
 *    Input. The descriptor of the position tensor. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[in] position_ptr
 *    Input. Pointer to the position parameter for quantization.
 *  @param[in] scale_desc
 *    Input. The descriptor of the scale tensor. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[in] scale_ptr
 *    Input. Pointer to the scale parameter for quantization.
 *  @param[in] offset_desc
 *    Input. The descriptor of the scale tensor. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[in] offset_ptr
 *    Input. Pointer to the offset parameter for quantization.
 *  @param[in] quant_scheme
 *    Input. An enum with information of quantization mode. For more information, see ::cnnlQuantizeScheme_t.
 *  @param[in] quant_mode
 *    Input. An enum with information of quantization parameters. For more information, see ::cnnlQuantizeMode_t.
 *  @param[in] pointer_mode
 *    Input. The pointer mode of the quantization parameters \p position_ptr \p scale_ptr and
 *    \p offset_ptr. For detailed information, see ::cnnlPointerMode_t.
 *    - If \p pointer_mode is \p CNNL_POINTER_MODE_DEVICE, \p position_ptr, \p scale_ptr or \p offset_ptr
 *    should be a device pointer.
 *    - If \p pointer_mode is \p CNNL_POINTER_MODE_HOST, \p position_ptr, \p scale_ptr or \p offset_ptr
 *    should be a host pointer.
 *  @param[in] onchip_dtype
 *    Input. The fixed-point data type after quantization.
 *  @param[in] channel_group_size
 *    Input. When \p quantize_mode is CNNL_QUANTIZE_GROUP_WISE, the channel dimension should be
 *    divided by \p channel_group_size.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - If the data that \p position_ptr, \p scale_ptr or \p offset_ptr is pointed to is a scalar, you need to
 *    call ::cnnlSetQuantizeExDescriptorScalarQuant. If the data is a tensor, you
 *    need to call ::cnnlSetQuantizeExDescriptor.
 *
 *  @par Requirements
 *  - \p quant_desc cannot be NULL.
 *  - \p position_desc, \p scale_desc and \p offset_desc cannot be NULL at the same time.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetQuantizeExDescriptor(cnnlQuantizeExDescriptor_t quant_desc,
                            cnnlTensorDescriptor_t position_desc,
                            const void *position_ptr,
                            cnnlTensorDescriptor_t scale_desc,
                            const void *scale_ptr,
                            cnnlTensorDescriptor_t offset_desc,
                            const void *offset_ptr,
                            cnnlQuantizeScheme_t quant_scheme,
                            cnnlQuantizeMode_t quant_mode,
                            cnnlPointerMode_t pointer_mode,
                            cnnlDataType_t onchip_dtype,
                            int64_t channel_group_size);
// Group:Common Interface
// Subgroup:QuantizeEx
/*!
 *  @brief Initializes the quantization descriptor \p quant_desc that was previously created
 *  with the ::cnnlCreateQuantizeExDescriptor function, and sets the information to the
 *  quantization descriptor \p quant_desc.
 *
 *  @param[in] quant_desc
 *    Input. The descriptor of the quantization. For detailed information,
 *    see ::cnnlQuantizeExDescriptor_t.
 *  @param[in] quant_scheme
 *    Input. An enum with information of quantization scheme. For more information, see ::cnnlQuantizeScheme_t.
 *  @param[in] onchip_dtype
 *    Input. The fixed-point data type after quantization.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetQuantizeExDescriptorQuantSchemeAndDtype(cnnlQuantizeExDescriptor_t quant_desc,
                                             cnnlQuantizeScheme_t quant_scheme,
                                             cnnlDataType_t onchip_dtype);

// Group:Common Interface
// Subgroup:QuantizeEx
/*!
 *  @brief Gets the quantization descriptor \p quant_desc that was previously created
 *  with ::cnnlCreateQuantizeExDescriptor and was set with the
 *  ::cnnlSetQuantizeExDescriptorScalarQuant function.
 *
 *  @param[in] quant_desc
 *    Input. The descriptor of the quantization. For detailed information,
 *    see ::cnnlQuantizeExDescriptor_t.
 *  @param[out] position_host
 *    Output. A host pointer to the data of the position parameter for quantization.
 *  @param[out] scale_host
 *    Output. A host pointer to the data of the scale parameter for quantization.
 *  @param[out] offset_host
 *    Output. A host pointer to the data of the offset parameter for quantization.
 *  @param[out] position_dev
 *    Output. A host pointer to the data pointer of the position parameter for quantization.
 *  @param[out] scale_dev
 *    Output. A host pointer to the data pointer of the scale parameter for quantization.
 *  @param[out] offset_dev
 *    Output. A host pointer to the data pointer of the offset parameter for quantization.
 *  @param[out] pointer_mode
 *    Output. A host pointer to the pointer_mode indicating that data pointers \p position_ptr,
 *    \p scale_ptr and \p offset_ptr are passed by reference on host.
 *  @param[out] quant_scheme
 *    Output. A pointer to host memory where the quantization scheme should be set. For detailed
 *    information, see ::cnnlQuantizeScheme_t.
 *  @param[out] quant_mode
 *    Output. A pointer to host memory where the quantization mode should be set. For detailed
 *    information, see ::cnnlQuantizeMode_t.
 *  @param[out] onchip_dtype
 *    Output. A host pointer to data type of fixed-point data after quantization.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateQuantizeExDescriptor
 *   and ::cnnlSetQuantizeExDescriptorScalarQuant functions. If you call
 *   ::cnnlSetQuantizeExDescriptor and then call this fucntion, errors may occur.
 *
 *  @par Requirements
 *  - The \p pointer_mode cannot be NULL.
 *  - If the data of \p pointer_mode pointed to is CNNL_POINTER_MODE_HOST, this function will get
 *    a host pointer of quantization parameters that are stored on the host, such as \p
 *    position_host, \p scale_host and \p offset_host. There is no need to set \p position_dev,
 *    \p scale_dev and \p offset_dev.
 *  - If the data of \p pointer_mode pointed to is CNNL_POINTER_MODE_DEVICE, this fucntion will
 *    get a host pointer to the data pointer of quantization parameters, such as \p position_dev,
 *    \p scale_dev and \p offset_dev. There is no need to set \p position_host, \p scale_host
 *    and \p offset_host.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetQuantizeExDescriptorScalarQuant(
                                                        const cnnlQuantizeExDescriptor_t quant_desc,
                                                        int *position_host,
                                                        float *scale_host,
                                                        int *offset_host,
                                                        const void **position_dev,
                                                        const void **scale_dev,
                                                        const void **offset_dev,
                                                        cnnlPointerMode_t *pointer_mode,
                                                        cnnlQuantizeScheme_t *quant_scheme,
                                                        cnnlQuantizeMode_t *quant_mode,
                                                        cnnlDataType_t *onchip_dtype);

// Group:Common Interface
// Subgroup:QuantizeEx
/*!
 *  @brief Gets the quantization descriptor \p quant_desc that was previously created
 *  with ::cnnlCreateQuantizeExDescriptor and was set with the
 *  ::cnnlSetQuantizeExDescriptor function.
 *
 *  @param[in] quant_desc
 *    Input. The descriptor of the quantization. For detailed information,
 *    see ::cnnlQuantizeExDescriptor_t.
 *  @param[out] position_desc
 *    Output. A pointer to the \p position_desc of data. For detailed information, see
 *    ::cnnlTensorDescriptor_t.
 *  @param[out] position_ptr
 *    Output. A host pointer to the data pointer of the position parameter for quantization.
 *  @param[out] scale_desc
 *    Output. A pointer to the \p scale_desc of data. For detailed information, see
 *    ::cnnlTensorDescriptor_t.
 *  @param[out] scale_ptr
 *    Output. A host pointer to the data pointer of the scale parameter for quantization.
 *  @param[out] offset_desc
 *    Output. A pointer to the \p offset_desc of data. For detailed information, see
 *    ::cnnlTensorDescriptor_t.
 *  @param[out] offset_ptr
 *    Output. A host pointer to the data pointer of the offset parameter for quantization.
 *  @param[out] pointer_mode
 *    Output. A host pointer to the pointer_mode indicating that data pointers \p position_ptr,
 *    \p scale_ptr and \p offset_ptr are passed by reference on host or device.
 *    For detailed information, see ::cnnlPointerMode_t.
 *  @param[out] quant_scheme
 *    Output. A pointer to host memory where the quantization mode should be set. For detailed
 *    information, see ::cnnlQuantizeScheme_t.
 *  @param[out] quant_mode
 *    Output. A pointer to host memory where the quantization mode should be set. For detailed
 *    information, see ::cnnlQuantizeMode_t.
 *  @param[out] onchip_dtype
 *    Output. A host pointer to data type of fixed-point data after quantization.
 *  @param[out] channel_group_size
 *    Output. A host pointer to \p channel_group_size when \p quantize_mode is
 *    CNNL_QUANTIZE_GROUP_SIZE.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateQuantizeExDescriptor function
 *   and the ::cnnlSetQuantizeExDescriptor function.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetQuantizeExDescriptor(const cnnlQuantizeExDescriptor_t quant_desc,
                                                      cnnlTensorDescriptor_t *position_desc,
                                                      const void **position_ptr,
                                                      cnnlTensorDescriptor_t *scale_desc,
                                                      const void **scale_ptr,
                                                      cnnlTensorDescriptor_t *offset_desc,
                                                      const void **offset_ptr,
                                                      cnnlPointerMode_t *pointer_mode,
                                                      cnnlQuantizeScheme_t *quant_scheme,
                                                      cnnlQuantizeMode_t *quant_mode,
                                                      cnnlDataType_t *onchip_dtype,
                                                      int64_t *channel_group_size);

// Group:Common Interface
// Subgroup:QuantizeEx
/*!
 *  @brief Gets the quantization descriptor \p quant_desc that was previously created
 *  with ::cnnlCreateQuantizeExDescriptor and was set with the
 *  ::cnnlSetQuantizeExDescriptorQuantSchemeAndDtype function.
 *
 *  @param[in] quant_desc
 *    Input. The descriptor of the quantization. For detailed information,
 *    see ::cnnlQuantizeExDescriptor_t.
 *  @param[out] quant_scheme
 *    Input. A pointer to host memory where the quantization mode should be set. For detailed
 *    information, see ::cnnlQuantizeScheme_t.
 *  @param[out] onchip_dtype
 *    Input. A host pointer to data type of fixed-point data after quantization.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateQuantizeExDescriptor function
 *   and the ::cnnlSetQuantizeExDescriptorQuantSchemeAndDtype function.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetQuantizeExDescriptorQuantSchemeAndDtype(
                                                      const cnnlQuantizeExDescriptor_t quant_desc,
                                                      cnnlQuantizeScheme_t *quant_scheme,
                                                      cnnlDataType_t *onchip_dtype);

// Group:Common Interface
// Subgroup:QuantizeEx
/*!
 * @brief Destroys a quantization descriptor \p quant_desc that was previously created with the
 * ::cnnlCreateQuantizeExDescriptor function.
 *
 * The quantizeEx descriptor is defined in ::cnnlQuantizeExDescriptor_t, and holds
 * the information about the quantization operation.
 *
 * @param[in] quant_desc
 *   Input. The quantizeEx descriptor to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - None.
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyQuantizeExDescriptor(cnnlQuantizeExDescriptor_t quant_desc);

// Group:Common Interface
// Subgroup:Version Management
/*!
 * @brief Retrieves the version of Cambricon CNNL library.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGetLibVersion instead.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlGetLibVersion)
size_t CNNL_WIN_API cnnlGetVersion(void);

// Group:Common Interface
// Subgroup:Version Management
/*!
 * @brief Retrieves the version of Cambricon CNNL library. The version of Cambricon CNNL is composed
 * of \p major, \p minor and \p patch. For instance, major = 1, minor = 2, patch = 3,
 * the version of Cambricon CNNL library is 1.2.3.
 *
 * @param[in] major
 * Input. A pointer to scale factor that gets the major version of Cambricon CNNL library.
 * @param[in] minor
 * Input. A pointer to scale factor that gets the minor version of Cambricon CNNL library.
 * @param[in] patch
 * Input. A pointer to scale factor that gets the patch version of Cambricon CNNL library.
 *
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
void CNNL_WIN_API cnnlGetLibVersion(int* major,
                       int* minor,
                       int* patch);
// Group:Common Interface
// Subgroup:Debugging
/*!
 * @brief Sets the mode of a Cambricon CNNL debugging tool that can generate operator
 * information files for all the operators that are called. The generated file
 * contains the operator information including inputs shapes, outputs shapes,
 * parameters and inputs real data based on the setting of \p mode. For more
 * information, see "Cambricon CNNL User Guide".
 *
 * @param[in] mode
 * Input. The parameter determines what mode the Cambricon CNNL debugging tool will turn on.
 *
 * @note
 * - When \p mode is set to 0, the Cambricon CNNL debugging tool will turn off, and do not
 *   generate operator information files.
 *
 * - When \p mode is set to 1, the Cambricon CNNL debugging tool will generate operator
 *   information files for all the operators that are called. And the inputs real
 *   data of the operators is not included in the files.
 *
 * - When \p mode is set to 2, the Cambricon CNNL debugging tool will generate operator
 *   information files for all the operators that are called. Only part of the inputs real
 *   data of the operators is included in the files. If the environment variable
 *   CNNL_GEN_CASE_DUMP_DATA is set to 1, all of the inputs real data of the operators will be
 *   included in the files. For more information about setting environment variable,
 *   see "Cambricon CNNL User Guide".
 *
 * - When \p mode is set to 3, the Cambricon CNNL debugging tool will print operator
 *   information on the screen without the inputs real data for all the operators
 *   that are called instead generating information files.
 *
 * - When \p mode is out of range [0, 3], the Cambricon CNNL debugging tool will turn off,
 *   and do not generate operator information files.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
void CNNL_WIN_API cnnlSetGenCaseMode(int mode);

// Group:IndexSelect
/*!
 * @brief Retrieves a new tensor, which indexes the input tensor \p input along \p dim using the
 *  entries in index tensor \p index. The \p index tensor is with the data type of integer.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the index select operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] dim
 *   Input. The dimension of the input tensor to be indexed.
 * @param[in] input_desc
 *   Input. The descriptors of the \p input tensors. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] index_desc
 *   Input. The descriptors of the \p index tensors. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] index
 *   Input. Pointer to the MLU memory that stores the index data.
 * @param[in] output_desc
 *   Input. The descriptors of the \p output tensors. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Index Select Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The (I/O)function supports the following data widths for \p input and \p output tensors.
 *   - input tensor: 1-byte, 2-byte, 4-byte, 8-byte.
 *   - index tensor: int32, int64.
 *   - dim: int32.
 *   - output tensor: 1-byte, 2-byte, 4-byte, 8-byte.
 *
 *   The byte width of a data type can be got with the ::cnnlGetSizeOfDataType function.
 *
 *   Note that the data type of input tensor \p input and output tensor \p output must be the same.
 *
 * @par Scale Limitation
 * - The value of the index tensor should be in range of [0, input[dim]-1].
 * - \p dim should be greater than or equal to negative number of input dimensions,
 *   and less than or equal to number of input dimensions -1.
 *
 * @par Reference
 * https://pytorch.org/docs/1.0.0/torch.html?highlight=index_select#torch.index_select
 *
 *
 * @par API Dependency
 * - None.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the index select operation is as follows:
     @verbatim
     input array
       input = [[0, 1, 2, 3],
                [4, 5, 6, 7],
                [8, 9, 10, 11]]
     index array
       index = [0, 2]
     dim = 0
     output array
       output = [[0, 1, 2, 3],
                 [8, 9, 10, 11]]
     @endverbatim
 *
 */

cnnlStatus_t CNNL_WIN_API cnnlIndexSelect(cnnlHandle_t handle,
                                          const int dim,
                                          const cnnlTensorDescriptor_t input_desc,
                                          const void *input,
                                          const cnnlTensorDescriptor_t index_desc,
                                          const void *index,
                                          const cnnlTensorDescriptor_t output_desc,
                                          void *output);
// Group:Transform
/*!
 * @brief Transforms linearly for an input tensor with the following formula:
 *
 * output = alpha * input + beta
 *
 * where, \p input and \p output are tensors, and \p alpha and \p beta are scaling factors
 * used in operation.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlTransform_v3 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of input tensor \p input. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. A device pointer to the MLU memory that stores the input tensor.
 * @param[in] alpha
 *   Input.  A host pointer to scaling factor of tensor input.
 * @param[in] beta
 *   Input.  A host pointer to bias factor of tensor input.
 * @param[out] output
 *   Output. A device pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Transform Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The combinations of data types for input tensor \p input and output tensor
 *   \p output must be half-half, float-float, int32-int32, bfloat16-bfloat16,
 *    int64-int64 or complex_float-complex_float.
 *
 *   bfloat16 is only supported on MLU500 series.
 * - \p alpha and \p beta:
 *   - If data type of tensors is float, half or bfloat16, the data
 *     type of \p alpha and \p beta should be float.
 *   - If data type of tensors is int32,
 *     the data type of \p alpha and \p beta should be int.
 *   - If data type of tensors is complex_float, the data type of \p alpha
 *     and \p beta should be two floating-point numbers stored continuously in memory,
 *     with each representing the real part and imaginary part.
 *
 * @par Scale Limitation
 * - The tensors descriptor of input and output tensors must be the same.
 * - When data type of \p input is int32, the intermediate result of \p input cannot
 *   exceed the value range of corresponding data type.
 * - When data type of \p input is int64, value of \p alpha and \p beta should be in
 *   range of [-140737488355328, 140737488355328).
 * - The number of dimensions should not exceed \p CNNL_DIM_MAX.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - Example of this operation is as follows:
     @verbatim
       Input tensor   :   [[1, 2, 3],
                           [4, 5, 6],
                           [7, 8, 9]]

       alpha          :   2

       beta           :   1

       Output tensor  :   [[3,  5,  7],
                           [9,  11, 13],
                           [15, 17, 19]]
     @endverbatim
 *
 */

cnnlStatus_t CNNL_WIN_API cnnlTransform(cnnlHandle_t handle,
                                        const void *alpha,
                                        const cnnlTensorDescriptor_t input_desc,
                                        const void *input,
                                        const void *beta,
                                        void *output);
// Group:Transform
/*!
 * @brief Transforms linearly for an input tensor with the following formula:
 *
 * output = alpha * input + beta
 *
 * where, \p input and \p output are tensors, and \p alpha and \p beta are scaling factors
 * used in operation. ::cnnlTransform_v3 supports host and device scale pointers \p alpha
 * and \p beta.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of input tensor \p input. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. A device pointer to the MLU memory that stores the input tensor.
 * @param[in] pointer_mode
 *   Input.  An enum value that indicates that scalar values \p alpha and \p beta are
 *   passed by reference on host or device. The information is defined in ::cnnlPointerMode_t.
 * @param[in] alpha
 *   Input.  A pointer to scaling factor of tensor input.
 *   - If \p pointer_mode is \p CNNL_POINTER_MODE_DEVICE, \p alpha should be a device pointer.
 *   - If \p pointer_mode is \p CNNL_POINTER_MODE_HOST, \p alpha should be a host pointer.
 * @param[in] beta
 *   Input.  A pointer to scaling factor of tensor input.
 *   - If \p pointer_mode is \p CNNL_POINTER_MODE_DEVICE, \p beta should be a device pointer.
 *   - If \p pointer_mode is \p CNNL_POINTER_MODE_HOST, \p beta should be a host pointer.
 * @param[in] output_desc
 *   Input. The descriptor of output tensor \p output. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. A device pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Transform Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The combinations of data types for input tensor \p input and output tensor
 *   \p output must be half-half, float-float, int32-int32, bfloat16-bfloat16,
 *   int64-int64, complex_float-complex_float or int64-float.
 *   The bfloat16 data type is supported only on MLU500 series.
 * - \p alpha and \p beta:
 *   - If data type of tensors is float, half or bfloat16, the data
 *     type of \p alpha and \p beta should be float.
 *   - If data type of tensors is int32,
 *     the data type of \p alpha and \p beta should be int.
 *   - If data type of tensors is complex_float, the data type of \p alpha
 *     and \p beta should be two floating-point numbers stored continuously in memory,
 *     with each representing the real part and imaginary part.
 *
 * @par Scale Limitation
 * - The tensor descriptors of input and output tensors must be the same.
 * - When data type of \p input is int64, value of \p alpha and \p beta should be in
 *   range of [-140737488355328, 140737488355328).
 * - The number of dimensions should not exceed \p CNNL_DIM_MAX.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
       Input tensor   :   [[1, 2, 3],
                           [4, 5, 6],
                           [7, 8, 9]]

       alpha          :   2

       beta           :   1

       Output tensor  :   [[3,  5,  7],
                           [9,  11, 13],
                           [15, 17, 19]]
     @endverbatim
 *
 */

cnnlStatus_t CNNL_WIN_API cnnlTransform_v2(cnnlHandle_t handle,
                                           const cnnlPointerMode_t pointer_mode,
                                           const void *alpha,
                                           const cnnlTensorDescriptor_t input_desc,
                                           const void *input,
                                           const void *beta,
                                           const cnnlTensorDescriptor_t output_desc,
                                           void *output);

// Group:Transform
/*!
 * @brief Linearly transforms an input tensor with the following formula:
 *
 * output = alpha * input + beta
 *
 * where, \p input and \p output are tensors, and \p alpha and \p beta are scaling factors
 * used in operation. Compared with ::cnnlTransform_v2 that supports host and device scale pointers
 * \p alpha and \p beta, this API supports tensors with stride.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of input tensor \p input. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. A device pointer to the MLU memory that stores the \p input tensor.
 * @param[in] pointer_mode
 *   Input.  An enum value that indicates that scalar values \p alpha and \p beta are
 *   passed by reference on host or device. The information is defined in ::cnnlPointerMode_t.
 * @param[in] alpha
 *   Input.  A pointer to scaling factor of tensor \p input.
 *   - If \p pointer_mode is \p CNNL_POINTER_MODE_DEVICE, \p alpha should be a device pointer.
 *   - If \p pointer_mode is \p CNNL_POINTER_MODE_HOST, \p alpha should be a host pointer.
 * @param[in] beta
 *   Input.  A pointer to scaling factor of tensor \p input.
 *   - If \p pointer_mode is \p CNNL_POINTER_MODE_DEVICE, \p beta should be a device pointer.
 *   - If \p pointer_mode is \p CNNL_POINTER_MODE_HOST, \p beta should be a host pointer.
 * @param[in] workspace
 *   Input. A device pointer to the MLU memory that is used as an extra workspace for this
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in this
 *   operation. You can get the size of workspace with ::cnnlGetTransformWorkspaceSize.
 * @param[in] output_desc
 *   Input. The descriptor of output tensor \p output. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. A device pointer to the MLU memory that stores the \p output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Transform Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The combinations of data types for input tensor \p input and output tensor
 *   \p output must be half-half, float-float, int32-int32, bfloat16-bfloat16,
 *   int64-int64, complex_float-complex_float or int64-float.
 *   The bfloat16 data type is supported only on MLU500 series.
 * - \p alpha and \p beta:
 *   - If data type of tensors is float, half or bfloat16, the data
 *     type of \p alpha and \p beta should be float.
 *   - If data type of tensors is int32,
 *     the data type of \p alpha and \p beta should be int.
 *   - If data type of tensors is complex_float, the data type of \p alpha
 *     and \p beta should be two floating-point numbers stored continuously in memory,
 *     with each representing the real part and imaginary part.
 *
 * @par Scale Limitation
 * - The tensor descriptors of input and output tensors must be the same.
 * - When data type of \p input is int64, value of \p alpha and \p beta should be in
 *   range of [-140737488355328, 140737488355328).
 * - The number of dimensions should not exceed \p CNNL_DIM_MAX.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
       Input tensor   :   [[1, 2, 3],
                           [4, 5, 6],
                           [7, 8, 9]]

       alpha          :   2

       beta           :   1

       Output tensor  :   [[3,  5,  7],
                           [9,  11, 13],
                           [15, 17, 19]]
     @endverbatim
 *
 */

cnnlStatus_t CNNL_WIN_API cnnlTransform_v3(cnnlHandle_t handle,
                                           const cnnlPointerMode_t pointer_mode,
                                           const void *alpha,
                                           const cnnlTensorDescriptor_t input_desc,
                                           const void *input,
                                           const void *beta,
                                           void * workspace,
                                           size_t workspace_size,
                                           const cnnlTensorDescriptor_t output_desc,
                                           void *output);


// Group:Transform
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory in bytes that is used as
 * an extra workspace to optimize the ::cnnlTransform_v3 operation.
 *
 * The size of the extra workspace is based on the given information of the input and output
 * tensor descriptors \p input_desc and \p output_desc.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor \p input. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor \p output. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is
 *   used in ::cnnlTransform_v3 operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */

cnnlStatus_t CNNL_WIN_API cnnlGetTransformWorkspaceSize(cnnlHandle_t handle,
                                                       const cnnlTensorDescriptor_t input_desc,
                                                       const cnnlTensorDescriptor_t output_desc,
                                                       size_t *workspace_size);

// Group:Tri
/*!
 * @brief Returns batches of the upper or lower triangular part of given matrices.
 *
 * When \p tri_up_mode is set to true, this function is in the triu mode and returns the upper
 * \p diagonal_k triangular part of the given matrix \p input with the other elements of the output
 * tensor set to zero.
 *
 * When \p tri_up_mode is set to false, this function is in the tril mode and returns the lower
 * \p diagonal_k triangular part of the given matrix \p input with the other elements of the output
 * tensor set to zero.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlTri_v2 instead for supporting large tensors.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the tri
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] diagonal_k
 *   Input. The diagonal used in this operation.
 * @param[in] tri_up_mode
 *   Input. A Boolean value that determines whether to use the tril or triu mode.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH,
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Tri Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \p input and output tensor
 *   \p output. Data type of input tensor and output tensor should be the same.
 *   - input tensor: bool, int8, int16, int32, half, float, bfloat16.
 *   - output tensor: bool, int8, int16, int32, half, float, bfloat16.
 *
 * @par Scale Limitation
 * - None.
 *
 * @note
 * - You can specify the stride of all dimensions for input_desc and output_desc
 *   with ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the tri operation is as follows:
     @verbatim
     input array by 4 * 4 --> input: [[1, 2, 3, 4],
                                      [5, 6, 7, 8],
                                      [9, 10,11,12],
                                      [13,14,15,16]]

     1.param:
       diagonal_k: 1, tri_up_mode: true

     output array by 4 * 4 --> output: [[0, 2, 3, 4],
                                        [0, 0, 7, 8],
                                        [0, 0, 0, 12],
                                        [0, 0, 0, 0]]

     2.param:
       diagonal_k: 1, tri_up_mode: false

     output array by 4 * 4 --> output: [[1, 2, 0, 0],
                                        [5, 6, 7, 0],
                                        [9, 10,11,12],
                                        [13,14,15,16]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.triu.html
 * - https://pytorch.org/docs/stable/generated/torch.tril.html
 */
CNNL_DEPRECATED_FOR(cnnlTri_v2)
cnnlStatus_t CNNL_WIN_API cnnlTri(cnnlHandle_t handle,
                                  const int diagonal_k,
                                  const bool tri_up_mode,
                                  const cnnlTensorDescriptor_t input_desc,
                                  const void *input,
                                  const cnnlTensorDescriptor_t output_desc,
                                  void *output);
// Group:Tri
/*!
 * @brief Returns batches of the upper or lower triangular part of given matrices.
 *
 * When \p tri_up_mode is set to true, this function is in the triu mode and returns the upper
 * \p diagonal_k triangular part of the given matrix \p input with the other elements of the output
 * tensor set to zero.
 *
 * When \p tri_up_mode is set to false, this function is in the tril mode and returns the lower
 * \p diagonal_k triangular part of the given matrix \p input with the other elements of the output
 * tensor set to zero.
 *
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the tri
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] diagonal_k
 *   Input. The diagonal used in this operation.
 * @param[in] tri_up_mode
 *   Input. A Boolean value that determines whether to use the tril or triu mode.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH,
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Tri Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \p input and output tensor
 *   \p output. Data type of input tensor and output tensor should be the same.
 *   - input tensor: bool, int8, int16, int32, half, float, bfloat16.
 *   - output tensor: bool, int8, int16, int32, half, float, bfloat16.
 *
 * @par Scale Limitation
 * - None.
 *
 * @note
 * - You can specify the stride of all dimensions for input_desc and output_desc
 *   with ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the tri operation is as follows:
     @verbatim
     input array by 4 * 4 --> input: [[1, 2, 3, 4],
                                      [5, 6, 7, 8],
                                      [9, 10,11,12],
                                      [13,14,15,16]]

     1.param:
       diagonal_k: 1, tri_up_mode: true

     output array by 4 * 4 --> output: [[0, 2, 3, 4],
                                        [0, 0, 7, 8],
                                        [0, 0, 0, 12],
                                        [0, 0, 0, 0]]

     2.param:
       diagonal_k: 1, tri_up_mode: false

     output array by 4 * 4 --> output: [[1, 2, 0, 0],
                                        [5, 6, 7, 0],
                                        [9, 10,11,12],
                                        [13,14,15,16]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.triu.html
 * - https://pytorch.org/docs/stable/generated/torch.tril.html
 */
cnnlStatus_t CNNL_WIN_API cnnlTri_v2(cnnlHandle_t handle,
                                  const int64_t diagonal_k,
                                  const bool tri_up_mode,
                                  const cnnlTensorDescriptor_t input_desc,
                                  const void *input,
                                  const cnnlTensorDescriptor_t output_desc,
                                  void *output);

// Group:Clip
/*!
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlClip_v3 instead, which supports \p min and \p max tensors.
 *
 * @brief Clips all elements from input into the range of [min, max].
 * If an element is less than \p min, the element will be set to \p min. If an element is bigger
 * than \p max, the element will be set to \p max. Otherwise, the element will not be reset.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the clamp operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] min
 *   Input. A host pointer to the lower-bound of the range to be clamped to.
 * @param[in] max
 *   Input. A host pointer to the upper-bound of the range to be clamped to.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 *   - input: half, float, int32, bfloat16, int64.
 *   - output: half, float, int32, bfloat16, int64.
 *
 *   The bfloat16 data type is supported only on MLU500 series.
 *
 * @note
 * - The data type of input should be the same as output.
 * - If the data type of input is int32, the value of input should be less than 4194304.
 * - You can specify the stride of all dimensions for input_desc with
 *   ::cnnlSetTensorDescriptorEx.
 *
 *
 * @par Scale Limitation
 * - According to the definition of Clip function, the parameters should meet the following
 *   conditions:
 *   - The shapes of input and output must match.
 *   - \p min and \p max should not be NULL at the same time.
 *
 * @par Example
 * - The example of the clip operation is as follows:
    @verbatim
    input one array by 2 * 3,
        input: [[1,2,3],[4,5,6]]

    param:
    min: 2, max: 5,
    output one array by 2 * 3 -->output: [[2,2,3],[4,5,5]]
    @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/torch.html
 *
 */
CNNL_DEPRECATED_FOR(cnnlClip_v3)
cnnlStatus_t CNNL_WIN_API cnnlClip(cnnlHandle_t handle,
                                   const cnnlTensorDescriptor_t x_desc,
                                   const void *x,
                                   const void *min,
                                   const void *max,
                                   void *y);

// Group:Clip
/*!
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlClip_v3 instead, which supports \p min and \p max tensors.
 *
 * @brief Clips all elements from input into the range of [min, max].
 * If an element is less than \p min, the element will be set to \p min. If an element is bigger
 * than \p max, the element will be set to \p max. Otherwise, the element will not be reset.
 * Compared with ::cnnlClip, this function supports host or device pointer modes for \p max and \p min.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the clamp operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] pointer_mode
 *   Input.  An enum value that indicates that the scalar values \p min and \p max are
 *   passed by reference on the host or device. The information is defined in ::cnnlPointerMode_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] min
 *   Input. A pointer to the lower-bound of the range to be clamped to.
 *   If the \p pointer_mode is \p CNNL_POINTER_MODE_DEVICE, the \p min should be a device pointer.
 *   If the \p pointer_mode is \p CNNL_POINTER_MODE_HOST, the \p min should be a host pointer.
 * @param[in] max
 *   Input. A pointer to the upper-bound of the range to be clamped to.
 *   If the \p pointer_mode is \p CNNL_POINTER_MODE_DEVICE, the \p max should be a device pointer.
 *   If the \p pointer_mode is \p CNNL_POINTER_MODE_HOST, the \p max should be a host pointer.
 * @param[in] y_desc
 *   Output. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Data Type
 *   - input: half, float, int32, bfloat16.
 *   - output: half, float, int32, bfloat16.
 *
 *   The bfloat16 data type is supported only on MLU500 series.
 *
 * @note
 * - The data type of input should be the same as output.
 * - If the data type of input is int32, the value of input should be less than 4194304.
 * - You can specify the stride of all dimensions for input_desc with
 *   ::cnnlSetTensorDescriptorEx.
 *
 *
 * @par Scale Limitation
 * - According to the definition of Clip function, the parameters should meet the following
 *   conditions:
 *   - The shapes of input and output must match.
 *   - \p min and \p max should not be NULL at the same time.
 *
 * @par Example
 * - The example of the clip operation is as follows:
    @verbatim
    input one array by 2 * 3,
        input: [[1,2,3],[4,5,6]]

    param:
    min: 2, max: 5,
    output one array by 2 * 3 -->output: [[2,2,3],[4,5,5]]
    @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/torch.html
 *
 */

CNNL_DEPRECATED_FOR(cnnlClip_v3)
cnnlStatus_t CNNL_WIN_API cnnlClip_v2(cnnlHandle_t handle,
                                   const cnnlPointerMode_t pointer_mode,
                                   const cnnlTensorDescriptor_t x_desc,
                                   const void *x,
                                   const void *min,
                                   const void *max,
                                   const cnnlTensorDescriptor_t y_desc,
                                   void *y);

// Group:Clip
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the ::cnnlClip_v3 function.
 *
 * The size of the extra workspace is based on the given information of the ::cnnlClip_v3 function,
 * including the input tensor descriptors \p input_desc, \p min_desc and \p max_desc.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   clip operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] min_desc
 *   Input. The descriptor of the min tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] max_desc
 *   Input. The descriptor of the max tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   ::cnnlClip_v3 function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH.
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlClip_v3 function to perform the
 *   select operation.
 *
 * @par Scale Limitation
 * - The number of dimensions is no more than \p CNNL_DIM_MAX.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetClipWorkspaceSize(cnnlHandle_t handle,
                         const cnnlTensorDescriptor_t input_desc,
                         const cnnlTensorDescriptor_t min_desc,
                         const cnnlTensorDescriptor_t max_desc,
                         size_t *workspace_size);


// Group:Clip
/*!
 * @brief Clips all elements from input into the range of [min, max].
 * If an element is less than \p min, the element will be set to \p min. If an element is bigger
 * than \p max, the element will be set to \p max. Otherwise, the element will not be reset.
 * Compared with ::cnnlClip_v2, this function supports \p min and \p max tensors.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the clamp operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
* @param[in] min_desc
 *   Input. The descriptor of the input tensor of lower-bound of the range to be clamped to. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] min
 *   Input. A pointer to the input tensor of lower-bound of the range to be clamped to.
* @param[in] max_desc
 *   Input. The descriptor of the input tensor of upper-bound of the range to be clamped to. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] max
 *   Input. A pointer to the input tensor of upper-bound of the range to be clamped to.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the ::cnnlClip_v3 function.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the ::cnnlClip_v3 function.
 *   You can get the size of the workspace by using ::cnnlGetClipWorkspaceSize.
 * @param[in] y_desc
 *   Output. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 *   - input: half, float, int32, bfloat16, int64.
 *   - output: half, float, int32, bfloat16, int64.
 *
 *   The bfloat16 data type is supported only on MLU500 series.
 *
 * @note
 * - The data type of input should be the same as output.
 * - If the data type of input is int32, the value of input should be less than 4194304.
 * - You can specify the stride of all dimensions for input_desc with
 *   ::cnnlSetTensorDescriptorEx.
 * - When the shape of \p min and \p max tensors is [1], the pointer_mode of tensor can be \p CNNL_POINTER_MODE_DEVICE or \p CNNL_POINTER_MODE_HOST.
 *   - If the \p pointer_mode is \p CNNL_POINTER_MODE_DEVICE, the \p value should be a device pointer.
 *   - If the \p pointer_mode is \p CNNL_POINTER_MODE_HOST, the \p value should be a host pointer.
 * - When the number elements of \p min and \p max tensors are larger than 1, the pointer_mode of tensor must be \p CNNL_POINTER_MODE_DEVICE.
 * - The pointer_mode of \p min and \p max tensors must be same.
 *
 * @par Scale Limitation
 * - According to the definition of Clip function, the parameters should meet the following
 *   conditions:
 *   - The shape of input and output must be matched.
 *   - \p min and \p max should not be NULL at the same time.
 *
 * @par Example
 * - The example of the clip operation is as follows:
    @verbatim
    input one array by 2 * 3,
        input: [[1,2,3],[4,5,6]]

    param:
    min one array by 2 * 3,
        min: [[2,2,2],[3,3,3]]
    max one array by 2 * 3,
        max: [[5,5,5],[5,5,5]]
    output one array by 2 * 3 --> output: [[2,2,3],[4,5,5]]
    @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/torch.html
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlClip_v3(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t x_desc,
                                      const void *x,
                                      const cnnlTensorDescriptor_t min_desc,
                                      const void *min,
                                      const cnnlTensorDescriptor_t max_desc,
                                      const void *max,
                                      void *workspace,
                                      size_t workspace_size,
                                      const cnnlTensorDescriptor_t y_desc,
                                      void *y);

// Group:Ax
/*!
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlOpTensor instead.
 *
 *  @brief Multiplies the input tensor \p a to the input tensor \p x.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *    ax operation. For detailed information, see ::cnnlHandle_t.
 *  @param[in] a_desc
 *    Input. Descriptor of input data \p a, including dimension, data type (half and float),
 *    and data layout.
 *  @param[in] a
 *    Input. Pointer to the MLU memory that stores the input tensor \p a.
 *  @param[in] x_desc
 *    Input. Descriptor of input data \p x, including dimension, data type (half and float),
 *    and data layout.
 *  @param[in,out] x
 *    Input and Output. Pointer to the MLU memory that stores the input and output tensor \p x.
 *  @param[in] workspace
 *    Input. Pointer to the MLU memory that is used as an extra workspace for the ax operation.
 *    For more information about workspace, see "Cambricon CNNL User Guide".
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @par Formula
 *  - \p x = \p a * \p x
 *  - See "Ax Operation" section in "Cambricon CNNL User Guide" for details.
 *
 *  @par Data Type
 *  - This function supports the following data types for input tensor \p a and \p x.
 *    Data type of all above tensors should be the same.
 *    - all above tensors: half, float.
 *
 *  @note
 *  - This operation supports broadcasting. Each dimension of the input tensor \p a must match the
 *    corresponding dimension of the tensor \p x, or equal to 1.
 *  - If tensor \p a does not need broadcasting, the workspace can be NULL.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - The example of the ax operation is as follows:
    @verbatim
    input two arrays by 2 * 1 and 2 * 2 --> a: [[2], [1]]

    --> x: [[4, 4], [4, 4]]

    output array by 2 * 2 --> x: [[8, 8], [4, 4]]
    @endverbatim
 */
CNNL_DEPRECATED_FOR(cnnlOpTensor)
cnnlStatus_t CNNL_WIN_API cnnlAx(cnnlHandle_t handle,
                                 const cnnlTensorDescriptor_t a_desc,
                                 const void *a,
                                 const cnnlTensorDescriptor_t x_desc,
                                 void *x,
                                 void *workspace);

// Group:Ax
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace to
 * optimize the ax operation.
 *
 * The size of the extra workspace is based on the given information of the ax operation,
 * including the input tensor descriptor \p a_desc. For more information about the workspace,
 * see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   ax operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] a_desc
 *   Input. Descriptor of input data \p a, including dimension, data type (half and float),
 *   and data layout.
 * @param[in] x_desc
 *   Input. Descriptor of input data \p x, including dimension, data type (half and float),
 *   and data layout.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *   the ax operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetAxWorkspaceSize(cnnlHandle_t handle,
                                                 const cnnlTensorDescriptor_t a_desc,
                                                 const cnnlTensorDescriptor_t x_desc,
                                                 size_t *size);

// Group:Ax
/*!
 *  @brief Multiplies the input tensor \p a to the input tensor \p x. Compared with ::cnnlAx,
 *  this function allows you to check the validation of \p workspace_size.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlOpTensor instead.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *    ax operation. For detailed information, see ::cnnlHandle_t.
 *  @param[in] a_desc
 *    Input. Descriptor of input data \p a, including dimension, data type (half and float),
 *    and data layout.
 *  @param[in] a
 *    Input. Pointer to the MLU memory that stores the input tensor \p a.
 *  @param[in] x_desc
 *    Input. Descriptor of input data \p x, including dimension, data type (half and float),
 *    and data layout.
 *  @param[in,out] x
 *    Input and Output. Pointer to the MLU memory that stores the input and output tensor \p x.
 *  @param[in] workspace
 *    Input. Pointer to the MLU memory that is used as an extra workspace for the ax operation.
 *    For more information about workspace, see "Cambricon CNNL User Guide".
 *  @param[in] workspace_size
 *    Input. The size of the extra workspace in bytes that needs to be used in the ax operation.
 *    You can get the size of the workspace with the ::cnnlGetAxWorkspaceSize function.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @par Formula
 *  - \p x = \p a * \p x
 *  - See "Ax Operation" section in "Cambricon CNNL User Guide" for details.
 *
 *  @par Data Type
 *  - This function supports the following data types for input tensor \p a and \p x.
 *    Data type of all above tensors should be the same.
 *    - all above tensors: half, float.
 *
 *  @note
 *  - This operation supports broadcasting. Each dimension of the input tensor \p a must match the
 *    corresponding dimension of the tensor \p x, or equal to 1.
 *  - If tensor \p a does not need broadcasting, the workspace can be NULL.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - The example of the ax operation is as follows:
    @verbatim
    input two arrays by 2 * 1 and 2 * 2 --> a: [[2], [1]]

    --> x: [[4, 4], [4, 4]]

    output array by 2 * 2 --> x: [[8, 8], [4, 4]]
    @endverbatim
 */
CNNL_DEPRECATED_FOR(cnnlOpTensor)
cnnlStatus_t CNNL_WIN_API cnnlAx_v2(cnnlHandle_t handle,
                                    const cnnlTensorDescriptor_t a_desc,
                                    const void *a,
                                    const cnnlTensorDescriptor_t x_desc,
                                    void *x,
                                    void *workspace,
                                    size_t workspace_size);


// Group:Axpy
/*!
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlOpTensor instead.
 *
 *  @brief Multiplies the input tensor \p a to the input tensor \p x, then adds input tensor \p y.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *    axpy operation. For detailed information, see ::cnnlHandle_t.
 *  @param[in] a_desc
 *    Input. Descriptor of input data \p a, including dimension, data type (half and float),
 *    and data layout.
 *  @param[in] a
 *    Input. Pointer to the MLU memory that stores the input tensor \p a.
 *  @param[in] x_desc
 *    Input. Descriptor of input data \p x, including dimension, data type (half and float),
 *    and data layout.
 *  @param[in,out] x
 *    Input and Output. Pointer to the MLU memory that stores the input and output tensor \p x.
 *  @param[in] y_desc
 *    Input. Descriptor of input data \p y, including dimension, data type (half and float),
 *    and data layout.
 *  @param[in] y
 *    Input. Pointer to the MLU memory that stores the input tensor \p y.
 *  @param[in] workspace
 *    Input. Pointer to the MLU memory that is used as an extra workspace for the axpy operation.
 *    For more information about workspace, see "Cambricon CNNL User Guide".
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @par Formula
 *  - \p x = \p a * \p x + \p y
 *  - See "Axpy Operation" section in "Cambricon CNNL User Guide" for details.
 *
 *  @par Data Type
 *  - This function supports the following data types for input tensor \p a, \p x and \p y.
 *    Data type of all above tensors should be the same.
 *    - all above tensors: half, float.
 *
 *  @note
 *  - This operation supports broadcasting. Each dimension of the input tensor \p a must match the
 *    corresponding dimension of the tensor \p x, or equal to 1.
 *  - The tensor \p x and \p y must have the same shape.
 *  - If tensor \p a does not need broadcasting, the workspace can be NULL.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - The example of the axpy operation is as follows:
    @verbatim
    input three arrays by 2 * 1, 2 * 2 and 2 * 2 --> a: [[2], [1]]

    --> x: [[4, 4], [4, 4]]

    --> y: [[6, 6], [6, 6]]

    output array by 2 * 2 --> x: [[14, 14], [10, 10]]
    @endverbatim
 */
CNNL_DEPRECATED_FOR(cnnlOpTensor)
cnnlStatus_t CNNL_WIN_API cnnlAxpy(cnnlHandle_t handle,
                                   const cnnlTensorDescriptor_t a_desc,
                                   const void *a,
                                   const cnnlTensorDescriptor_t x_desc,
                                   void *x,
                                   const cnnlTensorDescriptor_t y_desc,
                                   const void *y,
                                   void *workspace);

// Group:Axpy
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace to
 * optimize the axpy operation.
 *
 * The size of the extra workspace is based on the given information of the axpy operation,
 * including the input tensor descriptor \p a_desc. For more information about the workspace,
 * see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   axpy operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] a_desc
 *   Input. Descriptor of input data \p a, including dimension, data type (half and float),
 *   and data layout.
 * @param[in] x_desc
 *   Input. Descriptor of input data \p x, including dimension, data type (half and float),
 *   and data layout.
 * @param[in] y_desc
 *   Input. Descriptor of input data \p y, including dimension, data type (half and float),
 *   and data layout.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *   the axpy operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetAxpyWorkspaceSize(cnnlHandle_t handle,
                                                   const cnnlTensorDescriptor_t a_desc,
                                                   const cnnlTensorDescriptor_t x_desc,
                                                   const cnnlTensorDescriptor_t y_desc,
                                                   size_t *size);

// Group:Axpy
/*!
 *  @brief Multiplies the input tensor \p a to the input tensor \p x, then adds input tensor \p y.
 *  Compared with ::cnnlAxpy, this function allows you to check the validation of \p workspace_size.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlOpTensor instead.

 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *    axpy operation. For detailed information, see ::cnnlHandle_t.
 *  @param[in] a_desc
 *    Input. Descriptor of input data \p a, including dimension, data type (half and float),
 *    and data layout.
 *  @param[in] a
 *    Input. Pointer to the MLU memory that stores the input tensor \p a.
 *  @param[in] x_desc
 *    Input. Descriptor of input data \p x, including dimension, data type (half and float),
 *    and data layout.
 *  @param[in,out] x
 *    Input and Output. Pointer to the MLU memory that stores the input and output tensor \p x.
 *  @param[in] y_desc
 *    Input. Descriptor of input data \p y, including dimension, data type (half and float),
 *    and data layout.
 *  @param[in] y
 *    Input. Pointer to the MLU memory that stores the input tensor \p y.
 *  @param[in] workspace
 *    Input. Pointer to the MLU memory that is used as an extra workspace for the axpy operation.
 *    For more information about workspace, see "Cambricon CNNL User Guide".
 *  @param[in] workspace_size
 *    Input. The size of the extra workspace in bytes that needs to be used in the axpy operation.
 *    You can get the size of the workspace with the ::cnnlGetAxpyWorkspaceSize function.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @par Formula
 *  - \p x = \p a * \p x + \p y
 *  - See "Axpy Operation" section in "Cambricon CNNL User Guide" for details.
 *
 *  @par Data Type
 *  - This function supports the following data types for input tensor \p a, \p x and \p y.
 *    Data type of all above tensors should be the same.
 *    - all above tensors: half, float.
 *
 *  @note
 *  - This operation supports broadcasting. Each dimension of the input tensor \p a must match the
 *    corresponding dimension of the tensor \p x, or equal to 1.
 *  - The tensor \p x and \p y must have the same shape.
 *  - If tensor \p a does not need broadcasting, the workspace can be NULL.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - The example of the axpy operation is as follows:
    @verbatim
    input three arrays by 2 * 1, 2 * 2 and 2 * 2 --> a: [[2], [1]]

    --> x: [[4, 4], [4, 4]]

    --> y: [[6, 6], [6, 6]]

    output array by 2 * 2 --> x: [[14, 14], [10, 10]]
    @endverbatim
 */
CNNL_DEPRECATED_FOR(cnnlOpTensor)
cnnlStatus_t CNNL_WIN_API cnnlAxpy_v2(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t a_desc,
                                      const void *a,
                                      const cnnlTensorDescriptor_t x_desc,
                                      void *x,
                                      const cnnlTensorDescriptor_t y_desc,
                                      const void *y,
                                      void *workspace,
                                      size_t workspace_size);

// Group:Axpby
/*!
 * @deprecated
 * This function is deprecated and will be removed in future release. Use ::cnnlOpTensor instead.
 *
 *  @brief Multiplies the input tensors \p a and \p b to tensors \p x and \p y separately,
 *  then adds the results.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *    axpby operation. For detailed information, see ::cnnlHandle_t.
 *  @param[in] a_desc
 *    Input. Descriptor of input data \p a, including dimension, data type (half and float),
 *    and data layout.
 *  @param[in] a
 *    Input. Pointer to the MLU memory that stores the input tensor \p a.
 *  @param[in] x_desc
 *    Input. Descriptor of input data \p x, including dimension, data type (half and float),
 *    and data layout.
 *  @param[in,out] x
 *    Input and Output. Pointer to the MLU memory that stores the input and output tensor \p x.
 *  @param[in] b_desc
 *    Input. Descriptor of input data \p b, including dimension, data type (half and float),
 *    and data layout.
 *  @param[in] b
 *    Input. Pointer to the MLU memory that stores the input tensor \p b.
 *  @param[in] y_desc
 *    Input. Descriptor of input data \p y, including dimension, data type (half and float),
 *    and data layout.
 *  @param[in] y
 *    Input. Pointer to the MLU memory that stores the input tensor \p y.
 *  @param[in] workspace
 *    Input. Pointer to the MLU memory that is used as an extra workspace for the axpby operation.
 *    For more information about workspace, see "Cambricon CNNL User Guide".
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @par Formula
 *  - \p x = \p a * \p x + \p b * \p y
 *  - See "Axpby Operation" section in "Cambricon CNNL User Guide" for details.
 *
 *  @par Data Type
 *  - This function supports the following data types for input tensor \p a, \p x, \p b and \p y.
 *    Data type of all above tensors should be the same.
 *    - all above tensors: half, float.
 *
 *  @note
 *  - This operation supports broadcasting. Each dimension of the input tensor \p a must match the
 *    corresponding dimension of the tensor \p x, or equal to 1. The same rule applies to input
 *    tensors \p b and \p y.
 *  - The tensor \p x and \p y must have the same shape.
 *  - If tensors \p a and \p b does not need broadcasting, the workspace can be NULL.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - The example of the axpby operation is as follows:
    @verbatim
    input three arrays by 2 * 1, 2 * 2, 1 * 2 and 2 * 2 --> a: [[2], [1]]

    --> x: [[4, 4], [4, 4]]

    --> b: [[2, 1]]

    --> y: [[6, 6], [6, 6]]

    output array by 2 * 2 --> x: [[20, 14], [16, 10]]
    @endverbatim
 */
CNNL_DEPRECATED_FOR(cnnlOpTensor)
cnnlStatus_t CNNL_WIN_API cnnlAxpby(cnnlHandle_t handle,
                                    const cnnlTensorDescriptor_t a_desc,
                                    const void *a,
                                    const cnnlTensorDescriptor_t x_desc,
                                    void *x,
                                    const cnnlTensorDescriptor_t b_desc,
                                    const void *b,
                                    const cnnlTensorDescriptor_t y_desc,
                                    const void *y,
                                    void *workspace);

// Group:Axpby
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace to
 * optimize the axpby operation.
 *
 * The size of the extra workspace is based on the given information of the axpby operation,
 * including the input tensor descriptors \p a_desc and \p b_desc. For more information about the
 * workspace, see "Cambricon CNNL User Guide".
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   axpby operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] a_desc
 *   Input. Descriptor of input data \p a, including dimension, data type (half and float),
 *   and data layout.
 * @param[in] x_desc
 *   Input. Descriptor of input data \p x, including dimension, data type (half and float),
 *   and data layout.
 * @param[in] b_desc
 *   Input. Descriptor of input data \p b, including dimension, data type (half and float),
 *   and data layout.
 * @param[in] y_desc
 *   Input. Descriptor of input data \p y, including dimension, data type (half and float),
 *   and data layout.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *   the axpby operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlGetAxpbyWorkspaceSize(cnnlHandle_t handle,
                                                    const cnnlTensorDescriptor_t a_desc,
                                                    const cnnlTensorDescriptor_t x_desc,
                                                    const cnnlTensorDescriptor_t b_desc,
                                                    const cnnlTensorDescriptor_t y_desc,
                                                    size_t *size);

// Group:Axpby
/*!
 *  @brief Multiplies the input tensors \p a and \p b to tensors \p x and \p y separately,
 *  then adds the results. Compared with ::cnnlAxpby, this function allows you to check the
 *  validation of \p workspace_size.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release. Use ::cnnlOpTensor instead.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *    axpby operation. For detailed information, see ::cnnlHandle_t.
 *  @param[in] a_desc
 *    Input. Descriptor of input data \p a, including dimension, data type (half and float),
 *    and data layout.
 *  @param[in] a
 *    Input. Pointer to the MLU memory that stores the input tensor \p a.
 *  @param[in] x_desc
 *    Input. Descriptor of input data \p x, including dimension, data type (half and float),
 *    and data layout.
 *  @param[in,out] x
 *    Input and Output. Pointer to the MLU memory that stores the input and output tensor \p x.
 *  @param[in] b_desc
 *    Input. Descriptor of input data \p b, including dimension, data type (half and float),
 *    and data layout.
 *  @param[in] b
 *    Input. Pointer to the MLU memory that stores the input tensor \p b.
 *  @param[in] y_desc
 *    Input. Descriptor of input data \p y, including dimension, data type (half and float),
 *    and data layout.
 *  @param[in] y
 *    Input. Pointer to the MLU memory that stores the input tensor \p y.
 *  @param[in] workspace
 *    Input. Pointer to the MLU memory that is used as an extra workspace for the axpby operation.
 *    For more information about workspace, see "Cambricon CNNL User Guide".
 *  @param[in] workspace_size
 *    Input. The size of the extra workspace in bytes that needs to be used in the axpby operation.
 *    You can get the size of the workspace with the ::cnnlGetAxpbyWorkspaceSize function.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @par Formula
 *  - \p x = \p a * \p x + \p b * \p y
 *  - See "Axpby Operation" section in "Cambricon CNNL User Guide" for details.
 *
 *  @par Data Type
 *  - This function supports the following data types for input tensor \p a, \p x, \p b and \p y.
 *    Data type of all above tensors should be the same.
 *    - all above tensors: half, float.
 *
 *  @note
 *  - This operation supports broadcasting. Each dimension of the input tensor \p a must match the
 *    corresponding dimension of the tensor \p x, or equal to 1. The same rule applies to input
 *    tensors \p b and \p y.
 *  - The tensor \p x and \p y must have the same shape.
 *  - If tensors \p a and \p b does not need broadcasting, the workspace can be NULL.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - The example of the axpby operation is as follows:
    @verbatim
    input three arrays by 2 * 1, 2 * 2, 1 * 2 and 2 * 2 --> a: [[2], [1]]

    --> x: [[4, 4], [4, 4]]

    --> b: [[2, 1]]

    --> y: [[6, 6], [6, 6]]

    output array by 2 * 2 --> x: [[20, 14], [16, 10]]
    @endverbatim
 */
CNNL_DEPRECATED_FOR(cnnlOpTensor)
cnnlStatus_t CNNL_WIN_API cnnlAxpby_v2(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t a_desc,
                                       const void *a,
                                       const cnnlTensorDescriptor_t x_desc,
                                       void *x,
                                       const cnnlTensorDescriptor_t b_desc,
                                       const void *b,
                                       const cnnlTensorDescriptor_t y_desc,
                                       const void *y,
                                       void *workspace,
                                       size_t workspace_size);

// Group:Caxpby
/*!
 *  @brief Multiplies coefficient scalar \p alpha and \p beta to tensors \p x and \p y separately,
 *  then adds the results.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release. Use ::cnnlOpTensor instead.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *    caxpby operation. For detailed information, see ::cnnlHandle_t.
 *  @param[in] alpha
 *    Input. A host pointer to scaling factor of tensor \p x. The default value is 1.0.
 *  @param[in] x_desc
 *    Input. Descriptor of input data \p x, including dimension, data type (half and float),
 *    and data layout.
 *  @param[in,out] x
 *    Input and Output. Pointer to the MLU memory that stores the input and output tensor \p x.
 *  @param[in] beta
 *    Input. A host pointer to scaling factor of tensor \p y. The default value is 1.0.
 *  @param[in] y_desc
 *    Input. Descriptor of input data \p y, including dimension, data type (half and float),
 *    and data layout.
 *  @param[in] y
 *    Input. Pointer to the MLU memory that stores the input tensor \p y.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @par Formula
 *  - \p x = \p alpha * \p x + \p beta * \p y
 *  - See "Caxpby Operation" section in "Cambricon CNNL User Guide" for details.
 *
 *  @par Data Type
 *  - This function supports the following data types for input tensor \p x and \p y.
 *    Data type of all above tensors should be the same.
 *    - all above tensors: half, float.
 *  - Data type of \p alpha and \p beta must be float.
 *
 *  @note
 *  - The tensor \p x and \p y must have the same shape.
 *  - This operation is not supported on the 1V platforms.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - The example of the caxpby operation is as follows:
     @verbatim
     input two arrays by 2 * 2 and 2 * 2 --> x: [[4, 4], [4, 4]]
     --> y: [[6, 6], [6, 6]]

     alpha: 2.0
     beta: 3.0

     output array by 2 * 2 --> x: [[26, 26], [26, 26]]
     @endverbatim
 */
CNNL_DEPRECATED_FOR(cnnlOpTensor)
cnnlStatus_t CNNL_WIN_API cnnlCaxpby(cnnlHandle_t handle,
                                     const float *alpha,
                                     const cnnlTensorDescriptor_t x_desc,
                                     void *x,
                                     const float *beta,
                                     const cnnlTensorDescriptor_t y_desc,
                                     const void *y);

// Group:Addcdiv
/*!
 * @brief Performs the addition and division operations with the following formula:
 *
 * \p a + \p alpha * \p b / \p c
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the addcdiv
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] desc_a
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] alpha
 *   Input. A float value that scales the result of division.
 * @param[in] desc_b
 *   Input. The descriptor of the dividend tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. Pointer to the MLU memory that stores the dividend tensor.
 * @param[in] desc_c
 *   Input. The descriptor of the divisor tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] c
 *   Input. Pointer to the MLU memory that stores the divisor tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the addcdiv operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the addcdiv
 *   operation. You can get the size of the workspace with the ::cnnlGetAddcdivWorkspaceSize_v2
 *   function.
 * @param[in] desc_output
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Addcdiv Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \p a, dividend tensor \p b,
 *   divisor tensor \p c, and output tensor \p output. Data type of all above tensors should be the
 *   same.
 *   - all above tensors: half, float, bfloat16.
 *
 * @par Data Layout
 * - None.
 *
 * @par Scale Limitation
 * - The input tensor, dividend tensor, divisor tensor, and output tensor must meet the following
 *   requirements:
 *   - input tensor: Every dimension should be divisible by the same dimension in output tensor.
 *   - dividend tensor: Every dimension should be divisible by the same dimension in output tensor.
 *   - divisor tensor: Every dimension should be divisible by the same dimension in output tensor.
 *
 * @note
 * - You can specify the stride of all dimensions for desc_a, desc_b, desc_c and
 *   desc_output with ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the addcdiv operation is as follows:
     @verbatim
     input three arrays by 2 * 2, 1 * 2 and 2 * 1 --> a: [[1, 1], [1, 1]]

     --> b: [4, 4]

     --> c: [[2], [2]]

     param:
       alpha: 0.5

     output array by 2 * 2 --> output: [[2, 2], [2, 2]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/1.0.0/torch.html?highlight=std#torch.addcdiv
 */
cnnlStatus_t CNNL_WIN_API cnnlAddcdiv(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t desc_a,
                                      const void *a,
                                      const void *alpha,
                                      const cnnlTensorDescriptor_t desc_b,
                                      const void *b,
                                      const cnnlTensorDescriptor_t desc_c,
                                      const void *c,
                                      void *workspace,
                                      size_t workspace_size,
                                      const cnnlTensorDescriptor_t desc_output,
                                      void *output);

// Group:Addcdiv
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace to
 * optimize the addcdiv operation.
 *
 * The size of the extra workspace is based on the given information of the addcdiv operation,
 * including the input tensor descriptor \p desc_a, dividend tensor descriptor \p desc_b, and
 * divisor tensor descriptor \p desc_c. For more information about the workspace, see
 * "Cambricon CNNL User Guide".
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGetAddcdivWorkspaceSize_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the addcdiv
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] desc_a
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] desc_b
 *   Input. The descriptor of the dividend tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] desc_c
 *   Input. The descriptor of the divisor tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   addcdiv operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par API Dependency
 * - You need to call the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions to
 *   create and set the tensor descriptors \p desc_a, \p desc_b, and \p desc_c before calling this
 *   function.
 * - The allocated extra workspace should be passed to the ::cnnlAddcdiv function to perform the
 *   addcdiv operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */

CNNL_DEPRECATED_FOR(cnnlGetAddcdivWorkspaceSize_v2)
cnnlStatus_t CNNL_WIN_API cnnlGetAddcdivWorkspaceSize(cnnlHandle_t handle,
                                                      const cnnlTensorDescriptor_t desc_a,
                                                      const cnnlTensorDescriptor_t desc_b,
                                                      const cnnlTensorDescriptor_t desc_c,
                                                      size_t *size);
// Group:Addcdiv
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace to
 * optimize the addcdiv operation.
 *
 * The size of the extra workspace is based on the given information of the addcdiv operation,
 * including input tensor descriptor \p desc_a, dividend tensor descriptor \p desc_b,
 * divisor tensor descriptor \p desc_c and output tensor descriptor \p desc_output. For more information
 * about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the addcdiv
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] desc_a
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] desc_b
 *   Input. The descriptor of the dividend tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] desc_c
 *   Input. The descriptor of the divisor tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] desc_output
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   addcdiv operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par API Dependency
 * - Before calling this function, you need to call the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor
 *   functions to create and set tensor descriptors \p desc_a, \p desc_b, and \p desc_c.
 * - The allocated extra workspace should be passed to the ::cnnlAddcdiv function to perform the
 *   addcdiv operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetAddcdivWorkspaceSize_v2(cnnlHandle_t handle,
                                                         const cnnlTensorDescriptor_t desc_a,
                                                         const cnnlTensorDescriptor_t desc_b,
                                                         const cnnlTensorDescriptor_t desc_c,
                                                         const cnnlTensorDescriptor_t desc_output,
                                                         size_t *size);

// Group:EmbeddingBackward
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the embedding backward operation.
 *
 * The size of the extra workspace is determined based on the given information of the embedding
 * backward operation, including the input tensor descriptor \p diff, output tensor descriptor
 * \p output, and the parameter \p scale_grad_by_freq. For more information about the workspace,
 * see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the embedding
 *   backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] diff_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] scale_grad_by_freq
 *   Input. A Boolean value that determines whether to scale output tensor \p output by the
 *   inverse of frequency of the index.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   embedding backward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetEmbeddingBackwardWorkspaceSize(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t diff_desc,
                                      const cnnlTensorDescriptor_t output_desc,
                                      bool scale_grad_by_freq,
                                      size_t *workspace_size);

// Group:EmbeddingBackward
/*!
 * @brief Computes gradients of embedding.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the embedding
 *   backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] padding_idx
 *   Input. Determines which index of the embedding vector \p output should be initialized to zero.
 * @param[in] scale_grad_by_freq
 *   Input. A Boolean value that determines whether to scale output tensor \p output by the
 *   inverse of frequency of the index.
 * @param[in] indices_desc
 *   Input. The descriptor of the index tensor used to store index of each row of \p output in
 *   \p diff. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] indices
 *   Input. Pointer to the MLU memory that stores the tensor used to store index of each row of
 *   \p output in \p diff.
 * @param[in] diff_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] diff
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the embedding
 *   backward operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the embedding
 *   backward operation. You can get the size of the workspace with the
 *   ::cnnlGetEmbeddingBackwardWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH,
 *   ::CNNL_STATUS_NOT_SUPPORTED, ::CNNL_STATUS_INTERNAL_ERROR, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @par Formula
 * - See "EmbeddingBackward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for index tensor
 *   \p indices, input tensor \p diff, and output tensor \p output.
 *
 *   Note that the data type of input tensor and output tensor must be the same.
 *   - index tensor: int32.
 *   - input tensor: half, float, bfloat16.
 *   - output tensor: half, float, bfloat16.
 *
 * @par Note
 * - The value of \p indices must be in range of [\f$-2^{24}\f$, \f$2^{24}\f$].
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the embedding backward operation is as follows:
     @verbatim
     input two arrays by 2 * 2 and 2 * 2 * 3 --> indices: [[5, 1], [6, 5]]

     --> diff: [[[ 0.5356,  1.5739, -0.4864], [-0.6622, -0.4790,  0.8539]],
                [[-0.2285,  0.3081,  1.1171], [ 0.1585, -0.8696,  1.8683]]]

     param:
       padding_idx: 0, scale_grad_by_freq: false

     output array by 10 * 3 -->
         output: [[ 0.0000,  0.0000,  0.0000],
                  [-0.6622, -0.4790,  0.8539],
                  [ 0.0000,  0.0000,  0.0000],
                  [ 0.0000,  0.0000,  0.0000],
                  [ 0.0000,  0.0000,  0.0000],
                  [ 0.6941,  0.7042,  1.3819],
                  [-0.2285,  0.3081,  1.1171],
                  [ 0.0000,  0.0000,  0.0000],
                  [ 0.0000,  0.0000,  0.0000],
                  [ 0.0000,  0.0000,  0.0000]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html
 */
cnnlStatus_t CNNL_WIN_API cnnlEmbeddingBackward(cnnlHandle_t handle,
                                                int padding_idx,
                                                bool scale_grad_by_freq,
                                                const cnnlTensorDescriptor_t indices_desc,
                                                const void *indices,
                                                const cnnlTensorDescriptor_t diff_desc,
                                                const void *diff,
                                                void *workspace,
                                                size_t workspace_size,
                                                const cnnlTensorDescriptor_t output_desc,
                                                void *output);

// Group:Expand
/*!
 * @brief Copies and expands the input tensor \p input to the shape of output tensor \p output.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the expand
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Expand Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The (I/O)function supports the following data widths for \p input and \p output tensors.
 *   - input tensor: 1-byte, 2-byte, 4-byte, 8-byte.
 *   - output tensor: 1-byte, 2-byte, 4-byte, 8-byte.
 *
 *   The byte width of a data type can be got with the ::cnnlGetSizeOfDataType function.
 *
 *   Note that the data type of input tensor \p input and output tensor \p output must be the same.
 *
 * @par Data Layout
 * - None.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor must meet the following requirements:
 *   - Every dimension of the input tensor should be divisible by the same dimension of the output
 *     tensor.
 *
 * @note
 * - The input tensor \p input and output tensor \p output are multi-dimensional arrays, supporting
 *   up to \p CNNL_DIM_MAX dimensions.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the expand operation is as follows:
     @verbatim
     input one array by 2 * 2 --> input: [[1, 2], [3, 4]]

     output array by 3 * 2 * 2 --> output: [[[1, 2], [3, 4]],
                                            [[1, 2], [3, 4]],
                                            [[1, 2], [3, 4]]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/tensors.html#torch.Tensor.expand
 */
cnnlStatus_t CNNL_WIN_API cnnlExpand(cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const void *input,
                                     const cnnlTensorDescriptor_t output_desc,
                                     void *output);
// Group:Masked
/*!
 * @brief Fills the input tensor \p input with the specified value of the tensor \p value based on
 * the masked tensor \p masked.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlMasked_v5 instead, which supports input tensor or masked tensor broadcasting
 *   and supports setting \p CNNL_MASKED_SELECT and \p CNNL_MASKED_SCALE modes for parameter \p masked_mode.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the masked
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] masked_mode
 *   Input. The masked mode used to compute the masked operation. The mode is defined in
 *   ::cnnlMaskedOp_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor \p input.
 * @param[in] masked_desc
 *   Input. The descriptor of the masked tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] masked
 *   Input. Pointer to the MLU memory that stores the masked tensor.
 * @param[in] value_desc
 *   Input. The descriptor of the input tensor \p value to fill into \p input. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] value
 *   Input. Pointer to the MLU memory that stores the input tensor \p value to fill into \p input.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Masked Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor \p input, value tensor \p value and output tensor \p output must be the
 *   same. The supported data types of input tensor \p input, masked tensor \p masked, value tensor
 *   \p value and output tensor \p output are as follows:
 *   - When the \p masked_mode is \p CNNL_MASKED_FILL:
 *     - input tensor: bool, int8, int16, int32, half, bfloat16, float.
 *     - masked tensor: uint8, int8, bool.
 *     - value tensor: bool, int8, int16, int32, half, bfloat16, float.
 *     - output tensor: bool, int8, int16, int32, half, bfloat16, float.
 *   - When the \p masked_mode is \p CNNL_MASKED_SCATTER:
 *     - input tensor: half, float, int64.
 *     - masked tensor: uint8, int8, bool.
 *     - value tensor: half, float, int64.
 *     - output tensor: half, float, int64.
 *
 * @par Data Layout
 * - The supported data layout of the input, masked, value and output tensors must be
 *   \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 *
 * @note
 * - The masked tensor \p masked does not support broadcast.
 * - The shape of \p masked must be the same as the shape of the \p input.
 * - You can specify the stride of all dimensions for \p input_desc, \p masked_desc, \p value_desc and \p output_desc
 *   with ::cnnlSetTensorDescriptorEx.
 *   And the stride of mask is not supported when the number of elements in the input tensor is not equal to that in the
 *   masked tensor in \p CNNL_MASKED_SCATTER.
 * - The input tensor \p input, masked tensor \p masked and output tensor \p output are multi-dimensional
 *   array, supporting up to \p CNNL_DIM_MAX dimensions.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the masked operation is as follows:
     @verbatim
       Example 1:
       input three arrays by 1 * 1 * 2 * 3, 1 * 1 * 2 * 3 and 1 -->
         input: [[[[1,2,3],[4,5,6]]]]

         masked: [[[[0,0,1],[1,0,0]]]]

         value: [-1,]

       param:
       masked_mode: CNNL_MASKED_FILL

       output array by 1 * 1 * 2 * 3 --> [[[[1,2,-1],[-1,5,6]]]]

       Example 2:
       input three arrays by 1 * 1 * 2 * 3, 1 * 1 * 2 * 3 and 3 -->
         input: [[[[1,2,3],[4,5,6]]]]

         masked: [[[[0,0,1],[1,0,0]]]]

         value: [5,7,8]

       param:
       masked_mode: CNNL_MASKED_SCATTER

       output array by 1 * 1 * 2 * 3 --> [[[[1,2,5],[7,5,6]]]]
     @endverbatim
 *
 * @par Reference
 * - http://github.com/pytorch/pytorch/blob/master/aten/src/THC/THCTensorMasked.cuh
 *
 */

CNNL_DEPRECATED_FOR(cnnlMasked_v5)
cnnlStatus_t CNNL_WIN_API cnnlMasked(cnnlHandle_t handle,
                                     cnnlMaskedOp_t masked_mode,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const void *input,
                                     const cnnlTensorDescriptor_t masked_desc,
                                     const void *masked,
                                     const cnnlTensorDescriptor_t value_desc,
                                     const void *value,
                                     const cnnlTensorDescriptor_t output_desc,
                                     void *output);

// Group:Masked
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the masked operation.
 *
 * The size of the extra workspace is based on the given information of the masked operation,
 * including the tensor descriptors \p input_desc, \p masked_desc, \p value_desc and \p output_desc.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the masked
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] masked_mode
 *   Input. The masked mode used to compute the masked operation. The mode are defined in
 *   ::cnnlMaskedOp_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] masked_desc
 *   Input. The descriptor of the masked tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] value_desc
 *   Input. The descriptor of the value tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the masked
 *   operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Layout
 * - The supported data layout of the input, masked, value and output tensors must be
 *   \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - According to the rule of tensor broadcast, the parameters should satisfy the following
 *   conditions.
 *
 *   Take the lowest dimension for example, c1_dim and c2_dim represent the dimensions of \p x
 *   and \p y, c3_dim represents the dimension of \p z:
 *
 *   - min(c1_dim, c2_dim) == 1 or c1_dim == c2_dim
 *
 *   - max(c1_dim, c2_dim) == c3_dim
 *
 * @note
 * - When the \p masked_mode is set to \p CNNL_MASKED_SELECT or \p CNNL_MASKED_SCALE,
 *   the tensor \p value_desc should be null pointer.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetMaskedWorkspaceSize(cnnlHandle_t handle,
                                                     cnnlMaskedOp_t masked_mode,
                                                     const cnnlTensorDescriptor_t input_desc,
                                                     const cnnlTensorDescriptor_t masked_desc,
                                                     const cnnlTensorDescriptor_t value_desc,
                                                     const cnnlTensorDescriptor_t output_desc,
                                                     size_t *workspace_size);
// Group:Masked
/*!
 * @brief Fills the input tensor \p input with the specified value of the tensor \p value based on
 * the masked tensor \p masked.
 *
 * This function may need extra MLU memory as the workspace to improve the masked performance.
 * You can get the workspace size with the ::cnnlGetMaskedWorkspaceSize
 * function.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlMasked_v5 instead, which supports setting \p CNNL_MASKED_SELECT mode and \p CNNL_MASKED_SCALE
 *   mode for parameter \p masked_mode.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the masked
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] masked_mode
 *   Input. The masked mode used to compute the masked operation. The mode is defined in
 *   ::cnnlMaskedOp_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor \p input.
 * @param[in] masked_desc
 *   Input. The descriptor of the masked tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] masked
 *   Input. Pointer to the MLU memory that stores the masked tensor.
 * @param[in] value_desc
 *   Input. The descriptor of the input tensor \p value to fill into \p input. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] value
 *   Input. Pointer to the MLU memory that stores the input tensor \p value to fill into \p input.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the masked
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the masked
 *   operation. You can get the size of the workspace with the ::cnnlGetMaskedWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par API Dependency
 * - You need to call the ::cnnlGetMaskedWorkspaceSize function to allocate extra
 *   workspace for \p workspace.
 *
 * @par Formula
 * - See "Masked Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor \p input, value tensor \p value and output tensor \p output must be the
 *   same. The supported data types of input tensor \p input, masked tensor \p masked, value tensor
 *   \p value and output tensor \p output are as follows:
 *   - When the \p masked_mode is \p CNNL_MASKED_FILL:
 *     - input tensor: bool, int8, int16, int32, half, bfloat16, float.
 *     - masked tensor: uint8, int8, bool.
 *     - value tensor: bool, int8, int16, int32, half, bfloat16, float.
 *     - output tensor: bool, int8, int16, int32, half, bfloat16, float.
 *   - When the \p masked_mode is \p CNNL_MASKED_SCATTER:
 *     - input tensor: half, bfloat16, float, int64.
 *     - masked tensor: uint8, int8, bool.
 *     - value tensor: half, bfloat16, float, int64.
 *     - output tensor: half, bfloat16, float, int64.
 *
 * @par Data Layout
 * - The supported data layout of the input, masked, value and output tensors must be
 *   \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - When the \p masked_mode is set to \p CNNL_MASKED_FILL, the input tensor \p input and masked tensor
 *   \p masked support broadcast. According to the rule of tensor broadcast, the parameters should
 *   satisfy the following conditions. For example, c1_dim, c2_dim, and c3_dim represent the lowest
 *   dimension of \p input, \p masked, \p output respectively:
 *   - min(c1_dim, c2_dim) == 1 or c1_dim == c2_dim.
 *   - max(c1_dim, c2_dim) == c3_dim.
 *
 * @note
 * - When the \p masked_mode is set to \p CNNL_MASKED_SCATTER, the shape of \p input must be the same as the shape
 *   of the \p output.
 * - You can specify the stride of all dimensions for \p input_desc, \p masked_desc, \p value_desc and \p output_desc
 *   with ::cnnlSetTensorDescriptorEx.
 *   And the stride of mask is not supported when the number of elements in the input tensor is not equal to that in the
 *   masked tensor in \p CNNL_MASKED_SCATTER.
 * - The input tensor \p input, masked tensor \p masked and output tensor \p output are multi-dimensional
 *   array, supporting up to \p CNNL_DIM_MAX dimensions.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the masked operation is as follows:
     @verbatim
       Example 1:
       input three arrays by 1 * 1 * 2 * 3, 1 * 1 * 2 * 3 and 1 -->
         input: [[[[1,2,3],[4,5,6]]]]

         masked: [[[[0,0,1],[1,0,0]]]]

         value: [-1,]

       param:
       masked_mode: CNNL_MASKED_FILL

       output array by 1 * 1 * 2 * 3 --> [[[[1,2,-1],[-1,5,6]]]]

       Example 2:
       input three arrays by 1 * 1 * 2 * 3, 1 * 1 * 2 * 3 and 3 -->
         input: [[[[1,2,3],[4,5,6]]]]

         masked: [[[[0,0,1],[1,0,0]]]]

         value: [5,7,8]

       param:
       masked_mode: CNNL_MASKED_SCATTER

       output array by 1 * 1 * 2 * 3 --> [[[[1,2,5],[7,5,6]]]]
     @endverbatim
 *
 * @par Reference
 * - http://github.com/pytorch/pytorch/blob/master/aten/src/THC/THCTensorMasked.cuh
 *
 */
CNNL_DEPRECATED_FOR(cnnlMasked_v5)
cnnlStatus_t CNNL_WIN_API cnnlMasked_v2(cnnlHandle_t handle,
                                        cnnlMaskedOp_t masked_mode,
                                        const cnnlTensorDescriptor_t input_desc,
                                        const void *input,
                                        const cnnlTensorDescriptor_t masked_desc,
                                        const void *masked,
                                        const cnnlTensorDescriptor_t value_desc,
                                        const void *value,
                                        void *workspace,
                                        size_t workspace_size,
                                        const cnnlTensorDescriptor_t output_desc,
                                        void *output);
// Group:Masked
/*!
 * @brief Fills the input tensor \p input with the specified value of the tensor \p value based on
 * the masked tensor \p masked, or selects the input value of tensor \p input based on the masked
 * tensor \p masked.
 *
 * Compared with ::cnnlMasked, this function supports input tensor and masked tensor to broadcast.
 * Compared with ::cnnlMasked_v2, this function supports \p CNNL_MASKED_SELECT mode.
 *
 * This function may need extra MLU memory as the workspace to improve the operation performance.
 * You can get the workspace size with the ::cnnlGetMaskedWorkspaceSize
 * function.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlMasked_v5 instead, which supports setting \p CNNL_MASKED_SCALE mode for parameter
 *   \p masked_mode.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the masked
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] masked_mode
 *   Input. The masked mode used to compute the masked operation. The mode is defined in
 *   ::cnnlMaskedOp_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor \p input.
 * @param[in] masked_desc
 *   Input. The descriptor of the masked tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] masked
 *   Input. Pointer to the MLU memory that stores the masked tensor.
 * @param[in] value_desc
 *   Input. The descriptor of the tensor \p value to fill into \p input. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] value
 *   Input. Pointer to the MLU memory that stores the tensor \p value to fill into \p input.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the masked
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the masked
 *   operation. You can get the size of the workspace with the ::cnnlGetMaskedWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[out] number
 *   Output. Pointer to the MLU memory that stores the length of output selected data.

 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par API Dependency
 * - Before calling this function, you need to call the ::cnnlGetMaskedWorkspaceSize function to
 *   allocate extra workspace for \p workspace.
 *
 * @par Formula
 * - See "Masked Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor \p input, value tensor \p value and output tensor \p output must be the
 *   same. The supported data types of input tensor \p input, masked tensor \p masked, value tensor
 *   \p value and output tensor \p output are as follows:
 *   - When the \p masked_mode is \p CNNL_MASKED_FILL:
 *     - input tensor: bool, int8, int16, int32, half, bfloat16, float.
 *     - masked tensor: uint8, int8, bool.
 *     - value tensor: bool, int8, int16, int32, half, bfloat16, float.
 *     - output tensor: bool, int8, int16, int32, half, bfloat16, float.
 *   - When the \p masked_mode is \p CNNL_MASKED_SCATTER:
 *     - input tensor: half, bfloat16, float, int64.
 *     - masked tensor: uint8, int8, bool.
 *     - value tensor: half, bfloat16, float, int64.
 *     - output tensor: half, bfloat16, float, int64.
 *   - When the \p masked_mode is \p CNNL_MASKED_SELECT:
 *     - input tensor: bool, int8, uint8, int16, int32, bfloat16, half, float.
 *     - masked tensor: bool, half, bfloat16, float.
 *     - output tensor: bool, int8, uint8, int16, int32, bfloat16, half, float.
 *     - And if the type of masked tensor is not bool, its type should be the same as the input tensor.
 *
 * @par Data Layout
 * - The supported data layout of the input, masked, value and output tensors must be
 *   \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - When the \p masked_mode is set to \p CNNL_MASKED_FILL or \p CNNL_MASKED_SELECT, the input tensor
 *   \p input and masked tensor \p masked support broadcasting. According to the rule of tensor broadcasting,
 *   the parameters should satisfy the following conditions. For example, c1_dim, c2_dim, and c3_dim
 *   represent the lowest dimension of \p input, \p masked, \p output respectively:
 *   - min(c1_dim, c2_dim) == 1 or c1_dim == c2_dim.
 *   - max(c1_dim, c2_dim) == c3_dim.
 *
 * @note
 * - When the \p masked_mode is set to \p CNNL_MASKED_SCATTER, the shape of \p input must be the same as the shape
 *   of the \p output.
 * - When the \p masked_mode is set to \p CNNL_MASKED_SCATTER or \p CNNL_MASKED_FILL, you can specify the stride of
 *   all dimensions for \p input_desc, \p value_desc and \p output_desc with ::cnnlSetTensorDescriptorEx.
 *   And the stride of mask is not supported when the number of elements in the input tensor is not equal to that in the
 *   masked tensor in \p CNNL_MASKED_SCATTER.
 *   the stride of mask is supported when the mode is \p CNNL_MASKED_FILL.
 * - The input tensor \p input, masked tensor \p masked and output tensor \p output are multi-dimensional
 *   array, supporting up to \p CNNL_DIM_MAX dimensions.
 * - When the \p masked_mode is set to \p CNNL_MASKED_SELECT, the tensor \p value_desc and its related device memory
 *   pointer \p value should be null pointer.
 * - When the \p masked_mode is set to \p CNNL_MASKED_SCATTER or \p CNNL_MASKED_FILL, the tensor \p number should be null pointer.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the masked operation is as follows:
     @verbatim
       Example 1:
       input three arrays by 1 * 1 * 2 * 3, 1 * 1 * 2 * 3 and 1 -->
         input: [[[[1,2,3],[4,5,6]]]]

         masked: [[[[0,0,1],[1,0,0]]]]

         value: [-1,]

       param:
       masked_mode: CNNL_MASKED_FILL

       output array by 1 * 1 * 2 * 3 --> [[[[1,2,-1],[-1,5,6]]]]

       Example 2:
       input three arrays by 1 * 1 * 2 * 3, 1 * 1 * 2 * 3 and 3 -->
         input: [[[[1,2,3],[4,5,6]]]]

         masked: [[[[0,0,1],[1,0,0]]]]

         value: [5,7,8]

       param:
         masked_mode: CNNL_MASKED_SCATTER

       output array by 1 * 1 * 2 * 3 --> [[[[1,2,5],[7,5,6]]]]

       Example 3:
       input three arrays by 1 * 1 * 2 * 3, 1 * 1 * 2 * 3 and 3 -->
         input: [[[[1,2,3],[4,5,6]]]]

         masked: [[[[0,0,1],[1,0,0]]]]

       param:
         masked_mode: CNNL_MASKED_SELECT

       output array --> [3,4]
     @endverbatim
 *
 * @par Reference
 * - http://github.com/pytorch/pytorch/blob/master/aten/src/THC/THCTensorMasked.cuh
 *
 */
CNNL_DEPRECATED_FOR(cnnlMasked_v5)
cnnlStatus_t CNNL_WIN_API cnnlMasked_v3(cnnlHandle_t handle,
                                        cnnlMaskedOp_t masked_mode,
                                        const cnnlTensorDescriptor_t input_desc,
                                        const void *input,
                                        const cnnlTensorDescriptor_t masked_desc,
                                        const void *masked,
                                        const cnnlTensorDescriptor_t value_desc,
                                        const void *value,
                                        void *workspace,
                                        size_t workspace_size,
                                        const cnnlTensorDescriptor_t output_desc,
                                        void *output,
                                        uint32_t *number);

// Group:Masked
/*!
 * @brief Fills the input tensor \p input with the specified value of the tensor \p value based on
 * the masked tensor \p masked, or selects the input value of tensor \p input based on the masked
 * tensor \p masked.
 *
 * - Compared with ::cnnlMasked, this function supports input tensor and masked tensor to broadcast.
 * - Compared with ::cnnlMasked_v2, this function supports \p CNNL_MASKED_SELECT, \p CNNL_MASKED_SCALE and \p CNNL_MASKED_FILL_HOST modes.
 * - Compared with ::cnnlMasked_v3, this function supports \p CNNL_MASKED_SCALE and \p CNNL_MASKED_FILL_HOST modes.
 *
 * This function may need extra MLU memory as the workspace to improve the operation performance.
 * You can get the workspace size with the ::cnnlGetMaskedWorkspaceSize
 * function.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlMasked_v5 instead, which supports setting \p value_scale_desc to define \p value_scale information
 *   instead of using both \p value and \p scale.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the masked
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] masked_mode
 *   Input. The masked mode used to compute the masked operation. The mode is defined in
 *   ::cnnlMaskedOp_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor \p input.
 * @param[in] masked_desc
 *   Input. The descriptor of the masked tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] masked
 *   Input. Pointer to the MLU memory that stores the masked tensor.
 * @param[in] value_desc
 *   Input. The descriptor of the tensor \p value to fill into \p input. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] value
 *   Input. Pointer to the MLU memory that stores the tensor \p value to fill into \p input.
 * @param[in] scale
 *   Input. A host pointer.
 *   It points to multiplier factor of tensor \p output when \p masked_mode is set to \p CNNL_MASKED_SCALE.
 *   It points to the value to fill into \p input when \p masked_mode is set to \p CNNL_MASKED_FILL_HOST.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the masked
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the masked
 *   operation. You can get the size of the workspace with the ::cnnlGetMaskedWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[out] number
 *   Output. Pointer to the MLU memory that stores the length of output selected data.

 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par API Dependency
 * - Before calling this function, you need to call the ::cnnlGetMaskedWorkspaceSize function to
 *   allocate extra workspace for the operation.
 *
 * @par Formula
 * - See "Masked Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor \p input, value tensor \p value and output tensor \p output must be the
 *   same. The supported data types of input tensor \p input, masked tensor \p masked, value tensor \p value,
 *   \p scale and output tensor \p output are as follows:
 *   - When the \p masked_mode is \p CNNL_MASKED_FILL:
 *     - input tensor: bool, int8, int16, int32, half, bfloat16, float, int64.
 *     - masked tensor: uint8, int8, bool.
 *     - value tensor: bool, int8, int16, int32, half, bfloat16, float, int64.
 *     - output tensor: bool, int8, int16, int32, half, bfloat16, float, int64.
 *   - When the \p masked_mode is \p CNNL_MASKED_SCATTER:
 *     - input tensor: half, bfloat16, float, int64.
 *     - masked tensor: uint8, int8, bool.
 *     - value tensor: half, bfloat16, float, int64.
 *     - output tensor: half, bfloat16, float, int64.
 *   - When the \p masked_mode is \p CNNL_MASKED_SELECT:
 *     - input tensor: bool, int8, uint8, int16, int32, bfloat16, half, float, int64.
 *     - masked tensor: bool, half, bfloat16, float.
 *     - output tensor: bool, int8, uint8, int16, int32, bfloat16, half, float, int64.
 *     - If the \p masked tensor type is not bool, the \p masked tensor type should be the same as the \p input tensor type.
 *   - When the \p masked_mode is \p CNNL_MASKED_SCALE:
 *     - input tensor: half, bfloat16, float.
 *     - masked tensor: uint8, bool.
 *     - scale: float.
 *     - output tensor: half, bfloat16, float.
 *   - When the \p masked_mode is \p CNNL_MASKED_FILL_HOST:
 *     - input tensor: bool, int8, int16, int32, half, bfloat16, float, int64.
 *     - masked tensor: uint8, int8, bool.
 *     - scale: bool, int8, int16, int32, half, bfloat16, float, int64.
 *     - output tensor: bool, int8, int16, int32, half, bfloat16, float, int64.
 *     - Data types of input tensors \p input, \p scale and output tensor \p output must be the same.
 *
 *   The bfloat16 data type is supported only on MLU500 series.
 *
 * @par Data Layout
 * - The supported data layout of the input, masked, value and output tensors must be
 *   \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - When the \p masked_mode is set to \p CNNL_MASKED_FILL, \p CNNL_MASKED_FILL_HOST, \p CNNL_MASKED_SELECT
 *   or \p CNNL_MASKED_SCALE, the input tensor \p input and masked tensor \p masked support broadcasting.
 *   According to the rule of tensor broadcasting, the parameters should satisfy the following conditions.
 *   For example, c1_dim, c2_dim, and c3_dim represent the lowest dimension of \p input, \p masked and \p output
 *   respectively:
 *   - min(c1_dim, c2_dim) == 1 or c1_dim == c2_dim.
 *   - max(c1_dim, c2_dim) == c3_dim.
 *
 * @note
 * - When the \p masked_mode is set to \p CNNL_MASKED_SCATTER, the shape of \p input must be the same as the shape
 *   of the \p output.
 * - When the \p masked_mode is set to \p CNNL_MASKED_SELECT and the data type of masked tensor is not bool,
 *   the data type of input tensor must be the same as the data type of the masked tensor.
 * - When the \p masked_mode is set to \p CNNL_MASKED_SCATTER or \p CNNL_MASKED_FILL,
 *   you can specify the stride of all dimensions for \p input_desc, \p masked_desc,
 *   \p value_desc and \p output_desc with ::cnnlSetTensorDescriptorEx.
 * - When the \p masked_mode is set to \p CNNL_MASKED_FILL_HOST or \p CNNL_MASKED_SCALE,
 *   you can specify the stride of all dimensions for \p input_desc, \p masked_desc,
 *   and \p output_desc with ::cnnlSetTensorDescriptorEx.
 * - The input tensor \p input, masked tensor \p masked and output tensor \p output are multi-dimensional
 *   array, supporting up to \p CNNL_DIM_MAX dimensions.
 * - When the \p masked_mode is set to \p CNNL_MASKED_SELECT, \p CNNL_MASKED_FILL_HOST or \p CNNL_MASKED_SCALE,
 *   the tensor \p value_desc and its related device memory pointer \p value should be null pointer.
 * - When the \p masked_mode is set to \p CNNL_MASKED_SCATTER, \p CNNL_MASKED_FILL, \p CNNL_MASKED_SCALE or
 *   \p CNNL_MASKED_FILL_HOST the tensor \p number should be null pointer.
 * - When \p masked_mode is set to \p CNNL_MASKED_SCATTER, make sure that
 *   the number of elements of \p value is greater than the number of the ones in \p masked.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the masked operation is as follows:
     @verbatim
       Example 1:
       input three arrays by 1 * 1 * 2 * 3, 1 * 1 * 2 * 3 and 1 -->
         input: [[[[1,2,3],[4,5,6]]]]

         masked: [[[[0,0,1],[1,0,0]]]]

         value: [-1,]

       param:
         masked_mode: CNNL_MASKED_FILL

       output array by 1 * 1 * 2 * 3 --> [[[[1,2,-1],[-1,5,6]]]]

       Example 2:
       input three arrays by 1 * 1 * 2 * 3, 1 * 1 * 2 * 3 and 3 -->
         input: [[[[1,2,3],[4,5,6]]]]

         masked: [[[[0,0,1],[1,0,0]]]]

         value: [5,7,8]

       param:
         masked_mode: CNNL_MASKED_SCATTER

       output array by 1 * 1 * 2 * 3 --> [[[[1,2,5],[7,5,6]]]]

       Example 3:
       input three arrays by 1 * 1 * 2 * 3, 1 * 1 * 2 * 3 and 3 -->
         input: [[[[1,2,3],[4,5,6]]]]

         masked: [[[[0,0,1],[1,0,0]]]]

       param:
         masked_mode: CNNL_MASKED_SELECT

       output array --> [3,4]

       Example 4:
       input three arrays by 1 * 1 * 2 * 3, 1 * 1 * 2 * 3 and 3 -->
         input: [[[[1.0,2.0,3.0],[4.0,5.0,6.0]]]]

         masked: [[[[1,0,2],[1,0,2]]]]

         scale: 2.0

       param:
         masked_mode: CNNL_MASKED_SCALE

       output array by 1 * 1 * 2 * 3 --> [[[[2.0,0.0,12.0],[8.0,0.0,24.0]]]]

       Example 5:
       input three arrays by 1 * 1 * 2 * 3, 1 * 1 * 2 * 3 and 1 -->
         input: [[[[1,2,3],[4,5,6]]]]

         masked: [[[[0,0,1],[1,0,0]]]]

         scale: -1

       param:
         masked_mode: CNNL_MASKED_FILL_HOST

       output array by 1 * 1 * 2 * 3 --> [[[[1,2,-1],[-1,5,6]]]]
     @endverbatim
 *
 * @par Reference
 * - http://github.com/pytorch/pytorch/blob/master/aten/src/THC/THCTensorMasked.cuh
 *
 */
CNNL_DEPRECATED_FOR(cnnlMasked_v5)
cnnlStatus_t CNNL_WIN_API cnnlMasked_v4(cnnlHandle_t handle,
                                        cnnlMaskedOp_t masked_mode,
                                        const cnnlTensorDescriptor_t input_desc,
                                        const void *input,
                                        const cnnlTensorDescriptor_t masked_desc,
                                        const void *masked,
                                        const cnnlTensorDescriptor_t value_desc,
                                        const void *value,
                                        const void *scale,
                                        void *workspace,
                                        size_t workspace_size,
                                        const cnnlTensorDescriptor_t output_desc,
                                        void *output,
                                        uint32_t *number);

// Group:Masked
/*!
 * @brief Fills the input tensor \p input with the specified value of the tensor \p value_scale based on
 * the masked tensor \p masked, selects the input value of tensor \p input based on the masked
 * tensor \p masked or calculates the multiplication of the \p input, \p masked and \p value_scale.
 *
 * Compared with ::cnnlMasked, this function supports input tensor and masked tensor to broadcast.
 * Compared with ::cnnlMasked_v2, this function supports \p CNNL_MASKED_SELECT, \p CNNL_MASKED_SCALE and \p CNNL_MASKED_FILL_HOST modes.
 * Compared with ::cnnlMasked_v3, this function supports \p CNNL_MASKED_SCALE and \p CNNL_MASKED_FILL_HOST modes.
 * Compared with ::cnnlMasked_v4, this function supports setting \p value_scale_desc to define \p value_scale information
 * instead of using both \p value and \p scale.
 *
 * This function may need extra MLU memory as the workspace to improve the operation performance.
 * You can get the workspace size with the ::cnnlGetMaskedWorkspaceSize
 * function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the masked
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] masked_mode
 *   Input. The masked mode used to compute the masked operation. The mode is defined in
 *   ::cnnlMaskedOp_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor \p input.
 * @param[in] masked_desc
 *   Input. The descriptor of the masked tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] masked
 *   Input. Pointer to the MLU memory that stores the masked tensor.
 * @param[in] value_scale_desc
 *   Input.
 *   In CNNL_MASKED_FILL, CNNL_MASKED_FILL_HOST and CNNL_MASKED_SCATTER modes, the descriptor of the tensor value to be filled into \p input.
 *   In CNNL_MASKED_SCALE mode, the descriptor of the tensor scale to be used in multiplication.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] value_scale
 *   Input. Pointer to the MLU memory or host memory that stores the tensor value or scale.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the masked
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the masked
 *   operation. You can get the size of the workspace with the ::cnnlGetMaskedWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[out] number
 *   Output. Pointer to the MLU memory that stores the length of output selected data.

 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par API Dependency
 * - Before calling this function, you need to call the ::cnnlGetMaskedWorkspaceSize function to
 *   allocate extra workspace for the operation.
 *
 * @par Formula
 * - See "Masked Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor \p input, value tensor \p value and output tensor \p output must be the
 *   same. The supported data types of input tensor \p input, masked tensor \p masked, value tensor \p value,
 *   \p scale and output tensor \p output are as follows:
 *   - When the \p masked_mode is \p CNNL_MASKED_FILL or \p CNNL_MASKED_FILL_HOST:
 *     - input tensor: bool, int8, int16, int32, half, bfloat16, float, int64.
 *     - masked tensor: uint8, int8, bool.
 *     - value_scale tensor: bool, int8, int16, int32, half, bfloat16, float, int64.
 *     - output tensor: bool, int8, int16, int32, half, bfloat16, float, int64.
 *     - Data types of input tensors \p input, \p value_scale and output tensor \p output must be the same.
 *   - When the \p masked_mode is \p CNNL_MASKED_SCATTER:
 *     - input tensor: half, bfloat16, float, int64.
 *     - masked tensor: uint8, int8, bool.
 *     - value_scale tensor: half, bfloat16, float, int64.
 *     - output tensor: half, bfloat16, float, int64.
 *     - Data types of input tensors \p input, \p value_scale and output tensor \p output must be the same.
 *   - When the \p masked_mode is \p CNNL_MASKED_SELECT:
 *     - input tensor: bool, int8, uint8, int16, int32, bfloat16, half, float, int64.
 *     - masked tensor: bool, half, bfloat16, float.
 *     - output tensor: bool, int8, uint8, int16, int32, bfloat16, half, float, int64.
 *     - If the \p masked tensor type is not bool, the \p masked tensor type should be the same as the \p input tensor type.
 *   - When the \p masked_mode is \p CNNL_MASKED_SCALE:
 *     - input tensor: half, bfloat16, float.
 *     - masked tensor: uint8, bool.
 *     - value_scale tensor: half, bfloat16, float.
 *     - output tensor: half, bfloat16, float.
 *     - Data types of input tensors \p input and output tensor \p output must be the same.
 *     - Data types of scale tensor \p value_scale must be float or the same as \p input.
 *
 *   The bfloat16 data type is supported only on MLU500 series.
 *
 * @par Data Layout
 * - The supported data layout of the \p input, \p masked, \p value_scale and \p output tensors must be
 *   \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - When the \p masked_mode is set to \p CNNL_MASKED_FILL, \p CNNL_MASKED_FILL_HOST, \p CNNL_MASKED_SELECT
 *   or \p CNNL_MASKED_SCALE, the input tensor \p input and masked tensor \p masked support broadcasting.
 *   According to the rule of tensor broadcasting, the parameters should satisfy the following conditions.
 *   For example, c1_dim, c2_dim, and c3_dim represent the lowest dimension of \p input, \p masked and \p output
 *   respectively:
 *   - min(c1_dim, c2_dim) == 1 or c1_dim == c2_dim.
 *   - max(c1_dim, c2_dim) == c3_dim.
 *
 * @note
 * - When the \p masked_mode is set to \p CNNL_MASKED_FILL or \p CNNL_MASKED_SCATTER, \p value_scale must be a device pointer.
 *   When the \p masked_mode is set to \p CNNL_MASKED_FILL_HOST or \p CNNL_MASKED_SCALE, \p value_scale must be a host pointer.
 * - When the \p masked_mode is set to \p CNNL_MASKED_SCATTER, the shape of \p input must be the same as the shape
 *   of the \p output.
 * - When the \p masked_mode is set to \p CNNL_MASKED_SCATTER, \p CNNL_MASKED_FILL or \p CNNL_MASKED_FILL_HOST,
 *   you can specify the stride of all dimensions for \p input_desc, \p masked_desc,
 *   \p value_scale_desc and \p output_desc with ::cnnlSetTensorDescriptorEx.
 * - When the \p masked_mode is set to \p CNNL_MASKED_SCALE,
 *   you can specify the stride of all dimensions for \p input_desc, \p masked_desc,
 *   and \p output_desc with ::cnnlSetTensorDescriptorEx.
 * - The input tensor \p input, masked tensor \p masked and output tensor \p output are multi-dimensional
 *   array, supporting up to \p CNNL_DIM_MAX dimensions.
 * - When the \p masked_mode is set to \p CNNL_MASKED_SELECT,
 *   the tensor \p value_scale_desc and its related device memory pointer \p value_scale should be null pointer.
 * - When the \p masked_mode is set to \p CNNL_MASKED_SCATTER, \p CNNL_MASKED_FILL, \p CNNL_MASKED_SCALE or
 *   \p CNNL_MASKED_FILL_HOST, the tensor \p number should be null pointer.
 * - When \p masked_mode is set to \p CNNL_MASKED_SCATTER, make sure that
 *   the number of elements of \p value_scale is greater than the number of the ones in \p masked.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the masked operation is as follows:
     @verbatim
       Example 1:
       input three arrays by 1 * 1 * 2 * 3, 1 * 1 * 2 * 3 and 1 -->
         input: [[[[1,2,3],[4,5,6]]]]

         masked: [[[[0,0,1],[1,0,0]]]]

         value: [-1]

       param:
         masked_mode: CNNL_MASKED_FILL

       output array by 1 * 1 * 2 * 3 --> [[[[1,2,-1],[-1,5,6]]]]

       Example 2:
       input three arrays by 1 * 1 * 2 * 3, 1 * 1 * 2 * 3 and 3 -->
         input: [[[[1,2,3],[4,5,6]]]]

         masked: [[[[0,0,1],[1,0,0]]]]

         value: [5,7,8]

       param:
         masked_mode: CNNL_MASKED_SCATTER

       output array by 1 * 1 * 2 * 3 --> [[[[1,2,5],[7,5,6]]]]

       Example 3:
       input three arrays by 1 * 1 * 2 * 3, 1 * 1 * 2 * 3 and 3 -->
         input: [[[[1,2,3],[4,5,6]]]]

         masked: [[[[0,0,1],[1,0,0]]]]

       param:
         masked_mode: CNNL_MASKED_SELECT

       output array --> [3,4]

       Example 4:
       input three arrays by 1 * 1 * 2 * 3, 1 * 1 * 2 * 3 and 3 -->
         input: [[[[1.0,2.0,3.0],[4.0,5.0,6.0]]]]

         masked: [[[[1,0,2],[1,0,2]]]]

         scale: 2.0

       param:
         masked_mode: CNNL_MASKED_SCALE

       output array by 1 * 1 * 2 * 3 --> [[[[2.0,0.0,12.0],[8.0,0.0,24.0]]]]

       Example 5:
       input three arrays by 1 * 1 * 2 * 3, 1 * 1 * 2 * 3 and 1 -->
         input: [[[[1,2,3],[4,5,6]]]]

         masked: [[[[0,0,1],[1,0,0]]]]

         value: -1

       param:
         masked_mode: CNNL_MASKED_FILL_HOST

       output array by 1 * 1 * 2 * 3 --> [[[[1,2,-1],[-1,5,6]]]]
     @endverbatim
 *
 * @par Reference
 * - http://github.com/pytorch/pytorch/blob/master/aten/src/THC/THCTensorMasked.cuh
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlMasked_v5(cnnlHandle_t handle,
                                        cnnlMaskedOp_t masked_mode,
                                        const cnnlTensorDescriptor_t input_desc,
                                        const void *input,
                                        const cnnlTensorDescriptor_t masked_desc,
                                        const void *masked,
                                        const cnnlTensorDescriptor_t value_scale_desc,
                                        const void *value_scale,
                                        void *workspace,
                                        size_t workspace_size,
                                        const cnnlTensorDescriptor_t output_desc,
                                        void *output,
                                        uint32_t *number);

// Group:Pad
/*!
 *  @brief Pads the input tensor using \p padding_value based on \p paddings that specifies how the tensor is
 *  padded or cropped. This operation is usually used in ResNet and Transformer networks.
 *
 *  @param[in]  handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the pad operation. For
 *    detailed information, see ::cnnlHandle_t.
 *  @param[in]  input_desc
 *    Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 *  @param[in]  input
 *    Input. Pointer to the MLU memory that stores the input tensor.
 *  @param[in]  paddings
 *    Input. A host pointer to the \p paddings data that holds the padding size to be added for each dimension
 *    of \p input. Positive and negative padding values represent the padding size and the cropping size, respectively.
 *  @param[in]  padding_value
 *    Input. A host pointer to the \p padding_value that holds the constant value to be added.
 *    When input datatype is bool, uint8 or int8, allocate 1 byte for \p padding_value pointer.
 *    When input datatype is uint16, int16, half or bfloat16, allocate 2 bytes for \p padding_value pointer.
 *    When input datatype is uint32, int32, float or complex_float, allocate 4 bytes for \p padding_value pointer.
 *    When input datatype is uint64 or int64, allocate 8 bytes for \p padding_value pointer.
 *  @param[in]  output_desc
 *    Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 *  @param[out]  output
 *    Output. Pointer to the MLU memory that stores the output tensor.
 *
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 *  @par Data Type
 *  - Data types of input tensors \p input, \p padding_value and output tensor \p output must be the same. The
 *   supported data types are as follows:
 *    - input tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float, bfloat16, complex_float.
 *    - paddings: int32.
 *    - output tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float, bfloat16, complex_float.
 *
 *    The data type bfloat16 is only supported on MLU500 series or above.
 *
 *  @par Scale Limitation
 *  - Only supports padding with a constant value in \p padding_value.
 *  - The input tensor and output tensor must meet the following requirements:
 *    - The number of dimensions of \p input is no more than 8.
 *    - The shape of the integer array of \p paddings is \p [n, 2]. Length of the first dimension of \p paddings is
 *    \p n, which equals the number of dimensions of \p input. Length of the second dimension of \p paddings is 2,
 *    which means the two padding directions of before and after directions for each dimension of \p input. For each
 *    dimension \p k, \p paddings[k, 0] and \p paddings[k, 1] specify the padding size before and after the input data
 *    respectively.
 *    - \p output[k] = \p paddings[k, 0] + \p input[k] + \p paddings[k, 1]. \p k represents each dimension of \p input.
 *    \p output[k] represents the length of the dimension \p k in the output tensor. \p input[k] represents the length
 *    of the dimension \p k in the input tensor. \p paddings[k, 0] and \p paddings[k, 1] represent the length to be
 *    padded before and after the dimension \p k in the input tensor respectively.
 *    - \p paddings[k, 0] and \p paddings[k, 1] >= \p -input[k]. \p k represents each dimension of \p input.
 *    \p input[k] represents the length of the dimension \p k in the input tensor.
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - The example of pad is as follows:
    @verbatim
    For example below, if dimensions of input == 2, then the shape of paddings should be [n, 2] == [2, 2], where n holds the
    length of the first dimension of paddings. If paddings == [[len1, len2], [len3, len4]], then for the first dimension k,
    paddings[k, 0] == len1 == 1, paddings[k, 1] == len2 == 1. For the second dimension k + 1, paddings[k + 1, 0] == len3 == -1.
    paddings[k + 1, 1] == len4 == 2. So the paddings is [[1, 1], [-1, 2]].
      - len1 is the padding size to be padded before the first dimension k of the input data.
      - len2 is the padding size to be padded after the first dimension k of the input data.
      - len3 is the padding size to be padded before the second dimension k + 1 of the input data.
      - len4 is the padding size to be padded after the second dimension k + 1 of the input data.

    input: [[2, 4], [1, 3]]
    paddings: [[1, 1], [-1, 2]]
    padding_value: 0
    output: [[0, 0, 0], [4, 0, 0], [3, 0, 0], [0, 0, 0]]
    @endverbatim
 *
 *  @par Reference
 *  - https://www.tensorflow.org/api_docs/python/tf/pad
 *  - https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_ops.py
 *  - https://pytorch.org/docs/stable/nn.functional.html?highlight=pad#torch.nn.functional.pad
 */

cnnlStatus_t CNNL_WIN_API cnnlPad(cnnlHandle_t handle,
                                  const cnnlTensorDescriptor_t input_desc,
                                  const void *input,
                                  const void *paddings,
                                  const void *padding_value,
                                  const cnnlTensorDescriptor_t output_desc,
                                  void *output);
// Group:ReplicationPad
/*!
 * @brief Pads the input tensor using the replication of the input boundary. This operation is usually used in YoloV3 network.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the replication_pad2d operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in]  paddings
 *   Input.  A host pointer to the \p paddings data that holds the padding size to be added for certain
 *   dimensions of \p input in the order of left, right, top and bottom. Positive, negative and zero padding
 *   values represent the padding size, the cropping size and no padding size, respectively.
 * @param[out]  output_desc
 *   Output. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out]  output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \p input and output tensor
 *   \p output. Data type of both tensors should be the same.
 *   - input tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float, bfloat16.
 *   - paddings: int32.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float, bfloat16.
 *
 *
 * @par Data Layout
 * - The supported data layout of the input tensor is as follows:
 *   - \p CNNL_LAYOUT_NHWC or \p CNNL_LAYOUT_NCHW
 *
 * @par Scale Limitation
 * - Only supports 2D padding for replication padding mode now.
 * - The input tensor and output tensor must meet the following requirements:
 *   - dimensions of \p input equals 4, and length of \p paddings equals 4
 *   - \p output_h >= 1 || \p output_w >= 1. \p output_h represents the length of the H dimension in the output tensor.
 *     \p output_w represents the length of the W dimension in the output tensor.
 *   - \p output_h = \p input_h + \p pad_top + \p pad_bottom && \p output_w = \p input_w + \p pad_left + \p pad_right.
 *     \p input_h represents the length of the H dimension in the input tensor. \p input_w represents the length of the W dimension in the input tensor.
 *     \p pad_top, \p pad_bottom, \p pad_left and \p pad_right respectively represent the length of the corresponding dimension in \p paddings.
 *
 *  @par Performance Optimization
 *  - For best practices, to have better performance, set the layout of the input tensor and output tensor to NHWC.
 *
 *  @note
 *  - This operation is not supported on the 1V platforms.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - The example of replication_pad2d is as follows:
    @verbatim
    input: tensor([[[[0., 1., 2.],
                     [3., 4., 5.],
                     [6., 7., 8.]]]])
    paddings: array([2., 2., 2., 2.])
    output: tensor([[[[0., 0., 0., 1., 2., 2., 2.]
                      [0., 0., 0., 1., 2., 2., 2.]
                      [0., 0., 0., 1., 2., 2., 2.]
                      [3., 3., 3., 4., 5., 5., 5.]
                      [6., 6., 6., 7., 8., 8., 8.]
                      [6., 6., 6., 7., 8., 8., 8.]
                      [6., 6., 6., 7., 8., 8., 8.]]]])
    @endverbatim
 *
 *  @par Reference
 *  - https://pytorch.org/docs/master/generated/torch.nn.ReplicationPad2d.html
 */
cnnlStatus_t CNNL_WIN_API cnnlReplicationPad2d(cnnlHandle_t handle,
                                               const cnnlTensorDescriptor_t input_desc,
                                               const void *input,
                                               const int paddings[],
                                               const cnnlTensorDescriptor_t output_desc,
                                               void *output);
// Group:ReflectionPad
/*!
 * @brief Pads the input tensor using the reflection of the input boundary based on \p paddings that specifies
 * how the tensor is padded or cropped. This operation is usually used in YoloV3 network for PyTorch.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the reflection_pad2d
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in]  paddings
 *   Input. A host pointer to the host padding parameter that holds the padding size to be added for certain
 *   dimensions of \p input in the order of left, right, top and bottom. Positive, negative and zero padding
 *   values represent the padding size, the cropping size and no padding size, respectively.
 * @param[in]  output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out]  output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \p input and output tensor
 *   \p output. Data type of both tensors should be the same.
 *   - input tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float, bfloat16.
 *   - paddings: int32.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float, bfloat16.
 *
 * @par Data Layout
 * - Data layouts of input tensor \p input and output tensor \p output must be the same. The supported data
 *  layouts are as follows:
 *   - input tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NCHW.
 *   - output tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NCHW.
 *
 * @par Scale Limitation
 * - Only supports 2D padding for reflection padding mode now.
 * - The input tensor and output tensor must meet the following requirements:
 *   - The number of dimensions of \p input equals 4, and length of \p paddings equals 4.
 *   - \p pad_top < \p input_h && \p pad_bottom < input_h. \p input_h represents the length of the H dimension in
 *   the input tensor. \p pad_top and \p pad_bottom represent the length of the corresponding dimension in \p paddings
 *   respectively.
 *   - \p pad_left < \p input_w && \p pad_right < input_w. \p input_w represents the length of the W dimension in
 *   the input tensor. \p pad_left and \p pad_right represent the length of the corresponding dimension in \p paddings
 *   respectively.
 *   - \p output_h >= 1 || \p output_w >= 1. \p output_h represents the length of the H dimension in the output tensor.
 *   \p output_w represents the length of the W dimension in the output tensor.
 *   - \p output_h = \p input_h + \p pad_top + \p pad_bottom && \p output_w = \p input_w + \p pad_left + \p pad_right.
 *   \p input_h represents the length of the H dimension in the input tensor. \p input_w represents the length of the
 *   W dimension in the input tensor. \p pad_top, \p pad_bottom, \p pad_left and \p pad_right represent the length of
 *   the corresponding dimension in \p paddings respectively.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the layout of the input tensor and output tensor to NHWC.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of reflection_pad2d is as follows:
   @verbatim
   input: tensor([[[[0., 1., 2.],
                    [3., 4., 5.],
                    [6., 7., 8.]]]])
   paddings: array([2., 2., 2., 2.])
   output: tensor([[[[8., 7., 6., 7., 8., 7., 6.]
                     [5., 4., 3., 4., 5., 4., 3.]
                     [2., 1., 0., 1., 2., 1., 0.]
                     [5., 4., 3., 4., 5., 4., 3.]
                     [8., 7., 6., 7., 8., 7., 6.]
                     [5., 4., 3., 4., 5., 4., 3.]
                     [2., 1., 0., 1., 2., 1., 0.]]]])
   @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/master/generated/torch.nn.ReflectionPad2d.html
 */

cnnlStatus_t CNNL_WIN_API cnnlReflectionPad2d(cnnlHandle_t handle,
                                              const cnnlTensorDescriptor_t input_desc,
                                              const void *input,
                                              const int paddings[],
                                              const cnnlTensorDescriptor_t output_desc,
                                              void *output);
// Group:TopKTensor
/*!
 * @brief Computes the top \p k largest or smallest elements of the input tensor in the specified dimension.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlTopKTensor_v3 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the topk
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] k
 *   Input. An integer value that determines the number of top elements in \p input to be returned.
 * @param[in] dim
 *   Input. An integer value that determines the dimension to sort along.
 * @param[in] largest
 *   Input. A Boolean value that determines whether to return the largest data. When this parameter is true,
 *   return the top-k largest input values and their corresponding indices; when this parameter is false,
 *   return the top-k smallest input values and their corresponding indices.
 * @param[in] sorted
 *   Input. A Boolean value that determines whether to sort the returned data.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] index_desc
 *   Input. The descriptor of the index tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] index
 *   Output. Pointer to the MLU memory that stores the index tensor, which is the indices of the returned values.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "TopK Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - By the order of \p input - \p output - \p index, the supported combinations of data types are shown below:
 *   - uint8-uint8-int16.
 *   - uint8-uint8-uint16.
 *   - uint8-uint8-int32.
 *   - uint8-uint8-uint32.
 *   - int8-int8-int16.
 *   - int8-int8-uint16.
 *   - int8-int8-int32.
 *   - int8-int8-uint32.
 *   - int16-int16-int32.
 *   - int16-int16-uint32.
 *   - int32-int32-int32.
 *   - int32-int32-uint32.
 *   - half-half-int16.
 *   - half-half-uint16.
 *   - half-half-int32.
 *   - half-half-uint32.
 *   - float-float-int32.
 *   - float-float-uint32.
 *   - bfloat16-bfloat16-int32.
 *   - bfloat16-bfloat16-uint32.
 *
 * @par Scale Limitation
 * - None.
 *
 * @par Note
 * - The input tensor, output tensor, and index tensor must meet the following requirement:
 *   - The shape of \p output and \p index should be the same.
 * - When \p input contains NaN:
 *   - On MLU200 series:
 *     - NaN will be considered as saturation value.
 *   - On MLU300 series and MLU500 series: NaN is not supported.
 * - When the data type of \p input is CNNL_DTYPE_INT32, the value of \p input should
 *   be in range of [\f$-2^{24}\f$, \f$2^{24}\f$].
 * - When the data type of \p input is CNNL_DTYPE_FLOAT, the value of \p input should
 *   be in range of (\f$-2^{127}\f$, \f$2^{127}\f$).
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.topk.html
 */
CNNL_DEPRECATED_FOR(cnnlTopKTensor_v3)
cnnlStatus_t CNNL_WIN_API cnnlTopKTensor(cnnlHandle_t handle,
                                         const cnnlTensorDescriptor_t input_desc,
                                         const void *input,
                                         const int k,
                                         const int dim,
                                         const bool largest,
                                         const bool sorted,
                                         const cnnlTensorDescriptor_t output_desc,
                                         void *output,
                                         const cnnlTensorDescriptor_t index_desc,
                                         void *index);

// Group:TopKTensor
/*!
 * @brief Computes the top \p k largest or smallest elements of the input tensor in the specified dimension.
 *   Compared with ::cnnlTopKTensor, this function adds another parameter \p lower_index_first, which regulates
 *   behavior of selecting the elements with smaller indices when their values are the same.
 *
 * On MLU500 series, you can use only ::cnnlTopKTensor_v3.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlTopKTensor_v3 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the topk
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] k
 *   Input. An integer value that determines the number of top elements in \p input to be returned.
 * @param[in] dim
 *   Input. An integer value that determines the dimension to sort along.
 * @param[in] largest
 *   Input. A Boolean value that determines whether to return the largest data. When this parameter is true,
 *   return the top-k largest input values and their corresponding indices; when this parameter is false,
 *   return the top-k smallest input values and their corresponding indices.
 * @param[in] sorted
 *   Input. A Boolean value that determines whether to sort the returned data.
 * @param[in] lower_index_first
 *   Input. A Boolean value that determines whether the behavior of lower-index element coming first
 *   is guaranteed when multiple elements have the same value.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] index_desc
 *   Input. The descriptor of the index tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] index
 *   Output. Pointer to the MLU memory that stores the index tensor, which is the indices of the returned values.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "TopK Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - By the order of \p input - \p output - \p index, the supported combinations of data types are shown below:
 *   - uint8-uint8-int16.
 *   - uint8-uint8-uint16.
 *   - uint8-uint8-int32.
 *   - uint8-uint8-uint32.
 *   - int8-int8-int16.
 *   - int8-int8-uint16.
 *   - int8-int8-int32.
 *   - int8-int8-uint32.
 *   - int16-int16-int32.
 *   - int16-int16-uint32.
 *   - int32-int32-int32.
 *   - int32-int32-uint32.
 *   - half-half-int16.
 *   - half-half-uint16.
 *   - half-half-int32.
 *   - half-half-uint32.
 *   - float-float-int32.
 *   - float-float-uint32.
 *   - bfloat16-bfloat16-int32.
 *   - bfloat16-bfloat16-uint32.
 *
 * @par Scale Limitation
 * - None.
 *
 * @par Note
 * - The input tensor, output tensor, and index tensor must meet the following requirement:
 *   - The shape of \p output and \p index should be the same.
 * - When \p input contains NaN:
 *   - On MLU200 series:
 *     - NaN will be considered as saturation value.
 *   - On MLU300 series and MLU500 series: NaN is not supported.
 * - When the data type of \p input is CNNL_DTYPE_INT32, the value of \p input should
 *   be in range of [\f$-2^{24}\f$, \f$2^{24}\f$].
 * - When the data type of \p input is CNNL_DTYPE_FLOAT, the value of \p input should
 *   be in range of (\f$-2^{127}\f$, \f$2^{127}\f$).
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.topk.html
 */
CNNL_DEPRECATED_FOR(cnnlTopKTensor_v3)
cnnlStatus_t CNNL_WIN_API cnnlTopKTensor_v2(cnnlHandle_t handle,
                                            const cnnlTensorDescriptor_t input_desc,
                                            const void *input,
                                            const int k,
                                            const int dim,
                                            const bool largest,
                                            const bool sorted,
                                            const bool lower_index_first,
                                            const cnnlTensorDescriptor_t output_desc,
                                            void *output,
                                            const cnnlTensorDescriptor_t index_desc,
                                            void *index);

// Group:TopKTensor
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used to get
 *        extra space size in topk operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the topk operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The  descriptor of input tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] k
 *   Input. An integer value that determines the number of top elements in \p input to be returned.
 * @param[in] dim
 *   Input. An integer value that determines the dimension to sort along.
 * @param[in] output_desc
 *   Input. The  descriptor of output tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] index_desc
 *   Input. The descriptor of the index tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] largest
 *   Input. A Boolean value that determines whether to return the largest data. When this parameter is true,
 *   return the top-k largest input values and their corresponding indices; when this parameter is false,
 *   return the top-k smallest input values and their corresponding indices.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is
 *   used in the topk operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_INTERNAL_ERROR
 *
 * @par API Dependency
 * - This API is only used along with ::cnnlTopKTensor_v3. ::cnnlTopKTensor_v2 and ::cnnlTopKTensor do not require this API.
 * - On MLU500 series, you can use only ::cnnlTopKTensor_v3.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetTopKTensorWorkspaceSize(cnnlHandle_t handle,
                                                         const cnnlTensorDescriptor_t input_desc,
                                                         const int k,
                                                         const int dim,
                                                         const bool largest,
                                                         const cnnlTensorDescriptor_t output_desc,
                                                         const cnnlTensorDescriptor_t index_desc,
                                                         size_t *workspace_size);

// Group:TopKTensor
/*!
 * @brief Computes the top \p k largest or smallest elements of the input tensor in the specified dimension.
 *   Compared with ::cnnlTopKTensor_v2, this function adds another two parameters \p workspace and \p workspace_size_inbytes,
 *   which provide the extra space needed in computation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the topk
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] k
 *   Input. An integer value that determines the number of top elements in \p input to be returned.
 * @param[in] dim
 *   Input. An integer value that determines the dimension to sort along.
 * @param[in] largest
 *   Input. A Boolean value that determines whether to return the largest data. When this parameter is true,
 *   return the top-k largest input values and their corresponding indices; when this parameter is false,
 *   return the top-k smallest input values and their corresponding indices.
 * @param[in] sorted
 *   Input. A Boolean value that determines whether to sort the returned data.
 * @param[in] lower_index_first
 *   Input. A Boolean value that determines whether the behavior of lower-index element coming first
 *   is guaranteed when multiple elements have the same value.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   topk operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the
 *   topk operation. You can get the size of the workspace with the
 *   ::cnnlGetTopKTensorWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] index_desc
 *   Input. The descriptor of the index tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] index
 *   Output. Pointer to the MLU memory that stores the index tensor, which is the indices of the returned values.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED, ::CNNL_STATUS_INTERNAL_ERROR
 *
 * @par Formula
 * - See "TopK Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - By the order of \p input - \p output - \p index, the supported combinations of data types are shown below:
 *   - uint8-uint8-int16.
 *   - uint8-uint8-uint16.
 *   - uint8-uint8-int32.
 *   - uint8-uint8-uint32.
 *   - int8-int8-int16.
 *   - int8-int8-uint16.
 *   - int8-int8-int32.
 *   - int8-int8-uint32.
 *   - int16-int16-int32.
 *   - int16-int16-uint32.
 *   - int32-int32-int32.
 *   - int32-int32-uint32.
 *   - int32-int32-int64.
 *   - half-half-int16.
 *   - half-half-uint16.
 *   - half-half-int32.
 *   - half-half-uint32.
 *   - half-half-int64.
 *   - float-float-int32.
 *   - float-float-uint32.
 *   - float-float-int64.
 *   - bfloat16-bfloat16-int32.
 *   - bfloat16-bfloat16-uint32.
 *   - bfloat16-bfloat16-int64.
 *   - int64-int64-int32.
 *   - int64-int64-int64.
 *
 * @par Scale Limitation
 * - Large tensor is only supported for float, half, bfloat16, int32 and int64 input data types.
 *   Otherwise, the tensor size of input should be less than 2G bytes.
 *
 * @par Note
 * - The input tensor, output tensor, and index tensor must meet the following requirements:
 *   - The shape of \p output and \p index should be the same.
 * - NaN is supported only on MLU500 series or higher.
 * - bfloat16 is suuported only on MLU500 series or higher.
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.topk.html
 */
cnnlStatus_t CNNL_WIN_API cnnlTopKTensor_v3(cnnlHandle_t handle,
                                            const cnnlTensorDescriptor_t input_desc,
                                            const void *input,
                                            const int k,
                                            const int dim,
                                            const bool largest,
                                            const bool sorted,
                                            const bool lower_index_first,
                                            void *workspace,
                                            const size_t workspace_size,
                                            const cnnlTensorDescriptor_t output_desc,
                                            void *output,
                                            const cnnlTensorDescriptor_t index_desc,
                                            void *index);

// Group:NegTensor
/*!
 * @brief Retrieves the negation of the input tensor.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. Descriptor of the input tensor \p x.
 * @param[in] x
 *   Input.  Pointer to the MLU memory that stores the input data.
 * @param[in] y_desc
 *   Input.  Descriptor of the output tensor \p y.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output data.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Data Type
 * - Data type of input tensor and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input:  int32, float, half, bfloat16, int64, double, complex_double.
 *   - output: int32, float, half, bfloat16, int64, double, complex_double.
 *
 * @par Scale Limitation
 * - None.
 *
 * @note
 * - You can specify the stride of all dimensions for desc_input and desc_output with
 *  ::cnnlSetTensorDescriptorEx.
 *
 * @par Performance Optimization
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/raw_ops/Neg
 */
cnnlStatus_t CNNL_WIN_API cnnlNegTensor(cnnlHandle_t handle,
                                        const cnnlTensorDescriptor_t x_desc,
                                        const void *x,
                                        const cnnlTensorDescriptor_t y_desc,
                                        void *y);
// Group:Select
/*!
 * @brief Selects elements from then tensor \p p_then or else tensor \p p_else based on
 *        \p p_condition, and returns the results in the output tensor \p p_output.
 *
 * Compared with ::cnnlSelectV2, this function does not support broadcasting,
 * and the alignment is left while that of ::cnnlSelectV2 is right.
 *
 * If \p p_condition is true, \p p_output is equal to \p p_then; if \p p_condition is
 * false, \p p_output is equal to \p p_else.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the select operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] desc_then
 *   Input. The descriptor of the then tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] p_then
 *   Input. Pointer to the MLU memory that stores the then tensor.
 * @param[in] desc_else
 *   Input. The descriptor of the else tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] p_else
 *   Input. Pointer to the MLU memory that stores the else tensor.
 * @param[in] p_condition
 *   Input. Pointer to the MLU memory that stores the condition tensor.
 * @param[in] condition_size
 *   Input. The size of the condition tensor used in the select operation.
 * @param[in] desc_output
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] p_output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Select Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of then tensor, else tensor and output tensor must be the same.
 * - The supported data types of then tensor, else tensor, condition and output tensor are as follows:
 *   - then tensor: int8, int16, int32, int64, half, bfloat16, float, bool.
 *   - else tensor: int8, int16, int32, int64, half, bfloat16, float, bool.
 *   - output tensor: int8, int16, int32, int64, half, bfloat16, float, bool.
 *   - condition tensor: bool.
 *
 * @par Scale Limitation
 * - The number of dimensions is no more than \p CNNL_DIM_MAX.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - Shape of then tensor, else tensor and output tensor must be the same.
 * - If the shape of then tensor is [a, ..., b, ..., c], the supported sizes of condition tensor
 *   are as follows:
 *   - 1.
 *   - a.
 *   - a * ... * b * ... * c.
 *
 * @par Example
 * - The example of the select operation is as follows:
     @verbatim
      1. The size of condition is equal to 1
      input two arrays by 2 * 3 and 2 * 3
      --> then: [[1, 8, 9], [6, 4, 3]]

      --> else: [[2, 3, 6], [7, 5, 2]]

      param:
        condition: [true]
        condition size: 1

      output array by 2 * 3
      --> output: [[1, 8, 9], [6, 4, 3]]

      2. The size of condition is equal to the size of the first dimension of then tensor
      input two arrays by 2 * 3 and 2 * 3
      --> then: [[1, 8, 9], [6, 4, 3]]

      --> else: [[2, 3, 6], [7, 5, 2]]

      param:
        condition: [true, false]
        condition size: 2

      output array by 2 * 2
      --> output: [[1, 8, 9], [7, 5, 2]]

      3. The size of condition is equal to the size of the then tensor
      input two arrays by 2 * 3 and 2 * 3
      --> then: [[1, 8, 9], [6, 4, 3]]

      --> else: [[2, 3, 6], [7, 5, 2]]

      param:
        condition: [true, false, false, false, true, true]
        condition size: 6

      output array by 2 * 3
      --> output: [[1, 3, 6], [7, 4, 3]]

     @endverbatim
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/raw_ops/Select
 * - https://www.tensorflow.org/api_docs/python/tf/raw_ops/SelectV2
 */
cnnlStatus_t CNNL_WIN_API cnnlSelect(cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t desc_then,
                                     const void *p_then,
                                     const cnnlTensorDescriptor_t desc_else,
                                     const void *p_else,
                                     const cnnlTensorDescriptor_t desc_output,
                                     void *p_output,
                                     const bool *p_condition,
                                     const int condition_size);
// Group:SelectV2
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the select operation.
 *
 * The size of the extra workspace is based on the given information of the selectV2 operation,
 * including the input tensor descriptors \p condition_desc, \p then_desc and \p else_desc.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   selectV2 operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] condition_desc
 *   Input. The descriptor of the condition tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] then_desc
 *   Input. The descriptor of the then tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] else_desc
 *   Input. The descriptor of the else tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   selectV2 operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH.
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlSelectV2 function to perform the
 *   select operation.
 *
 * @par Scale Limitation
 * - The number of dimensions is no more than \p CNNL_DIM_MAX.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetSelectV2WorkspaceSize(cnnlHandle_t handle,
                             const cnnlTensorDescriptor_t condition_desc,
                             const cnnlTensorDescriptor_t then_desc,
                             const cnnlTensorDescriptor_t else_desc,
                             size_t *workspace_size);

// Group:SelectV2
/*!
 * @brief Selects elements from then tensor \p then_ptr or else tensor \p else_ptr based on
 *        \p condition_ptr, and returns the results in the output tensor \p output_ptr.
 *
 * Compared with ::cnnlSelect, this function supports multidirectional(i.e., Numpy-style) broadcasting,
 * and the alignment is right while that of ::cnnlSelect is left.
 *
 * If \p condition_ptr is true, \p output_ptr is equal to \p then_ptr; if \p condition_ptr is
 * false, \p output_ptr is equal to \p else_ptr.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the select operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] condition_desc
 *   Input. The descriptor of then condition tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] condition_ptr
 *   Input. Pointer to the MLU memory that stores the condition tensor.
 * @param[in] then_desc
 *   Input. The descriptor of the then tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] then_ptr
 *   Input. Pointer to the MLU memory that stores the then tensor.
 * @param[in] else_desc
 *   Input. The descriptor of the else tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] else_ptr
 *   Input. Pointer to the MLU memory that stores then else tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the selectV2
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the selectV2
 *   operation. You can get the size of the workspace with the ::cnnlGetSelectV2WorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output_ptr
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Select Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of then tensor, else tensor and output tensor must be the same.
 * - The supported data types of condition tensor, then tensor, else tensor and output tensor are as follows:
 *   - condition tensor: bool, uint8.
 *   - then tensor: bool, int8, int16, int32, int64, half, bfloat16, float.
 *   - else tensor: bool, int8, int16, int32, int64, half, bfloat16, float.
 *   - output tensor: bool, int8, int16, int32, int64, half, bfloat16, float.
 *
 * @par Scale Limitation
 * - The number of dimensions is no more than \p CNNL_DIM_MAX.
 * - According to the rule of tensor broadcasting, the parameters should satisfy the following
 *   conditions. The dimension number of the output tensor should be the largest dimension number in
 *   condition, then or else tensor. The tensors with smaller dimension number will be right aligned with
 *   the tensor with the largest dimension number and fill in 1 to the left to have the same dimension number
 *   with the largest dimension number of the tensor.
 *   Take the lowest dimension for example, c1_dim, c2_dim and c3_dim represent
 *   the dimensions of condition tensor, then tensor and else tensor,
 *   c4_dim represents the dimension of output tensor:
 *
 *   c4_dim == max(c1_dim, c2_dim, c3_dim)
 *   c1_dim == c4_dim or c1_dim == 1
 *   c2_dim == c4_dim or c2_dim == 1
 *   c3_dim == c4_dim or c3_dim == 1
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      1. The shape of condition array is equal to 1
      --> condition_ptr: [False]
      --> then_ptr: [1,2,3,4]
      --> else_ptr: [100]
      --> output_ptr: [100,100,100,100]

      2. no broadcasting (the same as cnnlSelect)
      --> condition_ptr: [True, False, False, True]
      --> then_ptr:  [1,2,3,4]
      --> else_ptr: [100,200,300,400]
      --> output_ptr: [1,200,300,4]

      3. broadcasting (right alignment)
      broadcast else array by 1 to 4
      --> condition_ptr: [True, False, False, True]
      --> then_ptr: [1,2,3,4]
      --> else_ptr: [100]
      --> output_ptr: [1,100,100,4]

      broadcast then array and else array by 1 to 4
      --> condition_ptr: [True, False, False, True]
      --> then_ptr: [1]
      --> else_ptr: [100]
      --> output_ptr: [1,100,100,1]

      broadcast condition array by 3 to 2*3
      --> condition_ptr: [True, False, True]
      --> then_ptr: [[1,2,3],[4,5,6]]
      --> else_ptr: [[100,200,300],[400,500,600]]
      --> output_ptr: [[1,200,3],[4,500,6]]
     @endverbatim
 *
 * @par Reference
 * - https://tensorflow.org/api_docs/python/tf/where
 * - https://github.com/onnx/onnx/blob/main/docs/Changelog.md#Where-9
 * - https://github.com/onnx/onnx/blob/main/docs/Broadcasting.md
 * - https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/select.cc#L215
 */
cnnlStatus_t CNNL_WIN_API cnnlSelectV2(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t condition_desc,
                                       const void *condition_ptr,
                                       const cnnlTensorDescriptor_t then_desc,
                                       const void *then_ptr,
                                       const cnnlTensorDescriptor_t else_desc,
                                       const void *else_ptr,
                                       void *workspace,
                                       size_t workspace_size,
                                       const cnnlTensorDescriptor_t output_desc,
                                       void *output_ptr);

// Group:RMSProp
/*!
 * @brief Creates an operation to update filters by Root Mean Square Prop (RMSProp) algorithm.
 *
 * @param[in] handle
 *  Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *  queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] lr
 *   Input. Pointer to the MLU memory that stores the \p lr parameter, which is the learning rate.
 * @param[in] rho
 *   Input. Pointer to the MLU memory that stores the \p rho parameter, which is the discounting
 *   factor of gradient.
 * @param[in] epsilon
 *   Input. Pointer to the MLU memory that stores the \p epsilon parameter, which is a small
 *   value to avoid zero denominator.
 * @param[in] momentum
 *   Input. Pointer to the MLU memory that stores the \p momentum parameter, which is a scale
 *   parameter.
 * @param[in] grad_desc
 *   Input. The descriptor of \p grad tensor, which is the gradient.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] grad
 *   Input. Pointer to the MLU memory that stores the \p grad tensor.
 * @param[in] var_desc
 *   Input. The descriptor of \p var tensor, which is the filter to be updated.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in,out] var
 *   Input and output. Pointer to the MLU memory that stores the \p var tensor.
 * @param[in] ms_desc
 *   Input. The descriptor of \p ms tensor, which is the mean square gradient to be updated.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in,out] ms
 *   Input and output. Pointer to the MLU memory that stores the \p ms tensor.
 * @param[in] mom_desc
 *   Input. The descriptor of \p mom tensor, which is the delta of \p var.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in,out] mom
 *   Input and output. Pointer to the MLU memory that stores the \p mom tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Apply RMSprop Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Scale Limitation
 * - The \p grad tensor, \p var tensor, \p ms tensor and \p mom tensor must meet the following
 * requirements:
 *   - The shape of \p grad equals the shape of \p var.
 *   - The shape of \p ms equals the shape of \p var.
 *   - The shape of \p mom equals the shape of \p var.
 *
 * @par Data Type
 * - The supported data types are as follows:
 *   - grad tensors: half, float.
 *   - var tensors: half, float.
 *   - ms tensors: half, float.
 *   - mom tensors: half, float.
 *   - lr parameter: half, float
 *   - rho parameter: half, float
 *   - epsilon parameter: half, float
 *   - momentum parameter: half, float
 * - The data type of \p grad tensor, \p var tensor, \p ms tensor, \p mom tensor must be the same.
 * - The data type of \p lr parameter, \p rho parameter, \p epsilon parameter and
 *   \p momentum parameter must be the same as \p var tensor.
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the operation is as follows:
    @verbatim
    --> grad: an array [64, 78, 1024];
    --> var: an array [64, 78, 1024];
    --> ms: an array [64, 78, 1024];
    --> mom: an array [64, 78, 1024];
    --> lr: an array [1];
    --> rho: an array [1];
    --> epsilon: an array [1];
    --> momentum: an array [1];
    Then we will get the output:
    --> var: an array [64, 78, 1024] same as grad;
    --> ms: an array [64, 78, 1024] same as grad;
    --> mom: an array [64, 78, 1024] same as grad;
    @endverbatim
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlRMSProp(cnnlHandle_t handle,
                                      const void *lr,
                                      const void *rho,
                                      const void *epsilon,
                                      const void *momentum,
                                      const cnnlTensorDescriptor_t grad_desc,
                                      const void *grad,
                                      const cnnlTensorDescriptor_t var_desc,
                                      void *var,
                                      const cnnlTensorDescriptor_t ms_desc,
                                      void *ms,
                                      const cnnlTensorDescriptor_t mom_desc,
                                      void *mom);

// Group:ApplyCenterRMSProp
/*!
 * @brief The CenterRMSProp algorithm uses an estimate of the centered second
 * moment (i.e., the variance) for normalization, as opposed to regular RMSProp,
 * which uses the (uncentered) second moment. This often helps with training,
 * but is slightly more expensive in terms of computation and memory.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  var_desc
 *   Input. A descriptor of input tensor \p var. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out]  var
 *   Input and output. Pointer to the MLU memory that stores the \p var tensor to be updated
 *   according to CenterRMSProp algorithm.
 *   This input refers to filter in the artificial intelligence network generally.
 * @param[in] mg_desc
 *   Input. A descriptor of input tensor \p mg. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in, out]  mg
 *   Input and output. Pointer to the MLU memory that stores the moving average gradient.
 * @param[in] ms_desc
 *   Input. A descriptor of input tensor \p ms. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in, out]  ms
 *   Input and output. Pointer to the MLU memory that stores the moving average square gradient.
 * @param[in] mom_desc
 *   Input. A descriptor of input tensor \p mom. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in, out]  mom
 *   Input and output. Pointer to the MLU memory that stores the momentum.
 * @param[in] grad_desc
 *   Input. A descriptor of input tensor \p grad. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  grad
 *   Input. Pointer to the MLU memory that stores the gradient.
 * @param[in]  lr
 *   Input. Pointer to the MLU memory that stores the learning rate.
 * @param[in]  rho
 *   Input. Pointer to the MLU memory that stores the moving average parameter.
 * @param[in]  momentum
 *   Input. Pointer to the MLU memory that stores the attenuation of impulse.
 * @param[in]  epsilon
 *   Input. Pointer to the MLU memory that stores a small positive number just as \f$10^{-8}\f$, to
 *   avoid division by 0.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "ApplyCenterRMSProp Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Scale Limitation
 * - The data shape of input tensors and output tensors must be the same.
 * - The number of dimensions of each input tensor should be no more than 8.
 * - The values in tensor \p mg should be in range of [-1,1].
 * - The values in tensor \p ms should be in range of [2,63486].
 * - The values in tensor \p grad should be in range of [-1,1].
 * - \p lr should be in range of [0,1].
 * - \p rho should be in range of [0,1].
 * - \p momentum should be in range of [0,1].
 * - \p epsilon should be in range of (0,1].
 *
 * @par Data Type
 * - Data type of input tensors and output tensors must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensors: half, float.
 *   - output tensors: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensors and output tensors must be \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the ApplyCenterRMSProp operation is as follows:
    @verbatim
     --> var: an array [64, 78, 1024];
     --> mg: an array [64, 78, 1024];
     --> ms: an array [64, 78, 1024];
     --> mom: an array [64, 78, 1024];
     --> grad: an array [64, 78, 1024];
     --> lr: an array [1];
     --> rho: an array [1];
     --> momentum: an array [1];
     --> epsilon: an array [1];
     Then we will get the output:
     --> var: an array [64, 78, 1024] same as input;
     --> mg: an array [64, 78, 1024] same as input;
     --> ms: an array [64, 78, 1024] same as input;
     --> mom: an array [64, 78, 1024] same as input;
    @endverbatim
 */

cnnlStatus_t CNNL_WIN_API cnnlApplyCenterRMSProp(cnnlHandle_t handle,
                                                 const cnnlTensorDescriptor_t var_desc,
                                                 void *var,
                                                 const cnnlTensorDescriptor_t mg_desc,
                                                 void *mg,
                                                 const cnnlTensorDescriptor_t ms_desc,
                                                 void *ms,
                                                 const cnnlTensorDescriptor_t mom_desc,
                                                 void *mom,
                                                 const cnnlTensorDescriptor_t grad_desc,
                                                 const void *grad,
                                                 const void *lr,
                                                 const void *rho,
                                                 const void *momentum,
                                                 const void *epsilon);

// Group:QR
/*!
 * @brief This function is used to compute the QR decomposition of a matrix or
 * a batch of matrices \p a with the transformation algorithm used in this operation, and returns \p q,
 * \p r of tensors such that \p a = \p q * \p r with \p q being an orthogonal matrix or batch
 * of orthogonal matrices and \p r being an upper triangular matrxi or batch of
 * upper triangular matrices. The \p a tensor of size (*,m,n) where * is zero or
 * more batch dimension consisting of matrices of dimension (m,n).
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  a_desc
 *   Input. A descriptor of input tensor \p a. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  a
 *   Input. Pointer to the MLU memory that stores the \p a tensor to be decomposed
 *   according to the transformation algorithm used in this operation.
 * @param[in] q_desc
 *   Input. A descriptor of output tensor \p q. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out]  q
 *   Output. Pointer to the MLU memory that is used to store the decomposition result q.
 * @param[in] r_desc
 *   Input. A descriptor of input tensor \p r. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out]  r
 *   Output. Pointer to the MLU memory that is used to store the decomposition result r.
 * @param[in]  workspace
 *   Input. Pointer to the MLU memory that is used to store the intermediate result.
 * @param[in] workspace_size
 *   Input. The size of \p workspace and can be calculated by ::cnnlGetQRWorkspaceSize.
 * @param[in]  some
 *   Input. Set to true for reduced QR decomposition and false for complete QR decomposition.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - None.
 *
 * @par Scale Limitation
 * - On 1V platforms, in the size (*, m, n) of input tensor \p a, m <=512, n <=512, and
 *   if * is too large, the process will be terminated by the watch dog. * is supposed
 *   to be less than or equal to 256. If \p some is false, \p q must be (*, m, m), and \p r
 *   must be (*, m, n); if \p some is true, \p q must be (*, m, min(m,n)), and \p r must
 *   be (*, min(m,n), n).
 * - The number of dimensions of each input tensor should be equal to or greater than 2,
 *   and no greater than 8.
 *
 * @par Data Type
 * - Data type of input tensors and output tensors must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensors: half, float.
 *   - output tensors: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensors and output tensors must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlGetQRWorkspaceSize function.
 *
 * @note
 * - Precision may be lost if the magnitudes of the elements of \a are large.
 * - This function returns a valid decomposition, but the result may not be the same as
 *   other third party libraries such as lapack, eigen, cusolver.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the QR operation is as follows:
    @verbatim
     --> some: false;
     --> a: an array [64, 256, 128];
     Then we will get the output:
     --> q: an array [64, 256, 256];
     --> r: an array [64, 256, 128];
    @endverbatim

    @verbatim
     --> some: true;
     --> a: an array [64, 256, 128];
     Then we will get the output:
     --> q: an array [64, 256, 128];
     --> r: an array [64, 128, 128];
    @endverbatim
 */

cnnlStatus_t CNNL_WIN_API cnnlQR(cnnlHandle_t handle,
                                 const cnnlTensorDescriptor_t a_desc,
                                 const void *a,
                                 const cnnlTensorDescriptor_t q_desc,
                                 void *q,
                                 const cnnlTensorDescriptor_t r_desc,
                                 void *r,
                                 void *workspace,
                                 size_t workspace_size,
                                 const bool some);

// Group:QR
/*!
 * @brief Returns in \p size the size of the MLU memory that is used to save the
 * intermediate result of ::cnnlQR operation.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the ::cnnlGetQRWorkspaceSize operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  a_desc
 *   Input. The descriptor of a tensor. The value must be the same as the \p a_desc in ::cnnlQR.
 *   This parameter is used to calculate the workspace size.
 * @param[in]  some
 *   Input. A Boolean value that must be the same as the \p some in ::cnnlQR.
 *   This parameter is used to calculate the workspace size.
 * @param[out]  size
 *   Output. A host pointer to the returned size of the extra workspace in bytes
 *   that is used in the QR operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Scale Limitation
 * - The \p a_desc must be the same as \p a_desc in ::cnnlQR function.
 * - The \p some must be the same as \p some in ::cnnlQR function.
 *
 * @par API Dependency
 * - This function must be called before the ::cnnlQR function.
 *
 * @note
 * - None.
 *
 * @par Example
 * - None.
 */

cnnlStatus_t CNNL_WIN_API cnnlGetQRWorkspaceSize(cnnlHandle_t handle,
                                                 const cnnlTensorDescriptor_t a_desc,
                                                 const bool some,
                                                 size_t *size);

// Group:Orgqr
/*!
 * @brief Returns in \p size the size of the MLU memory that is used to save the
 * intermediate result of ::cnnlOrgqr operation.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the function. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  input_desc
 *   Input. The descriptor of \p input tensor.
 * @param[out]  size
 *   Output. A host pointer to the returned size of the extra workspace in bytes
 *   that is used in the ::cnnlOrgqr function.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS
 *
 * @par API Dependency
 * - This function must be called before the ::cnnlOrgqr function.
 *
 * @note
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetOrgqrWorkspaceSize(cnnlHandle_t handle,
                                                    const cnnlTensorDescriptor_t input_desc,
                                                    size_t *size);

// Group:Orgqr
/*!
 * @brief Computes the first n columns of a product of Householder matrices.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  input_desc
 *   Input. The descriptor of input tensor \p input. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  input
 *   Input. Pointer to the MLU memory that stores the \p input tensor.
 * @param[in]  tau_desc
 *   Input. The descriptor of input tensor \p tau. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  tau
 *   Input. Pointer to the MLU memory that stores the \p tau tensor.
 * @param[in] output_desc
 *   Input. The descriptor of output tensor \p output. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out]  output
 *   Output. Pointer to the MLU memory that is used to store the output tensor \p output.
 * @param[in]  workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the operation.
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the operation. You can get the size of the workspace with
 *   the ::cnnlGetOrgqrWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Orgqr Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Scale Limitation
 * - The shape of tensor \p input, \p tau and \p output must meet the following restrictions:
 *   - input.dim >= 2
 *   - input.shape[-2] >= input.shape[-1]
 *   - input.dim - tau.dim == 1
 *   - input.shape == output.shape
 *   - input.shape[:-2] == tau.shape[:-1]
 *   - input.shape[-1] >= tau.shape[-1]
 *
 * @par Data Type
 * - The supported data types of input and output tensors are as follows:
 *   - \p input tensor: float.
 *   - \p tau tensor: float.
 *   - \p output tensor: float.
 *
 * @par Data Layout
 * - The supported data layout of the input, tau and output tensors must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par API Dependency
 * - Before using this API, you need to get the size of the workspace with the
 *   ::cnnlGetOrgqrWorkspaceSize function and pass the required extra workspace
 *   to the ::cnnlOrgqr function.
 *
 * @par Requirements
 * - None.
 *
 * @note
 * - When the input contains nan/inf, the result may not be equal with CPU/GPU.
 * - After multiple matrix multiplications, the output may overflow and has NaN or inf results,
 *   which acts differently with CPU/GPU. You can reduce the probability by generating
 *   the inputs using geqrf on CPU/GPU.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.linalg.householder_product.html
 */
cnnlStatus_t CNNL_WIN_API cnnlOrgqr(cnnlHandle_t handle,
                                    const cnnlTensorDescriptor_t input_desc,
                                    const void *input,
                                    const cnnlTensorDescriptor_t tau_desc,
                                    void *tau,
                                    const cnnlTensorDescriptor_t output_desc,
                                    void *output,
                                    void *workspace,
                                    size_t workspace_size);

// Group:IndexPut
/*!
 * @brief Inserts \p values into the input tensor according to the locations
 * described in the \p indices list.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the index put operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] indices_desc
 *   Input. The list of descriptors of the indices indicating the locations of input
 *   to be inserted. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] indices
 *   Input. A host pointer to a list of MLU pointers, which point to the MLU memory storing
 *   the indices tensors.
 * @param[in] indices_num
 *   Input. The length of the list of indices tensors.
 * @param[in] values_desc
 *   Input. The descriptor of the values to be inserted. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] values
 *   Input. Pointer to the MLU memory that stores the values tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the index put operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the index put operation.
 *   You can get the size of the workspace with the ::cnnlGetIndexPutWorkspaceSize function.
 * @param[in] accumulate
 *   Input. A Boolean that controls the behavior of inserting values to the input. If accumulate is false,
 *   the values will replace the original input. Otherwise, the values will accumulate to the
 *   original input.
 * @param[in] unsafe
 *   Input. A Boolean that controls whether to check all indices are valid. If unsafe is false, the operation
 *   will checkout whether any index values is over bound. Otherwise,
 *   the operation will not check the validity of index values.
 *   Note that this option is only valid when accumulate is true.
 *   Note that \p unsafe = false is not supported currently.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Index Put Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensors \p input, \p values and \p output must be the same.
 *   Data types of \p indices should be int32, int64, bool or uint8. Note that int64 is not supported on 1V.
 *   The data type of \p indices must be the same.
 *   If \p accumulate is false, \p input, \p values and \p output of the
 *   (I/O)function support the following data widths:
 *   - 1-byte, 2-byte, 4-byte, 8-byte.
 *   If \p accumulate is true, \p input, \p values and \p output of the function
 *   support the following data type(note that bf16 supported on MLU500 or higher):
 *   - half, bfloat16, float, int32, int16, int8, uint8.
 *
 * @par Scale Limitation
 * - If the data type of \p indices is bool or uint8, the number of non-null pointer in \p indices_desc should be 1.
 * - If the data type of \p indices is int32 or int64, the length of the \p indices
 *   list \p indices_num should be less than or equal to the dimension size of \p input.
 * - If the data type of \p indices is int32 or int64, inferred_indices_desc->dim + input_desc->dim - indices_not_nullptr_num <= 8,
 *   where inferred_indices_desc->dim represents the dimension of target indice shape inferred from \p indices_desc,
 *   input_desc->dim represents the dimension of the input tensor.
 *   indices_not_nullptr_num represents the number of non-null pointers in \p indices_desc list.
 * - The element in \p indices_desc can be NULL, but all of them cannot be NULL at the same time.
 * - When the element in \p indices_desc is NULL, the corresponding element in \p indices must be NULL.
 * - When the element in \p indices_desc is non-null, the corresponding element in \p indices must be non-null.
 * - The alignment of all indices in the \p indices list should be contiguous in the MLU memory.
 * - The alignment of \p values should be contiguous in the MLU memory.
 * - The alignment of \p input should be contiguous in the MLU memory.
 * - The alignment of \p output should be contiguous in the MLU memory.
 * - The data types of all indices in the \p indices_desc list should be the same.
 * - Mixed data type of indices is not supported.
 * - \p unsafe = false is not supported.
 * - When the data type of \p indices is int32 or int64, negative indexing is supported.
 *   However, unexpected results may occur if the values of indices are not in range of [-a, a),
 *   where a is the corresponding dimension size of input.
 * - When \p accumulate is true and the data type of \p input is int32, the accumulation result must be in range of (\f$-2^{23}\f$,\f$2^{23}\f$) on MLU200 series and 1V.
 * - When \p accumulate is true and the data type of \p input is int32, int16, int8 or uint8, the accumulation result supports overflow on MLU500 series. While on platforms earlier than MLU500 series, the overflow is not supported and undefined behaviors may occur if the overflow arises.
 * @par API Dependency
 * - Before calling this function to implement index put, you need to call
 * ::cnnlGetIndexPutWorkspaceSize to get the extra space size needed in index put operation.
 *
 * @note
 * - If accumulate is false and duplicate indices exist, which means that there are
 *   several values to be inserted into the same location, the result will be
 *   non-deterministic because the order of values to be inserted is not guaranteed.
 * - When \p accumulate is true, the type of \p indices is int32 or int64, and the type of \p values is half or float,
 *   the precision may be poor if the same location is added up too many times on MLU200 series.
 * - Supports in-place operation, that output equals input.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of index put operation are as follows:
     @verbatim
     input: [[0,0,0], [0,0,0]]
     indices[2]: [0,0,1], [1,2,1]
     values: [1.0]
     accumulate: false
     output: [[0,1,1],[0,1,0]]


     input: [[0,0,0],[0,0,0]]
     indices[1]: [True, False]
     values: [1, 2, 3]
     accumulate: false
     output: [[1,2,3],[0,0,0]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/cppdocs/notes/tensor_indexing.html
 * - https://numpy.org/docs/stable/reference/arrays.indexing.html
 */
cnnlStatus_t CNNL_WIN_API cnnlIndexPut(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t input_desc,
                                       const void *input,
                                       const cnnlTensorDescriptor_t indices_desc[],
                                       const void *const *indices,
                                       const int indices_num,
                                       const cnnlTensorDescriptor_t values_desc,
                                       const void *values,
                                       void *workspace,
                                       const size_t workspace_size,
                                       const bool accumulate,
                                       const bool unsafe,
                                       const cnnlTensorDescriptor_t output_desc,
                                       void *output);

// Group:IndexPut
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace to
 *        optimize the index put operation.
 *
 * @param[in] handle
 *  Input: Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *         queues operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] indices_desc
 *   Input. The list of descriptors of the indices indicating the locations of input
 *   to be inserted. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] indices_num
 *   Input. The length of the list of indices tensors.
 * @param[in] values_desc
 *   Input. The descriptor of the values to be inserted. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] accumulate
 *   Input. A Boolean that controls the behavior of inserting values to the input. If accumulate is false,
 *   the values will replace the original input. Otherwise, the values will accumulate to the
 *   original input.
 * @param[out] size
 *  Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *          the index put operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - Parameters must meet the following requirements:
 *   - The values of \p input_desc, \p indices_desc, \p indices_num, \p values_desc and \p accumulate
 *   should be set to be the same as the ones in the ::cnnlIndexPut operation.
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlIndexPut function to perform the
 *   index put operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetIndexPutWorkspaceSize(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t input_desc,
                                       const cnnlTensorDescriptor_t indices_desc[],
                                       const int indices_num,
                                       const cnnlTensorDescriptor_t values_desc,
                                       const bool accumulate,
                                       size_t *size);
// Group:Roll
/*!
 * @brief Rolls the input tensor according to the given dimensions and shifts. If no dimension is given,
 * the tensor will be regarded as a flattened tensor before rolling and restored to the original
 * shape.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlRoll_v2 instead, which supports the shape and shift of each dimension larger than
 *   2G number.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the rolling operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] shifts
 *   Input. A host pointer pointing to the host memory that stores the rolling length of each dimension.
 * @param[in] shifts_num
 *   Input. The length of input pointer \p shifts.
 * @param[in] dims
 *   Input. A host pointer pointing to the host memory that stores the rolling dimensions.
 * @param[in] dims_num
 *   Input. The length of input pointer \p dims.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the rolling operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the rolling operation.
 *   You can get the size of the workspace with the ::cnnlGetRollWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Roll Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor \p input must be the same as those of \p output.
 *   Input tensor \p input supports any data type supported by Cambricon CNNL except \p CNNL_DTYPE_INVALID.
 *
 * @par Scale Limitation
 * - The count of elements in input must be smaller than \f$2^{23}-1\f$.
 *
 * @par API Dependency
 * - Before calling this function, you need to call
 * ::cnnlGetRollWorkspaceSize to get the extra space size needed in rolling operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The examples of rolling operation are as follows:
     @verbatim
     input: [[0,1,2], [3,4,5]]
     shifts: [2]
     dims: []
     output: [[4,5,0],[1,2,3]]

     input: [[0,1,2],[3,4,5]]
     shifts: [1,2]
     dims: [0,1]
     output: [[4,5,3],[1,2,0]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/1.9.0/generated/torch.roll.html?highlight=roll#torch.roll
 */
CNNL_DEPRECATED_FOR(cnnlRoll_v2)
cnnlStatus_t CNNL_WIN_API cnnlRoll(cnnlHandle_t handle,
                                   const cnnlTensorDescriptor_t input_desc,
                                   const void *input,
                                   const int shifts[],
                                   const int shifts_num,
                                   const int dims[],
                                   const int dims_num,
                                   void *workspace,
                                   const size_t workspace_size,
                                   const cnnlTensorDescriptor_t output_desc,
                                   void *output);
// Group:Roll
/*!
 * @brief Rolls the input tensor according to the given dimensions and shifts. If no dimension is given,
 * the tensor will be regarded as a flattened tensor before rolling and restored to the original
 * shape. Compared with ::cnnlRoll, ::cnnlRoll_v2 supports the shape and shift of each dimension larger than
 * 2G number.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the rolling operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] shifts
 *   Input. A host pointer pointing to the host memory that stores the rolling length of each dimension.
 * @param[in] shifts_num
 *   Input. The length of input pointer \p shifts.
 * @param[in] dims
 *   Input. A host pointer pointing to the host memory that stores the rolling dimensions.
 * @param[in] dims_num
 *   Input. The length of input pointer \p dims.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the rolling operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the rolling operation.
 *   You can get the size of the workspace with the ::cnnlGetRollWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Roll Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor \p input must be the same as those of \p output.
 *   Input tensor \p input supports any data type supported by Cambricon CNNL except \p CNNL_DTYPE_INVALID.
 *
 * @par Scale Limitation
 * - The count of elements in input must be smaller than \f$2^{23}-1\f$.
 *
 * @par API Dependency
 * - Before calling this function, you need to call
 * ::cnnlGetRollWorkspaceSize to get the extra space size needed in rolling operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The examples of rolling operation are as follows:
     @verbatim
     input: [[0,1,2], [3,4,5]]
     shifts: [2]
     dims: []
     output: [[4,5,0],[1,2,3]]

     input: [[0,1,2],[3,4,5]]
     shifts: [1,2]
     dims: [0,1]
     output: [[4,5,3],[1,2,0]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/1.9.0/generated/torch.roll.html?highlight=roll#torch.roll
 */
cnnlStatus_t CNNL_WIN_API cnnlRoll_v2(cnnlHandle_t handle,
                                   const cnnlTensorDescriptor_t input_desc,
                                   const void *input,
                                   const int64_t shifts[],
                                   const int shifts_num,
                                   const int64_t dims[],
                                   const int dims_num,
                                   void *workspace,
                                   const size_t workspace_size,
                                   const cnnlTensorDescriptor_t output_desc,
                                   void *output);
// Group:Roll
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace to
 *        optimize the rolling operation.
 *
 * @param[in] handle
 *  Input: Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *         queues operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] size
 *  Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *          the rolling operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - Parameters must meet the following requirements:
 *   - The values of \p input_desc should be set to be the same as the ones in the ::cnnlRoll operation.
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlRoll function to perform the
 *   rolling operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetRollWorkspaceSize(cnnlHandle_t handle,
                                                   const cnnlTensorDescriptor_t input_desc,
                                                   size_t *size);

// Group:ApplyAdaMax
/*!
 * @brief Updates filter using AdaMax algorithm.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the applyadamax operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  var_desc
 *   Input. The descriptor of the variable tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out]  var
 *   Input. Pointer to the MLU memory that stores the variable tensor to be updated.
 * @param[in]  momentum_desc
 *   Input. The descriptor of the momentum tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out]  momentum
 *   Input. Pointer to the MLU memory that stores the first momentum vector.
 * @param[in]  velocity_desc
 *   Input. The descriptor of the velocity tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out]  velocity
 *   Input. Pointer to the MLU memory that stores the velocity tensor.
 * @param[in]  diff_desc
 *   Input. The descriptor of the gradient tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  diff
 *   Input. Pointer to the MLU memory that stores the gradient tensor.
 *   The diff is the gradients for updating variable. The values in this tensor
 *   should be in range of [-500,500] for high accuracy.
 * @param[in]  lr
 *   Input. The parameter of lr data. The device pointer to learning rate.
 *   The value of this parameter should be in range of [0,1].
 * @param[in]  beta1
 *   Input. The parameter of beta1 data. The device pointer to exponential decay rate for
 *   momentum. The value of this parameter should be in range of [0,1].
 * @param[in]  beta2
 *   Input. The parameter of beta2 data. The device pointer to exponential decay rate for
 *   velocity. The value of this parameter should be in range of [0,1].
 * @param[in]  beta1_power
 *   Input. The parameter of beta1_power data.
 *   The value of this parameter should be in range of [0,1).
 * @param[in]  epsilon
 *   Input. The parameter of epsilon data. The device pointer to a small positive number just as \f$10^{-8}\f$, to
 *   avoid division by 0. The value of this parameter should be in range of (0,1].
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - All inputs and outputs should be of the same size.
 * - The number of dimensions is no more than 8.
 *
 * @par Data Type
 * - input: float, half.
 * - output: float, half.
 *
 *  @par Formula
 *  - See "ApplyAdaMax Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlApplyAdaMax(cnnlHandle_t handle,
                                          const cnnlTensorDescriptor_t var_desc,
                                          void *var,
                                          const cnnlTensorDescriptor_t momentum_desc,
                                          void *momentum,
                                          const cnnlTensorDescriptor_t velocity_desc,
                                          void *velocity,
                                          const cnnlTensorDescriptor_t diff_desc,
                                          const void *diff,
                                          const void *lr,
                                          const void *beta1,
                                          const void *beta2,
                                          const void *beta1_power,
                                          const void *epsilon);

// Group:ApplyAdam
/*!
 * @brief Implements the Adam optimization algorithm. Adam is a stochastic gradient
 * descent method that computes individual adaptive learning rates for different parameters
 * from estimates of first-order and second moments of the gradients.
 * Set \p use_nesterov = true if you want to use Nesterov Adam, otherwise the default
 * Adam is used.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  var_desc
 *   Input. A descriptor of input tensor \p var. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out]  var
 *  Input and output. Pointer to the MLU memory that stores the \p var tensor to be updated
 *  according to Adam algorithm. This input refers to filter in the artificial intelligence generally.
 * @param[in]  momentum_desc
 *   Input. A descriptor of input tensor \p momentum. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out]  momentum
 *   Input and output. Pointer to the MLU memory that stores the first-order momentum.
 * @param[in]  velocity_desc
 *   Input. A descriptor of input tensor \p velocity. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out]  velocity
 *   Input and output. Pointer to the MLU memory that stores the second-order momentum.
 * @param[in]  diff_desc
 *   Input. The descriptor of \p diff tensor. For detailed information,
 *  see ::cnnlTensorDescriptor_t.
 * @param[in]  diff
 *  Input. Pointer to the MLU memory that stores the \p diff tensor. The \p diff is the gradient
 *  for updating \p var.
 * @param[in]  lr
 *   Input. Pointer to the MLU memory that stores the learning rate.
 * @param[in]  beta1
 *   Input. Pointer to the MLU memory that stores the first estimated exponential decay rate.
 * @param[in]  beta2
 *   Input. Pointer to the MLU memory that stores the second estimated exponential decay rate.
 * @param[in]  beta1_power
 *   Input. Pointer to the MLU memory that stores the accumulator of beta1.
 * @param[in]  beta2_power
 *   Input. Pointer to the MLU memory that stores the accumulator of beta2.
 * @param[in]  epsilon
 *   Input. Pointer to the MLU memory that stores a small positive number just as \f$10^{-8}\f$, to
 *   avoid division by 0.
 * @param[in]  use_nesterov
 *   Input. Determines whether to use Nesterov Adam to update \p var or not.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "ApplyAdam Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Scale Limitation
 * - The data shape of input tensors and output tensors must be the same.
 * - The number of dimensions of each input tensor should be no more than 8.
 * - The values in tensor \p velocity should be in range of [0,65504].
 * - The values in tensor \p diff should be in range of [-255,255].
 * - The value \p lr points to should be in range of [0,1].
 * - The value \p beta1 points to should be in range of [0,1].
 * - The value \p beta2 points to should be in range of [0,1].
 * - The value \p beta1_power points to should be in range of [0,1).
 * - The value \p beta2_power points to should be in range of [0,1].
 * - The value \p epsilon points to should be in range of (0,1].
 * - The var tensor size should be less than \f$2^{31}\f$.
 *
 * @par Data Type
 * - Data type of input tensors and output tensors must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensors: half, float.
 *   - output tensors: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensors and output tensors must be \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the ApplyAdam operation is as follows:
    @verbatim
     --> var: an array [64, 78, 1024];
     --> momentum: an array [64, 78, 1024];
     --> velocity: an array [64, 78, 1024];
     --> diff: an array [64, 78, 1024];
     --> lr: an array [1];
     --> beta1: an array [1];
     --> beta2: an array [1];
     --> beta1_power: an array [1];
     --> beta2_power: an array [1];
     --> epsilon: an array [1];
     --> use_nesterov: false;
     Then we will get the output:
     --> var: an array [64, 78, 1024] same as input;
     --> momentum: an array [64, 78, 1024] same as input;
     --> velocity: an array [64, 78, 1024] same as input;
    @endverbatim
 */

cnnlStatus_t CNNL_WIN_API cnnlApplyAdam(cnnlHandle_t handle,
                                        const cnnlTensorDescriptor_t var_desc,
                                        void *var,
                                        const cnnlTensorDescriptor_t momentum_desc,
                                        void *momentum,
                                        const cnnlTensorDescriptor_t velocity_desc,
                                        void *velocity,
                                        const cnnlTensorDescriptor_t diff_desc,
                                        const void *diff,
                                        const void *lr,
                                        const void *beta1,
                                        const void *beta2,
                                        const void *beta1_power,
                                        const void *beta2_power,
                                        const void *epsilon,
                                        const bool use_nesterov);

// Group:ApplyAdadelta
/*!
 * @brief Implements the adadelta optimization algorithm. adadelta is a per-dimension
 * learning rate method for stochastic gradient descent method.
 * The main advantage of adadelta is that we do not need to set a default learning rate when training the model.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] var_desc
 *   Input. A descriptor of input tensor \p var. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out] var
 *  Input and output. Pointer to the MLU memory that stores the \p var tensor to be updated
 *  according to Adadelta algorithm. This input refers to filter in the artificial intelligence generally.
 * @param[in] accum_desc
 *   Input. A descriptor of input tensor \p accum. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out]  accum
 *   Input and output. Pointer to the MLU memory that stores the
 *   moving average of the second momentum of the gradient.
 * @param[in] accum_update_desc
 *   Input. A descriptor of input tensor \p accum_update. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out] accum_update
 *   Input and output. Pointer to the MLU memory that stores the
 *    moving average of the second momentum of the change of parameters in the trainable model itself.
 * @param[in] diff_desc
 *   Input. The descriptor of \p diff tensor. For detailed information,
 *  see ::cnnlTensorDescriptor_t.
 * @param[in] diff
 *  Input. Pointer to the MLU memory that stores the \p diff tensor. The \p diff is the gradient
 *  for updating \p var.
 * @param[in] lr
 *   Input. Pointer to the MLU memory that stores the learning rate.
 * @param[in] rho
 *   Input. Pointer to the MLU memory. The \p rho is the decay rate used to update accum and accum_update.
 * @param[in] epsilon
 *   Input. Pointer to the MLU memory that stores a small positive number just as \f$10^{-8}\f$, to
 *   avoid division by 0.

 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "ApplyAdadelta Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Scale Limitation
 * - The data shape of input tensors and output tensors must be the same.
 * - The number of dimensions of each input tensor should be no more than \p CNNL_DIM_MAX.
 * - The values in tensor \p accum should be in range of [0, 1e6].
 * - The values in tensor \p accum_update should be in range of [0, 1e6].
 * - The values in tensor \p diff should be in range of [-1e3,1e3].
 * - The value \p lr points to should be in range of [0,1].
 * - The value \p rho points to should be in range of [0,1].
 * - The value \p epsilon points to should be in range of (0,1].
 *
 * @par Data Type
 * - Data types of input tensors and output tensors must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensors: half, float.
 *   - output tensors: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensors and output tensors must be \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the ApplyAdadelta operation is as follows:
    @verbatim
     --> var: an array [64, 78, 1024];
     --> accum: an array [64, 78, 1024];
     --> accum_update: an array [64, 78, 1024];
     --> diff: an array [64, 78, 1024];
     --> lr: an array [1];
     --> rho: an array [1];
     --> epsilon: an array [1];
     Then we will get the output:
     --> var: an array [64, 78, 1024] same shape with input;
     --> accum: an array [64, 78, 1024] same shape with input;
     --> a: an array [64, 78, 1024] same shape with input;
    @endverbatim
 *
 * @par Reference
 * - Zeiler,M.D.(2012). Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701
 * - http://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/training/adadelta.py
 */
cnnlStatus_t CNNL_WIN_API cnnlApplyAdadelta(cnnlHandle_t handle,
                                            const cnnlTensorDescriptor_t var_desc,
                                            void *var,
                                            const cnnlTensorDescriptor_t accum_desc,
                                            void *accum,
                                            const cnnlTensorDescriptor_t accum_update_desc,
                                            void *accum_update,
                                            const cnnlTensorDescriptor_t diff_desc,
                                            const void *diff,
                                            const void *lr,
                                            const void *rho,
                                            const void *epsilon);
// Group:ScatterNd
/*!
 * @brief Distributes the elements in tensor \p updates to tensor \p output according to the coordinates
 * in tensor \p indices. If \p indices contains duplicates, then their \p updates are accumulated. This
 * operator is the inverse of the ::cnnlGatherNd operator which extracts values or slices from a given tensor.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlScatterNd_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the scatter_nd operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] indices_desc
 *   Input. The descriptor of the \p indices tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] indices
 *   Input. Pointer to the MLU memory that stores the index data. The value of the lowest dimension in \p indices
 *   represents the coordinate of the element of \p updates in \p output tensor. If the coordinate contains negative
 *   numbers or numbers that exceeds the range, then it will be deprecated.
 * @param[in] updates_desc
 *   Input. The descriptor of the \p updates tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] updates
 *   Input. Pointer to the MLU memory that stores the input data.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output data.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "ScatterNd Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The ScatterNd operation supports the following data types for input tensor \p indices, \p updates,
 * and output tensor \p output.
 * The data type of \p updates and \p output must be the same.
 * - indices: int32, int64.
 * - updates: int32, half, bfloat16, float.
 * - output: int32, half, bfloat16, float.
 * - The bfloat16 data type is supported only on MLU500 series.
 * @par Scale Limitation
 * - If the rank of \p indices is n and indices[n-1] is ix, the shape of tensor \p indices, \p updates
 *   and \p output must meet the following restrictions:
 *
 *   indices.shape[0, n-2] = updates.shape[0, n-2]
 *   updates.rank - (n-1) = output.rank - ix
 *   updates.shape[n-1, updates.rank] = output.shape[ix, output.rank]
 *
 * @note
 * - This operator only supports TensorFlow framework.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of scatter_nd operation is as follows:
   @verbatim
    The shape of output: [4, 4, 4].
    indices: [[0], [2]]
    updates: [[[5, 5, 5, 5], [6, 6, 6, 6],
               [7, 7, 7, 7], [8, 8, 8, 8]],
              [[5, 5, 5, 5], [6, 6, 6, 6],
               [7, 7, 7, 7], [8, 8, 8, 8]]]

    -->output: [[[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]],
                [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],
                [[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]],
                [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]]

   @endverbatim
 */
CNNL_DEPRECATED_FOR(cnnlScatterNd_v2)
cnnlStatus_t CNNL_WIN_API cnnlScatterNd(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t indices_desc,
                                       const void *indices,
                                       const cnnlTensorDescriptor_t updates_desc,
                                       const void *updates,
                                       const cnnlTensorDescriptor_t output_desc,
                                       void *output);

// Group:ScatterNd
/*!
 * @brief Distributes the elements in tensor \p updates to tensor \p output according to the coordinates
 * in tensor \p indices. Compared with ::cnnlScatterNd, this function supports the \p mode parameter that contains
 * more calculation methods when \p indices contains duplicates. This operation is the inverse of the ::cnnlGatherNd
 * operator which extracts values or slices from a given tensor.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the scatter_nd operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] mode
 *  Input. The scatter_nd mode used when \p indices contains duplicates. For detailed information,
 *  see ::cnnlScatterNdMode_t.
 * @param[in] indices_desc
 *   Input. The descriptor of the \p indices tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] indices
 *   Input. Pointer to the MLU memory that stores the index data. The value of the lowest dimension in \p indices
 *   represents the coordinate of the element of \p updates in \p output tensor. If the coordinate contains negative
 *   numbers or numbers that exceeds the range, then it will be ignored.
 * @param[in] updates_desc
 *   Input. The descriptor of the \p updates tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] updates
 *   Input. Pointer to the MLU memory that stores the data for updating.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input data.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output data.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "ScatterNd Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The ScatterNd operation supports the following data types for input tensors \p indices and \p updates,
 *   and output tensor \p output.
 *   - indices: int32, int64, uint32, uint64.
 *   - updates: int32, half, bfloat16, float.
 *   - output: int32, half, bfloat16, float.
 * - When using the update mode (\p mode is ::CNNL_SCATTERND_UPDATE) on MLU300 series or MLU500 series,
 *   the ScatterNd operation supports the following data types for input tensors \p indices and \p updates
 *   and output tensor \p output.
 *   - indices: int32, int64, uint32, uint64.
 *   - updates: bool, int8, uint8, int16, uint16, half, bfloat16, int32, uint32, float, int64, uint64.
 *   - output: bool, int8, uint8, int16, uint16, half, bfloat16, int32, uint32, float, int64, uint64.
 *
 * The bfloat16 data type is supported only on MLU500 series.
 * @par Scale Limitation
 * - On MLU200 and 1V series, when the \p updates and \p output data type is int32 and \p mode = ::CNNL_SCATTERND_ADD,
 *   the values of \p updates should be in range of [\f$-2^{23}\f$, \f$2^{23}\f$].
 * - The number of dimensions is no more than \p CNNL_DIM_MAX.
 * - The shape of tensor \p indices, \p updates and \p output must meet the following restrictions:
 *   - indices.rank >= 2
 *   - indices.shape[-1] <= output.rank
 *   - if indices.shape[-1] == output.rank:
 *       updates.shape = indices.shape[:-1]
 *   - if indices.shape[-1] < output.rank:
 *       updates.shape = indices.shape[:-1] + output.shape[indices.shape[-1]:]
 *
 * @note
 * - When the \p input is NULL, it will be treated as zero vector. When the \p input is not NULL, the address
 *   can be equal to the \p output address, and in this case the performance is better.
 * - Note that the data types of \p updates and \p output must be the same.
 *   If \p input is not null, the data types of \p input and \p output must be the same.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of scatter_nd operation is as follows:
   @verbatim
    Example 1:
    The shape of output: [4,4,4].
    mode:    CNNL_SCATTERND_UPDATE
    indices: [[0], [0]]
    input:   [[[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]],
              [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]],
              [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]],
              [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]]]

    updates: [[[5, 5, 5, 5], [6, 6, 6, 6],
               [7, 7, 7, 7], [8, 8, 8, 8]],
              [[5, 5, 5, 5], [6, 6, 6, 6],
               [7, 7, 7, 7], [8, 8, 8, 8]]]

    -->output: [[[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]],
                [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]],
                [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]],
                [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]]]
    Example 2:
    The shape of output: [4,4,4].
    mode:    CNNL_SCATTERND_ADD
    indices: [[0], [0]]
    input:   NULL
    updates: [[[5, 5, 5, 5], [6, 6, 6, 6],
               [7, 7, 7, 7], [8, 8, 8, 8]],
              [[5, 5, 5, 5], [6, 6, 6, 6],
               [7, 7, 7, 7], [8, 8, 8, 8]]]

    -->output: [[[10, 10, 10, 10], [12, 12, 12, 12], [14, 14, 14, 14], [16, 16, 16, 16]],
                [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],
                [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],
                [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]]
   @endverbatim
 */
cnnlStatus_t CNNL_WIN_API cnnlScatterNd_v2(cnnlHandle_t handle,
                                           cnnlScatterNdMode_t mode,
                                           const cnnlTensorDescriptor_t indices_desc,
                                           const void *indices,
                                           const cnnlTensorDescriptor_t updates_desc,
                                           const void *updates,
                                           const cnnlTensorDescriptor_t input_desc,
                                           const void *input,
                                           const cnnlTensorDescriptor_t output_desc,
                                           void *output);

// Group:GatherNd
/*!
 * @brief Gathers slices from \p params with shape specified by \p indices.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGatherNd_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the gather_nd
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] desc_params
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] params
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] desc_indices
 *   Input. The descriptor of the index tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] indices
 *   Input. Pointer to the MLU memory that stores the index of each element of \p output in the
 *   corresponding dimension of input tensor \p params.
 * @param[in] desc_output
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @par Formula
 * - See "GatherNd Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The (I/O)function supports the following data widths for \p params and \p output tensors.
 *   - params tensor: 1-byte, 2-byte, 4-byte, 8-byte.
 *   - index tensor: int32, int64.
 *   - output tensor: 1-byte, 2-byte, 4-byte, 8-byte.
 *
 *   The byte width of a data type can be got with the ::cnnlGetSizeOfDataType function.
 *
 *   Note that the data type of input tensor \p params and output tensor \p output must be the same.
 *
 * @note
 * - The item in \p indices must be in range of [-rank, rank), where rank is the element size
 *   of each dimension of \p params. For example, params.shape is [3,2], indices' first data item be in
 *   [-3, 3) and second item must be in [-2, 2).
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the gather_nd operation is as follows:
     @verbatim
     input two arrays both by 3 * 2 --> params: [[1., 2.], [3., 4.], [5., 6.]]

     --> indices: [[-1, 0], [1, 1]]

     output array by 2 --> output: [5., 4.]
     @endverbatim
 *
 * @par Reference
 * - https://tensorflow.org/api_docs/python/tf/raw_ops/GatherNd
 */
CNNL_DEPRECATED_FOR(cnnlGatherNd_v2)
cnnlStatus_t CNNL_WIN_API cnnlGatherNd(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t desc_params,
                                       const void *params,
                                       const cnnlTensorDescriptor_t desc_indices,
                                       const void *indices,
                                       const cnnlTensorDescriptor_t desc_output,
                                       void *output);

// Group:GatherNd
/*!
 * @brief Gathers slices from \p params with shape specified by \p indices. Compared with ::cnnlGatherNd,
 * this function supports the \p mode parameter.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the gather_nd
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] mode
 *   Input. An integer value that determines the mode to use. For detailed information, see note.
 * @param[in] desc_params
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] params
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] desc_indices
 *   Input. The descriptor of the index tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] indices
 *   Input. Pointer to the MLU memory that stores the index of each element of \p output in the
 *   corresponding dimension of input tensor \p params.
 * @param[in] desc_output
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @par Formula
 * - See "GatherNd Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The (I/O)function supports the following data widths for \p params and \p output tensors.
 *   - params tensor: 1-byte, 2-byte, 4-byte, 8-byte.
 *   - index tensor: int32, int64.
 *   - output tensor: 1-byte, 2-byte, 4-byte, 8-byte.
 *
 *   The byte width of a data type can be got with the ::cnnlGetSizeOfDataType function.
 *
 *   Note that the data type of input tensor \p params and output tensor \p output must be the same.
 *
 * @note
 * - In \p mode = 0, the item in \p indices must be in range of [-rank, rank), where rank is the element size
 *   of each dimension of \p params. For example, params.shape is [3,2], indices' first item must be in
 *   [-3, 3) and second item must be in [-2, 2). If an out-of-bound index is found, core dump will occur.
 * - In \p mode = 1, the item in \p indices will be in range of [0, rank). if an out-of-bound index is found,
 *   a 0 is stored in the corresponding \p output value.
 *
 * @par Scale Limitation
 * - For mode = 1, the size of each tensor must be less than 2 GBytes.
 * - The shape of tensor \p params, \p indices, and \p output must meet the following restrictions:
 *   - indices.rank >= 2
 *   - indices.shape[-1] <= params.rank
 *   - if indices.shape[-1] == params.rank:
 *       output.shape = indices.shape[:-1]
 *   - if indices.shape[-1] < params.rank:
 *       output.shape = indices.shape[:-1] + params.shape[indices.shape[-1]:]
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the gather_nd operation is as follows:
     @verbatim
     input two arrays both by 3 * 2 --> params: [[1., 2.], [3., 4.], [5., 6.]]

     --> indices: [[-1, 0], [1, 1]]

     output array by 2 --> output: [5., 4.]
     @endverbatim
 *
 * @par Reference
 * - https://tensorflow.org/api_docs/python/tf/raw_ops/GatherNd
 */
cnnlStatus_t CNNL_WIN_API cnnlGatherNd_v2(cnnlHandle_t handle,
                                          const int mode,
                                          const cnnlTensorDescriptor_t desc_params,
                                          const void *params,
                                          const cnnlTensorDescriptor_t desc_indices,
                                          const void *indices,
                                          const cnnlTensorDescriptor_t desc_output,
                                          void *output);

// Group:Addcmul
/*!
 * @brief Performs addition and multiplication operations with the following formula:
 *
 * \p a + \p alpha * \p b * \p c
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the addcmul
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] desc_a
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] alpha
 *   Input. A host pointer to the compute result that scales the result of multiplication. The data type of this parameter is float.
 * @param[in] desc_b
 *   Input. The descriptor of the multiplier tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. Pointer to the MLU memory that stores the multiplier tensor.
 * @param[in] desc_c
 *   Input. The descriptor of the multiplier tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] c
 *   Input. Pointer to the MLU memory that stores the multiplier tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the addcmul operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the addcmul
 *   operation. You can get the size of the workspace with the ::cnnlGetAddcmulWorkspaceSize_v2
 *   function.
 * @param[in] desc_output
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Addcmul Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \p a, multiplier tensor \p b,
 *   multiplier tensor \p c, and output tensor \p output. Data type of all above tensors should be
 *   the same.
 *   - All above tensors: half, float, bfloat16.
 *
 * @par Data Layout
 * - None.
 *
 * @par Scale Limitation
 * - The input tensor, multiplier tensors, and output tensor must meet the following requirements:
 *   - input tensor: Every dimension should be divisible by the same dimension in output tensor.
 *   - multiplier tensors: Every dimension should be divisible by the same dimension in output
 *                         tensor.
 *
 * @note
 * - You can specify the stride of all dimensions for desc_a, desc_b, desc_c and
 *   desc_output with ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the addcmul operation is as follows:
     @verbatim
     input three arrays by 2 * 2, 1 * 2 and 2 * 1 --> a: [[1, 1], [1, 1]]

     --> b: [4, 4]

     --> c: [[2], [2]]

     param:
       alpha: 0.5

     output array by 2 * 2 --> output: [[5, 5], [5, 5]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/1.0.0/torch.html?highlight=std#torch.addcmul
 */
cnnlStatus_t CNNL_WIN_API cnnlAddcmul(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t desc_a,
                                      const void *a,
                                      const void *alpha,
                                      const cnnlTensorDescriptor_t desc_b,
                                      const void *b,
                                      const cnnlTensorDescriptor_t desc_c,
                                      const void *c,
                                      void *workspace,
                                      size_t workspace_size,
                                      const cnnlTensorDescriptor_t desc_output,
                                      void *output);

// Group:Addcmul
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace to
 * optimize the addcmul operation.
 *
 * The size of the extra workspace is based on the given information of the addcmul operation,
 * including the input tensor descriptor \p desc_a, multiplier tensor descriptor \p desc_b, and
 * multiplier tensor descriptor \p desc_c. For more information about the workspace, see
 * "Cambricon CNNL User Guide".
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlGetAddcmulWorkspaceSize_v2 instead.
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the addcmul
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] desc_a
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] desc_b
 *   Input. The descriptor of the multiplier tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] desc_c
 *   Input. The descriptor of the multiplier tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   addcmul operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par API Dependency
 * - You need to call the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions to
 *   create and set the tensor descriptors \p desc_a, \p desc_b, and \p desc_c before calling this
 *   function.
 * - The allocated extra workspace should be passed to the ::cnnlAddcmul function to perform the
 *   addcmul operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlGetAddcmulWorkspaceSize_v2)
cnnlStatus_t CNNL_WIN_API cnnlGetAddcmulWorkspaceSize(cnnlHandle_t handle,
                                                      const cnnlTensorDescriptor_t desc_a,
                                                      const cnnlTensorDescriptor_t desc_b,
                                                      const cnnlTensorDescriptor_t desc_c,
                                                      size_t *size);
// Group:Addcmul
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace to
 * optimize the addcmul operation.
 *
 * The size of the extra workspace is based on the given information of the addcmul operation,
 * including the input tensor descriptor \p desc_a, multiplier tensor descriptor \p desc_b,
 * multiplier tensor descriptor \p desc_c, and output tensor descriptor \p desc_output. For more information
 * about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the addcmul
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] desc_a
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] desc_b
 *   Input. The descriptor of the multiplier tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] desc_c
 *   Input. The descriptor of the multiplier tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] desc_output
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   addcmul operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par API Dependency
 * - Before calling this function, call ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor to
 *   create and set the tensor descriptors \p desc_a, \p desc_b, \p desc_c and \p desc_output.
 * - The allocated extra workspace should be passed to the ::cnnlAddcmul function to perform the
 *   addcmul operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetAddcmulWorkspaceSize_v2(cnnlHandle_t handle,
                                                        const cnnlTensorDescriptor_t desc_a,
                                                        const cnnlTensorDescriptor_t desc_b,
                                                        const cnnlTensorDescriptor_t desc_c,
                                                        const cnnlTensorDescriptor_t desc_output,
                                                        size_t *size);
// Group:Split
/*!
 * @brief Splits an input tensor into \p split_num tensors on the specified dimension.
 * This operator is the inverse of the ::cnnlConcat operator.
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the split operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] split_num
 *   Input. Number of split output tensors.
 * @param[in] axis
 *   Input. Dimension along which to be split. The value must be in range of [-rank, rank),
 *         where rank is the dimension of the input. Negative \p axis refers to 'axis + rank'.
 * @param[in]  input_desc
 *   Input. The descriptor of input. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in]  input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in]  workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for
 *   the split operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in]  workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the
 *   split operation. You can get the size of the workspace with the
 *   ::cnnlGetSplitWorkspaceSize function.
 * @param[in]  output_desc
 *   Output. The list of descriptors of output tensors. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out]  outputs
 *   Output. A host pointer to a list of pointers, which point to the MLU memory
 *   that store the output tensors.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par Data Type
 * - This function supports the following data types for the input tensor \p input
 * and output tensors \p outputs.
 *  Note that all the tensors must have the same data type. If the input tensor \p input is
 * in fixed-point data type, the quantization parameters of all the tensors should be the same.
 *
 * - input: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float, bfloat16.
 * - output: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float, bfloat16.
 *
 * @par Scale Limitation
 * - Parameters must meet the following requirements:
 *   - The parameter \p split_num should be greater than 0.
 *   - The number of dimensions of all tensors must match, including input and outputs.
 *   - The dimensions of all outputs except \p axis must be equal, and the size of input on the splitting dimension
 *     should be equal to the sum of the outputs on \p axis.
 * @par API Dependency
 * - Before calling this function to implement split, you need to call
 * ::cnnlGetSplitWorkspaceSize to get the extra space size needed in split operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of split operation is as follows:
   @verbatim
   input: a tensor of 5 * 3 --> [[1,2,3],[4,5,6],[7,8,9],[10,11,12],[13,14,15]]

   split_num: 3

   axis: 0

   num_splits: an array of [2, 2, 1]

   Then we will get the output:

   outputs: 3 tensors with the shapes of 2 * 3, 2 * 3 and 1 * 3, respectively
        --> [[1,2,3],[4,5,6]]
        --> [[7,8,9],[10,11,12]]
        --> [[13,14,15]]
   @endverbatim
 *
 * @par Reference
 * - http://www.tensorflow.org/api_docs/python/tf/split
 */
cnnlStatus_t CNNL_WIN_API cnnlSplit(cnnlHandle_t handle,
                                    const int split_num,
                                    const int axis,
                                    const cnnlTensorDescriptor_t input_desc,
                                    const void *input,
                                    void *workspace,
                                    size_t workspace_size,
                                    const cnnlTensorDescriptor_t output_desc[],
                                    void *outputs[]);
// Group:Split
/*!
 *  @brief Returns in \p size the size of the MLU memory that is used as an extra workspace
 *  to optimize the split operation.
 *
 *  @param[in]  handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *    queues in the split operation. For detailed information, see ::cnnlHandle_t.
 *  @param[in]  split_num
 *    Input. Number of split output tensors.
 *  @param[out]  size
 *    Output. A host pointer to the returned size of the extra workspace in bytes
 *    that is used in the split operation.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @par Scale Limitation
 *  - Parameters must meet the following requirements:
 *    - The parameter \p split_num should be greater than 0.
 *
 *  @par API Dependency
 *  - This function must be called before the ::cnnlSplit function.
 *
 *  @note
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetSplitWorkspaceSize(cnnlHandle_t handle,
                                                    const int split_num,
                                                    size_t *size);

// Group:ForeachBinaryOp
/*!
 * @brief Uses binary op to compute under the following situations:
 *        - Compute tensors in two tensorlists.
 *        - Compute tensors and scalars in tensorlist and scalarlist.
 *        - Compute tensors and one scalar in tensorlist.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *          ::cnnlForeachBinaryOp operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] foreach_op_mode
 *   Input. Integer that marks a different calculation formula. For detailed information, see
 *          ::cnnlForeachOpMode_t.
 * @param[in] binary_compute_mode
 *   Input. Integer that marks a different process formula. For detailed information, see
 *          ::cnnlForeachBinaryMode_t.
 * @param[in] tensor_num
 *   Input. The number of \p input_self tensors.
 * @param[in] input_self_desc
 *   Input. The list of descriptors of \p input_self tensors. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[in] input_self
 *   Input. A host pointer to a list of MLU pointers, which points to the MLU memory that store the
 *          \p input tensors.
 * @param[in] input_other_desc
 *   Input. The list of descriptors of \p input_other tensors. When \p scalar is computed,
 *          \p input_other_desc can be nullptr. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input_other
 *   Input. A host pointer to a list of MLU pointers, which points to the MLU memory that store the
 *          \p input tensors. When \p scalar is computed, \p input_other can be nullptr.
 * @param[in] scalar_desc
 *   Input. The descriptor of \p scalar, which represents the input of the operation.
 *          When \p input_other list is computed, \p scalar_desc can be nullptr.
 *          For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] scalar
 *   Input. It can be a host pointer or an MLU pointer, which points to the MLU memory that store one or \p tensor_num scalars.
 *          - When \p binary_compute_mode is FOREACH_BINARY_SCALAR_LIST, \p scalar is a host pointer,
 *            and the number of \p scalar is \p tensor_num.
 *          - When \p binary_compute_mode is FOREACH_BINARY_SCALAR, \p scalar is a host pointer,
 *            and the number of \p scalar is \p tensor_num is one.
 *          - When \p binary_compute_mode is FOREACH_BINARY_SCALAR_TENSOR, \p scalar is an MLU pointer,
 *            and the number of \p scalar is \p tensor_num is one.
 *          - When \p input_other list is computed, \p scalar can be nullptr.
 * @param[in] alpha
 *   Input. A host value that scales the data of tensors in \p input_other list.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the foreach unary operation.
 *          For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the foreach unary operation.
 *          You can get the size of the workspace with the ::cnnlGetForeachUnaryOpWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The list of descriptors of \p output tensors. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. A host pointer to a list of MLU pointers, which points to the MLU memory that store
 *           the \p output tensors.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par Formula
 * - See "Foreach Binary Op Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of \p input_self tensors, \p input_other tensors and \p output tensors should be the same.
 * - The supported data types of \p input_self tensors, \p input_other tensors, \p scalar, \p alpha and \p output tensors are as follows:
 *   - \p input_self: half, bfloat16, float.
 *   - \p input_other: half, bfloat16, float.
 *     - When \p binary_compute_mode is FOREACH_BINARY_TENOSR_LIST, the data type of \p input_other is equal to \p input_self.
 *   - \p scalar:
 *     - When \p binary_compute_mode is FOREACH_BINARY_SCALAR or FOREACH_BINARY_SCALAR_LIST, \p scalar: float.
 *     - When \p binary_compute_mode is FOREACH_BINARY_SCALAR_TENSOR, \p scalar: half, bfloat16, float.
 *   - \p alpha: float.
 *   - \p output: half, bfloat16, float.
 *
 *  The data type bfloat16 is only supported on MLU500 series or above.
 *
 * @par Scale Limitation
 * - Parameters must meet the following conditions:
 *   - \p tensor_num should be greater than 0 and less than 2147483647.
 *
 * @par API Dependency
 * - None.
 *
 * @note
 * - The length of the list of \p input_self tensors should be equal to \p tensor_num.
 * - The length of the list of \p input_other tensors should be equal to \p tensor_num.
 * - The length of the list of \p output tensors should be equal to \p tensor_num.
 * - The shapes of the i-th \p input_self tensor and the i-th \p output tensor must match.
 * - The shapes of the i-th \p input_other tensor and the i-th \p output tensor must match.
 * - \p foreach_binary_op supports the following modes: CNNL_FOREACH_ADD, CNNL_FOREACH_SUB,
 *   CNNL_FOREACH_MUL and CNNL_FOREACH_DIV.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - http://github.com/pytorch/pytorch/blob/main/aten/src/Aten/cuda/ForeachBinaryOpScalar.cu
 */
cnnlStatus_t CNNL_WIN_API cnnlForeachBinaryOp(cnnlHandle_t handle,
                                              const cnnlForeachOpMode_t foreach_op_mode,
                                              const cnnlForeachBinaryMode_t binary_compute_mode,
                                              const int64_t tensor_num,
                                              const cnnlTensorDescriptor_t *input_self_desc,
                                              const void * const *input_self,
                                              const cnnlTensorDescriptor_t *input_other_desc,
                                              const void * const *input_other,
                                              const cnnlTensorDescriptor_t scalar_desc,
                                              const void *scalar,
                                              const void *alpha,
                                              void *workspace,
                                              size_t workspace_size,
                                              const cnnlTensorDescriptor_t *output_desc,
                                              void **output);

// Group:ForeachBinaryOp
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace to
 *        optimize the foreach_binary_op operation.
 *
 * @param[in] handle
 *  Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *  queues in the foreach_binary_op operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] tensor_num
 *  Input. The number of \p input_self tensors.
 * @param[in] input_self_desc
 *  Input. The list of descriptors of \p input_self tensors. For detailed information, see
 *         ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *  Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *          the foreach_binary_op operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - Parameters must meet the following requirements:
 *   - \p tensor_num should be greater than 0.
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlForeachBinaryOp function to perform the
 *   foreach_binary_op operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetForeachBinaryOpWorkspaceSize(cnnlHandle_t handle,
                                    const int64_t tensor_num,
                                    const cnnlTensorDescriptor_t *input_self_desc,
                                    size_t *workspace_size);

// Group:ForeachBinaryOp
/*!
 * @brief Initializes the data space \p workspace on host.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *          ::cnnlForeachBinaryOp operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] tensor_num
 *   Input. The number of input tensors.
 * @param[in] binary_compute_mode
 *   Input. Integer that marks a different process formula. For detailed information, see
 *          ::cnnlForeachBinaryMode_t.
 * @param[in] input_self_desc
 *   Input. The list of descriptors of input_self tensors. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[in] input_self
 *   Input. A host pointer to a list of MLU pointers, which points to the MLU memory that store the
 *          input tensors.
 * @param[in] input_other
 *   Input. A host pointer to a list of MLU pointers, which points to the MLU memory that store the
 *          \p input tensors. When \p scalar is computed, \p input_other can be nullptr.
 * @param[in] scalar
 *   Input. It can be a host pointer or an MLU pointer, which points to the MLU memory that store one or \p tensor_num scalars.
 *          - When \p binary_compute_mode is FOREACH_BINARY_SCALAR_LIST, \p scalar is a host pointer,
 *            and the number of \p scalar is \p tensor_num.
 *          - When \p binary_compute_mode is FOREACH_BINARY_SCALAR, \p scalar is a host pointer,
 *            and the number of \p scalar is \p tensor_num is one.
 *          - When \p binary_compute_mode is FOREACH_BINARY_SCALAR_TENSOR, \p scalar is an MLU pointer,
 *            and the number of \p scalar is \p tensor_num is one.
 *          - When \p input_other list is computed, \p scalar can be nullptr.
 * @param[in] output_desc
 *   Input. The list of descriptors of output tensors. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[in] output
 *   Input. A host pointer to a list of MLU pointers, which points to the MLU memory that store
 *          the output tensors.
 * @param[out] workspace
 *   Output. Pointer to the host memory that is used as an extra input space for the
 *           ::cnnlForeachUnaryOp operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, you need to get the size of the extra input data with
 *   ::cnnlGetForeachBinaryOpWorkspaceSize, and make sure that the memory of the extra input data
 *   is allocated.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlInitForeachBinaryOpWorkspace(cnnlHandle_t handle,
                                 const int tensor_num,
                                 const cnnlForeachBinaryMode_t binary_compute_mode,
                                 const cnnlTensorDescriptor_t *input_self_desc,
                                 const void * const *input_self,
                                 const void * const *input_other,
                                 const void *scalar,
                                 const cnnlTensorDescriptor_t *output_desc,
                                 void **output,
                                 void *workspace);

// Group:ForeachLerp
/*!
 * @brief Implements a linear interpolation of two tensorlists \p input1 and \p input2 based on
 *        a scalar or tensorlist \p weight and returns the results in \p output tensorlist.
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *          ::cnnlForeachLerp operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] lerp_compute_mode
 *   Input. Integer that marks a different process formula. For detailed information, see
 *          ::cnnlForeachLerpMode_t.
 * @param[in] tensor_num
 *   Input. The number of \p input1 tensors.
 * @param[in] input1_desc
 *   Input. The list of descriptors of \p input1 tensors. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[in] input1
 *   Input. A host pointer to a list of MLU pointers, which points to the MLU memory that store the
 *          \p input tensors.
 * @param[in] input2_desc
 *   Input. The list of descriptors of \p input2 tensors. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[in] input2
 *   Input. A host pointer to a list of MLU pointers, which points to the MLU memory that store the
 *          \p input2 tensors.
 * @param[in] weight_desc
 *   Input. The descriptor of \p weight. When \p lerp_compute_mode is FOREACH_LERP_SCALAR, \p weight_desc can be nullptr.
 *          For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] weight
 *   Input. It can be pointer to a pointer to a scalar or a list of MLU pointers.
 *          - When \p lerp_compute_mode is FOREACH_LERP_SCALAR, \p weight is pointer to a pointer to a scalar.
 *          - When \p lerp_compute_mode is FOREACH_LERP_TENSOR_LIST, \p weight is a list of MLU pointers.
 * @param[in] extra_input_device
 *   Input. Pointer to the MLU memory that is used as an extra input for the ::cnnlForeachLerp operation.
 *          For more information about extra_input, see "Cambricon CNNL User Guide".
 * @param[in] extra_input_size
 *   Input. The size of the extra input in bytes that needs to be used in the ::cnnlForeachLerp operation.
 *          You can get the size of the extra input with the ::cnnlGetForeachLerpExtraInputSize function.
 * @param[in] output_desc
 *   Input. The list of descriptors of \p output tensors. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. A host pointer to a list of MLU pointers, which points to the MLU memory that store
 *           the \p output tensors.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par Formula
 * - See "Foreach Lerp Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of \p input1 tensors, \p input2 tensors, \p weight tensors and \p output tensors should be the same.
 * - The supported data types of \p input1 tensors, \p input2 tensors, \p weight tensors, \p output tensors are as follows:
 *   - \p input1: half, bfloat16, float.
 *   - \p input2: half, bfloat16, float.
 *   - \p weight: half, bfloat16, float.
 *   - \p output: half, bfloat16, float.
 *
 *  The data type bfloat16 is only supported on MLU500 series or above.
 *
 * @par Scale Limitation
 * - Parameters must meet the following conditions:
 *   - \p tensor_num should be greater than 0 and less than 2147483647.
 *
 * @par API Dependency
 * - None.
 *
 * @note
 * - The length of the list of \p input1 tensors should be equal to \p tensor_num.
 * - The length of the list of \p input2 tensors should be equal to \p tensor_num.
 * - The length of the list of \p output tensors should be equal to \p tensor_num.
 * - The shapes of the i-th \p input1 tensor and the i-th \p output tensor must match.
 * - The shapes of the i-th \p input2 tensor and the i-th \p output tensor must match.
 * - When \p lerp_compute_mode is FOREACH_LERP_TENSOR_LIST:
 *   - The length of the list of \p weight tensors should be equal to \p tensor_num.
 *   - The shapes of the i-th \p weight tensor and the i-th \p output tensor must match.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - http://github.com/pytorch/pytorch/blob/main/aten/src/Aten/cuda/ForeachTernaryOp.cu
 */
cnnlStatus_t CNNL_WIN_API cnnlForeachLerp(cnnlHandle_t handle,
                                          const cnnlForeachLerpMode_t lerp_compute_mode,
                                          const int64_t tensor_num,
                                          const cnnlTensorDescriptor_t *input1_desc,
                                          const void * const *input1,
                                          const cnnlTensorDescriptor_t *input2_desc,
                                          const void * const *input2,
                                          const cnnlTensorDescriptor_t *weight_desc,
                                          const void * const *weight,
                                          void *extra_input_device,
                                          size_t extra_input_size,
                                          const cnnlTensorDescriptor_t *output_desc,
                                          void **output);

// Group:ForeachLerp
/*!
 * @brief Returns in \p extra_input_size the size of the MLU memory and host memory that is used as an extra
 * input data to optimize the ::cnnlForeachLerp operation. You need to allocate memory both on host and MLU based on
 * the size returned in \p extra_input_size.
 *
 * @param[in] handle
 *  Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *  queues in the ::cnnlForeachLerp operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] lerp_compute_mode
 *   Input. Integer that marks a different process formula. For detailed information, see
 *          ::cnnlForeachLerpMode_t.
 * @param[in] tensor_num
 *  Input. The number of \p input1 tensors.
 * @param[in] input1_desc
 *  Input. The list of descriptors of \p input1 tensors. For detailed information, see
 *         ::cnnlTensorDescriptor_t.
 * @param[out] extra_input_size
 *  Output. A host pointer to the returned size of the extra input in bytes that is used in the
 *         ::cnnlForeachLerp operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - Parameters must meet the following requirements:
 *   - \p tensor_num should be greater than 0.
 *
 * @par API Dependency
 * - This function must be called before the ::cnnlForeachLerp function.
 * - After calling this function, you need to call ::cnnlInitForeachLerpExtraInput to initialize
 *   the memory on host.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetForeachLerpExtraInputSize(cnnlHandle_t handle,
                                 const cnnlForeachLerpMode_t lerp_compute_mode,
                                 const int64_t tensor_num,
                                 const cnnlTensorDescriptor_t *input1_desc,
                                 size_t *extra_input_size);

// Group:ForeachLerp
/*!
 * @brief Initializes the extra input data space \p extra_input on host.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *          ::cnnlForeachLerp operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] tensor_num
 *   Input. The number of input tensors.
 * @param[in] lerp_compute_mode
 *   Input. Integer that marks a different process formula. For detailed information, see
 *          ::cnnlForeachLerpMode_t.
 * @param[in] input1_desc
 *   Input. The list of descriptors of \p input1 tensors. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[in] input1
 *   Input. A host pointer of \p input1 to a list of MLU pointers, which points to the MLU memory that store the
 *          input tensors.
 * @param[in] input2
 *   Input. A host pointer of \p input2 to a list of MLU pointers, which points to the MLU memory that store the
 *          input tensors.
 * @param[in] weight
 *   Input. It can be a host pointer to a pointer scalar or an MLU pointer, which points to the MLU memory that store \p tensor_num scalars.
 *          - When \p lerp_compute_mode is FOREACH_LERP_SCALAR, \p weight can be nullptr.
 *          - When \p lerp_compute_mode is FOREACH_LERP_TENSOR_LIST, \p weight is an MLU pointer list.
 * @param[in] output_desc
 *   Input. The list of descriptors of \p output tensors. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[in] output
 *   Input. A host pointer to a list of MLU pointers, which points to the MLU memory that store
 *          the output tensors.
 * @param[out] extra_host_input
 *   Output. Pointer to the host memory that is used as an extra input space for the
 *           ::cnnlForeachLerp operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, you need to get the size of the extra input data with
 *   ::cnnlGetForeachLerpExtraInputSize, and make sure that the memory of the extra input data
 *   is allocated.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */

cnnlStatus_t CNNL_WIN_API
cnnlInitForeachLerpExtraInput(cnnlHandle_t handle,
                              const cnnlForeachLerpMode_t lerp_compute_mode,
                              const int tensor_num,
                              const cnnlTensorDescriptor_t *input1_desc,
                              const void * const *input1,
                              const void * const *input2,
                              const void * const *weight,
                              const cnnlTensorDescriptor_t *output_desc,
                              void **output,
                              void *extra_host_input);

// Group:Concat
/*!
 * @brief Concatenates the list of input tensors \p inputs along the given dimension \p axis.
 * This operator is the inverse of the ::cnnlSplit operator.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 * @param[in] handle
 *  Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *  queues in the concat operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] concat_num
 *   Input. Number of tensors needed to be concatenated.
 * @param[in] axis
 *   Input. Dimension along which to be concatenated. The value must be in range of [-rank, rank),
 *          where rank is the number of dimensions in the input tensors,
 *          and negative \p axis refers to 'axis + rank'.
 * @param[in] inputs_desc
 *   Input. The list of descriptors of input tensors. For detailed information,
 *          see ::cnnlTensorDescriptor_t.
 * @param[in] inputs
 *   Input. A host pointer to a list of MLU pointers, which point to the MLU memory that store the
 *          input tensors.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the concat operation.
 *          For more information about workspace, see "Cambricon CNNL User Guide". Because ::cnnlConcat
 *          does not need extra workspace, the \p workspace can be set to NULL.
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the concat operation.
 *          You can get the size of the workspace with the ::cnnlGetConcatWorkspaceSize function.
 *          Because ::cnnlConcat does not need extra workspace, the \p workspace_size can be set to 0.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *          see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par Formula
 * - See "Concat Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The (I/O)function supports the following data widths for \p input and \p output tensors.
 *   - input tensor: 1-byte, 2-byte, 4-byte, 8-byte.
 *   - output tensor: 1-byte, 2-byte, 4-byte, 8-byte.
 *
 *   The byte width of a data type can be got with the ::cnnlGetSizeOfDataType function.
 *
 *   Note that all the tensors must have the same data type. If the tensors are in
 *   fixed-point data type, the quantization parameters of all the tensors should be the same.
 *
 * @par Scale Limitation
 * - Parameters must meet the following requirements:
 *   - The parameter \p concat_num should be greater than 0.
 *   - The number of dimensions of all tensors must match, including inputs and output.
 *   - All dimensions except \p axis must be equal, and the dimension of output on \p axis must be
 *     equal to the sum of input dimensions on \p axis.
 *
 * @par API Dependency
 * - Before calling this function to implement concat, you need to call
 *   ::cnnlGetConcatWorkspaceSize to get the extra space size needed in concat operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of concat operation is as follows:
     @verbatim
     input: 3 tensors with the shapes of 2 * 3, 2 * 3 and 1 * 3, respectively
             --> [[1,2,3],[4,5,6]]
             --> [[7,8,9],[10,11,12]]
             --> [[13,14,15]]

     concat_num: 3

     axis: 0

     Then we will get the output:

     output: a tensor of 5 * 3 --> [[1,2,3],[4,5,6],[7,8,9],[10,11,12],[13,14,15]]
     @endverbatim
 *
 * @par Reference
 * - http://www.tensorflow.org/api_docs/python/tf/concat
 */
cnnlStatus_t CNNL_WIN_API cnnlConcat(cnnlHandle_t handle,
                                     const int concat_num,
                                     const int axis,
                                     const cnnlTensorDescriptor_t inputs_desc[],
                                     const void *const inputs[],
                                     void *workspace,
                                     size_t workspace_size,
                                     const cnnlTensorDescriptor_t output_desc,
                                     void *output);

// Group:Concat
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace to
 *        optimize the concat operation.
 *
 * @param[in] handle
 *  Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *  queues in the concat operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] concat_num
 *   Input. Number of tensors needed to be concatenated.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *   the concat operation. At present, because ::cnnlConcat does not need extra workspace,
 *   the \p size will be returned with 0.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - Parameters must meet the following requirements:
 *   - The parameter \p concat_num should be greater than 0.
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlConcat function to perform the
 *   concat operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetConcatWorkspaceSize(cnnlHandle_t handle,
                                                     const int concat_num,
                                                     size_t *size);
// Group:ForeachCopy
/*!
 * @brief Copies tensors from input tensor list to output tensor list.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *          ::cnnlForeachCopy operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] tensor_num
 *   Input. The number of \p input tensors.
 * @param[in] input_desc
 *   Input. The list of descriptors of \p input tensors. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. A host pointer to a list of MLU pointers, which points to the MLU memory that stores the
 *          \p input tensors.
 * @param[in] output_desc
 *   Input. The list of descriptors of \p output tensors. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. A host pointer to a list of MLU pointers, which points to the MLU memory that stores
 *           the \p output tensors.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Foreach Copy Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of \p input tensors and \p output tensors should be the same.
 * - The supported data types of \p input tensors and \p output tensors are as follows:
 *   - \p input: 1-byte, 2-byte, 4-byte, 8-byte, 16-byte.
 *   - \p output: 1-byte, 2-byte, 4-byte, 8-byte, 16-byte.
 *
 * @par Scale Limitation
 * - Parameters must meet the following conditions:
 *   - \p tensor_num should be greater than 0 and less than 2147483647.
 *
 * @par API Dependency
 * - None.
 *
 * @note
 * - The length of the list of \p input tensors should be equal to that of \p tensor_num.
 * - The shapes of the i-th \p input tensor and the i-th \p output tensor must match.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - http://github.com/pytorch/pytorch/blob/main/aten/src/Aten/cuda/ForeachCopy.cu
 */
cnnlStatus_t CNNL_WIN_API cnnlForeachCopy(cnnlHandle_t handle,
                                          const int64_t tensor_num,
                                          const cnnlTensorDescriptor_t input_desc[],
                                          const void * const *input,
                                          const cnnlTensorDescriptor_t output_desc[],
                                          void **output);

// Group:ForeachUnaryOp
/*!
 * @brief Uses unary op to compute tensors in a tensor list.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *          ::cnnlForeachUnaryOp operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] mode
 *   Input. Integer that marks a different calculation formula. For detailed information, see
 *          ::cnnlForeachOpMode_t.
 * @param[in] tensor_num
 *   Input. The number of \p input_self tensors.
 * @param[in] input_desc
 *   Input. The list of descriptors of \p input_self tensors. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. A host pointer to a list of MLU pointers, which points to the MLU memory that stores the
 *          \p input tensors.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the foreach unary operation.
 *          You need to use ::cnnlInitForeachUnaryOpWorkspace to copy \p input and \p output data
 *          from host to \p workspace. For more information about workspace,
 *          see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the foreach unary operation.
 *          You can get the size of the workspace with the ::cnnlGetForeachUnaryOpWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The list of descriptors of \p output tensors. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. A host pointer to a list of MLU pointers, which points to the MLU memory that stores
 *           the \p output tensors.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par Formula
 * - See "Foreach Unary Op Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of \p input tensors and \p output tensors should be the same.
 * - The supported data types of \p input tensors and \p output tensors are as follows:
 *   - \p input: half, bfloat16, float.
 *   - \p output: half, bfloat16, float.
 *
 *   The data type bfloat16 is only supported on MLU500 series or above.
 *
 * @par Scale Limitation
 * - Parameters must meet the following conditions:
 *   - \p tensor_num should be greater than 0 and less than 2147483647.
 *
 * @par API Dependency
 * - None.
 *
 * @note
 * - The length of the list of \p input tensors should be equal to that of \p tensor_num.
 * - The shapes of the i-th \p input tensor and the i-th \p output tensor must match.
 * - The \p mode supports CNNL_FOREACH_ZERO and CNNL_FOREACH_SQRT.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - http://github.com/pytorch/pytorch/blob/main/aten/src/Aten/cuda/ForeachUnaryOp.cu
 */
cnnlStatus_t CNNL_WIN_API cnnlForeachUnaryOp(cnnlHandle_t handle,
                                             const cnnlForeachOpMode_t mode,
                                             const int64_t tensor_num,
                                             const cnnlTensorDescriptor_t input_desc[],
                                             const void * const *input,
                                             void *workspace,
                                             size_t workspace_size,
                                             const cnnlTensorDescriptor_t output_desc[],
                                             void **output);

// Group:ForeachUnaryOp
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace to
 *        optimize the foreach_unary_op operation.
 *
 * @param[in] handle
 *  Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *  queues in the foreach_unary_op operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] tensor_num
 *  Input. The number of \p input tensors.
 * @param[in] input_desc
 *  Input. The list of descriptors of \p input tensors. For detailed information, see
 *         ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *  Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *          the foreach_binary_op operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - Parameters must meet the following requirements:
 *   - \p tensor_num should be greater than 0 and less than 2147483647.
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlForeachUnaryOp function to perform the
 *   foreach_unary_op operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetForeachUnaryOpWorkspaceSize(cnnlHandle_t handle,
                                   const int64_t tensor_num,
                                   const cnnlTensorDescriptor_t *input_desc,
                                   size_t *workspace_size);

// Group:ForeachUnaryOp
/*!
 * @brief Initializes the data space \p workspace on host.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *          ::cnnlForeachUnaryOp operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] tensor_num
 *   Input. The number of input tensors.
 * @param[in] input_desc
 *   Input. The list of descriptors of input tensors. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. A host pointer to a list of MLU pointers, which points to the MLU memory that store the
 *          input tensors.
 * @param[in] output_desc
 *   Input. The list of descriptors of output tensors. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[in] output
 *   Input. A host pointer to a list of MLU pointers, which points to the MLU memory that store
 *          the output tensors.
 * @param[out] workspace
 *   Output. Pointer to the host memory that is used as an extra input space for the
 *           ::cnnlForeachUnaryOp operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, you need to get the size of the extra input data with
 *   ::cnnlGetForeachUnaryOpWorkspaceSize, and make sure that the memory of the extra input data
 *   is allocated.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlInitForeachUnaryOpWorkspace(cnnlHandle_t handle,
                                const int tensor_num,
                                const cnnlTensorDescriptor_t *input_desc,
                                const void * const *input,
                                const cnnlTensorDescriptor_t *output_desc,
                                void **output,
                                void *workspace);

// Group:ForeachNorm
/*!
 * @brief Calculates Lp normalization tensor by tensor in a tensorlist. The following modes are supported:
 *        L1 Norm, L2 Norm and LInf Norm.
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *          ::cnnlForeachNorm operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] tensor_num
 *   Input. The number of \p input tensors.
 * @param[in] input_desc
 *   Input. The list of descriptors of \p input tensors. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. A host pointer to a list of MLU pointers, which points to the MLU memory that store the
 *          \p input tensors.
 * @param[in] pnorm
 *   Input. A host pointer that is used to distinguish the norm mode.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the ::cnnlForeachNorm operation.
 *          For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the ::cnnlForeachNorm operation.
 *          You can get the size of the workspace with the ::cnnlGetForeachNormWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The list of descriptors of \p output tensors. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. A host pointer to a list of MLU pointers, which points to the MLU memory that store
 *           the \p output tensors.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED, ::CNNL_STATUS_ALLOC_FAILED.
 *
 * @par Formula
 * - See "Foreach Norm Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - By the order of \p input - \p output - \p pnorm , the supported data types are as follows:
 *   - float - float - float
 *   - half - half - float
 *   - half - float - float
 *   - bfloat16 - bfloat16_t - float
 *   - bfloat16 - float - float
 *
 *   The data type bfloat16 is only supported on MLU500 series or above.
 * - Data types of each tensor in \p input should be the same.
 * - Data types of each tensor in \p output should be the same.
 *
 * @par Scale Limitation
 * - Parameters must meet the following conditions:
 *   - \p tensor_num should be greater than 0 and less than 2147483647.
 *   - \p pnorm only supports the following three values: 1, 2 and +inf.
 *
 * @par API Dependency
 * - None.
 *
 * @note
 * - The length of the list of \p input tensors should be equal to \p tensor_num.
 * - The length of the list of \p output tensors should be equal to 1.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - http://github.com/pytorch/pytorch/blob/main/aten/src/Aten/cuda/ForeachReduceOp.cu
 */
cnnlStatus_t CNNL_WIN_API cnnlForeachNorm(cnnlHandle_t handle,
                                          const int64_t tensor_num,
                                          const cnnlTensorDescriptor_t *input_desc,
                                          const void * const *input,
                                          const void *pnorm,
                                          void *workspace,
                                          const size_t workspace_size,
                                          const cnnlTensorDescriptor_t *output_desc,
                                          void **output);

// Group:ForeachNorm
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace to
 *        optimize the ::cnnlForeachNorm operation.
 *
 * @param[in] handle
 *  Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *  queues in the ::cnnlForeachNorm operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] tensor_num
 *   Input. The number of \p input tensors.
 * @param[in] input_desc
 *   Input. The list of descriptors of \p input tensors. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *  Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *          the ::cnnlForeachNorm operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The allocated extra workspace must be passed to the ::cnnlForeachNorm function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetForeachNormWorkspaceSize(cnnlHandle_t handle,
                                const int64_t tensor_num,
                                const cnnlTensorDescriptor_t *input_desc,
                                size_t *workspace_size);

// Group:ScatterRef
/*!
 * @brief Calculates sparse \p update to the \p dim dimension of the \p ref referenced
 * by \p indices. Where operation modes include add, sub, mul, div, max, min and update.
 * And update represents replacing \p ref with \p update directly.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 * @param[in] handle
 *  Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *  in the scatterref operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] ref_desc
 *  Input. The descriptor of \p ref tensor which represents the input of the operation.
 *  For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in, out] ref
 *  Input. Pointer to the MLU memory that stores the \p ref tensor.
 * @param[in] indices_desc
 *  Input. The descriptor of \p indices tensor. \p indices contains some index values,
 *  which references to the dim dimension of \p ref. So that \p ref can do some operation
 *  with \p update. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] indices
 *  Input. Pointer to the MLU memory that stores the \p indices tensor.
 * @param[in] update_desc
 *  Input. The descriptor of \p update tensor. \p update is used to do some operation with
 *  the dim dimension of \p ref referenced by \p indices. For detailed information, see
 *  ::cnnlTensorDescriptor_t.
 * @param[in] update
 *  Input. Pointer to the MLU memory that stores the \p update tensor.
 * @param[in] dim
 *  Input. Specified which dimension to do sparse computing.
 * @param[in] mode
 *  Input. The descriptor of the scatterref mode. For detailed information,
 *  see ::cnnlScatterRefMode_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "ScatterRef Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *   ref - indices - update - output.
 *   - float - int32 - float - float
 *   - half - int32 - half - half
 *   - int32 - int32 - int32 - int32
 *   - float - int64 - float - float
 *   - half - int64 - half - half
 *   - int32 - int64 - int32 - int32
 *
 * @par Scale Limitation
 * - Parameters must meet the following requirements:
 *   - The length of the \p dim dimension of the \p update is equal to the length of \p indices,
 *   and the length of other dimensions of \p update is equal to the length of the corresponding
 *   dimension of \p ref.
 * - If the data type of output is int32, each element in \p output should in range of
 *   [\f$-2^{23}\f$, \f$2^{23}-1\f$] in MLU200 series. And the intermediate results cannot exceed the value range
 *   of int32.
 * - In the update mode of MLU500 series, the total bytes of ref tensor need to be less than or equal to UINT_MAX
 *   (i.e. 4294967295) to achieve better performance.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the ScatterRef operation is as follows:
 *   @verbatim
 *   input array:
 *   --> ref: [1, 2, 3, 4, 5, 6]
 *
 *   --> indices: [2, 0, 5]
 *
 *   --> update: [9, 10, 11]
 *
 *   param:
 *     mode: CNNL_SCATTERREF_ADD, dim: 0
 *
 *   output one dimension array --> output: [11, 2, 12, 4, 5, 17]
 *   @endverbatim
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/compat/v1/scatter_add
 * - https://www.tensorflow.org/api_docs/python/tf/compat/v1/scatter_sub
 * - https://www.tensorflow.org/api_docs/python/tf/compat/v1/scatter_mul
 * - https://www.tensorflow.org/api_docs/python/tf/compat/v1/scatter_div
 * - https://www.tensorflow.org/api_docs/python/tf/compat/v1/scatter_max
 * - https://www.tensorflow.org/api_docs/python/tf/compat/v1/scatter_min
 * - https://www.tensorflow.org/api_docs/python/tf/compat/v1/scatter_update
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlScatterRef(cnnlHandle_t handle,
                                         const cnnlTensorDescriptor_t ref_desc,
                                         const void *ref,
                                         const cnnlTensorDescriptor_t indices_desc,
                                         const void *indices,
                                         const cnnlTensorDescriptor_t update_desc,
                                         const void *update,
                                         const int dim,
                                         cnnlScatterRefMode_t mode);
// Group:PoolingIndex
/*!
 * @brief Computes the index of maximum pooling forward, and returns the results in
 * the output tensor \p y.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the ::cnnlPoolingIndex operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] pooling_desc
 *   Input. The descriptor of pooling operation. For detailed information,
 *   see ::cnnlPoolingDescriptor_t.
 * @param[in] x_desc
 *   Input. The descriptor of \p x tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the \p y tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor. The
 *   \p y tensor is the result of this operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_EXECUTION_FAILED
 * @par Formula
 * - See "PoolingIndex Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for
 *   \p x tensor, \p y tensor.
 *   Note that the combinations of these tensors must be half-int16 or float-int32.
 *   - \p x tensor: half, float.
 *   - \p y tensor: int16, int32.
 *
 * @par Data Layout
 * - The supported data layouts of the \p x tensor and \p y tensor are as follows:
 *   Note that the layout of these tensors must be the same.
 *   -\p x tensor: \p CNNL_LAYOUT_NHWC.
 *   -\p y tensor: \p CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - According to the definition of pooling, the parameters in the \p pooling_desc should satisfy the following conditions:
 *   padding >= 0, stride >= 1.
 * - The parameters in the \p x_desc should satisfy the following conditions: in_batch > 0, in_height > 0, in_width > 0, in_channel > 0.
 *   The in_batch is the batch of the input tensor. The in_height is the height of the input tensor.
 *   The in_width is the width of the input tensor. The in_channel is the channel of the input tensor.
 * - The parameters in the \p y_desc should satisfy the following conditions: out_batch > 0, out_height > 0, out_width > 0, out_channel > 0.
 *   The out_batch is the batch of the output tensor. The out_height is the height of the output tensor.
 *   The out_width is the width of the output tensor. The out_channel is the channel of the output tensor.
 * - The parameters kernel in the \p pooling_desc should satisfy the following conditions:
 *   kw * kh < 3070. The kw and kh represent the width and the height of the pooling kernel size respectively.
 * - This function supports input large tensors only on MLU500 series in which H*W does not exceed INT32_MAX and any single dimension cannot exceed INT32_MAX.
 *
 * @note
 * - Currently, only 2D pooling with the CNNL_LAYOUT_NHWC layout has been supported and only supports CNNL_POOLING_MAX mode.
 * - Currently only supports dilation equal to 1.
 * - When the kernel of the pooling contains NaN:
 *   - On MLU200 series:
 *    - The output index is the index of the last NaN.
 *   - On MLU300 series and CE3226:
 *    - If the last value in the kernel of the pooling is NaN, the output index is
 *    the index of the last value. Otherwise, the output index is the index of the maximum value after the last NaN.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 *  - The example of this operation is as follows:
     @verbatim
      input layout is NHWC, and input shape is (1,4,4,1).
        input: [[[1,2,3,4],
                 [5,6,7,8],
                 [9,10,11,12],
                 [13,14,15,16]]]
      param:
        pad:(0,0,0,0), stride:(2,2), kernel:(2,2), mode: CNNL_POOLING_MAX

      output:  [[[3],[3],[3],[3]]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/nn.functional.html#max-pool2d
 */
cnnlStatus_t CNNL_WIN_API cnnlPoolingIndex(cnnlHandle_t handle,
                                           const cnnlPoolingDescriptor_t pooling_desc,
                                           const cnnlTensorDescriptor_t x_desc,
                                           const void *x,
                                           const cnnlTensorDescriptor_t y_desc,
                                           void *y);

// Group:Pooling
/*!
 * @brief Creates a descriptor pointed by \p desc for pooling operation, and allocates
 * memory for holding the information about the pooling forward or backward operation.
 * The information is defined in ::cnnlPoolingDescriptor_t. For more information about descriptor,
 * see "Cambricon CNNL User Guide".
 *
 * @param[out]  desc
 *   Output. Pointer to the pooling descriptor that holds information about the pooling operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetPooling2dDescriptor, ::cnnlSetPooling2dDescriptor_v2,
 * ::cnnlSetPoolingNdDescriptor or ::cnnlSetPoolingNdDescriptor_v2 function to initialize and set the information to the pooling descriptor.
 * - You need to call the ::cnnlDestroyPoolingDescriptor function to destroy the descriptor.
 *   Otherwise, the memory leak may occur.
 *
 * @note
 * - Only supports 2D and 3D pooling. N-D pooling will be supported in the future.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreatePoolingDescriptor(cnnlPoolingDescriptor_t *desc);

// Group:Pooling
/*!
 * @brief Destroys a pooling descriptor \p desc that was previously created with the
 * ::cnnlCreatePoolingDescriptor function.
 *
 * The pooling descriptor is defined in ::cnnlPoolingDescriptor_t and holds
 * the information about the pooling forward or backward operation.
 *
 * @param[in] desc
 *   Input. The pooling descriptor to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - You need to call this function after calling the pooling operation.
 *   Otherwise, the memory leak may occur.
 * - Only supports 2D and 3D pooling. N-D pooling will be supported in the future.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */

cnnlStatus_t CNNL_WIN_API cnnlDestroyPoolingDescriptor(cnnlPoolingDescriptor_t desc);

// Group:Pooling
/*!
 * @brief Initializes the pooling descriptor \p pooling_desc that was previously created with the
 * ::cnnlCreatePoolingDescriptor function, and sets the information about the pooling forward
 * operation to the pooling descriptor \p pooling_desc. The information includes the pooling mode,
 * the NaN propagation mode \p maxpooling_nan_opt, the number of the pooling dimension \p dims,
 * the window size for each dimension \p window, the padding size for each dimension \p pad,
 * the stride of the sliding window for each dimension \p stride. To specify the stride of elements
 * in the window and whether to use ceil or floor to compute the output shape,
 * call the ::cnnlSetPoolingNdDescriptor_v2 function.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlSetPoolingNdDescriptor_v2 instead, which supports the parameters of \p dilation
 *   and \p ceil_mode that determines whether to use ceil or floor to compute the shape of output.
 *
 * @param[in,out] pooling_desc
 *   Input/output. The descriptor of pooling operation. For detailed information,
 *   see ::cnnlPoolingDescriptor_t.
 * @param[in] mode
 *   Input. The \p mode describes the algorithm that is used in the implementation of the pooling operation.
 *   For detailed information, see ::cnnlPoolingMode_t.
 * @param[in]  maxpooling_nan_opt
 *   Input. The maxpooling_nan_opt describes whether to propagate NaN numbers.
 *   Only supports CNNL_NOT_PROPAGATE_NAN now.
 *   For detailed information, see ::cnnlNanPropagation_t.
 * @param[in] dims
 *   Input. The number of dimensions in the input tensor of the pooling operation.
 *   The value of this parameter should be the same as the one you set in the input tensor descriptor.
 * @param[in] window
 *   Input. An array that stores the kernel for each dimension of the input tensor
 *   used in the pooling operation. If the dimension of input tensor is 5, the \p window is
 *   on depth, height and width. The value of this parameter should be greater than or equal
 *   to 1.
 * @param[in] stride
 *   Input. An array that stores the filter stride for each dimension of the input tensor
 *   used in the pooling operation. For each dimension, the filter stride represents
 *   the number of elements to slide over the input tensor. If the dimension of input tensor is 5, the \p stride is
 *   on depth, height and width. The value of this parameter should be greater than or equal
 *   to 1.
 * @param[in] padding
 *   Input. An array that stores the padding size for each dimension of the input tensor
 *   used in the pooling operation.
 *   For each dimension, the padding size represents the number of zeros to be concatenated at the
 *   starting and ending of that dimension. If \p dims is set to 5, the padding is on front, back, top, bottom, left,
 *   and right. The value of this parameter should be greater than or equal to 0.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - ::cnnlCreatePoolingDescriptor needs to be called before this function.
 * - You need to call the ::cnnlDestroyPoolingDescriptor function to destroy the descriptor.
 *   Otherwise, the memory leak may occur.
 *
 * @note
 * - Currently, only supports 5D input tensor for pooling operation.
 * - All pointer arguments should not be NULL.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
CNNL_DEPRECATED_FOR(cnnlSetPoolingNdDescriptor_v2)
cnnlStatus_t CNNL_WIN_API cnnlSetPoolingNdDescriptor(cnnlPoolingDescriptor_t pooling_desc,
                                                     cnnlPoolingMode_t mode,
                                                     cnnlNanPropagation_t maxpooling_nan_opt,
                                                     int dims,
                                                     const int window[],
                                                     const int padding[],
                                                     const int stride[]);

// Group:Pooling
/*!
 * @brief Initializes the pooling descriptor \p pooling_desc that was previously created with the
 * ::cnnlCreatePoolingDescriptor function, and sets the information about the pooling forward
 * operation to the pooling descriptor \p pooling_desc. Compared with ::cnnlSetPoolingNdDescriptor,
 * this function supports the stride of elements in the window \p dilation and whether to use ceil
 * or floor to compute the output shape \p ceil_mode. The information includes the pooling mode,
 * the NaN propagation mode \p maxpooling_nan_opt, the number of the pooling dimension \p dims,
 * the window size for each dimension \p window, the padding size for each dimension \p pad,
 * the stride of the sliding window for each dimension \p stride.
 *
 * @param[in,out] pooling_desc
 *   Input/output. The descriptor of pooling operation. For detailed information,
 *   see ::cnnlPoolingDescriptor_t.
 * @param[in] mode
 *   Input. The \p mode describing the algorithm that is used in the implementation of the pooling operation.
 *   For detailed information, see ::cnnlPoolingMode_t.
 * @param[in]  maxpooling_nan_opt
 *   Input. The maxpooling_nan_opt describes whether to propagate NaN numbers.
 *   Only supports CNNL_NOT_PROPAGATE_NAN now.
 *   For detailed information, see ::cnnlNanPropagation_t.
 * @param[in] dims
 *   Input. The number of dimensions in the input tensor of the pooling operation.
 *   The value of this parameter should be the same as the one you set in the input tensor descriptor.
 * @param[in] window
 *   Input. An array that stores the kernel for each dimension of the input tensor
 *   used in the pooling operation. If the dimension of input tensor is 5, the \p window is
 *   on depth, height and width. The value of this parameter should be greater than or equal
 *   to 1.
 * @param[in] stride
 *   Input. An array that stores the filter stride for each dimension of the input tensor
 *   used in the pooling operation. For each dimension, the filter stride represents
 *   the number of elements to slide over the input tensor. If the dimension of input tensor is 5, the \p stride is
 *   on depth, height and width. The value of this parameter should be greater than or equal
 *   to 1.
 * @param[in] padding
 *   Input. An array that stores the padding size for each dimension of the input tensor
 *   used in the pooling operation.
 *   For each dimension, the padding size represents the number of zeros to be concatenated at the
 *   starting and ending of that dimension. If \p dims is set to 5, the padding is on front, back, top, bottom, left,
 *   and right. The value of this parameter should be greater than or equal to 0.
 * @param[in] dilation
 *   Input. A parameter that controls the stride of elements in the window.
 * @param[in] ceil_mode
 *   Input. When \p ceil_mode is true, the operator will use ceil mode to compute the output shape,
 *   otherwise, floor mode would be used.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par API Dependency
 * - ::cnnlCreatePoolingDescriptor needs to be called before this function.
 * - You need to call the ::cnnlDestroyPoolingDescriptor function to destroy the descriptor.
 *   Otherwise, the memory leak may occur.
 *
 * @note
 * - Currently, only supports 5D input tensor for pooling operation.
 * - All pointer arguments should not be NULL.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlSetPoolingNdDescriptor_v2(cnnlPoolingDescriptor_t pooling_desc,
                                                        cnnlPoolingMode_t mode,
                                                        cnnlNanPropagation_t maxpooling_nan_opt,
                                                        int dims,
                                                        const int window[],
                                                        const int padding[],
                                                        const int stride[],
                                                        const int dilation[],
                                                        const bool ceil_mode);

// Group:Pooling
/*!
 * @brief Computes the pooling forward with the ::cnnlPoolingDescriptor_t \p pooling_desc,
 * and returns the results in the output tensor \p y.
 *
 * To set the factors for quantization:
 * - If offline symmetric quantization with position is used, you need to call the
 *   ::cnnlSetTensorDescriptorPosition function to set the position factor used in the fixed-point
 *   quantization.
 * - If offline symmetric quantization with position and scale factors is used,
 *   you need to call the ::cnnlSetTensorDescriptorPositionAndScale function to set the position
 *   and scale factors used in fixed-point quantization.
 * - If offline asymmetric quantization is used, you need to call the
 *   ::cnnlSetTensorDescriptorPositionScaleAndOffset function to set the position, scale and offset
 *   factors used in fixed-point quantization.
 * - For more information about quantization, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the pooling operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] pooling_desc
 *   Input. The descriptor of pooling operation. For detailed information,
 *   see ::cnnlPoolingDescriptor_t.
 * @param[in] alpha, beta
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] x_desc
 *   Input. The descriptor of \p x tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the \p y tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor. The
 *   \p y tensor is the result of pooling operation.
 * @param[in]  workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlPoolingForward. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in]  workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the ::cnnlPoolingForward. You can get the size of the workspace with
 *   the ::cnnlGetPoolingWorkspaceSize or ::cnnlGetPoolingWorkspaceSize_v2 function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_INTERNAL_ERROR, ::CNNL_STATUS_EXECUTION_FAILED, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Pooling Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - If the \p mode is not CNNL_POOLING_MAX, the supported combinations of data types are shown below with the following order:
 *   \p x - onchip_dtype  - \p y.
 *   - half-half-half.
 *   - half-float-half.
 *   - float-float-float.
 * - On MLU500 series, this function additionally supports the following data type combinations:
 *   - bfloat16-bfloat16-bfloat16.
 *   - bfloat16-float-bfloat16.
 * - If the \p mode is CNNL_POOLING_MAX, the supported combinations of data types are shown below with the following order:
 *   \p x - onchip_dtype  - \p y.
 *   - float-float-float.
 *   - float-float-int16.
 *   - float-float-int8.
 *   - half-half-half.
 *   - half-half-int8.
 *   - int16-float-float.
 *   - int16-float-int16.
 *   - int8-half-half.
 *   - int8-half-int8.
 *   - int8-float-float.
 *   - int8-float-int8.
 * - On MLU500 series, this function additionally supports the following data type combinations:
 *   - bfloat16-bfloat16-bfloat16.
 * - The onchip data type of \p x_desc must be float, half, or bfloat16. If the data type of \p x tensor
 *   is int16 and int8, the onchip data type must be half or float.
 *
 * @par Data Layout
 * - In the 2D pooling forward operation, the supported data layouts of the \p x tensor and \p y tensor are as follows:
 *   - x tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NCHW.
 *   - y tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NCHW.
 * - In the 3D pooling forward operation, the supported data layouts of the \p x tensor and \p y tensor are as follows:
 *   Note that the layout of these tensors must be the same.
 *   - x tensor: \p CNNL_LAYOUT_NDHWC.
 *   - y tensor: \p CNNL_LAYOUT_NDHWC.
 *
 * @par Scale Limitation
 * - In the 2D pooling forward operation, the parameters should satisfy the following conditions:
 *    - padding >= 0, stride >= 1.
 *    - The parameters in the \p x_desc should satisfy the following conditions: in_batch > 0, in_height > 0, in_width > 0, in_channel > 0.
 *      The in_batch is the batch of the input tensor. The in_height is the height of the input tensor.
 *      The in_width is the width of the input tensor. The in_channel is the channel of the input tensor.
 *    - The parameters in the \p y_desc should satisfy the following conditions: out_batch > 0, out_height > 0, out_width > 0, out_channel > 0.
 *      The out_batch is the batch of the output tensor. The out_height is the height of the output tensor.
 *      The out_width is the width of the output tensor. The out_channel is the channel of the output tensor.
 * - In the 3D pooling forward operation, the parameters should satisfy the following conditions:
 *    - padding >= 0, stride >= 1.
 *    - The parameters in the \p x_desc should satisfy the following conditions: in_batch > 0, in_depth > 0, in_height > 0, in_width > 0, in_channel > 0.
 *      The in_batch is the batch of the input tensor. The in_depth is the depth of the input tensor. The in_height is the height of the input tensor.
 *      The in_width is the width of the input tensor. The in_channel is the channel of the input tensor.
 *    - The parameters in the \p y_desc should satisfy the following conditions: out_batch > 0, out_depth > 0, out_height > 0, out_width > 0, out_channel > 0.
 *      The out_batch is the batch of the output tensor. The out_depth is the depth of the output tensor. The out_height is the height of the output tensor.
 *      The out_width is the width of the output tensor. The out_channel is the channel of the output tensor.
 *    - In 3D pooling forward operation, the pooling window size is limited to the capacity of NRAM. The maximum pooling window varies by the MLU platform.
 *      When the pooling window size exceeds the maximum supported window size, the function will return ::CNNL_STATUS_NOT_SUPPORTED.
 * - Large tensors are supports only on MLU 500 series:
 *    - In 2D pooling: Output tensor cannot be large tensor. Input tensor can be large tensor, which means that the element number can exceed INT32_MAX, but any single dimension of the input tensor cannot exceed INT32_MAX, and if the \p mode is CNNL_POOLING_MAX, input tensor's H*W cannot exceed INT32_MAX.
*     - In 3D pooling: Both input and output tensors can be large tensors, which means that the element number can exceed INT32_MAX, but any single dimension of the tensor cannot exceed INT32_MAX, and if the \p mode is CNNL_POOLING_MAX, input tensor's D*H*W*C cannot exceed INT32_MAX.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the layout of the \p x
 *   tensor, \p y tensor, \p index tensor to NHWC in the 2D pooling forward operation.
 *
 * @note
 * - Only supports 2D and 3D pooling.
 * - Currently only supports dilation equal to 1.
 * - For ::CNNL_POOLING_AVERAGE_COUNT_INCLUDE_PADDING and ::CNNL_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING,
 *   When the data type of input is half or bfloat16, users can set the data type to float for actual on-chip computation
 *   with the ::cnnlSetTensorDescriptorOnchipDataType function.
 *
 * - In the 2D maximum pooling, when the kernel of the pooling contains NaN:
 *   - On MLU200 series:
 *    - The \p output value is the NaN.
 *   - On MLU300 series and CE3226:
 *    - If the last value in the kernel of the pooling is NaN, the \p output value is NaN.
 *      Otherwise, the \p output value is the maximum value after the last NaN.
 * - In the 3D maximum pooling, the \p output is unpredictable since a NaN value is noncomparable.
 * - The following data type combinations are deprecated and will be removed in future release with
 *   the order:
 *   \p x - onchip_dtype  - \p y.
 *   - float-float-int16.
 *   - float-float-int8.
 *   - half-half-int8.
 *   - int16-float-float.
 *   - int16-float-int16.
 *   - int8-half-half.
 *   - int8-half-int8.
 *   - int8-float-float.
 *   - int8-float-int8.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
     input layout is NHWC, and input shape is (1,4,4,1).
        input: [[[1,2,3,4],
                 [5,6,7,8],
                 [9,10,11,12],
                 [13,14,15,16]]]
      param:
        pad:(0,0,0,0), stride:(2,2), kernel:(2,2), mode: CNNL_POOLING_MAX

      output: [[[6],[8],[14],[16]]]

     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/nn.functional.html#avg-pool2d
 * - https://pytorch.org/docs/stable/nn.functional.html#max-pool2d
 *
 */
CNNL_DEPRECATED_FOR(cnnlPoolingForward_v2)
cnnlStatus_t CNNL_WIN_API cnnlPoolingForward(cnnlHandle_t handle,
                                             const cnnlPoolingDescriptor_t pooling_desc,
                                             const void *alpha,
                                             const cnnlTensorDescriptor_t x_desc,
                                             const void *x,
                                             const void *beta,
                                             const cnnlTensorDescriptor_t y_desc,
                                             void *y,
                                             void *workspace,
                                             size_t workspace_size);

// Group:Pooling
/*!
 * @brief Computes the pooling forward with the ::cnnlPoolingDescriptor_t \p pooling_desc,
 * returns the results in the output tensor \p y. Compared with ::cnnlPoolingForward,
 * ::cnnlPoolingForward_v2 provides better performance with extra input space.
 *
 * If you want to use the quantization function, set these factors as follows:
 *
 * - If offline symmetric quantization with position is used, you need to call the
 *   ::cnnlSetTensorDescriptorPosition function to set the position factor used in the fixed-point
 *   quantization.
 * - If offline symmetric quantization with position and scale factors is used,
 *   you need to call the ::cnnlSetTensorDescriptorPositionAndScale function to set the position
 *   and scale factors used in fixed-point quantization.
 * - If offline asymmetric quantization is used, you need to call the
 *   ::cnnlSetTensorDescriptorPositionScaleAndOffset function to set the position, scale and offset
 *   factors used in fixed-point quantization.
 * - For more information about quantization, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the pooling operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] pooling_desc
 *   Input. The descriptor of pooling operation. For detailed information,
 *   see ::cnnlPoolingDescriptor_t.
 * @param[in] alpha, beta
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] x_desc
 *   Input. The descriptor of \p x tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] extra_device_input
 *   Input. Pointer to the MLU memory that stores the extra input data.
 *   You need to copy the extra input data to MLU from the host that is
 *   initialized with ::cnnlInitPoolingExtraInput. For more information
 *   about extra input data, see Cambricon CNNL User Guide.
 * @param[in] y_desc
 *   Input. The descriptor of the \p y tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor. The
 *   \p y tensor is the result of pooling operation.
 * @param[in]  workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlPoolingForward_v2. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in]  workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the ::cnnlPoolingForward_v2. You can get the size of the workspace with
 *   the ::cnnlGetPoolingWorkspaceSize or ::cnnlGetPoolingWorkspaceSize_v2 function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_INTERNAL_ERROR, ::CNNL_STATUS_EXECUTION_FAILED, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Pooling Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - If the \p mode is not CNNL_POOLING_MAX, the supported combinations of data types are shown below with the following order:
 *   \p x - onchip_dtype  - \p y.
 *   - half-half-half.
 *   - half-float-half.
 *   - float-float-float.
 * - On MLU500 series, this function additionally supports the following data type combinations:
 *   - bfloat16-bfloat16-bfloat16.
 *   - bfloat16-float-bfloat16.
 * - If the \p mode is CNNL_POOLING_MAX, the supported combinations of data types are shown below with the following order:
 *   \p x - onchip_dtype  - \p y.
 *   - float-float-float.
 *   - float-float-int16.
 *   - float-float-int8.
 *   - half-half-half.
 *   - half-half-int8.
 *   - int16-float-float.
 *   - int16-float-int16.
 *   - int8-half-half.
 *   - int8-half-int8.
 *   - int8-float-float.
 *   - int8-float-int8.
 * - On MLU500 series, this function additionally supports the following data type combinations:
 *   - bfloat16-bfloat16-bfloat16.
 * - The onchip data type of \p x_desc must be float, half, or bfloat16. If the data type of \p x tensor
 *   is int16 and int8, the onchip data type must be half or float.
 *
 * @par Data Layout
 * - In the 2D pooling forward operation, the supported data layouts of the \p x tensor and \p y tensor are as follows:
 *   - x tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NCHW.
 *   - y tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NCHW.
 * - In the 3D pooling forward operation, the supported data layouts of the \p x tensor and \p y tensor are as follows:
 *   Note that the layout of these tensors must be the same.
 *   - x tensor: \p CNNL_LAYOUT_NDHWC.
 *   - y tensor: \p CNNL_LAYOUT_NDHWC.
 *
 * @par Scale Limitation
 * - In the 2D pooling forward operation, the parameters should satisfy the following conditions:
 *    - padding >= 0, stride >= 1.
 *    - The parameters in the \p x_desc should satisfy the following conditions: in_batch > 0, in_height > 0, in_width > 0, in_channel > 0.
 *      The in_batch is the batch of the input tensor. The in_height is the height of the input tensor.
 *      The in_width is the width of the input tensor. The in_channel is the channel of the input tensor.
 *    - The parameters in the \p y_desc should satisfy the following conditions: out_batch > 0, out_height > 0, out_width > 0, out_channel > 0.
 *      The out_batch is the batch of the output tensor. The out_height is the height of the output tensor.
 *      The out_width is the width of the output tensor. The out_channel is the channel of the output tensor.
 * - In the 3D pooling forward operation, the parameters should satisfy the following conditions:
 *    - padding >= 0, stride >= 1.
 *    - The parameters in the \p x_desc should satisfy the following conditions: in_batch > 0, in_depth > 0, in_height > 0, in_width > 0, in_channel > 0.
 *      The in_batch is the batch of the input tensor. The in_depth is the depth of the input tensor. The in_height is the height of the input tensor.
 *      The in_width is the width of the input tensor. The in_channel is the channel of the input tensor.
 *    - The parameters in the \p y_desc should satisfy the following conditions: out_batch > 0, out_depth > 0, out_height > 0, out_width > 0, out_channel > 0.
 *      The out_batch is the batch of the output tensor. The out_depth is the depth of the output tensor. The out_height is the height of the output tensor.
 *      The out_width is the width of the output tensor. The out_channel is the channel of the output tensor.
 *    - In 3D pooling forward operation, the pooling window size is limited to the capacity of NRAM. The maximum pooling window varies by the MLU platform.
 *      When the pooling window size exceeds the maximum supported window size, the function will return ::CNNL_STATUS_NOT_SUPPORTED.
 *
 * @par API Dependency
 * - You need to initialize the \p extra_device_input with ::cnnlGetPoolingExtraInputSize and ::cnnlInitPoolingExtraInput
 *   before calling this function.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the layout of the \p x
 *   tensor, \p y tensor, \p index tensor to NHWC in the 2D pooling forward operation.
 *
 * @note
 * - Only supports 2D and 3D pooling.
 * - Currently only supports dilation equal to 1.
 * - For ::CNNL_POOLING_AVERAGE_COUNT_INCLUDE_PADDING and ::CNNL_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING,
 *   When the data type of input is half or bfloat16, users can set the data type to float for actual on-chip computation
 *   with the ::cnnlSetTensorDescriptorOnchipDataType function.
 *
 * - In the 2D maximum pooling, when the kernel of the pooling contains NaN:
 *   - On MLU200 series:
 *    - The \p output value is the NaN.
 *   - On MLU300 series and CE3226:
 *    - If the last value in the kernel of the pooling is NaN, the \p output value is NaN.
 *      Otherwise, the \p output value is the maximum value after the last NaN.
 * - The following data type combinations are deprecated and will be removed in future release
 *   with the order:
 *   \p x - onchip_dtype  - \p y.
 *   - float-float-int16.
 *   - float-float-int8.
 *   - half-half-int8.
 *   - int16-float-float.
 *   - int16-float-int16.
 *   - int8-half-half.
 *   - int8-half-int8.
 *   - int8-float-float.
 *   - int8-float-int8.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
     input layout is NHWC, and input shape is (1,4,4,1).
       input: [[[1,2,3,4],
                [5,6,7,8],
                [9,10,11,12],
                [13,14,15,16]]]
       param:
         pad:(0,0,0,0), stride:(2,2), kernel:(2,2), mode: CNNL_POOLING_MAX

     output: [[[6],[8],[14],[16]]]

     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/nn.functional.html#avg-pool2d
 * - https://pytorch.org/docs/stable/nn.functional.html#max-pool2d
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlPoolingForward_v2(cnnlHandle_t handle,
                                                const cnnlPoolingDescriptor_t pooling_desc,
                                                const void *alpha,
                                                const cnnlTensorDescriptor_t x_desc,
                                                const void *x,
                                                const void *beta,
                                                const void *extra_device_input,
                                                const cnnlTensorDescriptor_t y_desc,
                                                void *y,
                                                void *workspace,
                                                size_t workspace_size);
// Group:Pooling
/*!
 * @brief Returns in \p extra_input_size the size of the MLU memory and host memory that is used as an extra
 * input data to optimize the pooling operation. You need to allocate memory both on host and MLU based on
 * the size returned in \p extra_input_size.
 *
 * The size of the extra workspace is based on the given information of the pooling
 * forward operation, including the pooling \p mode, the width of pooling output
 * \p out_w_size, the height of pooling output \p out_h_size.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the pooling operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  mode
 *   Input. The \p mode describes the algorithm that is used in the implementation of the pooling operation.
 *   For detailed information, see ::cnnlPoolingMode_t.
 * @param[in]  out_w_size
 *   Input. The width of pooling operation output.
 * @param[in]  out_h_size
 *   Input. The height of pooling operation output.
 * @param[out] extra_input_size
 *   Output. A host pointer to the returned size of the extra input data in bytes that is used in
 *   the ::cnnlPoolingForward_v2 operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - According to the definition of pooling, the parameters should satisfy the following conditions:
 *   - The \p out_w_size and \p out_h_size should be greater than zero.
 *
 * @par API Dependency
 * - You need to call ::cnnlCreatePoolingDescriptor and ::cnnlSetPooling2dDescriptor or ::cnnlSetPooling2dDescriptor_v2
 *   functions accordingly before calling this function.
 * - After calling this function, you need to call ::cnnlInitPoolingExtraInput to initialize the memory on host.
 * - The allocated extra input should be passed to the ::cnnlPoolingForward_v2 function
 *   to perform the pooling operation.
 *
 * @note
 * - Only supports 2D pooling.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */

cnnlStatus_t CNNL_WIN_API cnnlGetPoolingExtraInputSize(cnnlHandle_t handle,
                                                       cnnlPoolingMode_t mode,
                                                       const int out_w_size,
                                                       const int out_h_size,
                                                       size_t *extra_input_size);
// Group:Pooling
/*!
 * @brief Initializes the extra input data space \p extra_host_input on host. The data of
 * extra input is based on the given information of the pooling operation, including
 * the pooling descriptor \p pooling_desc, the input tensor descriptor \p x_desc
 * and the output tensor descriptor \p y_desc.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the pooling operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] pooling_desc
 *   Input. The descriptor of pooling operation. For detailed information,
 *   see ::cnnlPoolingDescriptor_t.
 * @param[in] x_desc
 *   Input. The descriptor of \p x tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] y_desc
 *   Input. The descriptor of the \p y tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] extra_host_input
 *   Output. Pointer to the host memory that is used as extra input space for
 *   ::cnnlPoolingForward_v2 operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call ::cnnlCreatePoolingDescriptor, ::cnnlSetPooling2dDescriptor or ::cnnlSetPooling2dDescriptor_v2
 *   functions accordingly before calling this function.
 * - You need to get the size of the extra input data with ::cnnlGetPoolingExtraInputSize.
 *   The memory of the extra input data should be allocated before calling this function.
 * - The allocated extra input should be passed to the function
 *   to perform the ::cnnlPoolingForward_v2 operation.
 *
 * @note
 * - Only supports 2D pooling.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlInitPoolingExtraInput(cnnlHandle_t handle,
                                                    const cnnlPoolingDescriptor_t pooling_desc,
                                                    const cnnlTensorDescriptor_t x_desc,
                                                    const cnnlTensorDescriptor_t y_desc,
                                                    void * extra_host_input);

// Group:PoolingBackward
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace
 * in the ::cnnlPoolingBackward_v2 operation.
 *
 * The size of the extra workspace is based on the given information of the pooling backward
 * operation, including the tensor descriptors \p pooling_desc, \p y_desc, \p diff_y_desc, \p
 * x_desc and \p diff_x_desc.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in ::cnnlGetPoolingBackwardWorkspaceSize. For detailed information, see ::cnnlHandle_t.
 * @param[in] pooling_desc
 *   Input. The descriptor of the pooling backward operation. For detailed information,
 *   see ::cnnlPoolingDescriptor_t.
 * @param[in] y_desc
 *   Input. The descriptor of the \p y tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] diff_y_desc
 *   Input. The descriptor of \p diff_y tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x_desc
 *   Input. The descriptor of the \p x tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] diff_x_desc
 *   Output. The descriptor of the \p diff_x tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in
 *   the ::cnnlPoolingBackward_v2 operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called before the ::cnnlPoolingBackward_v2 function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetPoolingBackwardWorkspaceSize(cnnlHandle_t handle,
                                    cnnlPoolingDescriptor_t pooling_desc,
                                    cnnlTensorDescriptor_t y_desc,
                                    cnnlTensorDescriptor_t diff_y_desc,
                                    cnnlTensorDescriptor_t x_desc,
                                    cnnlTensorDescriptor_t diff_x_desc,
                                    size_t *size);

// Group:PoolingBackward
/*!
 * @brief Computes gradients of average or maximum pooling. This operation only supports 2D and 3D pooling backward.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlPoolingBackward_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the pooling backward operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] pooling_desc
 *   Input. The descriptor of the pooling backward operation. For detailed information,
 *   see ::cnnlPoolingDescriptor_t.
 * @param[in] alpha, beta
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] y_desc
 *   Input. The descriptor of the \p y tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the index of maximum in kernel.
 * @param[in] diff_y_desc
 *   Input. The descriptor of \p diff_y tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] diff_y
 *   Input. Pointer to the MLU memory that stores the \p diff_y tensor that is the gradient used in average or maximum pooling operation.
 * @param[in] x_desc
 *   Input. The descriptor of the \p x tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the \p x tensor. This tensor is used to compute the index.
 * @param[out] diff_x_desc
 *   Output. The descriptor of the \p diff_x tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the \p diff_x tensor that is the gradient of average or maximum pooling.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @par Formula
 * - See "PoolingBackward operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below in the order of input - index - output:
 *   - half - int16 - half.
 *   - half - int64 - half.
 *   - float - int32 - float.
 *   - float - int64 - float.
 * - On MLU500 series, this function additionally supports the following data type combinations:
 *   - bfloat16-int16-bfloat16.
 *   - bfloat16-int64-bfloat16.
 *
 * @par Data Layout
 * - The layout of input and output must be the same.
 * - In the 2D pooling backward operation, the layout of input and output must be CNNL_LAYOUT_NHWC.
 * - In the 3D pooling backward operation, the layout of input and output must be CNNL_LAYOUT_NDHWC.
 *
 * @par Scale Limitation
 * - In both 2D and 3D pooling backward operations, the pooling window size is limited to the capacity of NRAM.
 *   The maximum pooling window size varies depending on the the MLU platform, pooling mode and data type combinations.
 *   When the pooling window size exceeds the supported maximum value, the function will report the maximum supported kernel size and return ::CNNL_STATUS_NOT_SUPPORTED.
 * - In the 2D pooling backward operation, the parameters should satisfy the following conditions:
 *    - padding >= 0, kernel >= 1, stride >= 1.
 *    - Parameters in the \p diff_y_desc should satisfy the following conditions: in_batch > 0, in_height > 0, in_width > 0, in_channel > 0.
 *
 *      The in_batch is the batch of the input tensor. The in_height is the height of the input tensor.
 *      The in_width is the width of the input tensor. The in_channel is the channel of the input tensor.
 *    - Parameters in the \p diff_x_desc should satisfy the following conditions: out_batch > 0, out_height > 0, out_width > 0, out_channel > 0.
 *
 *      The out_batch is the batch of the output tensor. The out_height is the height of the output tensor.
 *      The out_width is the width of the output tensor. The out_channel is the channel of the output tensor.
 * - In the 3D pooling backward operation, the parameters should satisfy the following conditions:
 *    - padding >= 0, kernel >= 1, stride >= 1.
 *    - Parameters in the \p diff_y_desc should satisfy the following conditions: in_batch > 0, in_depth > 0, in_height > 0, in_width > 0, in_channel > 0.
 *
 *      The in_batch is the batch of the input tensor. The in_depth is the depth of the input tensor. The in_height is the height of the input tensor.
 *      The in_width is the width of the input tensor. The in_channel is the channel of the input tensor.
 *    - The parameters in the \p diff_x_desc should satisfy the following conditions: out_batch > 0, out_depth > 0, out_height > 0, out_width > 0, out_channel > 0.
 *
 *      The out_batch is the batch of the output tensor. The out_depth is the depth of the output tensor. The out_height is the height of the output tensor.
 *      The out_width is the width of the output tensor. The out_channel is the channel of the output tensor.
 * - Large tensors are now only supported on MLU500 series. Paramaters should satisfy the following conditions:
 *    - In the 2D pooling backward operation, only when \p mode is CNNL_POOLING_MAX, output tensor could be large tensor, which means
 *      that the element number could exceed INT32_MAX.
 *    - In the 3D pooling backward operation, all the pooling modes in \p mode support large tensors.
 * @note
 * - In the 2D maximum pooling backward operation, you can use the input \p x to compute the index of maximum pooling with
 *   the optional input \p y. In this situation, if \p x has the NaN value, the output \p diff_x is unpredictable.
 * - In the 3D maximum pooling backward operation, the input tensor \p x is not supported and the input \p y cannot be NULL.
 * - The pooling mode must be \p CNNL_POOLING_MAX, \p CNNL_POOLING_AVERAGE_COUNT_INCLUDE_PADDING,
 *   or \p CNNL_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING.
 * - When kernel_size > stride_size, if the data type of input tensor is half or bfloat16, accuracy problem may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 *   For 2D poolingbackward:
     @verbatim
     4D input[batch, in_height, in_width, in_channel].
     4D index[batch, in_height, in_width, in_channel], maxpoolbackward need.
     4D output[batch, out_height, out_width, in_channel].
     4D padding[padding_top, padding_bottom, padding_left, padding_right].
     2D kernel[kernel_height, kernel_width].
     2D stride[stride_height, stride_width].
     Maxpoolbackward:
           input                index                              output
     ------------------ ------------------               -------------------------
     |       |        | |       |        |               |  1  |  0  |  0  |  0  |
     |   1   |  0.8   | |   0   |   2    |               |-----+-----+-----+-----|
     |       |        | |       |        |               |  0  |  0  | 0.8 |  0  |
     |-------+--------| |-------+--------| --backward--> |-----+-----+-----+-----|
     |       |        | |       |        |               |  0  |  0  |  0  | 0.6 |
     |  0.4  |  0.6   | |   3   |   1    |               |-----+-----+-----+-----|
     |       |        | |       |        |               |  0  | 0.4 |  0  |  0  |
     ------------------ ------------------               -------------------------

     Avgpoolbackward:
           input                                                   output
     ------------------                                  -----------------------------
     |       |        |                                  | 0.25 | 0.25 | 0.2  | 0.2  |
     |   1   |  0.8   |                                  |------+------+------+------|
     |       |        |                                  | 0.25 | 0.25 | 0.2  | 0.2  |
     |-------+--------|           --backward-->          |------+------+------+------|
     |       |        |                                  | 0.1  | 0.1  | 0.15 | 0.15 |
     |  0.4  |  0.6   |                                  |------+------+------+------|
     |       |        |                                  | 0.1  | 0.1  | 0.15 | 0.15 |
     ------------------                                  -----------------------------
     @endverbatim
 *
 */
CNNL_DEPRECATED_FOR(cnnlPoolingBackward_v2)
cnnlStatus_t CNNL_WIN_API cnnlPoolingBackward(cnnlHandle_t handle,
                                              const cnnlPoolingDescriptor_t pooling_desc,
                                              const void *alpha,
                                              const cnnlTensorDescriptor_t y_desc,
                                              const void *y,
                                              const cnnlTensorDescriptor_t diff_y_desc,
                                              const void *diff_y,
                                              const cnnlTensorDescriptor_t x_desc,
                                              const void *x,
                                              const void *beta,
                                              const cnnlTensorDescriptor_t diff_x_desc,
                                              void *diff_x);

// Group:PoolingBackward
/*!
 * @brief Computes gradients of average or maximum pooling. This operation only supports 2D and 3D
 pooling backward.
 * Compared with ::cnnlPoolingBackward, this API provides higher precision in
 certain scenarios with extra input space.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the pooling backward operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] pooling_desc
 *   Input. The descriptor of the pooling backward operation. For detailed information,
 *   see ::cnnlPoolingDescriptor_t.
 * @param[in] alpha, beta
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] y_desc
 *   Input. The descriptor of the \p y tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the index of maximum in kernel.
 * @param[in] diff_y_desc
 *   Input. The descriptor of \p diff_y tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] diff_y
 *   Input. Pointer to the MLU memory that stores the \p diff_y tensor that is the gradient used in average or maximum pooling operation.
 * @param[in] x_desc
 *   Input. The descriptor of the \p x tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the \p x tensor. This tensor is used to compute the index.
 * @param[out] diff_x_desc
 *   Output. The descriptor of the \p diff_x tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the \p diff_x tensor that is the gradient of average or maximum pooling.
 * @param[in]  workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for
 *   ::cnnlPoolingBackward_v2. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in]  workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   ::cnnlPoolingBackward_v2. You can get the size of the workspace with
 *   ::cnnlGetPoolingBackwardWorkspaceSize.
 *
 * @par API Dependency
 * - Before calling this function, call ::cnnlGetPoolingBackwardWorkspaceSize.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_EXECUTION_FAILED, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "PoolingBackward operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below in the order of input - index - output:
 *   - half - int16 - half.
 *   - half - int64 - half.
 *   - float - int32 - float.
 *   - float - int64 - float.
 * - On MLU500 series, this function additionally supports the following data type combinations:
 *   - bfloat16-int16-bfloat16.
 *   - bfloat16-int64-bfloat16.
 *
 * @par Data Layout
 * - The layout of input and output must be the same.
 * - In the 2D pooling backward operation, the layout of input and output must be CNNL_LAYOUT_NHWC.
 * - In the 3D pooling backward operation, the layout of input and output must be CNNL_LAYOUT_NDHWC.
 *
 * @par Scale Limitation
 * - In both 2D and 3D pooling backward operations, the pooling window size is limited to the capacity of NRAM.
 *
 *   The maximum pooling window size varies depending on the the MLU platform, pooling mode and data type combinations.
 *   When the pooling window size exceeds the supported maximum value, the function will report the maximum supported kernel size and return ::CNNL_STATUS_NOT_SUPPORTED.
 * - In the 2D pooling backward operation, the parameters should satisfy the following conditions:
 *    - padding >= 0, kernel >= 1, stride >= 1.
 *    - Parameters in the \p diff_y_desc should satisfy the following conditions: in_batch > 0, in_height > 0, in_width > 0, in_channel > 0.
 *
 *      The in_batch is the batch of the input tensor. The in_height is the height of the input tensor.
 *      The in_width is the width of the input tensor. The in_channel is the channel of the input tensor.
 *    - Parameters in the \p diff_x_desc should satisfy the following conditions: out_batch > 0, out_height > 0, out_width > 0, out_channel > 0.
 *
 *      The out_batch is the batch of the output tensor. The out_height is the height of the output tensor.
 *      The out_width is the width of the output tensor. The out_channel is the channel of the output tensor.
 * - In the 3D pooling backward operation, the parameters should satisfy the following conditions:
 *    - padding >= 0, kernel >= 1, stride >= 1.
 *    - Parameters in the \p diff_y_desc should satisfy the following conditions: in_batch > 0, in_depth > 0, in_height > 0, in_width > 0, in_channel > 0.
 *
 *      The in_batch is the batch of the input tensor. The in_depth is the depth of the input tensor. The in_height is the height of the input tensor.
 *      The in_width is the width of the input tensor. The in_channel is the channel of the input tensor.
 *    - Parameters in the \p diff_x_desc should satisfy the following conditions: out_batch > 0, out_depth > 0, out_height > 0, out_width > 0, out_channel > 0.
 *
 *      The out_batch is the batch of the output tensor. The out_depth is the depth of the output tensor. The out_height is the height of the output tensor.
 *      The out_width is the width of the output tensor. The out_channel is the channel of the output tensor.
 * - Large tensors are now only supported on MLU500 series. Paramaters should satisfy the following conditions:
 *    - In the 2D pooling backward operation, only when \p mode is CNNL_POOLING_MAX, output tensor could be large tensor,
 *      which means that the element number could exceed INT32_MAX.
 *    - In the 3D pooling backward operation, all the pooling modes in \p mode support large tensors.
 * @note
 * - In the 2D maximum pooling backward operation, you can use the input \p x to compute the index of maximum pooling with
 *   the optional input \p y. In this situation, if \p x has the NaN value, the output \p diff_x is unpredictable.
 * - In the 3D maximum pooling backward operation, the input tensor \p x is not supported and the input \p y cannot be NULL.
 * - The pooling mode must be \p CNNL_POOLING_MAX, \p CNNL_POOLING_AVERAGE_COUNT_INCLUDE_PADDING
 *   or \p CNNL_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 *   For 2D poolingbackward:
     @verbatim
     4D input[batch, in_height, in_width, in_channel].
     4D index[batch, in_height, in_width, in_channel], maxpoolbackward need.
     4D output[batch, out_height, out_width, in_channel].
     4D padding[padding_top, padding_bottom, padding_left, padding_right].
     2D kernel[kernel_height, kernel_width].
     2D stride[stride_height, stride_width].
     Maxpoolbackward:
           input                index                              output
     ------------------ ------------------               -------------------------
     |       |        | |       |        |               |  1  |  0  |  0  |  0  |
     |   1   |  0.8   | |   0   |   2    |               |-----+-----+-----+-----|
     |       |        | |       |        |               |  0  |  0  | 0.8 |  0  |
     |-------+--------| |-------+--------| --backward--> |-----+-----+-----+-----|
     |       |        | |       |        |               |  0  |  0  |  0  | 0.6 |
     |  0.4  |  0.6   | |   3   |   1    |               |-----+-----+-----+-----|
     |       |        | |       |        |               |  0  | 0.4 |  0  |  0  |
     ------------------ ------------------               -------------------------

     Avgpoolbackward:
           input                                                   output
     ------------------                                  -----------------------------
     |       |        |                                  | 0.25 | 0.25 | 0.2  | 0.2  |
     |   1   |  0.8   |                                  |------+------+------+------|
     |       |        |                                  | 0.25 | 0.25 | 0.2  | 0.2  |
     |-------+--------|           --backward-->          |------+------+------+------|
     |       |        |                                  | 0.1  | 0.1  | 0.15 | 0.15 |
     |  0.4  |  0.6   |                                  |------+------+------+------|
     |       |        |                                  | 0.1  | 0.1  | 0.15 | 0.15 |
     ------------------                                  -----------------------------
     @endverbatim
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlPoolingBackward_v2(cnnlHandle_t handle,
                       const cnnlPoolingDescriptor_t pooling_desc,
                       const void *alpha,
                       const cnnlTensorDescriptor_t y_desc,
                       const void *y,
                       const cnnlTensorDescriptor_t diff_y_desc,
                       const void *diff_y,
                       const cnnlTensorDescriptor_t x_desc,
                       const void *x,
                       const void *beta,
                       const cnnlTensorDescriptor_t diff_x_desc,
                       void *diff_x,
                       void *workspace,
                       size_t workspace_size);

// Group:Pooling
/*!
 * @brief Initializes the pooling descriptor \p pooling_desc that was previously created with the
 * ::cnnlCreatePoolingDescriptor function, and sets the information about the pooling forward
 * or backward operation to the pooling descriptor \p pooling_desc. The information includes the
 * pooling mode, the NaN propagation mode \p maxpooling_nan_op, the size of kernel for each dimension
 * \p window_height and \p window_width, the padding size for each dimension, the stride of the sliding
 * window for each dimension. To specify the stride of elements in the window and whether to use ceil or
 *  floor to compute the output shape, call the ::cnnlSetPooling2dDescriptor_v2 function.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlSetPooling2dDescriptor_v2 instead, which supports the parameters of \p vertical_dilation,
 *   \p horizon_dilation and the \p ceil_mode that determines whether to use ceil or floor to compute the
 *   shape of output.
 *
 * @param[in,out] pooling_desc
 *   Input/output. The descriptor of pooling operation. For detailed information,
 *   see ::cnnlPoolingDescriptor_t.
 * @param[in] mode
 *   Input. The \p mode describes the algorithm that is used in the implementation of the pooling operation.
 *   For detailed information, see ::cnnlPoolingMode_t.
 * @param[in] maxpooling_nan_opt
 *   Input. The maxpooling_nan_opt describes whether to propagate NaN numbers.
 *   Only supports CNNL_NOT_PROPAGATE_NAN now. For detailed information,
 *   see ::cnnlNanPropagation_t.
 * @param[in] window_height
 *   Input. Height of pooling window.
 * @param[in] window_width
 *   Input. Width of pooling window.
 * @param[in] top_padding
 *   Input. Size of top padding.
 * @param[in] bottom_padding
 *   Input. Size of bottom padding.
 * @param[in] left_padding
 *   Input. Size of left padding.
 * @param[in] right_padding
 *   Input. Size of right padding.
 * @param[in] vertical_stride
 *   Input. The vertical stride of the pooling.
 * @param[in] horizon_stride
 *   Input. The horizon stride of the pooling.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - ::cnnlCreatePoolingDescriptor needs to be called before this function.
 * - You need to call the ::cnnlDestroyPoolingDescriptor function to destroy the descriptor.
 *   Otherwise, the memory leak may occur.
 *
 * @note
 * - Currently, only supports 4D input tensor for pooling operation.
 * - All pointer arguments should not be NULL.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
CNNL_DEPRECATED_FOR(cnnlSetPooling2dDescriptor_v2)
cnnlStatus_t CNNL_WIN_API cnnlSetPooling2dDescriptor(cnnlPoolingDescriptor_t pooling_desc,
                                                     const cnnlPoolingMode_t mode,
                                                     const cnnlNanPropagation_t maxpooling_nan_opt,
                                                     const int window_height,
                                                     const int window_width,
                                                     const int top_padding,
                                                     const int bottom_padding,
                                                     const int left_padding,
                                                     const int right_padding,
                                                     const int vertical_stride,
                                                     const int horizon_stride);

// Group:Pooling
/*!
 * @brief Initializes the pooling descriptor \p pooling_desc that was previously created with the
 * ::cnnlCreatePoolingDescriptor function, and sets the information about the pooling forward
 * operation to the pooling descriptor \p pooling_desc.
 * The information includes the pooling mode, the NaN propagation mode \p maxpooling_nan_opt,
 * the number of the pooling dimension \p dims, the window size for each dimension \p window_height & \p window_width,
 * the padding size for each dimension \p top_padding & \p bottom_padding & \p left_padding & \p right_padding,
 * the stride of the sliding window for each dimension \p vertical_stride & \p horizon_stride.
 * Compared with ::cnnlSetPooling2dDescriptor, this function supports the stride of elements in the window \p vertical_dilation & \p horizon_dilation
 * and whether to use ceil or floor to compute the output shape \p ceil_mode.
 *
 * @param[in,out] pooling_desc
 *   Input/output. The descriptor of pooling operation. For detailed information,
 *   see ::cnnlPoolingDescriptor_t.
 * @param[in] mode
 *   Input. The \p mode describes the algorithm that is used in the implementation of the pooling operation.
 *   For detailed information, see ::cnnlPoolingMode_t.
 * @param[in] maxpooling_nan_opt
 *   Input. The maxpooling_nan_opt describes whether to propagate NaN numbers.
 *   Only supports CNNL_NOT_PROPAGATE_NAN now. For detailed information,
 *   see ::cnnlNanPropagation_t.
 * @param[in] window_height
 *   Input. Height of pooling window.
 * @param[in] window_width
 *   Input. Width of pooling window.
 * @param[in] top_padding
 *   Input. Size of top padding.
 * @param[in] bottom_padding
 *   Input. Size of bottom padding.
 * @param[in] left_padding
 *   Input. Size of left padding.
 * @param[in] right_padding
 *   Input. Size of right padding.
 * @param[in] vertical_stride
 *   Input. The vertical stride of the pooling.
 * @param[in] horizon_stride
 *   Input. The horizon stride of the pooling.
 * @param[in] vertical_dilation
 *   Input. The vertical dilation of the pooling.
 * @param[in] horizon_dilation
 *   Input. The horizon dilation of the pooling.
 * @param[in] ceil_mode
 *   Input. Describes how the output shape is used. When \p ceil_mode is true, the operator will use ceil mode to compute the output shape,
 *   otherwise, floor mode would be used.
 *   For example, Maxpoolforward with ceil mode:
     @verbatim
                input                                output
          -------------------                   ----------------
          |  1  |  0  |  0  |                   |   1   | 0.8  |
          |-----+-----+-----+                   |-------+------+
          |  0  |  0  | 0.8 |  --forward-->     |   1   |   1  |
          |-----+-----+-----+                   --------+------+
          |  1  |  0  |  1  |
          |-----+-----+-----+
     @endverbatim
 *   Maxpoolforward without ceil mode:
     @verbatim
                input
          -------------------
          |  1  |  0  |  0  |                        output
          |-----+-----+-----+                       --------
          |  0  |  0  | 0.8 |  --forward-->         |   1  |
          |-----+-----+-----+                       -------+
          |  1  |  0  |  1  |
          |-----+-----+-----+
     @endverbatim
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - ::cnnlCreatePoolingDescriptor needs to be called before this function.
 * - This function is prepared for ::cnnlPoolingForward or ::cnnlPoolingForward_v2 to compute pooling
 * - After finishing ::cnnlPoolingForward or ::cnnlPoolingForward_v2, you need to call the ::cnnlDestroyPoolingDescriptor function to destroy the descriptor.
 *   Otherwise, the memory leak may occur.
 *
 * @note
 * - Currently, only supports 4D input tensor for pooling operation.
 * - All pointer arguments should not be NULL.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetPooling2dDescriptor_v2(cnnlPoolingDescriptor_t pooling_desc,
                                           const cnnlPoolingMode_t mode,
                                           const cnnlNanPropagation_t maxpooling_nan_opt,
                                           const int window_height,
                                           const int window_width,
                                           const int top_padding,
                                           const int bottom_padding,
                                           const int left_padding,
                                           const int right_padding,
                                           const int vertical_stride,
                                           const int horizon_stride,
                                           const int vertical_dilation,
                                           const int horizon_dilation,
                                           const bool ceil_mode);

// Group:Pooling
/*!
 * @brief Returns the information of pooling descriptor \p pooling_desc. The information includes the
 * pooling mode, the NaN propagation mode \p maxpooling_nan_op, the size of kernel for each dimension
 * \p window_height and \p window_width, the padding size for each dimension, the stride of the sliding
 * window for each dimension.
 *
 * @param[in] pooling_desc
 *   Input. The descriptor of pooling operation. For detailed information,
 *   see ::cnnlPoolingDescriptor_t.
 * @param[in] mode
 *   Input. The \p mode describes the algorithm that is used in the implementation of the pooling operation.
 *   For detailed information, see ::cnnlPoolingMode_t.
 * @param[in] maxpooling_nan_opt
 *   Input. The maxpooling_nan_opt describes whether to propagate NaN numbers.
 *   Only supports CNNL_NOT_PROPAGATE_NAN now. For detailed information,
 *   see ::cnnlNanPropagation_t.
 * @param[in] window_height
 *   Input. Height of pooling window.
 * @param[in] window_width
 *   Input. Width of pooling window.
 * @param[in] top_padding
 *   Input. Size of top padding.
 * @param[in] bottom_padding
 *   Input. Size of bottom padding.
 * @param[in] left_padding
 *   Input. Size of left padding.
 * @param[in] right_padding
 *   Input. Size of right padding.
 * @param[in] vertical_stride
 *   Input. The vertical stride of the pooling.
 * @param[in] horizontal_stride
 *   Input. The horizon stride of the pooling.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - ::cnnlCreatePoolingDescriptor needs to be called before this function at first.
 * - ::cnnlSetPooling2dDescriptor or ::cnnlSetPooling2dDescriptor_v2 needs to be called at second before this function.
 *
 * @note
 * - Only supports 2D pooling.
 * - All pointer arguments should not be NULL.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetPooling2dDescriptor(const cnnlPoolingDescriptor_t pooling_desc,
                                                     cnnlPoolingMode_t *mode,
                                                     cnnlNanPropagation_t *maxpooling_nan_opt,
                                                     int *window_height,
                                                     int *window_width,
                                                     int *top_padding,
                                                     int *bottom_padding,
                                                     int *left_padding,
                                                     int *right_padding,
                                                     int *vertical_stride,
                                                     int *horizontal_stride);
// Group:Pooling
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra workspace
 * to optimize the pooling forward operation.
 *
 * The size of the extra workspace is based on the given information of the pooling
 * forward operation, including the pooling \p mode, the width of pooling output
 * \p out_w_size, the height of pooling output \p out_h_size.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGetPoolingWorkspaceSize_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the pooling forward operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  mode
 *   Input. The \p mode describes the algorithm that is used in the implementation of the pooling operation.
 *   For detailed information, see ::cnnlPoolingMode_t.
 * @param[in]  out_w_size
 *   Input. The width of pooling operation output.
 * @param[in]  out_h_size
 *   Input. The height of pooling operation output.
 * @param[out]  workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *   the pooling forward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - According to the definition of pooling, the parameters should satisfy the following conditions:
 *   - \p out_w_size and \p out_h_size should be greater than zero.
 *
 * @par API Dependency
 * - Some functions need to be called before and after this function. The dependencies are as follows:
 * - ::cnnlCreatePoolingDescriptor needs to be called before this function.
 * - ::cnnlSetPooling2dDescriptor or ::cnnlSetPooling2dDescriptor_v2 needs to be called before this function.
 * - The allocated extra workspace should be passed to the ::cnnlPoolingForward or ::cnnlPoolingForward_v2 function
 *   to perform the pooling forward operation.

 * @note
 * - Only supports 2D pooling.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
CNNL_DEPRECATED_FOR(cnnlGetPoolingWorkspaceSize_v2)
cnnlStatus_t CNNL_WIN_API cnnlGetPoolingWorkspaceSize(cnnlHandle_t handle,
                                                      cnnlPoolingMode_t mode,
                                                      const int out_w_size,
                                                      const int out_h_size,
                                                      size_t *workspace_size);

// Group:Pooling
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra workspace
 * to optimize the pooling forward operation.
 *
 * The size of the extra workspace is based on the given information of the pooling forward operation,
 * including the tensor descriptors \p pooling_desc, \p x_desc, \p y_desc.
 * Compare with ::cnnlGetPoolingWorkspaceSize, in global pooling operation this function will calculate
 * the size of the extra workspace required to call ::cnnlReduce to obtain better performance.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the pooling forward operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] pooling_desc
 *   Input. The descriptor of pooling operation. For detailed information,
 *   see ::cnnlPoolingDescriptor_t.
 * @param[in] x_desc
 *   Input. The descriptor of input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] y_desc
 *   Input. The descriptor of output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *   the pooling forward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Some functions need to be called before and after this function. The dependencies are as follows:
 * - ::cnnlCreatePoolingDescriptor needs to be called before this function.
 * - ::cnnlSetPooling2dDescriptor or ::cnnlSetPooling2dDescriptor_v2 needs to be called before this function.
 * - The allocated extra workspace should be passed to the ::cnnlPoolingForward or ::cnnlPoolingForward_v2 function
 *   to perform the pooling forward operation.

 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlGetPoolingWorkspaceSize_v2(cnnlHandle_t handle,
                                                         const cnnlPoolingDescriptor_t pooling_desc,
                                                         const cnnlTensorDescriptor_t x_desc,
                                                         const cnnlTensorDescriptor_t y_desc,
                                                         size_t *workspace_size);
// Group:Sign
/*!
 * @brief Returns an output tensor \p y with signs of the elements of
 * input tensor \p x.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the sign operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in]  y_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out]  y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH,
 *   ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Sign Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The combinations of the data types for input tensor \p x and
 *   output tensor \p y must be bfloat16-bfloat16, half-half, float-float, or complex_float-complex_float.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor must meet the following requirements:
 *   - input tensor: each dimension must be more than zero
 *   - output tensor: each dimension must be more than zero
 *   - input tensor shape must be the same as output tensor shape.
 *
 * @note
 * - You can specify the stride of all dimensions for \p x_desc and \p y_desc with
 *   ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the sign operation is as follows:
     @verbatim
      input an array by 1 * 2 * 2 * 1
      --> input: [[[[-1.0]], [[0.0]]], [[[2.0]], [[-6.5]]]]

      output an array by 1 * 2 * 2 * 1
      --> output: [[[[-1.0]], [[0.0]]], [[[1.0]], [[-1.0]]]]

      input an array by 1 * 4
      --> input: [5+3j, 0, 3-4j, 1+2j]

      output an array by 1 * 4
      --> output: [0.8575+0.5145j, 0.0000+0.0000j, 0.6000-0.8000j, 0.4472+0.8944j]
      @endverbatim
 * @par Reference
 * - https://tensorflow.google.cn/api_docs/cc/class/tensorflow/ops/sign
 * - https://pytorch.org/docs/stable/generated/torch.sign.html?highlight=sign
 * - https://pytorch.org/docs/stable/generated/torch.sgn.html?highlight=sgn
 */
cnnlStatus_t CNNL_WIN_API cnnlSign(cnnlHandle_t handle,
                                   const cnnlTensorDescriptor_t x_desc,
                                   const void *x,
                                   const cnnlTensorDescriptor_t y_desc,
                                   void *y);
// Group:Tile
/*!
 * @brief Copies and expands the input tensor to the shape of output tensor.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the tile
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "Tile Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The (I/O)function supports the following data widths for \p input and \p output tensors.
 *   - input tensor: 1-byte, 2-byte, 4-byte, 8-byte
 *   - output tensor: 1-byte, 2-byte, 4-byte, 8-byte
 *
 *   The byte width of a data type can be got with the ::cnnlGetSizeOfDataType function.
 *
 *   Note that the data type of input tensor \p input and output tensor \p output must be the same.
 *
 * @par Data Layout
 * - The input tensor \p input and output tensor \p output are multi-dimensional arrays, supporting
 *   up to \p CNNL_DIM_MAX dimensions.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor must meet the following requirements:
 *   - Every dimension of the input tensor should be divisible by the same dimension of the output
 *     tensor.
 *
 * @note
 * - The input tensor \p input and output tensor \p output are multi-dimensional arrays, supporting
 *   up to \p CNNL_DIM_MAX dimensions.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the tile operation is as follows:
     @verbatim
     input one array by 2 * 2 --> input: [[1, 2], [3, 4]]

     output array by 3 * 2 * 2 --> output: [[[1, 2], [3, 4]],
                                            [[1, 2], [3, 4]],
                                            [[1, 2], [3, 4]]]
     @endverbatim
 *
 * @par Reference
 * - https://tensorflow.org/api_docs/python/tf/tile
 */
cnnlStatus_t CNNL_WIN_API cnnlTile(cnnlHandle_t handle,
                                   const cnnlTensorDescriptor_t input_desc,
                                   const void *input,
                                   const cnnlTensorDescriptor_t output_desc,
                                   void *output);
// Group:GradientDescent
/*!
 * @brief Creates an operation to update filter by gradient descent algorithm.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in this operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] grad_desc
 *   Input. The descriptor of \p grad tensor, which is the gradient.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] grad
 *   Input. Pointer to the MLU memory that stores the \p grad tensor.
 * @param[in] lr
 *   Input. Pointer to the MLU memory that stores the \p lr parameter, which is the learning rate.
 * @param[in] var_desc
 *   Input. The descriptor of \p var tensor, which is the filter to be updated.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in,out] var
 *   Input and output. Pointer to the MLU memory that stores the \p var tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Gradient Descent Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Scale Limitation
 * - The \p grad tensor and \p var tensor must meet the following requirements:
 *   - The shape of \p grad tensor equals the shape of \p var tensor.
 *
 * @par Data Type
 * - Data type of \p grad tensor, \p var tensor and \p lr parameter must be the same.
 * - The supported data types are as follows:
 *   - grad tensors: half, float.
 *   - var tensors: half, float.
 *   - lr parameter: half, float
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the operation is as follows:
    @verbatim
    --> grad: an array [64, 78, 1024];
    --> var: an array [64, 78, 1024];
    --> lr: an array [1];
    Then we will get the output:
    --> var: an array [64, 78, 1024] same as grad;
    @endverbatim
 *
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlGradientDescent(cnnlHandle_t handle,
                                              const cnnlTensorDescriptor_t grad_desc,
                                              const void *grad,
                                              const void *lr,
                                              const cnnlTensorDescriptor_t var_desc,
                                              void *var);

// Group:Arange
/*!
 * @brief Creates a sequence of numbers that begins at the value pointed by \p start,
 * and extends by increments of the value pointed by \p step, up to but not including
 * the value pointed by \p end.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlArange_v3 instead, which offers tensor descriptor of output.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the arange operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] start
 *   Input. Pointer to the host memory that stores the first value of output sequence.
 * @param[in] end
 *   Input. Pointer to the host memory that stores the upper limit of output sequence.
 * @param[in] step
 *   Input. Pointer to the host memory that stores the increment of output sequence.
 * @param[in] output_dtype
 *   Input. The data type of output sequence. Only supports int32, int64, float, bfloat16 and half.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output sequence.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Arange Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *   start - end - step - output.
 *   - int32 - int32 - int32 - int32.
 *   - int64 - int64 - int64 - int64.
 *   - float - float - float - float.
 *   - float - float - float - half.
 *   - float - float - float - bfloat16.
 *
 *   The bfloat16 data type is supported only on MLU500 series.
 *
 *   The data type int64 is supported only on MLU300 and MLU500 series.
 *
 * @par Scale Limitation
 * - The input parameters should meet the following requirements:
 *   - The element number of \p output should be less than or equal to 16777216 on 1V.
 *   - When the data type of output is set to int32, output value should be in range of
 *   [-8388608, 8388607] on MLU200 series and [-16777216, 16777215] on 1V.
 *   - When the data type of output is set to int64, output value should be in range of
 *   [-140737488355328, 140737488355328).
 *   - The sign of \p end and \p start should be consistent with the sign of
 *     \p step.
 *   - The \p step should be non-zero.
 *   - The \p end should not be equal to the \p start.
 *   - The \p start, \p step and \p end cannot be NaN or infinity.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the \p output_dtype to ::CNNL_DTYPE_FLOAT.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the arange operation is as follows:
     @verbatim
     int start = 0;
     int end = 9;
     int step = 3;
     cnnlArange(handle, &start, $end, &step, CNNL_DTYPE_INT32, output);

     Then we will get the output:
     output: --> [0,3,6]
     @endverbatim
 *
 * @par Reference
 * - http://www.tensorflow.org/api_docs/python/tf/range
 */

CNNL_DEPRECATED_FOR(cnnlArange_v3)
cnnlStatus_t CNNL_WIN_API cnnlArange(cnnlHandle_t handle,
                                     const void *start,
                                     const void *end,
                                     const void *step,
                                     const cnnlDataType_t output_dtype,
                                     void *output);

// Group:Arange
/*!
 * @brief Creates a sequence of numbers that begins at the value pointed by \p start,
 * and extends by increments of the value pointed by \p step. The length of the sequence can
 * be inferred from output tensor descriptor \p output_desc.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release. Use ::cnnlArange_v3
 *   instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the arange operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. The algorithm used to compute the output. It only supports high precision and
 *   is deprecated.
 *   For detailed information, see ::cnnlComputationPreference_t.
 * @param[in] start
 *   Input. Pointer to the host memory that stores the first value of output sequence.
 * @param[in] step
 *   Input. Pointer to the host memory that stores the increment of output sequence.
 * @param[in] output_desc
 *   Output. The descriptor of \p output tensor. For detailed information, see
 *  ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output sequence.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Arange Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *   start - step - output.
 *   - int32 - int32 - int32.
 *   - int64 - int64 - int64.
 *   - float - float - float.
 *   - float - float - half.
 *   - float - float - bfloat16.
 *
 *   The bfloat16 data type is supported only on MLU500 series.
 *
 *   The data type int64 is supported only on MLU300 and MLU500 series.
 *
 * @par Scale Limitation
 * - The element number of \p output should be less than or equal to 16777216 on 1V.
 * - When the data type of output is set to int32, output value should be in range of
 *   [-8388608, 8388607] on MLU200 series and [-16777216, 16777215] on 1V.
 * - When the data type of output is set to int64, output value should be in range of
 *   [-140737488355328, 140737488355328).
 * - The \p step_value should be non-zero.
 * - The \p start_value and \p step_value cannot be NaN or infinity.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set \p output_dtype to ::CNNL_DTYPE_FLOAT.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the arange operation is as follows:
     @verbatim
     int start = 0;
     int step = 3;
     cnnlTensorDescriptor_t desc;
     cnnlCreateTensorDescriptor(&desc);
     cnnlSetTensorDescriptor(desc, CNNL_LAYOUT_ARRAY, CNNL_DTYPE_INT32, 1, [3]);
     cnnlArange_v2(handle, CNNL_COMPUTATION_FAST, &start, &step, desc, output);
     cnnlDestroyTensorDescriptor(desc);

     Then we will get the output:
     output: --> [0,3,6]
     @endverbatim
 *
 * @par Reference
 * - http://www.tensorflow.org/api_docs/python/tf/range
 */

CNNL_DEPRECATED_FOR(cnnlArange_v3)
cnnlStatus_t CNNL_WIN_API cnnlArange_v2(cnnlHandle_t handle,
                                        const cnnlComputationPreference_t prefer,
                                        const void *start,
                                        const void *step,
                                        const cnnlTensorDescriptor_t output_desc,
                                        void *output);

// Group:Arange
/*!
 * @brief Creates a sequence of numbers that begins at the value pointed by \p start,
 * and extends by increments of the value pointed by \p step. The length of the sequence can
 * be inferred from output tensor descriptor \p output_desc. Compared with ::cnnlArange_v2,
 * this API removes param \p prefer, but the function is the same as ::cnnlArange_v2.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the arange operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] start
 *   Input. Pointer to the host memory that stores the first value of output sequence.
 * @param[in] step
 *   Input. Pointer to the host memory that stores the increment of output sequence.
 * @param[in] output_desc
 *   Output. The descriptor of \p output tensor. For detailed information, see
 *  ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output sequence.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Arange Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *   start - step - output.
 *   - int32 - int32 - int32.
 *   - int64 - int64 - int64.
 *   - float - float - float.
 *   - float - float - half.
 *   - float - float - bfloat16.
 *
 *   The data type bfloat16 is supported only on MLU500 series.
 *
 *   The data type int64 is supported only on MLU300 and MLU500 series.
 *
 * @par Scale Limitation
 * - The element number of \p output should be less than or equal to 16777216 on 1V.
 * - When the data type of output is set to int32, output value should be in range of
 *   [-8388608, 8388607] on MLU200 series and [-16777216, 16777215] on 1V.
 * - When the data type of output is set to int64, output value should be in range of
 *   [-140737488355328, 140737488355328).
 * - The \p step_value should be non-zero.
 * - The \p start_value and \p step_value cannot be NaN or infinity.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the \p output_dtype to ::CNNL_DTYPE_FLOAT.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the arange operation is as follows:
     @verbatim
     int start = 0;
     int step = 3;
     cnnlTensorDescriptor_t desc;
     cnnlCreateTensorDescriptor(&desc);
     cnnlSetTensorDescriptor(desc, CNNL_LAYOUT_ARRAY, CNNL_DTYPE_INT32, 1, [3]);
     cnnlArange_v3(handle, &start, &step, desc, output);
     cnnlDestroyTensorDescriptor(desc);

     Then we will get the output:
     output: --> [0,3,6]
     @endverbatim
 *
 * @par Reference
 * - http://www.tensorflow.org/api_docs/python/tf/range
 */

cnnlStatus_t CNNL_WIN_API cnnlArange_v3(cnnlHandle_t handle,
                                        const void *start,
                                        const void *step,
                                        const cnnlTensorDescriptor_t output_desc,
                                        void *output);

// Group:Linspace
/*!
 * @brief Creates a one-dimensional evenly spaced number sequence over a specified interval.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the cnnlLinspace operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] start
 *   Input. A float number that is the starting value of the sequence.
 * @param[in] end
 *   Input. A float number that is the ending value of the sequence.
 * @param[in] output_desc
 *   Input. The descriptor of \p output tensor. For detailed information, see
 *  ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output sequence.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Linspace Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported data types of output tensor are:
 *   float, half, bfloat16, int32, int64
 *
 * @par Performance Optimization
 * - To have better performance, set the data type of output to ::CNNL_DTYPE_FLOAT.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Scale Limitation
 * - On MLU200 series, when the data type of output is set to int32, output value should be in the
 *   range of [-8388608, 8388607].
 * @par Example
 * - The example of the linspace operation is as follows:
     @verbatim
     float start = 0;
     float end = 9;
     shape of output = [3];
     data type = CNNL_DTYPE_FLOAT
     cnnlLinspace(handle, start, end, output_desc, output);

     Then we will get the output:
     output: --> [0,4.5,9]
     @endverbatim
 *
 * @par Reference
 * - https://tensorflow.google.cn/versions/r2.1/api_docs/python/tf/linspace
 * - https://pytorch.org/docs/1.9.0/generated/torch.linspace.html
 * - https://numpy.org/doc/stable/reference/generated/numpy.linspace.html
 */

cnnlStatus_t CNNL_WIN_API cnnlLinspace(cnnlHandle_t handle,
                                       const float start,
                                       const float end,
                                       const cnnlTensorDescriptor_t output_desc,
                                       void *output);
// Group:MaskZero
/*!
 * @brief Maskzero is the fusion of masking and prelu, which is mainly used in voice network.
 *
 * This operation performs two steps.
 *
 * 1. If \p pad_label is equal to the lowest dimension of \p label, set the corresponding H * C
 * to zero in \p input. If the length of the lowest dimension of \p label(l_label) is greater than
 * the axis dimension of \p input(l_input), then define stride = l_label / l_input,  the step of
 * \p label is stride and the step of \p input is 1. So there may have useless data in \p label.
 * 2. If \p fuse_relu is true, then \p input multiplies \p slope when \p input is smaller
 * than zero.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *  Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *  maskzero operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *  Input. The descriptor of \p input tensor. For detailed information, see
 *  ::cnnlTensorDescriptor_t.
 * @param[in] input
 *  Input. Pointer to the MLU memory that stores the \p input tensor.
 * @param[in] label_desc
 *  Input. The descriptor of \p label tensor. It is used to compare with \p pad_label. If it is
 *  equal to \p pad_label, the corresponding dimension in \p input is set to zero. Otherwise,
 *  this parameter is not used in the operation. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] label
 *  Input. Pointer to the MLU memory that stores the \p label tensor.
 * @param[in] output_desc
 *  Output. The descriptor of \p output tensor. For detailed information, see
 *  ::cnnlTensorDescriptor_t.
 * @param[out] output
 *  Output. Pointer to the MLU memory that stores the \p output tensor.
 * @param[in] fuse_relu
 *  Input. The parameter determines whether to multiply slope.
 * @param[in] axis
 *  Input. The dimension of \p input corresponding to the lowest dimension of \p label.
 * @param[in] pad_label
 *  Input. The parameter used to compare with \p label.
 * @param[in] slope
 *  Input. The parameter used to multiply \p input when \p input is smaller than zero.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "MaskZero Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for the input tensor \p input,
 *   label tensor \p label and output tensor \p output.
 *   Note that all the tensors must have the same data type.
 *   - input: half, float.
 *   - label: half, float.
 *   - output: half, float.
 *
 * @par Scale Limitation
 * - Parameters must meet the following requirements:
 *   - input.dim[0] = label.dim[0]
 *   - input.dim[axis] <= label.dim[1]
 *   - \p axis should be greater than zero and less than the dimension of input.
 *   - The shape of input should be the same as output.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the maskzero operation is as follows:
     @verbatim
     input two arrays by 2 * 2 * 2 * 3 and 2 * 2
     --> input: [[[[-1, 0, 1], [2, 3, 4]],
                  [[5, 6, 7], [8, 9, 10]]],
                 [[[11, 12, 13], [14, 15, 16]],
                  [[17, 18, 19], [20, 21, 22]]]]

     --> label: [[0, 1], [2, 3]]

     param:
       pad_label: 1.0, fuse_relu: 1, slope: 2.0, axis: 2,

     output array by 2 * 2 * 2 * 3 --> output: [[[[-2, 0, 1], [0, 0, 0]],
                                                 [[5, 6, 7], [0, 0, 0]]],
                                                [[[11, 12, 13], [14, 15, 16]],
                                                 [[17, 18, 19], [20, 21, 22]]]]
     @endverbatim
 *
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlMaskZero(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t input_desc,
                                       const void *input,
                                       const cnnlTensorDescriptor_t label_desc,
                                       const void *label,
                                       const cnnlTensorDescriptor_t output_desc,
                                       const void *output,
                                       const bool fuse_relu,
                                       const int axis,
                                       const float pad_label,
                                       const float slope);
// Group:SoftmaxBackward
/*!
 * @brief Computes gradients of ::cnnlSoftmaxBackward with input tensors \p y and \p diff_y,
 * and returns the results in the output tensor \p diff_x.
 *
 * This function is used to create an operation of softmaxbackward and logsoftmaxbackward,
 * then is applied to a 3D input tensor rescaling them so that the elements of
 * the 3D output tensor is in range of (0,1) and the summary along the reduction
 * dimension is 1.
 *
 * The softmax backward operation is only used in PyTorch.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   softmax backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] algorithm
 *   Input. The computing algorithm used to compute softmax backward, defined in
 *   ::cnnlSoftmaxAlgorithm_t.
 * @param[in] mode
 *   Input. The reduction dimension in the computing procedure, defined in ::cnnlSoftmaxMode_t.
 * @param[in] alpha
 *   Input. The parameter to scale the result of this function. This parameter is not supported
 *   currently. The value of alpha is 1 by default.
 * @param[in] beta
 *   Input. The parameter to scale the previous result in the output tensor \p diff_x. This
 *   parameter is not supported currently. The value of beta is 0 by default.
 * @param[in] y_desc
 *   Input. The descriptor of the tensors. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] diff_y_desc
 *   Input. The descriptor of the tensors. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] diff_y
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[out] diff_x_desc
 *   Output. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Softmax Backward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensors \p y and \p diff_y must be the same.
 * - When data types of \p y and \p diff_y are float, data type of \p diff_x can be float or half.
 * - When data types of \p y and \p diff_y are half, data type of \p diff_x must be half.
 * - The supported data types of the input tensor and output tensor are as follows:
 *   - input tensor: float, half, bfloat16.
 *   - output tensor: float, half, bfloat16.
 * - When \p algorithm is set to \p CNNL_SOFTMAX_LOG, bfloat16 is supported on MLU300 and MLU500
 *   series. Otherwise, bfloat16 is supported only on MLU500 series.
 *
 * @par Scale Limitation
 * On MLU200 series, when \p algorithm is CNNL_SOFTMAX_LOG, the range of input tensor \p y is recommended
 * to be in range of (-inf, 0.5] for higher precision for both half and float data type.
 *
 * @par API Requirements
 * - You need to reshape the input tensor to 3 dimension before invoking this operation.
 *   and reshape the output tensor to original shape after invoking this operation.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, set \p algorithm to \p CNNL_SOFTMAX_FAST,
 *   reshape the reduction dimension to the lowest dimension and set the \p mode to
 *   \p CNNL_SOFTMAX_MODE_LOW_DIMENSION. The result produced by \p CNNL_SOFTMAX_FAST algorithm is
 *   less precise than \p CNNL_SOFTMAX_ACCURATE. If the precision does not meet your requirement,
 *   you must replace the \p algorithm with \p CNNL_SOFTMAX_ACCURATE.
 *
 * @par Example
 * - The example of the softmax backward operation is as follows:
    @verbatim
      Example 1:
      input two arrays by 1 * 1 * 3 and 1 * 1 * 3
      --> y: [[[1,1,1]]]

      --> dy: [[[1,1,1]]]

      param:
        mode: CNNL_SOFTMAX_MODE_LOW_DIMENSION, algorithm: CNNL_SOFTMAX_FAST/CNNL_SOFTMAX_ACCURATE

      output an array by 1 * 1 * 3
      --> dx: [[[-2,-2,-2]]]

      Example 2:
      input two arrays by 1 * 1 * 3 and 1 * 1 * 3
      --> y: [[[1,1,1]]]

      --> dy: [[[1,1,1]]]

      params:
        mode: CNNL_SOFTMAX_MODE_LOW_DIMENSION, algorithm: CNNL_SOFTMAX_LOG

      output an array by 1 * 1 * 3
      --> dx: [[[-7.154,-7.154,-7.154]]]
    @endverbatim
 *
 * @par Reference
 * - https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/activation.py
 */
cnnlStatus_t CNNL_WIN_API cnnlSoftmaxBackward(cnnlHandle_t handle,
                                              cnnlSoftmaxAlgorithm_t algorithm,
                                              cnnlSoftmaxMode_t mode,
                                              const void *alpha,
                                              const cnnlTensorDescriptor_t y_desc,
                                              const void *y,
                                              const cnnlTensorDescriptor_t diff_y_desc,
                                              const void *diff_y,
                                              const void *beta,
                                              const cnnlTensorDescriptor_t diff_x_desc,
                                              void *diff_x);

// Group:Copy
/*!
 * @brief Returns a copy of input tensor \p input in the output tensor \p output on MLU devices.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlCopy_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the copy operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - The (I/O) function supports the following data widths for \p input and \p output
 *   tensors.
 *   - input tensor: 1-byte, 2-byte, 4-byte, 8-byte, 16-byte.
 *   - output tensor: 1-byte, 2-byte, 4-byte, 8-byte, 16-byte.
 *
 *   The byte width of a data type can be got with ::cnnlGetSizeOfDataType.
 *
 *   Note that the data type of input tensor \p input and output tensor \p output must be the same.
 *
 * @par Formula
 * - See "Copy Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @note
 * - You can specify the stride of all dimensions for \p input_desc and \p output_desc with
 *   ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - Data type of input tensor and output tensor must be the same.
 * - Data layout of input tensor and output tensor must be the same.
 * - Shape of input tensor and output tensor must be the same.
 *
 * @par Scale Limitation
 * - When the input or output tensor is non-contiguous, for example, with non-contiguous strides set
 *   in the tensor descriptor, the total number spanned by either of the input or output tensor
 *   should be less than or equal to INT_MAX on MLU300 series and CE3226.
 *
 * @par Example
 * - The example of the copy operation is as follows:
     @verbatim
      input array by 2 * 2
      --> then: [[1, 8], [6, 4]]

      output array by 2 * 2
      --> output: [[1, 8], [6, 4]]

     @endverbatim
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/raw_ops/Snapshot
 */
CNNL_DEPRECATED_FOR(cnnlCopy_v2)
cnnlStatus_t CNNL_WIN_API cnnlCopy(cnnlHandle_t handle,
                                   const cnnlTensorDescriptor_t input_desc,
                                   const void *input,
                                   const cnnlTensorDescriptor_t output_desc,
                                   void *output);

// Group:Copy
/*!
 * @brief Returns a copy of input tensor \p input in the output tensor \p output on MLU devices.
 *        Compared with ::cnnlCopy, it provides better performance for tensors with strides
 *       in most cases with an extra input space.

 * This function needs extra MLU memory as the workspace to work. You can get the workspace_size
 * with ::cnnlGetCopyWorkspaceSize.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the copy operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   copy operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the copy operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - The (I/O) function supports the following data widths for \p input and \p output
 *   tensors.
 *   - input tensor: 1-byte, 2-byte, 4-byte, 8-byte, 16-byte.
 *   - output tensor: 1-byte, 2-byte, 4-byte, 8-byte, 16-byte.
 *
 *   The byte width of a data type can be got with ::cnnlGetSizeOfDataType.
 *
 *   Note that the data type of input tensor \p input and output tensor \p output must be the same.
 *
 * @par Formula
 * - See "Copy Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @note
 * - You can specify the stride of all dimensions for \p input_desc and \p output_desc with
 *   ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - Data type of input tensor and output tensor must be the same.
 * - Data layout of input tensor and output tensor must be the same.
 * - Shape of input tensor and output tensor must be the same.
 *
 * @par Scale Limitation
 * - When the input or output tensor is non-contiguous, for example, with non-contiguous strides set
 *   in the tensor descriptor, the total number spanned by either of the input or output tensor
 *   should be less than or equal to INT_MAX on MLU300 series and CE3226.
 *
 * @par Example
 * - The example of the copy operation is as follows:
     @verbatim
      input array by 2 * 2
      --> then: [[1, 8], [6, 4]]

      output array by 2 * 2
      --> output: [[1, 8], [6, 4]]

     @endverbatim
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/raw_ops/Snapshot
 */
cnnlStatus_t CNNL_WIN_API cnnlCopy_v2(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t input_desc,
                                      const void *input,
                                      const cnnlTensorDescriptor_t output_desc,
                                      void *output,
                                      void *workspace,
                                      size_t workspace_size);

// Group:Copy
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace
 * to optimize the copy operation.
 *
 * The size of the extra workspace is based on the given information of the copy operation,
 * including the input tensor descriptor \p input_desc and output tensor descriptor \p output_desc.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the copy operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output_desc
 *   Output. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *   the copy operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, you must call ::cnnlCreateTensorDescriptor and
 *   ::cnnlSetTensorDescriptor to create and set the tensor descriptors \p input_desc and
 *   \p output_desc.
 * - The allocated extra workspace should be passed to the ::cnnlCopy_v2 to perform the copy
 *   operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */

cnnlStatus_t CNNL_WIN_API cnnlGetCopyWorkspaceSize(cnnlHandle_t handle,
                                                   const cnnlTensorDescriptor_t input_desc,
                                                   const cnnlTensorDescriptor_t output_desc,
                                                   size_t *size);

// Group:Lrn
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace
 * to optimize the lrn operation.
 *
 * The size of the extra workspace is based on the given information of the lrn operation,
 * including the input tensor descriptor \p input_desc, output tensor descriptor \p output_desc and
 * the lrn window width \p lrn_n.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlGetLrnWorkspaceSize_v2 instead.
 *
 *  @param[in]  handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *    queues in the cnnlLrn operation. For detailed information,
 *    see ::cnnlHandle_t.
 *  @param[in] input_desc
 *    Input. The descriptor of input tensor. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[in]  output_desc
 *    Output. The descriptor of output tensor. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[in] lrn_n
 *    Input. The width of lrn window.
 *  @param[out]  size
 *   Output. An extra space size needed in lrn operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - The shape of input should be the same as output's.
 * - \p lrn_n should be in range of (0, 15], and \p lrn_n % 2 must be equal to 1.
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlCreateTensorDescriptor and
 *   ::cnnlSetTensorDescriptor functions.
 * - The allocated extra workspace should be passed to the ::cnnlLrn or ::cnnlLrn_v2 function
 *   to perform the lrn operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlGetLrnWorkspaceSize_v2)
cnnlStatus_t CNNL_WIN_API cnnlGetLrnWorkspaceSize(cnnlHandle_t handle,
                                                  const cnnlTensorDescriptor_t input_desc,
                                                  const cnnlTensorDescriptor_t output_desc,
                                                  unsigned lrn_n,
                                                  size_t *size);

// Group:Lrn
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace
 * to optimize the lrn operation. Compared with ::cnnlGetLrnWorkspaceSize,
 * ::cnnlGetLrnWorkspaceSize_v2 supports lrn within channel computing mode.
 *
 * The size of the extra workspace is based on the given information of the lrn operation,
 * including the input tensor descriptor \p input_desc, output tensor descriptor \p output_desc and
 * the lrn window width \p lrn_n.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 *  @param[in]  handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *    queues in the cnnlLrn operation. For detailed information,
 *    see ::cnnlHandle_t.
 *  @param[in] input_desc
 *    Input. The descriptor of input tensor. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[in]  output_desc
 *    Input. The descriptor of output tensor. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 * @param[in]  lrn_mode
 *   Input. The computing mode of lrn operation. For detailed information, see ::cnnlLrnMode_t.
 *  @param[in] lrn_n
 *    Input. The width of lrn window.
 *  @param[out]  size
 *     Output. An extra space size needed in lrn operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - The shape of input should be the same as output's.
 * - \p lrn_n should be in range of (0, 15], and \p lrn_n % 2 must be equal to 1.
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlCreateTensorDescriptor and
 *   ::cnnlSetTensorDescriptor functions.
 * - The allocated extra workspace should be passed to the ::cnnlLrn or ::cnnlLrn_v2 function
 *   to perform the lrn operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetLrnWorkspaceSize_v2(cnnlHandle_t handle,
                                                  const cnnlTensorDescriptor_t input_desc,
                                                  const cnnlTensorDescriptor_t output_desc,
                                                  cnnlLrnMode_t lrn_mode,
                                                  unsigned lrn_n,
                                                  size_t *size);

// Group:Lrn
/*!
 * @brief Returns in \p extra_input_size the size of the MLU memory and host memory that is used
 * as an extra input data to optimize the lrn operation. You need to allocate memory both on host
 * and MLU based on the size returned in \p extra_input_size.
 *
 * The size of extra input data is based on the given information of the lrn operation, including
 * the output tensor descriptor \p output_desc and lrn window width \p lrn_n.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlGetLrnExtraInputSize_v2 instead.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the cnnlLrn operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] lrn_n
 *   Input. The width of lrn window.
 * @param[in]  output_desc
 *   Input. The descriptor of output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out]  extra_input_size
 *    Output. A host pointer to the returned size of the extra input data in bytes
 *    that is used in the lrn operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - \p lrn_n should be in range of (0, 15], and \p lrn_n % 2 must be equal to 1.
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlCreateTensorDescriptor and
 * ::cnnlSetTensorDescriptor functions.
 * - After calling this function, you need to call ::cnnlInitLrnExtraInput to initialize the memory on host.
 * - The allocated extra input should be passed to the ::cnnlLrn_v2 function to perform the lrn operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlGetLrnExtraInputSize_v2)
cnnlStatus_t CNNL_WIN_API cnnlGetLrnExtraInputSize(cnnlHandle_t handle,
                                                  const cnnlTensorDescriptor_t output_desc,
                                                  unsigned lrn_n,
                                                  size_t *extra_input_size);
// Group:Lrn
/*!
 * @brief Returns in \p extra_input_size the size of the MLU memory and host memory that is used
 * as an extra input data to optimize the lrn operation. You need to allocate memory both on host
 * and MLU based on the size returned in \p extra_input_size. Compared with ::cnnlGetLrnExtraInputSize,
 * ::cnnlGetLrnExtraInputSize_v2 supports lrn within channel computing mode.
 *
 * The size of extra input data is based on the given information of the lrn operation, including
 * the output tensor descriptor \p output_desc and lrn window width \p lrn_n.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the cnnlLrn operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  output_desc
 *   Input. The descriptor of output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  lrn_mode
 *   Input. The computing mode of lrn operation. For detailed information, see ::cnnlLrnMode_t.
 * @param[in] lrn_n
 *   Input. The width of lrn window.
 * @param[out]  extra_input_size
 *    Output. A host pointer to the returned size of the extra input data in bytes
 *    that is used in the lrn operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - \p lrn_n should be in range of (0, 15], and \p lrn_n % 2 must be equal to 1.
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlCreateTensorDescriptor and
 * ::cnnlSetTensorDescriptor functions.
 * - After calling this function, you need to call ::cnnlInitLrnExtraInput to initialize the memory on host.
 * - The allocated extra input should be passed to the ::cnnlLrn_v2 function to perform the lrn operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetLrnExtraInputSize_v2(cnnlHandle_t handle,
                                                  const cnnlTensorDescriptor_t output_desc,
                                                  cnnlLrnMode_t lrn_mode,
                                                  unsigned lrn_n,
                                                  size_t *extra_input_size);

// Group:Lrn
/*!
 * @brief Initializes the extra input data space \p extra_host_input on host.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the cnnlLrn operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  lrn_mode
 *   Input. The computing mode of lrn operation. For detailed information, see ::cnnlLrnMode_t.
 * @param[in] lrn_n
 *   Input. The width of lrn window.
 * @param[in]  lrn_alpha
 *   Input. Value of the alpha parameter in lrn formula, as a scale factor for variance.
 * @param[in]  lrn_beta
 *   Input. Value of the beta parameter in the lrn formula, as a power parameter.
 * @param[in]  lrn_k
 *   Input. Value of the k parameter in the formula, as a hyper-parameter.
 * @param[in] input_desc
 *   Input. The descriptor of input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  output_desc
 *   Input. The descriptor of output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out]  extra_host_input
 *   Output. Pointer to the host memory that is used as an extra input space
 *   for the lrn operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - The shape of input should be the same as output's.
 * - \p lrn_n should be in range of (0, 15], and \p lrn_n % 2 must be equal to 1.
 * - \p lrn_k should be greater than 1e-5.
 * - \p lrn_beta should be greater than 0.01.
 *
 * @par API Dependency
 * - You need to get the size of the extra input data with ::cnnlGetLrnExtraInputSize_v2.
 *   The memory of the extra input data should be allocated before calling this function.
 * - This function must be called after the ::cnnlCreateTensorDescriptor and
 *   ::cnnlSetTensorDescriptor functions.
 * - The allocated extra input should be passed to the ::cnnlLrn_v2 function
 *   to perform the reorg operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlInitLrnExtraInput(cnnlHandle_t handle,
                                  cnnlLrnMode_t lrn_mode,
                                  unsigned lrn_n,
                                  double lrn_alpha,
                                  double lrn_beta,
                                  double lrn_k,
                                  const cnnlTensorDescriptor_t input_desc,
                                  const cnnlTensorDescriptor_t output_desc,
                                  void* extra_host_input);

/*! The parameters of the ::cnnlSpace2batch or ::cnnlBatch2space operation that
 * hold space to batch or batch to space information including the height
 * dilation and the width dilation of the input tensor.
 *
 * The dilation is usually used in the convolution function. It represents the
 * size of which the convolution kernel increases in certain dimension. Based on
 * the dilation, the shape of the tensor in ::cnnlSpace2batch and
 * ::cnnlBatch2space functions are changed. If the dilation is set to 1,
 * the convolution is implemented normally. If the dilation is set to 2, the
 * interval of the input elements should be set to 1. The dilation may be in
 * different dimensions.
 */
typedef struct cnnlSpaceBatchParam_s {
  unsigned int dilation_height; /*!< The dilation in the height dimension. */
  unsigned int dilation_width;  /*!< The dilation in the width dimension. */
} cnnlSpaceBatchParam_t;

/*! The parameters of the ::cnnlSpace2batchNd or ::cnnlBatch2spaceNd operation
 * that hold space to batch or batch to space information including the \p block
 * array and the \p pad array.
 *
 * The ::cnnlSpace2batchNd and ::cnnlBatch2spaceNd function pairs need the
 * parameter \p block, this can get the dilation value of every dimension. The function
 * pairs can add or cut the pads in some dimensions. So, they need the
 * parameter \p pad that can get the pad value of every dimension.
 *
 * You can allocate the memory at static data area or at stack. If you allocate
 * it at static data area, it will occupy the memory at whole life cycle of process.
 * If you allocate memory as a local variable, the memory will be allocated and
 * freed automatically. You can also allocate the memory dynamically using
 * ``malloc()`` function and free the memory using ``free()`` function.
 *
 * You can set value for this structure fields using "=" operator.
 *
 * It is deprecated and will be removed in future release.
 *   Use ::cnnlSpaceBatchNdDescriptor_t instead.
 */
typedef struct cnnlSpaceBatchNdParam_s {
  uint32_t *block;
  /*!< The \p block parameter is an array. The element of the \p block array means
   * the dilation of the input tensor dimension. For example, if the input
   * layout is NHWC, the shape is [3, 4, 4, 1], then, the \p block_num parameter
   * may be 2, the \p block shape is [2, 2], this means the dilation of H and W is
   * 2.
   */
  uint32_t block_num;
  /*!< The number of elements in the \p block array. */
  uint32_t *pad;
  /*!< An array that stores the pre-pad and post-pad of
   * every dimension. For example, if you add pad on the H and W dimensions,
   * before H dimension and after H dimension add 1 pad, before W dimension and
   * after W dimension add 1 pad, then the \p pad_num parameter should be set
   * to 4, the pad shape should be set to [1, 1, 1, 1].
   */
  uint32_t pad_num;
  /*!< The number of elements in the \p pad array. */
} cnnlSpaceBatchNdParam_t;

// Group:Space2batch
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra
 * workspace to run the ::cnnlSpace2batch operation.
 *
 * The \p size of the extra workspace is based on the given information of the
 * ::cnnlSpace2batch operation, according to algorithm and input dimension. The
 * parameter \p size is the output parameter, when the function returns
 * ::CNNL_STATUS_SUCCESS, also, the value of \p size is the memory size you
 * should allocate and pass to the ::cnnlSpace2batch operation function.
 *
 * If the operation does not need the extra memory, the \p size value will
 * return zero. If the \p size parameter returns zero, you do not need to call
 * the memory allocation function for the operation workspace any more.
 *
 * This function supports 4D tensor only. Usually it uses in the
 * convolutional artificial intelligence network. For more information about the workspace,
 * see "Cambricon CNNL User Guide".
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  input_desc
 *   Input. The descriptor of the input tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  output_desc
 *   Input. The descriptor of the output tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out]  size
 *   Output. A host pointer to the returned size of the extra workspace
 *   in bytes that is used in the ::cnnlSpace2batch operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_INTERNAL_ERROR.
 *
 * @par Data Type
 * - This function supports the following data types for the input and output tensors.
 *   And the data type must be the same between input tensor and output tensor.
 *   - input tensor: bool, uint8, int8, uint16, int16, uint32, int32, uint64, int64,
 *   half, float, double(only supported in NHWC layout).
 *   - output tensor: bool, uint8, int8, uint16, int16, uint32, int32, uint64, int64,
 *   half, float, double(only supported in NHWC layout).
 *
 * @par Data Layout
 * - The supported data layouts of the input tensor and output tensor are as
 *   follows:
 *   Note that this function supports any combinations of the layout.
 *   - input tensor: \p CNNL_LAYOUT_NHWC and \p CNNL_LAYOUT_NCHW and \p CNNL_LAYOUT_HWCN.
 *   - output tensor: \p CNNL_LAYOUT_NHWC and \p CNNL_LAYOUT_NCHW and \p CNNL_LAYOUT_HWCN.
 *
 * @par Scale Limitation
 * - The total number of the input tensor dimensions should be equal to the output's.
 * - The parameter \p size is a pointer and should not be NULL.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetSpace2batchWorkspaceSize(cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const cnnlTensorDescriptor_t output_desc,
                                     size_t *size);
// Group:Space2batch
/*!
 * @brief Changes tensor shape from space to batch. It is usually used in
 * TensorFlow framework.
 *
 * Space to batch makes the convolution operation with dilation faster
 * and easier. The idea is, reform the NHWC block to another shape. Usually,
 * it uses in the artificial intelligence, and this function supports
 * 4D tensor only. Take the following diagram for example:
 *
   @verbatim
               # + # + # +       # # #
               x o x o x o       # # #    batch 0_0
    batch 0    # + # + # +  -->  # # #
               x o x o x o       -----
               # + # + # +       + + +
               x o x o x o       + + +    batch 0_1
                                 + + +
                                 -----
                                 x x x
                                 x x x    batch 0_2
                                 x x x
                                 -----
                                 o o o
                                 o o o    batch 0_3
                                 o o o
   @endverbatim
 *
 * As shown in the schematic diagram, each batch will be extended into several
 * small batches, the total number of pixels will not be changed. In this way,
 * in each small batch, the convolution operation can be operated without
 * dilation. Generally this will accelerate the convolution calculation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] param
 *   Input. The dilation value, including the height dimension and the width
 *   dimension of the input tensor. For detailed information,
 *   see ::cnnlSpaceBatchParam_t.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for
 *   the operation. For more information about workspace, see "Cambricon CNNL
 *   User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the operation. You can get the size of the workspace with the
 *   ::cnnlGetSpace2batchWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_INTERNAL_ERROR, ::CNNL_STATUS_EXECUTION_FAILED.
 *
 * @par Formula
 * - See "Space2batch_batch2space Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for the input and output tensors.
 *   And the data type must be the same between input tensor and output tensor.
 *   - input tensor: bool, uint8, int8, uint16, int16, uint32, int32, uint64, int64,
 *   half, float, double(only supported in NHWC layout).
 *   - output tensor: bool, uint8, int8, uint16, int16, uint32, int32, uint64, int64,
 *   half, float, double(only supported in NHWC layout).
 *
 *
 * @par Data Layout
 * - The supported data layouts of the input tensor and output tensor are as
 *   follows:
 *   Note that this function supports any combinations of the layout.
 *   - input tensor: \p CNNL_LAYOUT_NHWC and \p CNNL_LAYOUT_NCHW and \p CNNL_LAYOUT_HWCN.
 *   - output tensor: \p CNNL_LAYOUT_NHWC and \p CNNL_LAYOUT_NCHW and \p CNNL_LAYOUT_HWCN.
 *
 * @par Scale Limitation
 *  - \p param.dilation_width > 0
 *  - \p param.dilation_height > 0
 *  - \p ni * \p hi * \p wi * \p ci < 0xffffffff
 *  - \p ni * \p hi * \p wi * \p ci == \p no * \p ho * \p wo * \p co
 *  - \p no == \p ni * \p param.dilation_height * \p param.dilation_width
 *  - \p hi == \p ho * \p param.dilation_height
 *  - \p wi == \p wo * \p param.dilation_width
 *  - \p ci == \p co
 *  Where \p ni represents the input number of N-dimension, \p hi represents
 *  the input number of the height dimension, \p wi represents the input number
 *  of the width dimension, \p ci represents the input number of the channel
 *  dimension, \p no represents the output number of the N-dimension, \p ho
 *  represents the output number of the height dimension, \p wo represents the
 *  output number of the width dimension, \p co represents the output number of
 *  the channel dimension.About param.dilation_height and param.dilation_width,
 *  see ::cnnlSpaceBatchParam_t.
 *
 * @par API Dependency
 * - Before you call this function, you can first call the
 *   ::cnnlGetSpace2batchWorkspaceSize function to get the size of the memory
 *   workspace in MLU device that the function uses.
 * - You need to call the ::cnnlBatch2space function to reform the shape to the
 *   original shape after the convolution operation.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the layout of the input
 *   tensor and output tensor to NHWC.
 * - Also, the input channel and the output channel dimension should not be too
 *   small. For example, if the channel dimension is less than 256, the I/O
 *   performance may be very low. It is recommended that the size of channel
 *   dimension is the times of 64 and greater than 256.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/space_to_batch
 */
cnnlStatus_t CNNL_WIN_API cnnlSpace2batch(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t input_desc,
                                      const void *input,
                                      const cnnlTensorDescriptor_t output_desc,
                                      void *output,
                                      const cnnlSpaceBatchParam_t param,
                                      void *workspace,
                                      size_t workspace_size);
// Group:Space2batchNd
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used
 * as an extra workspace to run the ::cnnlSpace2batchNd operation.
 *
 * The size of the extra workspace is based on the given information of the
 * ::cnnlSpace2batchNd operation, according to algorithm and input dimension.
 * The parameter \p workspace_size is the output parameter, when the function
 * return ::CNNL_STATUS_SUCCESS, also, the value of \p workspace_size is the
 * memory size you should allocate and pass to the ::cnnlSpace2batchNd
 * operation function.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGetSpace2batchNdExtraInputSize instead.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  input_desc
 *   Input. The descriptor of the input tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  param
 *   Input. Descriptor of parameter. For detailed information,
 *   see ::cnnlSpaceBatchNdParam_t.
 * @param[out]  workspace_size
 *   Output. A host pointer to the returned size of the extra workspace
 *   in bytes that is used in the ::cnnlSpace2batchNd operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED.
 *
 * @par Scale Limitation
 * - The parameter \p workspace_size is a pointer and should not be NULL.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlGetSpace2batchNdExtraInputSize)
cnnlStatus_t CNNL_WIN_API cnnlGetSpace2batchNdWorkspaceSize(cnnlHandle_t handle,
                                        const cnnlTensorDescriptor_t input_desc,
                                        const cnnlSpaceBatchNdParam_t *param,
                                        size_t *workspace_size);
// Group:Space2batchNd
/*!
 * @brief Changes N-Dimensional tensor shape from space to batch. It is usually
 * used in TensorFlow framework, and in the artificial intelligence.
 *
 * Space to batch makes the convolution operation with dilation faster
 * and easier. Take the following diagram for example:
 *
   @verbatim
               # + # + # +       # # #
               x o x o x o       # # #    batch 0_0
    batch 0    # + # + # +  -->  # # #
               x o x o x o       -----
               # + # + # +       + + +
               x o x o x o       + + +    batch 0_1
                                 + + +
                                 -----
                                 x x x
                                 x x x    batch 0_2
                                 x x x
                                 -----
                                 o o o
                                 o o o    batch 0_3
                                 o o o
   @endverbatim
 *
 * As shown in the schematic diagram, each batch will be extended into several
 * small batches. In this way, in each small batch, the convolution operation
 * can be operated without dilation. Generally this will accelerate the
 * convolution calculation.
 *
 * Compared with the ::cnnlSpace2batch function, this function supports more
 * dimensional tensor than the ::cnnlSpace2batch function. The ::cnnlSpace2batch
 * function only supports 4D tensor, ::cnnlSpace2batchNd function supports
 * a larger dimensional tensor(the maximum dimension value is 8). But the tensor layout
 * parameter is not used in ::cnnlSpace2batchNd function. It will not change
 * the layout of the input tensor, so you must prepare appropriate data layout
 * for the function before you call it.
 *
 * In addition, this function supports adding pad in every dimension before
 * implementing the space to batch operation. This can fuse pad operation
 * and increase the performance.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlSpace2batchNd_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] param
 *   Input. The block and pad value. For detailed information,
 *   see ::cnnlSpaceBatchNdParam_t.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for
 *   the operation. For more information about workspace, see "Cambricon CNNL
 *   User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the operation. You can get the size of the workspace with the
 *   ::cnnlGetSpace2batchNdWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_ALLOC_FAILED, ::CNNL_STATUS_EXECUTION_FAILED.
 *
 * @par Formula
 * - See "Space2batch_batch2space Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for the input and output tensors.
 *   And the data type must be the same between input tensor and output tensor.
 *   - input tensor: bool, uint8, int8, uint16, int16, uint32, int32, uint64, int64,
 *   half, float, double.
 *   - output tensor: bool, uint8, int8, uint16, int16, uint32, int32, uint64, int64,
 *   half, float, double.
 *
 * @par Scale Limitation
 * - The "spatial" dimensions `[1, ..., M]` of input and block and pad parameters must meet
 *   the following requirements:
 *   - \p input_desc->dims[i] + \p param->pad[2*(i-1)] + \p param->pad[2*(i-1)+1]
 *     == \p output->dims[i] * \p param->block[i-1].
 * - \p input_desc->dim  = \p output_desc->dim.
 * - \p input_desc->dim -1 = \p param->block_num.
 * - \p param->pad_num = 2* \p param->block_num.
 * - The value range of \p param->block_num is [1, 4].
 * - All elements of the parameter \p param->block should be greater than 0.
 * - The element number of the input and output tensors should be less than \f$2^{32}\f$.
 *
 * @par API Dependency
 * - Before you call this function, you can first call the
 *   ::cnnlGetSpace2batchNdWorkspaceSize function to get the size of the memory
 *   workspace in MLU device that the function uses.
 * - You need to call the ::cnnlBatch2spaceNd function to reform the shape to the
 *   original shape after the convolution operation.
 *
 * @par Performance Optimization
 * - The input channel and the output channel dimension should not be too
 *   small. For example, if the channel dimension is less than 256, the I/O
 *   performance may be very low. It is recommended that the size of channel
 *   dimension is the times of 64 and greater than 256.
 *
 * @note
 * - The input tensor \p input and output tensor \p output are multi-dimensional arrays, supporting
 *   up to \p CNNL_DIM_MAX dimensions.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/space_to_batch_nd
 */
CNNL_DEPRECATED_FOR(cnnlSpace2batchNd_v2)
cnnlStatus_t CNNL_WIN_API cnnlSpace2batchNd(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t input_desc,
                                       const void *input,
                                       const cnnlTensorDescriptor_t output_desc,
                                       void *output,
                                       const cnnlSpaceBatchNdParam_t *param,
                                       void *workspace,
                                       size_t workspace_size);

// Group:Batch2space
/*!
 * @brief Returns in \p size the size of the MLU memory that is used
 * as an extra workspace to run the ::cnnlBatch2space operation.
 *
 * The size of the extra workspace is based on the given information of the
 * ::cnnlBatch2space operation, according to algorithm and input dimension.
 * The parameter \p size is the output parameter, when the function return
 * ::CNNL_STATUS_SUCCESS, also, the value of \p size is the memory size you
 * should allocate and pass to the ::cnnlBatch2space operation function.
 *
 * If the operation does not need the extra memory, the \p size value will
 * return zero. If the \p size parameter returns zero, you do not need to call
 * the memory allocation function for the operation workspace any more.
 *
 * Usually this function uses in the artificial intelligence, and this
 * function supports 4D tensor only.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  input_desc
 *   Input. The descriptor of the input tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  output_desc
 *   Input. The descriptor of the output tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out]  size
 *   Output. A host pointer to the returned size of the extra workspace
 *   in bytes that is used in the ::cnnlBatch2space operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_INTERNAL_ERROR.
 *
 * @par Data Type
 * - This function supports the following data types for the input and output tensors.
 *   And the data type must be the same between input tensor and output tensor.
 *   - input tensor: bool, uint8, int8, uint16, int16, uint32, int32, uint64, int64,
 *   half, float, double(only supported in NHWC layout).
 *   - output tensor: bool, uint8, int8, uint16, int16, uint32, int32, uint64, int64,
 *   half, float, double(only supported in NHWC layout).
 *
 * @par Data Layout
 * - The supported data layouts of the input tensor and output tensor are as
 *   follows:
 *   Note that this function supports any combinations of the layout.
 *   - input tensor: \p CNNL_LAYOUT_NHWC and \p CNNL_LAYOUT_NCHW and \p CNNL_LAYOUT_HWCN.
 *   - output tensor: \p CNNL_LAYOUT_NHWC and \p CNNL_LAYOUT_NCHW and \p CNNL_LAYOUT_HWCN.
 *
 * @par Scale Limitation
 * - The dimension size of the input tensor should be equal to that of the output tensor.
 * - The parameter \p size is a pointer and should not be NULL.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetBatch2spaceWorkspaceSize(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t input_desc,
                                       const cnnlTensorDescriptor_t output_desc,
                                       size_t *size);

// Group:Batch2space
/*!
 * @brief Changes tensor shape from batch to space. It is usually used in
 * TensorFlow framework.
 *
 * Batch to space is the reverse operation of the space to batch. The idea is,
 * reform the changed block back to the original shape. Usually it uses in the
 * artificial intelligence, and this function supports 4D tensor
 * only. Take the following diagram for example:
 *
   @verbatim
               # + # + # +       # # #
               x o x o x o       # # #    batch 0_0
    batch 0    # + # + # +  <--  # # #
               x o x o x o       -----
               # + # + # +       + + +
               x o x o x o       + + +    batch 0_1
                                 + + +
                                 -----
                                 x x x
                                 x x x    batch 0_2
                                 x x x
                                 -----
                                 o o o
                                 o o o    batch 0_3
                                 o o o
   @endverbatim
 *
 * After the convolution operation, the operation named ::cnnlBatch2space
 * should be called to reform the shape to the original shape.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] param
 *   Input. The dilation value, including the height dimension and the width
 *   dimension of the input tensor. For detailed information,
 *   see ::cnnlSpaceBatchParam_t.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for
 *   the operation. For more information about workspace, see "Cambricon CNNL
 *   User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the operation. You can get the size of the workspace with the
 *   ::cnnlGetBatch2spaceWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_EXECUTION_FAILED, ::CNNL_STATUS_INTERNAL_ERROR.
 *
 * @par Formula
 * - See "Space2batch_batch2space Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for the input and output tensors.
 *   And the data type must be the same between input tensor and output tensor.
 *   - input tensor: bool, uint8, int8, uint16, int16, uint32, int32, uint64, int64,
 *   half, float, double(only supported in NHWC layout).
 *   - output tensor: bool, uint8, int8, uint16, int16, uint32, int32, uint64, int64,
 *   half, float, double(only supported in NHWC layout).
 *
 * @par Data Layout
 * - This function supports the following data layout of the input
 *   and output tensors:
 *   - input tensor: \p CNNL_LAYOUT_NHWC and \p CNNL_LAYOUT_NCHW and \p CNNL_LAYOUT_HWCN.
 *   - output tensor: \p CNNL_LAYOUT_NHWC and \p CNNL_LAYOUT_NCHW and \p CNNL_LAYOUT_HWCN.
 *
 * @par Scale Limitation
 * - \p param.dilation_width > 0.
 * - \p param.dilation_height > 0.
 * - \p ni * \p hi * \p wi * \p ci < 0xffffffff.
 *   \p ni represents the input number of N-dimension, \p hi represents
 *   the input number of the height dimension, \p wi represents the input number
 *   of the width dimension, and \p ci represents the input number of the channel
 *   dimension.
 * - \p ni * \p hi * \p wi * \p ci == \p no * \p ho * \p wo * \p co.
 * - \p no == \p ni / (\p param.dilation_height * \p param.dilation_width).
 *   \p no represents the output number of the N-dimension.
 * - \p ho == \p hi * \p param.dilation_height. \p ho represents the output number of the height dimension.
 * - \p wo == \p wi * \p param.dilation_width. \p wo represents the output number of the width dimension.
 * - \p ci == \p co. \p co represents the output number of the channel dimension
 *
 * @par API Dependency
 * - Before you call this function, you can first call the
 *   ::cnnlGetBatch2spaceWorkspaceSize function to get the size of the memory
 *   workspace in MLU device that the function uses.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the layout of the input
 *   tensor and output tensor to NHWC.
 * - The input channel and the output channel dimension should not be too
 *   small. For example, if the channel dimension is less than 256, the I/O
 *   performance may be very low. It is recommended that the size of channel
 *   dimension is the times of 64 and greater than 256.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/batch-to-space-n-d
 */
cnnlStatus_t CNNL_WIN_API cnnlBatch2space(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t input_desc,
                                      const void *input,
                                      const cnnlTensorDescriptor_t output_desc,
                                      void *output,
                                      const cnnlSpaceBatchParam_t param,
                                      void *workspace,
                                      size_t workspace_size);

// Group:Batch2spaceNd
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used
 * as an extra workspace to run the ::cnnlBatch2spaceNd operation.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGetBatch2spaceNdExtraInputSize instead.
 *
 * The size of the extra workspace is based on the given information of the
 * ::cnnlBatch2spaceNd operation, according to algorithm and input dimension.
 * The parameter \p workspace_size is the output parameter, when the function
 * return ::CNNL_STATUS_SUCCESS, also, the value of \p workspace_size is the
 * memory size you should allocate and pass to the ::cnnlBatch2spaceNd
 * operation function.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  input_desc
 *   Input. The descriptor of the input tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  param
 *   Input. The block and pad value. For detailed information,
 *   see ::cnnlSpaceBatchNdParam_t.
 * @param[out]  workspace_size
 *   Output. A host pointer to the returned size of the extra workspace
 *   in bytes that is used in the ::cnnlBatch2spaceNd operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_INTERNAL_ERROR.
 *
 * @par Scale Limitation
 * - The parameter \p workspace_size is a pointer and should not be NULL.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlGetBatch2spaceNdExtraInputSize)
cnnlStatus_t CNNL_WIN_API cnnlGetBatch2spaceNdWorkspaceSize(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t input_desc,
                                      const cnnlSpaceBatchNdParam_t *param,
                                      size_t *workspace_size);
// Group:Batch2spaceNd
/*!
 * @brief Changes N-Dimensional tensor shape from batch to space.
 * It is usually used in TensorFlow framework, and in the artificial intelligence.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlBatch2spaceNd_v2 instead.
 *
 * It is the reverse operation of the ::cnnlSpace2batchNd operation. The idea
 * is, reform the changed block back to the original shape. Take the following
 * diagram for example:
 *
   @verbatim
               # + # + # +       # # #
               x o x o x o       # # #    batch 0_0
    batch 0    # + # + # +  <--  # # #
               x o x o x o       -----
               # + # + # +       + + +
               x o x o x o       + + +    batch 0_1
                                 + + +
                                 -----
                                 x x x
                                 x x x    batch 0_2
                                 x x x
                                 -----
                                 o o o
                                 o o o    batch 0_3
                                 o o o
   @endverbatim
 *
 * After the convolution operation, the operation named ::cnnlBatch2spaceNd
 * should be called to reform the shape to the original shape.
 *
 * Compared with the ::cnnlBatch2space function, this function supports more
 * dimensions than the ::cnnlBatch2space function.
 * The ::cnnlBatch2space function only supports 4D tensor,
 * ::cnnlBatch2spaceNd function supports a larger dimensional tensor(the maximum
 * dimension value is 8). But the tensor layout parameter is not used in
 * ::cnnlBatch2spaceNd function. It will not change the layout of the input
 * tensor, so you must prepare appropriate data layout for the function before
 * you call it.
 *
 * In addition, this function supports cutting pad in every dimension after
 * implementing the batch to space N-Dimensional operation.
 * This can fuse pad operation and increase the performance.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] param
 *   Input. The block and pad value. For detailed information,
 *   see ::cnnlSpaceBatchNdParam_t.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for
 *   the operation. For more information about workspace, see "Cambricon CNNL
 *   User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the operation. You can get the size of the workspace with the
 *   ::cnnlGetBatch2spaceNdWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_ALLOC_FAILED, ::CNNL_STATUS_EXECUTION_FAILED.
 *
 * @par Formula
 * - See "Space2batch_batch2space Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for the input and output tensors.
 *   And the data type must be the same between input tensor and output tensor.
 *   - input tensor: bool, uint8, int8, uint16, int16, uint32, int32, uint64, int64,
 *   half, float, double.
 *   - output tensor: bool, uint8, int8, uint16, int16, uint32, int32, uint64, int64,
 *   half, float, double.
 *
 * @par Scale Limitation
 * - The "spatial" dimensions `[1, ..., M]` of input and block and pad parameters must meet
 *   the following requirements:
 *   - \p input->dims[i] * \p param->block[i-1] == \p output_desc->dims[i]
 *      + \p param->pad[2*(i-1)] + \p param->pad[2*(i-1)+1].
 * - \p input_desc->dim  = \p output_desc->dim.
 * - \p input_desc->dim -1 = \p param->block_num.
 * - \p param->pad_num = 2* \p param->block_num.
 * - The value range of \p param->block_num is [1, 4].
 * - All elements of the parameter \p param->block should be greater than 0.
 * - The element number of the input and output tensors should be less than \f$2^{32}\f$.
 *
 * @par API Dependency
 * - Before you call this function, you can first call the
 *   ::cnnlGetBatch2spaceNdWorkspaceSize function to get the size of the memory
 *   workspace in MLU device that the function uses.
 *
 * @par Performance Optimization
 * - The input channel and the output channel dimension should not be too
 *   small. For example, if the channel dimension is less than 256, the I/O
 *   performance may be very low. It is recommended that the size of channel
 *   dimension is the times of 64 and greater than 256.
 *
 * @note
 * - The input tensor \p input and output tensor \p output are multi-dimensional arrays, supporting
 *   up to \p CNNL_DIM_MAX dimensions.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/batch_to_space_nd
 */
CNNL_DEPRECATED_FOR(cnnlBatch2spaceNd_v2)
cnnlStatus_t CNNL_WIN_API cnnlBatch2spaceNd(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t input_desc,
                                      const void *input,
                                      const cnnlTensorDescriptor_t output_desc,
                                      void *output,
                                      const cnnlSpaceBatchNdParam_t *param,
                                      void *workspace,
                                      size_t workspace_size);

// Group:Batch2spaceNd
/*!
 * @brief Initializes the ::cnnlSpace2batchNd_v2 or ::cnnlBatch2spaceNd_v2
 * operation descriptor \p param that was previously created with the
 * ::cnnlCreateSpaceBatchNdDescriptor function, and sets the information about
 * the ::cnnlSpace2batchNd_v2 or ::cnnlBatch2spaceNd_v2 operation to the descriptor
 * \p param. The information includes the block array \p block, the number of
 * the block dimensions \p block_num, the padding size for each dimension
 * \p pad, the number of pad dimensions \p pad_num.
 *
 * @param[in,out] param
 *   Input/output. The descriptor of the ::cnnlSpace2batchNd_v2 or ::cnnlBatch2spaceNd_v2
 *   operation. For detailed information, see ::cnnlSpaceBatchNdDescriptor_t.
 * @param[in] block
 *   Input. An array that stores the dilation of the input tensor dimension.
 * @param[in] block_num
 *   Input. The number of elements in the \p block array.
 *   Currently, the value of this parameter can only be in range of [1, 8].
 * @param[in] pad
 *   Input. An array that stores the zero-padding size for each dimension of
 *   the input tensor used in the operation. For each dimension, the padding
 *   size represents the number of zeros to be concatenated at the start and end
 *   of that dimension. If the tensor dimension number is set to 4,
 *   the padding is on top, bottom, left, and right. If \p dimension number is
 *   set to 5, the padding is on front, back, top, bottom, left, and right.
 *   The value of this parameter should be greater than or equal to 0.
 * @param[in] pad_num
 *   Input. The number of elements in the \p pad array. Currently, the value of
 *   this parameter should be set to an even value, and the value is greater or
 *   equal to 2. Usually it is set two times larger than \p block_num.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - See ::cnnlSpace2batchNd_v2
 */
cnnlStatus_t CNNL_WIN_API cnnlSetSpaceBatchNdDescriptor(
                                     cnnlSpaceBatchNdDescriptor_t param,
                                     const uint32_t block[],
                                     uint32_t block_num,
                                     const uint32_t pad[],
                                     uint32_t pad_num);

// Group:Batch2spaceNd
/*!
 * @brief Creates a descriptor pointed by \p param for
 * ::cnnlSpace2batchNd_v2 or ::cnnlBatch2spaceNd_v2 operation, and allocates memory
 * for holding the information about the operation. The information is defined
 * in ::cnnlSpaceBatchNdDescriptor_t. For more information about descriptor,
 * see "Cambricon CNNL User Guide".
 *
 * @param[out] param
 *   Output. A host pointer to the ::cnnlSpace2batchNd_v2 or ::cnnlBatch2spaceNd_v2
 *   descriptor that holds information about the operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the
 *   ::cnnlSetSpaceBatchNdDescriptor function to initialize and set the
 *   information to the descriptor.
 * - You need to call the ::cnnlDestroySpaceBatchNdDescriptor function to
 *   destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - See ::cnnlSpace2batchNd_v2
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateSpaceBatchNdDescriptor(
                                     cnnlSpaceBatchNdDescriptor_t *param);

// Group:Batch2spaceNd
/*!
 * @brief Destroys a ::cnnlSpace2batchNd_v2 or ::cnnlBatch2spaceNd_v2 descriptor
 * \p param that was previously created with the
 * ::cnnlCreateSpaceBatchNdDescriptor function.
 *
 * The descriptor is defined in ::cnnlSpaceBatchNdDescriptor_t and holds the
 * information about the ::cnnlSpace2batchNd_v2 or ::cnnlBatch2spaceNd_v2
 * operation.
 *
 * @param[in] param
 *   Input. The ::cnnlSpace2batchNd_v2 or ::cnnlBatch2spaceNd_v2 descriptor to be
 *   destroyed.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - This function should be called to destroy the descriptor.
 *   Otherwise, the memory leak may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - See ::cnnlSpace2batchNd_v2
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroySpaceBatchNdDescriptor(
                                     cnnlSpaceBatchNdDescriptor_t param);

// Group:Space2batchNd
/*!
 * @brief Returns in \p extra_input_size the size of the MLU memory and host
 * memory that is used as an extra input data for the ::cnnlSpace2batchNd_v2
 * operation. You need to allocate memory both on host and MLU based on
 * the size returned in \p extra_size.
 *
 * The size of extra input data is based on the given information of the
 * ::cnnlSpace2batchNd_v2 operation, according to algorithm and input dimension.
 * The parameter \p extra_input_size is the output parameter, if the function
 * returns ::CNNL_STATUS_SUCCESS. The value of \p extra_input_size is the
 * memory size you should allocate in host. You can allocate the memory
 * dynamically using ``malloc()`` function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  input_desc
 *   Input. The descriptor of the input tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  param
 *   Input. Descriptor of parameter. For detailed information,
 *   see ::cnnlSpaceBatchNdDescriptor_t.
 * @param[out]  extra_input_size
 *   Output. A host pointer to the returned size of the extra input data
 *   in bytes that is used in the ::cnnlSpace2batchNd_v2 operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED.
 *
 * @par Scale Limitation
 * - The parameter \p extra_input_size is a pointer and should not be NULL.
 *
 * @par API Dependency
 * - After calling this function, you can allocate the right size host memory
 *   and call the ::cnnlInitSpace2batchNdExtraInput function to initialize the
 *   memory.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - See ::cnnlSpace2batchNd_v2
 */
cnnlStatus_t CNNL_WIN_API cnnlGetSpace2batchNdExtraInputSize(
                                     cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const cnnlSpaceBatchNdDescriptor_t param,
                                     size_t *extra_input_size);

// Group:Space2batchNd
/*!
 * @brief Initializes the extra input data space \p extra_host_input on host.
 *
 * When the ::cnnlSpace2batchNd_v2 operation runs, it needs a memory buffer on
 * MLU device, this buffer must be initialized before uses. So, you must give
 * the same size buffer in the host and initialize the memory first. This
 * function initializes the host memory buffer.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  input_desc
 *   Input. The descriptor of the input tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  param
 *   Input. Descriptor of parameter. For detailed information,
 *   see ::cnnlSpaceBatchNdDescriptor_t.
 * @param[out]  extra_host_input
 *   Output. Pointer to the host memory that is used as an extra input space
 *   for the ::cnnlSpace2batchNd_v2 operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED.
 *
 * @par Scale Limitation
 * - The parameter \p extra_host_input is a pointer and should not be NULL.
 *
 * @par API Dependency
 * - Before you call this function, you should call the
 *   ::cnnlGetSpace2batchNdExtraInputSize function first to get the size of the
 *   host memory that the operation needs.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - See ::cnnlSpace2batchNd_v2
 */
cnnlStatus_t CNNL_WIN_API cnnlInitSpace2batchNdExtraInput(
                                     cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const cnnlSpaceBatchNdDescriptor_t param,
                                     void *extra_host_input);

// Group:Space2batchNd
/*!
 * @brief Changes N-Dimensional tensor shape from space to batch. It is usually
 * used in TensorFlow framework, and in the artificial intelligence.
 *
 * Space to batch makes the convolution operation with dilation faster
 * and easier. Take the following diagram for example:
 *
   @verbatim
               # + # + # +       # # #
               x o x o x o       # # #    batch 0_0
    batch 0    # + # + # +  -->  # # #
               x o x o x o       -----
               # + # + # +       + + +
               x o x o x o       + + +    batch 0_1
                                 + + +
                                 -----
                                 x x x
                                 x x x    batch 0_2
                                 x x x
                                 -----
                                 o o o
                                 o o o    batch 0_3
                                 o o o
   @endverbatim
 *
 * As shown in the schematic diagram, each batch will be extended into several
 * small batches. In this way, in each small batch, the convolution operation
 * can be operated without dilation. Generally this will accelerate the
 * convolution calculation.
 *
 * Compared with the ::cnnlSpace2batch function, this function supports more
 * dimensions for input tensor than the ::cnnlSpace2batch function's. The
 * ::cnnlSpace2batch function only supports 4D tensor,
 * ::cnnlSpace2batchNd function supports a larger dimensional tensor (the
 * maximum dimension value is 8). But the tensor layout parameter is not used
 * in ::cnnlSpace2batchNd function. It will not change the layout of the input
 * tensor, so you must prepare appropriate data layout for the function before
 * you call it.
 *
 * In addition, this function supports adding pad in every dimension before
 * implementing the space to batch operation. This can fuse pad operation
 * and increase the performance.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] param
 *   Input. The block and pad value. For detailed information,
 *   see ::cnnlSpaceBatchNdDescriptor_t.
 * @param[in] extra_device_input
 *   Input. Pointer to the MLU memory that stores the extra input data.
 *   You need to copy the extra input data to MLU from the host that is
 *   initialized with ::cnnlInitSpace2batchNdExtraInput. For more information
 *   about extra input data, see Cambricon CNNL User Guide.
 * @param[in] extra_input_size
 *   Input. The size of the extra input in bytes that needs to be used in
 *   the operation. You can get the size of the extra input with the
 *   ::cnnlGetSpace2batchNdExtraInputSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_EXECUTION_FAILED.
 *
 * @par Formula
 * - See "Space2batch_batch2space Operator" section in "Cambricon CNNL User
 *   Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for the input and output tensors.
 *   And the data type must be the same between input tensor and output tensor.
 *   - input tensor: bool, uint8, int8, uint16, int16, uint32, int32, uint64, int64,
 *   half, float, double.
 *   - output tensor: bool, uint8, int8, uint16, int16, uint32, int32, uint64, int64,
 *   half, float, double.
 *
 * @par Scale Limitation
 * - The "spatial" dimensions `[1, ..., M]` of input and block and pad parameters must meet
 *   the following requirements:
 *   - \p input_desc->dims[i] + \p param->pad[2*(i-1)] + \p param->pad[2*(i-1)+1]
 *     == \p output->dims[i] * \p param->block[i-1].
 * - \p input_desc->dim  = \p output_desc->dim.
 * - \p input_desc->dim -1 = \p param->block_num.
 * - \p param->pad_num = 2* \p param->block_num.
 * - The value range of \p param->block_num is [1, 4].
 * - All elements of the parameter \p param->block should be greater than 0.
 * - The element number of the input and output tensors should be less than \f$2^{32}\f$.
 *
 * @note
 * - The input tensor \p input are multi-dimensional arrays, supporting
 *   up to \p CNNL_DIM_MAX dimensions.
 *
 * @par API Dependency
 * - Before you call this function, you can first call the
 *   ::cnnlGetSpace2batchNdExtraInputSize function to get the size of the memory
 *   in MLU device and host that the function uses.
 * - You need to call the ::cnnlBatch2spaceNd_v2 function to reform the shape to
 *   the original shape after the convolution operation.
 *
 * @par Performance Optimization
 * - The input channel and the output channel dimension should not be too
 *   small. For example, if the channel dimension is less than 256, the I/O
 *   performance may be very low. It is recommended that the size of channel
 *   dimension is the times of 64 and greater than 256.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - Gives a simple example for this function, note:
 *   - If gives "fake" comment at the line tail, means it is the fake code.
 *   - For more easier to understand, the following code does not check the
 *     return value of the function, check the return value in the actual
 *     code.
 *   - The usage of the batch to space N-dimension operation is similar to the
 *     space to batch N-dimension operation, so only gives the usage of space to
 *     batch N-dimension operation.
   @verbatim
   // prepare the parameter, usually these parameters is given from caller:
   void *input = ...;  // fake
   void *output = ...;  // fake
   cnnlTensorDescriptor_t input_desc = ...;  // fake
   cnnlTensorDescriptor_t output_desc = ...;  // fake

   // create and set op descriptor:
   cnnlSpaceBatchNdDescriptor_t op_desc;
   uint32_t block[] = {2, 2};
   uint32_t block_num = 2;
   uint32_t pad[] = {1, 0, 1, 0};
   uint32_t pad_num = 4;
   cnnlCreateSpaceBatchNdDescriptor(&op_desc);
   cnnlSetSpaceBatchNdDescriptor(op_desc, block, block_num, pad, pad_num);

   // do the space to batch N-dimension process:
   size_t extra_input_size;
   cnnlGetSpace2batchNdExtraInputSize(handle, input_desc, op_desc, &extra_input_size);
   void *extra_host_input = allocate_host_memory(extra_input_size);  // fake
   void *extra_device_input = allocate_device_memory(extra_input_size);  // fake
   cnnlInitSpace2batchNdExtraInput(handle, input_desc, op_desc, extra_host_input);
   copy_from_host_to_device(extra_host_input, extra_device_input);  // fake
   free_the_host_memory(extra_host_input);  // fake
   cnnlSpace2batchNd_v2(handle,
       input_desc, input,
       output_desc, output,
       op_desc,
       extra_device_input,
       extra_input_size);

   // destroy the object which you have created:
   cnnlDestroySpaceBatchNdDescriptor(op_desc);

   // free the device memory when the queue is end:
   free_device_memory(extra_device_input);
   @endverbatim
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/space_to_batch_nd
 */
cnnlStatus_t CNNL_WIN_API cnnlSpace2batchNd_v2(
                                     cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const void *input,
                                     const cnnlTensorDescriptor_t output_desc,
                                     void *output,
                                     const cnnlSpaceBatchNdDescriptor_t param,
                                     const void *extra_device_input,
                                     size_t extra_input_size);

// Group:Batch2spaceNd
/*!
 * @brief Returns in \p extra_input_size the size of the MLU memory and host
 * memory that is used as an extra input data for the ::cnnlBatch2spaceNd_v2
 * operation. You need to allocate memory both on host and MLU based on
 * the size returned in \p extra_size.
 *
 * The size of extra input data is based on the given information of the
 * ::cnnlBatch2spaceNd_v2 operation, according to algorithm and input dimension.
 * The parameter \p extra_input_size is the output parameter, if the function
 * returns ::CNNL_STATUS_SUCCESS. The value of \p extra_input_size is the
 * memory size you should allocate in host.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  input_desc
 *   Input. The descriptor of the input tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  param
 *   Input. Descriptor of parameter. For detailed information,
 *   see ::cnnlSpaceBatchNdDescriptor_t.
 * @param[out]  extra_input_size
 *   Output. A host pointer to the returned size of the extra host input
 *   in bytes that is used in the ::cnnlBatch2spaceNd_v2 operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED.
 *
 * @par Scale Limitation
 * - The parameter \p extra_input_size is a pointer and should not be NULL.
 *
 * @par API Dependency
 * - After calling this function, you can allocate the right size host memory
 *   and call the ::cnnlInitBatch2spaceNdExtraInput function to initialize the
 *   memory.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - See ::cnnlSpace2batchNd_v2
 */
cnnlStatus_t CNNL_WIN_API cnnlGetBatch2spaceNdExtraInputSize(
                                     cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const cnnlSpaceBatchNdDescriptor_t param,
                                     size_t *extra_input_size);

// Group:Batch2spaceNd
/*!
 * @brief Initializes the extra input data space \p extra_host_input on host.
 *
 * When the ::cnnlBatch2spaceNd_v2 operation runs, it needs a memory buffer on
 * MLU device, this buffer must be initialized before uses. So in the host must
 * have the same size buffer and be initialized first. This function initializes
 * the host memory buffer.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  input_desc
 *   Input. The descriptor of the input tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  param
 *   Input. Descriptor of parameter. For detailed information,
 *   see ::cnnlSpaceBatchNdDescriptor_t.
 * @param[out]  extra_host_input
 *   Output. Pointer to the host memory that is used as an extra input space
 *   for the ::cnnlBatch2spaceNd_v2 operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED.
 *
 * @par Scale Limitation
 * - The parameter \p extra_host_input is a pointer and should not be NULL.
 *
 * @par API Dependency
 * - Before you call this function, you should call the
 *   ::cnnlGetBatch2spaceNdExtraInputSize function first to get the size of the
 *   host memory that the operation needs.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - See ::cnnlSpace2batchNd_v2
 */
cnnlStatus_t CNNL_WIN_API cnnlInitBatch2spaceNdExtraInput(
                                     cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const cnnlSpaceBatchNdDescriptor_t param,
                                     void *extra_host_input);

// Group:Batch2spaceNd
/*!
 * @brief Changes N-Dimensional tensor shape from batch to space.
 * It is usually used in TensorFlow framework, and in the artificial intelligence.
 *
 * It is the reverse operation of the ::cnnlSpace2batchNd_v2 operation. The idea
 * is, reform the changed block back to the original shape. Take the following
 * diagram for example:
 *
   @verbatim
               # + # + # +       # # #
               x o x o x o       # # #    batch 0_0
    batch 0    # + # + # +  <--  # # #
               x o x o x o       -----
               # + # + # +       + + +
               x o x o x o       + + +    batch 0_1
                                 + + +
                                 -----
                                 x x x
                                 x x x    batch 0_2
                                 x x x
                                 -----
                                 o o o
                                 o o o    batch 0_3
                                 o o o
   @endverbatim
 *
 * After the convolution operation, the operation named ::cnnlBatch2spaceNd_v2
 * should be called to reform the shape to the original shape.
 *
 * Compared with the ::cnnlBatch2space function, this function supports more
 * dimensions for input tensor than the ::cnnlBatch2space function's.
 * The ::cnnlBatch2space function only supports 4D tensor,
 * ::cnnlBatch2spaceNd_v2 function supports a larger dimensional tensor (the
 * maximum dimension value is 8). But the tensor layout parameter is not used in
 * ::cnnlBatch2spaceNd_v2 function. It will not change the layout of the input
 * tensor, so you must prepare appropriate data layout for the function before
 * you call it.
 *
 * In addition, this function supports cutting pad in every dimension after
 * implementing the batch to space N-Dimensional operation.
 * This can fuse pad operation and increase the performance.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] param
 *   Input. The block and pad value. For detailed information,
 *   see ::cnnlSpaceBatchNdDescriptor_t.
 * @param[in] extra_device_input
 *   Input. Pointer to the MLU memory that stores the extra input data.
 *   You need to copy the extra input data to MLU from the host that is
 *   initialized with ::cnnlInitBatch2spaceNdExtraInput. For more information
 *   about extra input data, see Cambricon CNNL User Guide.
 * @param[in] extra_input_size
 *   Input. The size of the extra input in bytes that needs to be used in
 *   the operation. You can get the size of the extra input with the
 *   ::cnnlGetBatch2spaceNdExtraInputSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED.
 *
 * @par Formula
 * - See "Space2batch_batch2space Operator" section
 *   in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for the input and output tensors.
 *   And the data type must be the same between input tensor and output tensor.
 *   - input tensor: bool, uint8, int8, uint16, int16, uint32, int32, uint64, int64,
 *   half, float, double.
 *   - output tensor: bool, uint8, int8, uint16, int16, uint32, int32, uint64, int64,
 *   half, float, double.
 *
 * @par Scale Limitation
 * - The "spatial" dimensions `[1, ..., M]` of input and block and pad parameters must meet
 *   the following requirements:
 *   - \p input->dims[i] * \p param->block[i-1] == \p output_desc->dims[i]
 *      + \p param->pad[2*(i-1)] + \p param->pad[2*(i-1)+1].
 * - \p input_desc->dim  = \p output_desc->dim.
 * - \p input_desc->dim -1 = \p param->block_num.
 * - \p param->pad_num = 2* \p param->block_num.
 * - The value range of range of \p param->block_num is [1, 4].
 * - All elements of the parameter \p param->block should be greater than 0.
 * - The element number of the input and output tensors should be less than \f$2^{32}\f$.
 *
 * @par API Dependency
 * - Before you call this function, you can first call the
 *   ::cnnlGetBatch2spaceNdExtraInputSize function to get the size of the memory
 *   in MLU device and host that the function uses.
 *
 * @par Performance Optimization
 * - The input channel and the output channel dimension should not be too
 *   small. For example, if the channel dimension is less than 256, the I/O
 *   performance may be very low. It is recommended that the size of channel
 *   dimension is the times of 64 and greater than 256.
 *
 * @note
 * - The input tensor \p input and output tensor \p output are multi-dimensional arrays, supporting
 *   up to \p CNNL_DIM_MAX dimensions.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The usage is similar to space to batch operation,
 *   see ::cnnlSpace2batchNd_v2
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/batch_to_space_nd
 */
cnnlStatus_t CNNL_WIN_API cnnlBatch2spaceNd_v2(
                                     cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const void *input,
                                     const cnnlTensorDescriptor_t output_desc,
                                     void *output,
                                     const cnnlSpaceBatchNdDescriptor_t param,
                                     const void *extra_device_input,
                                     size_t extra_input_size);
// Group:Momentum
/*!
 * @brief Updates \p var according to the momentum scheme.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  var_desc
 *   Input. A descriptor of input tensor \p var. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out]  var
 *  Input and output. Pointer to the MLU memory that stores the \p var tensor to be updated
 *  according to momentum algorithm. This input refers to filter in the artificial intelligence generally.
 * @param[in] accum_desc
 *   Input. A descriptor of \p accum tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out]  accum
 *   Input and output. Pointer to the MLU memory that stores the \p accum tensor. The \p accum
 *   is the impulse, which indicates the accumulation of \p diff.
 * @param[in]  diff_desc
 *   Input. The descriptor of \p diff tensor. For detailed information,
 *  see ::cnnlTensorDescriptor_t.
 * @param[in]  diff
 *  Input. Pointer to the MLU memory that stores the \p diff tensor. The \p diff is the gradient
 *  for updating \p var.
 * @param[in]  lr
 *   Input. Pointer to the MLU memory that stores the learning rate.
 * @param[in]  moment
 *   Input. Pointer to the MLU memory that stores the attenuation of impulse.
 * @param[in]  use_nesterov
 *   Input. Determines whether to use Nesterov momentum to update \p var or not.
 *   Set \p use_nesterov = true if you want to use Nesterov momentum,
 *   otherwise the default momentum is used.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Momentum Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Scale Limitation
 * - The data shape of input tensors and output tensors must be the same.
 * - The number of dimensions of each input tensor should be no more than 8.
 *
 * @par Data Type
 * - Data type of input tensors and output tensors must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensors: half, float.
 *   - output tensors: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensors and output tensors must be \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the momentum operation is as follows:
    @verbatim
     --> var: an array [64, 78, 1024];
     --> accum: an array [64, 78, 1024];
     --> diff: an array [64, 78, 1024];
     --> lr: an array [1];
     --> moment: an array [1];
     --> use_nesterov: false;
     Then we will get the output:
     --> var: an array [64, 78, 1024] same as input;
     --> accum: an array [64, 78, 1024] same as input;
    @endverbatim
 */

cnnlStatus_t CNNL_WIN_API cnnlMomentum(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t var_desc,
                                       void *var,
                                       const cnnlTensorDescriptor_t accum_desc,
                                       void *accum,
                                       const cnnlTensorDescriptor_t diff_desc,
                                       const void *diff,
                                       const void *lr,
                                       const void *moment,
                                       const bool use_nesterov);

// Group:KerasMomentum
/*!
 * @brief Updates \p var using Keras Momentum or Nesterov Momentum algorithm.
 *
 * This function is used to update \p var according to the momentum scheme. Set \p
 * use_nesterov to true if you want to use Nesterov Momentum, otherwise the default
 * Keras Momentum is used.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] var_desc
 *   Input. The descriptor of \p var tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out] var
 *   Input and output. Pointer to the MLU memory that stores the \p var tensor to be updated
 *   according to momentum scheme, which generally refers to filter in the convolutional network.
 * @param[in] accum_desc
 *   Input. The descriptor of \p accum tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out] accum
 *   Input and output. Pointer to the MLU memory that stores the \p accum tensor. The \p accum
 *   is the impulse, which indicates the accumulation of \p diff.
 * @param[in] diff_desc
 *   Input. The descriptor of \p diff tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] diff
 *   Input. Pointer to the MLU memory that stores the \p diff tensor. The \p diff is the gradient
 *   for updating \p var.
 * @param[in] lr
 *   Input. The learning rate.
 * @param[in] momentum
 *   Input. The attenuation of impulse.
 * @param[in] use_nesterov
 *   Input. Determines whether to use Nesterov momentum to update \p var or not.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "KerasMomentum Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Scale Limitation
 * - The data shape of input tensors and output tensors must be the same.
 * - The number of dimensions of each input tensor should be no more than 8.
 *
 * @par Data Type
 * - The data type of input tensors and output tensors must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensors: half, float.
 *   - output tensors: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensors and output tensors must be \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the keras momentum operation is as follows:
     @verbatim
      --> var: an array [64, 78, 1024];
      --> accum: an array [64, 78, 1024];
      --> diff: an array [64, 78, 1024];
      --> lr: an array [1];
      --> momentum: an array [1];
      --> use_nesterov: false;
      Then we will get the output:
      --> var: an array [64, 78, 1024] same as input;
      --> accum: an array [64, 78, 1024] same as input;
     @endverbatim
 */

cnnlStatus_t CNNL_WIN_API cnnlKerasMomentum(cnnlHandle_t handle,
                                            const cnnlTensorDescriptor_t var_desc,
                                            void *var,
                                            const cnnlTensorDescriptor_t accum_desc,
                                            void *accum,
                                            const cnnlTensorDescriptor_t diff_desc,
                                            const void *diff,
                                            const void *lr,
                                            const void *momentum,
                                            bool use_nesterov);

// Group:LayerNormForward
 /*!
 * @brief Computes layer normalization forward over the last certain number of dimensions on input tensors
 *        \p x, \p filter and \p bias, and returns the results in output tensor \p y. Compared with ::cnnlLayerNormForward,
 *        this API provides \p layernorm_desc parameter to control the \p y data type and whether to allow reduced precision computation.
 *
 * This function may need extra MLU memory as the workspace to improve the layer normalization performance.
 * You can get the workspace size with ::cnnlGetLayerNormOpWorkspaceSize.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the layer normalization
 *   forward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] layernorm_desc
 *   Input. The descriptor of the layernorm forward operation. For detailed
 *   information, see ::cnnlNormDescriptor_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] axis
 *    Input. Normalize over the last dimensions from axis to end.
 * @param[in] filter_bias_desc
 *   Input. The descriptor of the input filter and bias tensors. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the filter tensor. Its value can be NULL.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the bias tensor. Its value can be NULL.
 *  @param[in] eps
 *    Input. A float value added to the denominator for numerical stability.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the layer normalization forward
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the layer normalization forward
 *   operation. You can get the size of the workspace with the ::cnnlGetLayerNormOpWorkspaceSize function.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] mean_rstd_desc
 *   Input. The descriptor of the output mean and rstd tensors. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] saved_mean
 *   Output. Pointer to the MLU memory that stores the output mean tensor.
 * @param[out] saved_rstd
 *   Output. Pointer to the MLU memory that stores the inverse of the variance tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "LayerNorm Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are as follows in the order of
 *   x - filter/bias - y - saved_mean/saved_rstd:
 *   - half - half - half - half
 *   - half - half - half - float
 *   - float - float - float - float
 *   - half - float - half - half
 *   - half - float - half - float
 *   - float - half - float - float
 * - When layernorm_desc->output_type_same_as_weight is true, this function additionally
 *   supports the following data type combinations:
 *   - half - float - float - half
 *   - half - float - float - float
 *   - float - half - half - float
 * - On MLU500 series, this function additionally supports the following data type combinations:
 *   - bfloat16 - bfloat16 - bfloat16 - bfloat16
 *   - bfloat16 - bfloat16 - bfloat16 - float
 *   - bfloat16 - half - bfloat16 - bfloat16
 *   - bfloat16 - float - bfloat16 - bfloat16
 *   - bfloat16 - half - bfloat16 - float
 *   - bfloat16 - float - bfloat16 - float
 *   - half - bfloat16 - half - half
 *   - half - bfloat16 - half - float
 *   - float - bfloat16 - float - float
 * - On MLU500 series and when layernorm_desc->output_type_same_as_weight is true, this function
 *   additionally supports the following data type combinations:
 *   - bfloat16 - half - half - bfloat16
 *   - bfloat16 - float - float - bfloat16
 *   - bfloat16 - half - half - float
 *   - bfloat16 - float - float - float
 *   - half - bfloat16 - bfloat16 - half
 *   - half - bfloat16 - bfloat16 - float
 *   - float - bfloat16 - bfloat16 - float
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlLayerNormForward_v2 function to perform the
 *   layer normalization operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the layer normalization forward operation is as follows:
     @verbatim
      x: an array [64, 78, 1024]
      axis: 2 (layer normalization for the last dimension)
      filter and bias: [1024] or [1, 1, 1024]
      Then we will get the output:
      y: an array [64, 78, 1024] same as input
      saved_mean and saved_rstd: [64, 78] or [64, 78, 1]
     @endverbatim
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlLayerNormForward_v2(cnnlHandle_t handle,
                                                  const cnnlNormDescriptor_t layernorm_desc,
                                                  const cnnlTensorDescriptor_t x_desc,
                                                  const void *x,
                                                  int axis,
                                                  const cnnlTensorDescriptor_t filter_bias_desc,
                                                  const void *filter,
                                                  const void *bias,
                                                  float eps,
                                                  void *workspace,
                                                  size_t workspace_size,
                                                  const cnnlTensorDescriptor_t y_desc,
                                                  void *y,
                                                  const cnnlTensorDescriptor_t mean_rstd_desc,
                                                  void *saved_mean,
                                                  void *saved_rstd);

// Group:LayerNormForward
 /*!
 * @brief Computes layer normalization forward over last certain number of dimensions on input tensor
 *        \p x, \p filter and \p bias, and returns the results in the output tensor \p y.
 *
 * This function may need extra MLU memory as the workspace to improve the layer normalization performance.
 * You can get the workspace size with the ::cnnlGetLayerNormOpWorkspaceSize
 * function.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlLayerNormForward_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the layer normalization
 *   forward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] axis
 *    Input. Normalize over the last dimensions from axis to end.
 * @param[in] filter_bias_desc
 *   Input. The descriptor of the input filter and bias tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the filter tensor. The value of this pointer can be NULL.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the bias tensor. The value of this pointer can be NULL.
 *  @param[in] eps
 *    Input. A float value added to the denominator for numerical stability.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the layer normalization forward
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the layer normalization forward
 *   operation. You can get the size of the workspace with the ::cnnlGetLayerNormOpWorkspaceSize function.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] mean_rstd_desc
 *   Input. The descriptor of the output mean and rstd tensors. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] saved_mean
 *   Output. Pointer to the MLU memory that stores the output mean tensor.
 * @param[out] saved_rstd
 *   Output. Pointer to the MLU memory that stores the inverse of the variance tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *   ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "LayerNorm Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are as follows in the order of
 *   x - filter/bias - y - saved_mean/saved_rstd:
 *   - half - half - half - half
 *   - half - half - half - float
 *   - float - float - float - float
 * - On MLU500 series, this function additionally supports the following data type combinations:
 *   - bfloat16 - bfloat16 - bfloat16 - bfloat16
 *   - bfloat16 - bfloat16 - bfloat16 - float
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlLayerNormForward function to perform the
 *   layer normalization operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the layer normalization forward operation is as follows:
     @verbatim
      x: an array [64, 78, 1024]
      axis: 2 (layer normalization for the last dimension)
      filter and bias: [1024] or [1, 1, 1024]
      Then we will get the output:
      y: an array [64, 78, 1024] same as input
      saved_mean and saved_rstd: [64, 78] or [64, 78, 1]
     @endverbatim
 *
 * @par Reference
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlLayerNormForward_v2)
cnnlStatus_t CNNL_WIN_API cnnlLayerNormForward(cnnlHandle_t handle,
                                               const cnnlTensorDescriptor_t x_desc,
                                               const void *x,
                                               int axis,
                                               const cnnlTensorDescriptor_t filter_bias_desc,
                                               const void *filter,
                                               const void *bias,
                                               float eps,
                                               void *workspace,
                                               size_t workspace_size,
                                               const cnnlTensorDescriptor_t y_desc,
                                               void *y,
                                               const cnnlTensorDescriptor_t mean_rstd_desc,
                                               void *saved_mean,
                                               void *saved_rstd);

// Group:LayerNormForward
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the layer normalization forward operation.
 *
 * The size of the extra workspace is based on the given information of the layer normalization forward
 * operation, including the input tensor descriptors \p x_desc, and reduction axis \p axis. For more
 * information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the layer
 *   normalization forward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] axis
 *   Input. Normalize over the last dimensions from \p axis to the end.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] size
 *    Output. Pointer to the returned size of the extra workspace in bytes that is used in the layer
 *    normalization forward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetLayerNormOpWorkspaceSize(cnnlHandle_t handle,
                                                          int axis,
                                                          const cnnlTensorDescriptor_t x_desc,
                                                          size_t *size);

// Group:RmsNormForward
 /*!
 * @brief Computes rms normalization forward over the last certain number of dimensions on input tensors
 *        \p x, \p scale and \p bias, and returns the results in the output tensor \p y.
 *
 * This function may need extra MLU memory as the workspace to improve the rms normalization performance.
 * You can get the workspace size with the ::cnnlGetRmsNormOpWorkspaceSize
 * function.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlRmsNormForward_v2 instead, which supports parameter \p rmsnormforward_desc.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the rms normalization
 *   forward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] axis
 *    Input. Normalize over the last dimensions from \p axis to end.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] scale_bias_desc
 *   Input. The descriptor of the input scale and bias tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] scale
 *   Input. Pointer to the MLU memory that stores the scale tensor. The value of this pointer can be NULL.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the bias tensor.
 *   Now, the \p bias tensor is not supported and the value of this pointer must be NULL.
 * @param[in] eps
 *    Input. A float value added to the denominator for numerical stability.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the rms normalization forward
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the rms normalization forward
 *   operation.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] rms_desc
 *   Input. The descriptor of the output mean and rstd tensors. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] saved_rms
 *   Output. Pointer to the MLU memory that stores the output mean tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "RmsNorm Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are as follows in the order of
 *   x - scale - y - saved_rms:
 *   - half - half - half - half
 *   - half - half - half - float
 *   - half - float - half - half
 *   - half - float - half - float
 *   - half - float - float - half
 *   - half - float - float - float
 *   - float - half - half - half
 *   - float - half - half - float
 *   - float - half - float - half
 *   - float - half - float - float
 *   - float - float - float - float
 * - On MLU500 series, this function additionally supports the following data type combinations:
 *   - bfloat16 - bfloat16 - bfloat16 - bfloat16
 *   - bfloat16 - bfloat16 - bfloat16 - float
 *   - bfloat16 - float - bfloat16 - bfloat16
 *   - bfloat16 - float - bfloat16 - float
 *   - bfloat16 - float - float - bfloat16
 *   - bfloat16 - float - float - float
 *   - float - bfloat16 - bfloat16 - bfloat16
 *   - float - bfloat16 - bfloat16 - float
 *   - float - bfloat16 - float - bfloat16
 *   - float - bfloat16 - float - float
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlRmsNormForward function to perform the
 *   rms normalization operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the rms normalization forward operation is as follows:
     @verbatim
      x: an array [64, 78, 1024]
      axis: 2 (rms normalization for the last dimension)
      scale and bias: [1024] or [1, 1, 1024]
      Then we will get the output:
      y: an array [64, 78, 1024] same as input
      saved_rms: [64, 78] or [64, 78, 1]
     @endverbatim
 *
 * @par Reference
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlRmsNormForward_v2)
cnnlStatus_t CNNL_WIN_API cnnlRmsNormForward(cnnlHandle_t handle,
                                             int axis,
                                             const cnnlTensorDescriptor_t x_desc,
                                             const void *x,
                                             const cnnlTensorDescriptor_t scale_bias_desc,
                                             const void *scale,
                                             const void *bias,
                                             float eps,
                                             void *workspace,
                                             size_t workspace_size,
                                             const cnnlTensorDescriptor_t y_desc,
                                             void *y,
                                             const cnnlTensorDescriptor_t rms_desc,
                                             void *saved_rms);

// Group:RmsNormForward
 /*!
 * @brief Computes rms normalization forward over the last certain number of dimensions on input tensors
 *        \p x, \p scale and \p bias, and returns the results in the output tensor \p y.
 *
 * This function may need extra MLU memory as the workspace to improve the rms normalization performance.
 * You can get the workspace size with the ::cnnlGetRmsNormOpWorkspaceSize
 * function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the rms normalization
 *   forward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] axis
 *    Input. Normalize over the last dimensions from \p axis to end.
 * @param[in] eps
 *    Input. A float value added to the denominator for numerical stability.
 * @param[in] rmsnormforward_desc
 *    Input. The descriptor of the rmsnorm forward operation. For detailed
 *    information, see ::cnnlNormDescriptor_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] scale_bias_desc
 *   Input. The descriptor of the input scale and bias tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] scale
 *   Input. Pointer to the MLU memory that stores the scale tensor. The value of this pointer can be NULL.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the bias tensor.
 *   Now, the \p bias tensor is not supported and the value of this pointer must be NULL.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the rms normalization forward
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the rms normalization forward
 *   operation.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] rms_desc
 *   Input. The descriptor of the output mean and rstd tensors. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] saved_rms
 *   Output. Pointer to the MLU memory that stores the output mean tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "RmsNorm Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are as follows in the order of
 *   x - scale - y - saved_rms:
 *   - half - half - half - half
 *   - half - half - half - float
 *   - half - float - half - half
 *   - half - float - half - float
 *   - half - float - float - half
 *   - half - float - float - float
 *   - float - half - half - half
 *   - float - half - half - float
 *   - float - half - float - half
 *   - float - half - float - float
 *   - float - float - float - half
 *   - float - float - float - float
 * - On MLU500 series, this function additionally supports the following data type combinations:
 *   - bfloat16 - bfloat16 - bfloat16 - bfloat16
 *   - bfloat16 - bfloat16 - bfloat16 - float
 *   - bfloat16 - float - bfloat16 - bfloat16
 *   - bfloat16 - float - bfloat16 - float
 *   - bfloat16 - float - float - bfloat16
 *   - bfloat16 - float - float - float
 *   - float - bfloat16 - bfloat16 - bfloat16
 *   - float - bfloat16 - bfloat16 - float
 *   - float - bfloat16 - float - bfloat16
 *   - float - bfloat16 - float - float
 *   - float - float - float - bfloat16
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlRmsNormForward_v2 function to perform the
 *   rms normalization operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the rms normalization forward operation is as follows:
     @verbatim
      x: an array [64, 78, 1024]
      axis: 2 (rms normalization for the last dimension)
      scale and bias: [1024] or [1, 1, 1024]
      Then we will get the output:
      y: an array [64, 78, 1024] same as input
      saved_rms: [64, 78] or [64, 78, 1]
     @endverbatim
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlRmsNormForward_v2(cnnlHandle_t handle,
                                                int axis,
                                                float eps,
                                                const cnnlNormDescriptor_t rmsnormforward_desc,
                                                const cnnlTensorDescriptor_t x_desc,
                                                const void *x,
                                                const cnnlTensorDescriptor_t scale_bias_desc,
                                                const void *scale,
                                                const void *bias,
                                                void *workspace,
                                                size_t workspace_size,
                                                const cnnlTensorDescriptor_t y_desc,
                                                void *y,
                                                const cnnlTensorDescriptor_t rms_desc,
                                                void *saved_rms);

// Group:RmsNormForward
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the rms normalization forward operation.
 *
 * The size of the extra workspace is based on the given information of the rms normalization forward
 * operation, including the input tensor descriptor \p x_desc, and reduction axis \p axis. For more
 * information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the rms
 *   normalization forward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] axis
 *   Input. Normalize over the last dimensions from \p axis to the end.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *    Output. Pointer to the returned size of the extra workspace in bytes that is used in the rms
 *    normalization forward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetRmsNormOpWorkspaceSize(cnnlHandle_t handle,
                                                        int axis,
                                                        const cnnlTensorDescriptor_t x_desc,
                                                        size_t *workspace_size);

// Group:Norm
/*!
 * @brief Creates a descriptor pointed by \p norm_desc for a norm operation,
 *        and allocates memory for holding the information about the norm operation.
 *        The information is defined in ::cnnlNormDescriptor_t.
 *
 * @param[out] norm_desc
 *   Output. A host pointer to the norm descriptor that holds information about the
 *   norm operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetNormDescAttr function to initialize
 *   and set the information to the norm descriptor.
 * - At the end, you need to call the ::cnnlDestroyNormDesc function to destroy the descriptor.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
    cnnlCreateNormDesc(cnnlNormDescriptor_t *norm_desc);

// Group:Norm
/*!
 * @brief Destroys the norm descriptor \p norm_desc
 *        that was previously created with ::cnnlCreateNormDesc.
 *
 * The norm descriptor is defined in ::cnnlNormDescriptor_t
 * and holds the information about the norm operation.
 *
 * @param[in] norm_desc
 *   Input. The norm descriptor to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
    cnnlDestroyNormDesc(cnnlNormDescriptor_t norm_desc);

// Group:Norm
/*!
 * @brief Initializes the norm descriptor \p norm_desc
 * that was previously created with the ::cnnlCreateNormDesc function, and sets
 * the information about the norm operation to the norm
 * descriptor \p norm_desc. The information includes the attribute \p attr defined in
 * ::cnnlNormDescAttribute_t, the host pointer \p buf to the attribute value, and
 * the size of buffer for verification.
 *
 * @param[in,out] norm_desc
 *   Input/output. The descriptor of the norm operation. For detailed
 *   information, see ::cnnlNormDescriptor_t.
 * @param[in] attr
 *   Input. Attributes of the norm descriptor to be set. For detailed
 *   information, see ::cnnlNormDescAttribute_t.
 * @param[in] buf
 *   Input. A host pointer to the attribute value set by this function.
 * @param[in] size_in_bytes
 *   Input. Buffer in bytes for verification.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetNormDescAttr(const cnnlNormDescriptor_t norm_desc,
                                              cnnlNormDescAttribute_t attr,
                                              const void *buf,
                                              size_t size_in_bytes);

// Group:ActivationForward
/*!
 * @brief Initializes the activation descriptor \p activation_desc, and sets the information
 * about the activation operation to the activation descriptor \p activation_desc. The
 * information includes the attribute \p attr defined in ::cnnlActivationDescAttribute_t,
 * the host pointer \p buf to the attribute value, and the size of buffer for verification.
 *
 * @param[in,out] activation_desc
 *   Input/output. The descriptor of the activation operation. For detailed
 *   information, see ::cnnlActivationDescriptor_t.
 * @param[in] attr
 *   Input. Attribute of the activation descriptor to be set. For detailed
 *   information, see ::cnnlActivationDescAttribute_t.
 * @param[in] buf
 *   Input. A host pointer to the attribute value that is set by this function.
 * @param[in] size_in_bytes
 *   Input. Buffer in bytes for verification.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetActivationDescAttr(cnnlActivationDescriptor_t activation_desc,
                                                    cnnlActivationDescAttribute_t attr,
                                                    const void *buf,
                                                    size_t size_in_bytes);

// Group:ActivationForward
/*!
 * @brief Returns the pointer to the retrieved buffer \p buf, and the size of the buffer
 * \p size_written, based on the given activation descriptor \p activation_desc and attribute \p attr.
 *
 * You can set the attribute in the activation descriptor based on the return value
 * of this function. For detailed information, see ::cnnlSetActivationDescAttr.
 *
 * @param[in] activation_desc
 *   Input. The descriptor of the activation operation. For detailed
 *   information, see ::cnnlActivationDescriptor_t.
 * @param[in] attr
 *   Input. Attribute of the activation descriptor to be retrieved.
 * @param[out] buf
 *   Output. A host pointer to the attribute value to be retrieved by this function.
 * @param[in] size_in_bytes
 *   Input. Buffer in bytes for verification, which is used to check whether the memory size is the same as \p size_written.
 * @param[out] size_written
 *   Output. A host pointer to the number of bytes actually written to the buffer.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetActivationDescAttr(cnnlActivationDescriptor_t activation_desc,
                                                    cnnlActivationDescAttribute_t attr,
                                                    void *buf,
                                                    size_t size_in_bytes,
                                                    size_t *size_written);

// Group:ActivationForward
/*!
 * @brief Creates a descriptor pointed by \p activation_desc for an activation forward or
 *        backward operation, and allocates memory for holding the information about the
 *        activation operation. The information is defined in ::cnnlActivationDescriptor_t.
 *        For more information about descriptor, see "Cambricon CNNL User Guide".
 *
 * @param[out] activation_desc
 *   Output. A host pointer to the activation descriptor that holds information about the
 *   activation operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - After calling this function, you need to call the ::cnnlSetActivationDescAttr function to
 *   initialize and set the information to the activation descriptor.
 * - You need to call the ::cnnlDestroyActivationDescriptor function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCreateActivationDescriptor(cnnlActivationDescriptor_t *activation_desc);

// Group:ActivationForward
/*!
 * @brief Destroys an activation descriptor \p activation_desc that was previously created with the
 *        ::cnnlCreateActivationDescriptor function.
 *
 * The activation descriptor is defined in ::cnnlActivationDescriptor_t and holds the information
 * about the activation forward or backward operation.
 *
 * @param[in] activation_desc
 *   Input. The activation descriptor to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - You need to call this function after calling the ::cnnlActivationForward
 *   or ::cnnlActivationBackward function.
 *   Otherwise, ::CNNL_STATUS_BAD_PARAM is returned.
 * - This function should be called to destroy the activation descriptor.
 *   Otherwise, the memory leak may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlDestroyActivationDescriptor(cnnlActivationDescriptor_t activation_desc);

// Group:ActivationForward
/*!
 * @brief Initializes the activation descriptor \p activation_desc that was previously created
 * with the ::cnnlCreateActivationDescriptor function, and sets the information about the
 * activation forward or backward operation to the activation descriptor \p activation_desc.
 * You can specify the activation function to be used in \p mode. The following activation
 * functions are supported:
 * - Activation forward:
 *   - ReLU, ReLU6, Sigmoid, Tanh, SELU, GELU, ELU, LeakyReLU, TF_LeakyReLU, Caffe_ReLU6, Logsigmoid, Mish and GLU.
 * - Activation backward:
 *   - ReLU, ReLU6, Sigmoid, Tanh, GELU, LeakyReLU, Mish and TF_LeakyReLU.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlSetActivationDescAttr instead, which supports parameter of \p sliced_dim
 *   that determines which dimension of the input to be sliced when \p mode is set to \p CNNL_ACTIVATION_GLU.
 *
 * @param[in,out] activation_desc
 *   Input/output. The descriptor of the activation operation. For detailed information,
 *   see ::cnnlActivationDescriptor_t.
 * @param[in] mode
 *   Input. The activation \p mode defined in ::cnnlActivationMode_t enum.
 * @param[in] nan_prop
 *   Input. The NaN propagation modes defined in ::cnnlNanPropagation_t enum.
 *   This parameter is only used when you set \p mode to \p CNNL_ACTIVATION_RELU or
 *   \p CNNL_ACTIVATION_RELU6 in ::cnnlActivationForward.
 * @param[in] coef
 *   Input. A scalar value that is only used when you set \p mode to \p CNNL_ACTIVATION_CLIPPED_RELU,
 *   \p CNNL_ACTIVATION_ELU, \p CNNL_ACTIVATION_LEAKYRELU, \p CNNL_ACTIVATION_TF_LEAKYRELU, or
 *   \p CNNL_ACTIVATION_CAFFE_RELU6.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_NOT_INITIALIZED
 *
 * @note
 * - When \p mode is \p CNNL_ACTIVATION_RELU or \p CNNL_ACTIVATION_RELU6 in ::cnnlActivationForward:
 *   - PyTorch: \p nan_prop = CNNL_PROPAGATE_NAN.
 *   - TensorFlow 2.5: \p nan_prop = CNNL_PROPAGATE_NAN.
 *   - TensorFlow 2.4 and earlier versions: \p nan_prop = CNNL_NOT_PROPAGATE_NAN.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlSetActivationDescAttr)
cnnlStatus_t CNNL_WIN_API cnnlSetActivationDescriptor(cnnlActivationDescriptor_t activation_desc,
                                                      cnnlActivationMode_t mode,
                                                      cnnlNanPropagation_t nan_prop,
                                                      float coef);

// Group:ActivationForward
/*!
 * @brief Initializes the activation descriptor \p activation_desc that was previously created
 * with the ::cnnlCreateActivationDescriptor function, and sets the information about the
 * activation forward or backward operation to the activation descriptor \p activation_desc.
 * You can specify the activation function to be used in \p mode. The following activation
 * functions are supported for activation forward or backward operation separately:
 *
 * - Activation forward:
 *   - ReLU, ReLU6, Sigmoid, Tanh, SELU, GELU, ELU, LeakyReLU, TF_LeakyReLU, Caffe_ReLU6, Logsigmoid, Mish and GLU.
 * - Activation backward:
 *   - ReLU, ReLU6, Sigmoid, Tanh, GELU, LeakyReLU, Mish and TF_LeakyReLU.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlSetActivationDescAttr instead, which supports parameter of \p sliced_dim
 *   that determines which dimension of the input to be sliced when \p mode is set to \p CNNL_ACTIVATION_GLU.
 *
 * @param[in,out] activation_desc
 *   Input/output. The descriptor of the activation operation. For detailed information,
 *   see ::cnnlActivationDescriptor_t.
 * @param[in] mode
 *   Input. The activation \p mode defined in ::cnnlActivationMode_t enum.
 * @param[in] prefer
 *   Input. The \p prefer modes defined in ::cnnlActivationPreference_t enum.
 *   The following activation functions support \p prefer modes for activation forward or backward
 *   operation separately:
 *   - Activation forward:
 *     - Sigmoid, Tanh, SELU, GELU, and ELU.
 *   - Activation backward:
 *     - GELU.
 *   The default value of this parameter is \p CNNL_ACTIVATION_HIGH_PRECISION.
 * @param[in] nan_prop
 *   Input. The NaN propagation modes defined in ::cnnlNanPropagation_t enum.
 *   This parameter is only used when you set \p mode to \p CNNL_ACTIVATION_RELU or
 *   \p CNNL_ACTIVATION_RELU6 in ::cnnlActivationForward.
 * @param[in] coef
 *   Input. A scalar value that is only used when you set \p mode to \p CNNL_ACTIVATION_CLIPPED_RELU,
 *   \p CNNL_ACTIVATION_ELU, \p CNNL_ACTIVATION_LEAKYRELU, \p CNNL_ACTIVATION_TF_LEAKYRELU, or
 *   \p CNNL_ACTIVATION_CAFFE_RELU6.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_NOT_INITIALIZED
 *
 * @par API Dependency
 * - Before calling this function, you need to call the ::cnnlCreateActivationDescriptor function to
 *   create the activation descriptor.
 * - You need to call the ::cnnlDestroyActivationDescriptor function to destroy the descriptor.
 *
 * @note
 * - When \p mode is \p CNNL_ACTIVATION_RELU or \p CNNL_ACTIVATION_RELU6 in ::cnnlActivationForward:
 *   - PyTorch: \p nan_prop = CNNL_PROPAGATE_NAN.
 *   - TensorFlow 2.5: \p nan_prop = CNNL_PROPAGATE_NAN.
 *   - TensorFlow 2.4 and earlier versions: \p nan_prop = CNNL_NOT_PROPAGATE_NAN.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlSetActivationDescAttr)
cnnlStatus_t CNNL_WIN_API cnnlSetActivationDescriptor_v2(cnnlActivationDescriptor_t activation_desc,
                                                         cnnlActivationMode_t mode,
                                                         cnnlActivationPreference_t prefer,
                                                         cnnlNanPropagation_t nan_prop,
                                                         float coef);

// Group:ActivationForward
/*!
 * @brief Initializes the activation descriptor \p activation_desc that was previously created
 * with the ::cnnlCreateActivationDescriptor function, and sets the information about the
 * activation forward or backward operation to the activation descriptor \p activation_desc.
 * You can specify the activation function to be used in \p mode. The following activation
 * functions are supported for activation forward or backward operation separately:
 *
 * - Activation forward:
 *   - ReLU, ReLU6, Sigmoid, Tanh, SELU, GELU, ELU, LeakyReLU, TF_LeakyReLU, Caffe_ReLU6, Logsigmoid, Mish and GLU.
 * - Activation backward:
 *   - ReLU, ReLU6, Sigmoid, Tanh, GELU, LeakyReLU, Mish and TF_LeakyReLU.
 *
 * Compared with ::cnnlSetActivationDescriptor and cnnlSetActivationDescriptor_v2
 * If set \p mode to \p CNNL_ACTIVATION_GLU, this function allows you to choose whether to
 * perform activation operations with \p sliced_dim.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlSetActivationDescAttr instead, which supports parameters of \p prefer
 *   that determines to compute with faster algorithm or higher precision, and \p sliced_dim that
 *   determines which dimension of the input to be sliced when \p mode is set to \p CNNL_ACTIVATION_GLU.
 *
 * @param[in,out] activation_desc
 *   Input/output. The descriptor of the activation operation. For detailed information,
 *   see ::cnnlActivationDescriptor_t.
 * @param[in] mode
 *   Input. The activation \p mode defined in ::cnnlActivationMode_t enum.
 * @param[in] prefer
 *   Input. The \p prefer modes defined in ::cnnlActivationPreference_t enum.
 *   The following activation functions support \p prefer modes for activation forward or backward
 *   operation separately:
 *   - Activation forward:
 *     - Sigmoid, Tanh, SELU, GELU, and ELU.
 *   - Activation backward:
 *     - GELU.
 *   The default value of this parameter is \p CNNL_ACTIVATION_HIGH_PRECISION.
 * @param[in] nan_prop
 *   Input. The NaN propagation modes defined in ::cnnlNanPropagation_t enum.
 *   This parameter is only used when you set \p mode to \p CNNL_ACTIVATION_RELU or
 *   \p CNNL_ACTIVATION_RELU6 in ::cnnlActivationForward.
 * @param[in] coef
 *   Input. A scalar value that is only used when you set \p mode to \p CNNL_ACTIVATION_CLIPPED_RELU,
 *   \p CNNL_ACTIVATION_ELU, \p CNNL_ACTIVATION_LEAKYRELU, \p CNNL_ACTIVATION_TF_LEAKYRELU, or
 *   \p CNNL_ACTIVATION_CAFFE_RELU6.
 * @param[in] sliced_dim
 *   Input. An integer value deciding which dimension of the input to be sliced.
 *   The element number of sliced input dimension must be an even number.
 *   The activation input tensor \p x is sliced in half along \p sliced_dim
 *   to form a (in the lower dimension part) and b (in the higher dimension part).
 *   This parameter is only used when you set \p mode to \p CNNL_ACTIVATION_GLU.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_NOT_INITIALIZED
 *
 * @par API Dependency
 * - Before calling this function, you need to call the ::cnnlCreateActivationDescriptor function to
 *   create the activation descriptor.
 * - You need to call the ::cnnlDestroyActivationDescriptor function to destroy the descriptor.
 *
 * @note
 * - When \p mode is \p CNNL_ACTIVATION_RELU or \p CNNL_ACTIVATION_RELU6 in ::cnnlActivationForward:
 *   - PyTorch: \p nan_prop = CNNL_PROPAGATE_NAN.
 *   - TensorFlow 2.5: \p nan_prop = CNNL_PROPAGATE_NAN.
 *   - TensorFlow 2.4 and earlier versions: \p nan_prop = CNNL_NOT_PROPAGATE_NAN.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlSetActivationDescAttr)
cnnlStatus_t CNNL_WIN_API cnnlSetActivationDescriptor_v3(cnnlActivationDescriptor_t activation_desc,
                                                         cnnlActivationMode_t mode,
                                                         cnnlActivationPreference_t prefer,
                                                         cnnlNanPropagation_t nan_prop,
                                                         float coef,
                                                         int sliced_dim);

// Group:ActivationForward
/*!
 * @brief Initializes the activation descriptor \p activation_desc that was previously created
 * with the ::cnnlCreateActivationDescriptor function, and sets the information about the
 * activation forward or backward operation to the activation descriptor \p activation_desc.
 * You can specify the activation function to be used in \p mode. The following activation
 * functions are supported for activation forward or backward operation separately:
 *
 * - Activation forward:
 *   - ReLU, ReLU6, Sigmoid, Tanh, SELU, GELU, ELU, LeakyReLU, TF_LeakyReLU, Caffe_ReLU6,
 *   - Hardsigmoid, Hardswish, Mish and GLU.
 *
 * - Activation backward:
 *   - ReLU, ReLU6, Sigmoid, Tanh, GELU, LeakyReLU, Hardsigmoid, Hardswish, Mish and TF_LeakyReLU.
 *
 * Compared with ::cnnlSetActivationDescriptor, cnnlSetActivationDescriptor_v2 and
 * cnnlSetActivationDescriptor_v3, this function allows you to choose whether to perform activation
 * operations with \p gamma and \p scale when \p mode is \p CNNL_ACTIVATION_ELU,
 * \p CNNL_ACTIVATION_HARDSIGMOID or \p CNNL_ACTIVATION_SELU.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlSetActivationDescAttr instead, which supports parameter of \p sliced_dim
 *   that determines which dimension of the input to be sliced when \p mode is set to \p CNNL_ACTIVATION_GLU.
 *
 * @param[in] activation_desc
 *   Input. The descriptor of the activation operation. For detailed information,
 *   see ::cnnlActivationDescriptor_t.
 * @param[in] mode
 *   Input. The activation \p mode defined in ::cnnlActivationMode_t enum.
 * @param[in] prefer
 *   Input. The \p prefer modes defined in ::cnnlActivationPreference_t enum.
 *   The following activation functions support \p prefer modes for activation forward or backward
 *   operation separately:
 *   - Activation forward:
 *     - Sigmoid, Tanh, SELU, GELU, Hardsigmoid, Hardswish and ELU.
 *   - Activation backward:
 *     - GELU and Hardswish.
 *   The default value of this parameter is \p CNNL_ACTIVATION_HIGH_PRECISION.
 * @param[in] nan_prop
 *   Input. The NaN propagation modes defined in ::cnnlNanPropagation_t enum.
 *   This parameter is only used when you set \p mode to \p CNNL_ACTIVATION_RELU or
 *   \p CNNL_ACTIVATION_RELU6 in ::cnnlActivationForward.
 * @param[in] coef
 *   Input. A scalar value that is only used when you set \p mode to \p CNNL_ACTIVATION_CLIPPED_RELU,
 *   \p CNNL_ACTIVATION_ELU, \p CNNL_ACTIVATION_LEAKYRELU, \p CNNL_ACTIVATION_TF_LEAKYRELU, or
 *   \p CNNL_ACTIVATION_CAFFE_RELU6.
 * @param[in] sliced_dim
 *   Input. An integer value deciding which dimension of the input to be sliced.
 *   The element number of sliced input dimension must be an even number.
 *   The activation input tensor \p x is sliced in half along \p sliced_dim
 *   to form a (in the lower dimension part) and b (in the higher dimension part).
 *   This parameter is only used when you set \p mode to \p CNNL_ACTIVATION_GLU.
 * @param[in] gamma
 *   Input. A hyper parameter which is only used when you set \p mode to \p CNNL_ACTIVATION_ELU,
 *   \p CNNL_ACTIVATION_HARDSIGMOID or \p CNNL_ACTIVATION_SELU.
 *   This parameter has a default value: 1.67326319217681884765625.
 * @param[in] scale
 *   Input. A hyper parameter which is only used when you set \p mode to \p CNNL_ACTIVATION_ELU,
 *   \p CNNL_ACTIVATION_HARDSIGMOID or \p CNNL_ACTIVATION_SELU.
 *   This parameter has a default value: 1.05070102214813232421875.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_NOT_INITIALIZED
 *
 * @par API Dependency
 * - Before calling this function, you need to call the ::cnnlCreateActivationDescriptor function to
 *   create the activation descriptor.
 * - You need to call the ::cnnlDestroyActivationDescriptor function to destroy the descriptor.
 *
 * @note
 * - The parameter settings for different frameworks are as follows:
 *   - When \p mode is \p CNNL_ACTIVATION_HARDSIGMOID in Activation forward and backward, the semantics of
 *     hardsigmoid in different frameworks can be achieved by setting different values for \p gamma and \p scale. Details:
 *     - PyTorch Framework: \p gamma = 1/6 and \p scale = 1/2.
 *     - TensorFlow Framework: \p gamma = 1/5 and \p scale = 1/2.
 *     - Onnx Framework: \p gamma and \p scale are set according to requirements.
 *   - When \p mode is \p CNNL_ACTIVATION_RELU or \p CNNL_ACTIVATION_RELU6 in ::cnnlActivationForward:
 *     - PyTorch: \p nan_prop = CNNL_PROPAGATE_NAN.
 *     - TensorFlow 2.5: \p nan_prop = CNNL_PROPAGATE_NAN.
 *     - TensorFlow 2.4 and earlier versions: \p nan_prop = CNNL_NOT_PROPAGATE_NAN.
 *   - When \p mode is \p CNNL_ACTIVATION_SELU in ::cnnlActivationForward:
 *     - PyTorch: \p gamma and \p scale should be the default value.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlSetActivationDescAttr)
cnnlStatus_t CNNL_WIN_API cnnlSetActivationDescriptor_v4(cnnlActivationDescriptor_t activation_desc,
                                                         cnnlActivationMode_t mode,
                                                         cnnlActivationPreference_t prefer,
                                                         cnnlNanPropagation_t nan_prop,
                                                         float coef,
                                                         int sliced_dim,
                                                         float gamma,
                                                         float scale);

// Group:ActivationForward
/*!
 * @brief Initializes the activation descriptor \p activation_desc that was previously created
 * with the ::cnnlCreateActivationDescriptor function, and sets the information about the
 * activation forward or backward operation to the activation descriptor \p activation_desc.
 * You can specify the activation function to be used in \p mode. The following activation
 * functions are supported for activation forward or backward operation separately:
 *
 * - Activation forward:
 *   - ReLU, ReLU6, Sigmoid, Tanh, SELU, GELU, ELU, LeakyReLU, TF_LeakyReLU, Caffe_ReLU6,
 *     Hardsigmoid, Hardswish, ELU_V2, Mish and GLU.
 *
 * - Activation backward:
 *   - ReLU, ReLU6, Sigmoid, Tanh, GELU, LeakyReLU, Hardsigmoid, Hardswish, ELU,
 *     ELU_V2, Mish and TF_LeakyReLU.
 *
 * Compared with ::cnnlSetActivationDescriptor, cnnlSetActivationDescriptor_v2,
 * cnnlSetActivationDescriptor_v3 and cnnlSetActivationDescriptor_v4, this function supports the
 * \p is_result parameter that allows users to choose from different implementation methods when \p mode
 * is \p CNNL_ACTIVATION_ELU.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlSetActivationDescAttr instead.
 *
 * @deprecated
 * - \p CNNL_ACTIVATION_ELU mode will be deprecated gradually in future release.
 *   Use \p CNNL_ACTIVATION_ELU_V2 instead.
 *
 * @param[in,out] activation_desc
 *   Input/output. The descriptor of the activation operation. For detailed information,
 *   see ::cnnlActivationDescriptor_t.
 * @param[in] mode
 *   Input. The activation \p mode defined in ::cnnlActivationMode_t enum.
 * @param[in] prefer
 *   Input. The \p prefer modes defined in ::cnnlActivationPreference_t enum.
 *   The following activation functions support \p prefer modes for activation forward or backward
 *   operation separately:
 *   - Activation forward:
 *     - Sigmoid, Tanh, SELU, GELU, Hardsigmoid, Hardswish, ELU and ELU_V2.
 *   - Activation backward:
 *     - GELU and Hardswish.
 *   The default value of this parameter is \p CNNL_ACTIVATION_HIGH_PRECISION.
 * @param[in] nan_prop
 *   Input. The NaN propagation modes defined in ::cnnlNanPropagation_t enum.
 *   This parameter is only used when you set \p mode to \p CNNL_ACTIVATION_RELU or
 *   \p CNNL_ACTIVATION_RELU6 in ::cnnlActivationForward.
 * @param[in] coef
 *   Input. A scalar value that is only used when you set \p mode to \p CNNL_ACTIVATION_CLIPPED_RELU,
 *   \p CNNL_ACTIVATION_ELU, \p CNNL_ACTIVATION_ELU_V2, \p CNNL_ACTIVATION_LEAKYRELU,
 *   \p CNNL_ACTIVATION_TF_LEAKYRELU, or \p CNNL_ACTIVATION_CAFFE_RELU6.
 * @param[in] sliced_dim
 *   Input. An integer value deciding which dimension of the input to be sliced.
 *   The element number of sliced input dimension must be an even number.
 *   The activation input tensor \p x is sliced in half along \p sliced_dim
 *   to form a (in the lower dimension part) and b (in the higher dimension part).
 *   This parameter is only used when you set \p mode to \p CNNL_ACTIVATION_GLU.
 * @param[in] gamma
 *   Input. A hyper parameter which is only used when you set \p mode to \p CNNL_ACTIVATION_ELU,
 *   \p CNNL_ACTIVATION_ELU_V2, \p CNNL_ACTIVATION_HARDSIGMOID or \p CNNL_ACTIVATION_SELU.
 *   (The gamma and scale, as a pair of hyper parameters, have multiple meanings in different
 *   activation \p mode. For details, refer to the "Note" section described below.)
 * @param[in] scale
 *   Input. A hyper parameter which is only used when you set \p mode to \p CNNL_ACTIVATION_ELU,
 *   \p CNNL_ACTIVATION_ELU_V2, \p CNNL_ACTIVATION_HARDSIGMOID or \p CNNL_ACTIVATION_SELU.
 *   (The gamma and scale, as a pair of hyper parameters, have multiple meanings in different
 *   activation \p mode. For details, refer to the "Note" section described below.)
 * @param[in] is_result
 *   Input. A Boolean value describes the implementation logic of ::cnnlActivationBackward when
 *   the \p mode is \p CNNL_ACTIVATION_ELU or \p CNNL_ACTIVATION_ELU_V2. The default value of
 *   this parameter is true.
 *   When the \p mode is \p CNNL_ACTIVATION_ELU and this parameter is true, the implementation
 *   logic of ELU is as follows:
     @verbatim
      diff_x = scale * diff_y, x > 0;
      diff_x = gamma * (x + scale * coef) * diff_y, x <= 0;
     @endverbatim
 *   When the \p mode is \p CNNL_ACTIVATION_ELU and this parameter is false, the implementation
 *   logic of ELU is as follows:
     @verbatim
      diff_x = scale * diff_y, x > 0;
      diff_x = scale * coef * gamma * exp(gamma * x) * diff_y, x <= 0;
     @endverbatim
 *   When the \p mode is \p CNNL_ACTIVATION_ELU_V2 and this parameter is true, the implementation
 *   logic of ELU_V2 is as follows:
     @verbatim
      diff_x = scale * diff_y, x > 0;
      diff_x = coef * (x + scale * gamma) * diff_y, x <= 0;
     @endverbatim
 *   When the \p mode is \p CNNL_ACTIVATION_ELU_V2 and this parameter is false, the implementation
 *   logic of ELU_V2 is as follows:
     @verbatim
      diff_x = scale * diff_y, x > 0;
      diff_x = scale * coef * gamma * exp(coef * x) * diff_y, x <= 0;
     @endverbatim
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_NOT_INITIALIZED
 *
 * @par API Dependency
 * - Before calling this function, you need to call the ::cnnlCreateActivationDescriptor function to
 *   create the activation descriptor.
 * - You need to call the ::cnnlDestroyActivationDescriptor function to destroy the descriptor.
 *
 * @note
 * - The parameter settings for different frameworks are as follows:
 *
 *   - When \p mode is \p CNNL_ACTIVATION_HARDSIGMOID in Activation forward and backward, the semantics of
 *     hardsigmoid in different frameworks can be achieved by setting different values for \p gamma and \p scale. Details:
 *     - PyTorch Framework: \p gamma = 1/6 and \p scale = 1/2.
 *     - TensorFlow Framework: \p gamma = 1/5 and \p scale = 1/2.
 *     - Onnx Framework: \p gamma and \p scale are set according to requirements.
 *     The implementation of HARDSIGMOID in ::cnnlActivationForward is given below:
       @verbatim
        HARDSIGMOID = max(0, min(1, gamma * x + scale));
       @endverbatim
 *
 *   - When \p mode is \p CNNL_ACTIVATION_SELU in ::cnnlActivationForward:
 *     - PyTorch: \p gamma and \p scale should be the default value,
 *       where \p gamma is 1.67326319217681884765625 and \p scale is 1.05070102214813232421875.
 *     The implementation of SELU in ::cnnlActivationForward is given below:
       @verbatim
        SELU = scale * gamma * (exp(x) - 1), x <= 0;
        SELU = scale * x, x > 0;
       @endverbatim
 *
 *   - When \p mode is \p CNNL_ACTIVATION_RELU or \p CNNL_ACTIVATION_RELU6 in ::cnnlActivationForward:
 *     - PyTorch: \p nan_prop = CNNL_PROPAGATE_NAN.
 *     - TensorFlow 2.5: \p nan_prop = CNNL_PROPAGATE_NAN.
 *     - TensorFlow 2.4 and earlier versions: \p nan_prop = CNNL_NOT_PROPAGATE_NAN.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlSetActivationDescAttr)
cnnlStatus_t CNNL_WIN_API cnnlSetActivationDescriptor_v5(cnnlActivationDescriptor_t activation_desc,
                                                         cnnlActivationMode_t mode,
                                                         cnnlActivationPreference_t prefer,
                                                         cnnlNanPropagation_t nan_prop,
                                                         float coef,
                                                         int sliced_dim,
                                                         float gamma,
                                                         float scale,
                                                         bool is_result);

// Group:ActivationForward
/*!
 * @brief Initializes the activation descriptor \p activation_desc that was previously created
 * with the ::cnnlCreateActivationDescriptor function, and sets the information about the
 * activation forward or backward operation to the activation descriptor \p activation_desc.
 * You can specify the activation function to be used in \p mode. The following activation
 * functions are supported for activation forward or backward operation separately:
 *
 * - Activation forward:
 *   - ReLU, ReLU6, Sigmoid, Tanh, SELU, GELU, ELU, LeakyReLU, TF_LeakyReLU, Caffe_ReLU6,
 *   - Hardsigmoid, Hardswish, Hardshrink, Softshrink, GLU, Mish and ERFC.
 *
 * - Activation backward:
 *   - ReLU, ReLU6, Sigmoid, Tanh, GELU, LeakyReLU, Hardsigmoid, Hardswish, Mish and TF_LeakyReLU.
 *
 * Compared with ::cnnlSetActivationDescriptor, cnnlSetActivationDescriptor_v2,
 * cnnlSetActivationDescriptor_v3, cnnlSetActivationDescriptor_v4 and cnnlSetActivationDescriptor_v5,
 * this function supports the \p approximate parameter that allows users to choose different
 * approximation algorithm when \p mode is \p CNNL_ACTIVATION_GELU.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlSetActivationDescAttr instead.
 *
 * @param[in,out] activation_desc
 *   Input/output. The descriptor of the activation operation. For detailed information,
 *   see ::cnnlActivationDescriptor_t.
 * @param[in] mode
 *   Input. The activation \p mode defined in ::cnnlActivationMode_t enum.
 * @param[in] prefer
 *   Input. The \p prefer modes defined in ::cnnlActivationPreference_t enum.
 *   The following activation functions support \p prefer modes for activation forward or backward
 *   operation separately:
 *   - Activation forward:
 *     - Sigmoid, Tanh, SELU, GELU, Hardsigmoid, Hardswish, ELU, ELU_V2, SILU and ERFC.
 *   - Activation backward:
 *     - GELU, Hardswish and SILU.
 *   The default value of this parameter is \p CNNL_ACTIVATION_HIGH_PRECISION.
 * @param[in] nan_prop
 *   Input. The \p nan_prop modes defined in ::cnnlNanPropagation_t enum.
 *   This parameter is only used when you set \p mode to \p CNNL_ACTIVATION_RELU,
 *   \p CNNL_ACTIVATION_RELU6 or \p CNNL_ACTIVATION_SIGMOID in ::cnnlActivationForward.
 * @param[in] coef
 *   Input. A scalar value that is only used when you set \p mode to \p CNNL_ACTIVATION_CLIPPED_RELU,
 *   \p CNNL_ACTIVATION_ELU, \p CNNL_ACTIVATION_LEAKYRELU, \p CNNL_ACTIVATION_TF_LEAKYRELU,
 *   \p CNNL_ACTIVATION_SOFTSHRINK, \p CNNL_ACTIVATION_HARDSHRINK, \p CNNL_ACTIVATION_CAFFE_RELU6,
 *   or \p CNNL_ACTIVATION_ELU_V2.
 * @param[in] sliced_dim
 *   Input. An integer value deciding which dimension of the input to be sliced.
 *   The element number of sliced input dimension must be an even number.
 *   The activation input tensor \p x is sliced in half along \p sliced_dim
 *   to form a (in the lower dimension part) and b (in the higher dimension part).
 *   This parameter is only used when you set \p mode to \p CNNL_ACTIVATION_GLU.
 * @param[in] gamma
 *   Input. A hyper parameter which is only used when you set \p mode to \p CNNL_ACTIVATION_ELU,
 *   \p CNNL_ACTIVATION_HARDSIGMOID or \p CNNL_ACTIVATION_SELU.
 *   (The gamma and scale, as a pair of hyper parameters, have multiple meanings in different
 *   activation \p mode. For details, refer to the "Note" section described below.)
 * @param[in] scale
 *   Input. A hyper parameter which is only used when you set \p mode to \p CNNL_ACTIVATION_ELU,
 *   \p CNNL_ACTIVATION_HARDSIGMOID or \p CNNL_ACTIVATION_SELU.
 *   (The gamma and scale, as a pair of hyper parameters, have multiple meanings in different
 *   activation \p mode. For details, refer to the "Note" section described below.)
 * @param[in] is_result
 *   Input. A Boolean value describes the implementation logic of ::cnnlActivationBackward when
 *   the \p mode is \p CNNL_ACTIVATION_ELU. The default value of this parameter is true.
 *   When the value of this parameter is true, the implementation logic of ELU is as follows:
     @verbatim
      diff_x = scale * diff_y, x > 0;
      diff_x = gamma * (x + scale * coef) * diff_y, x <= 0;
     @endverbatim
 *   When the value of this parameter is false, the implementation logic of ELU is as follows:
     @verbatim
      diff_x = scale * diff_y, x > 0;
      diff_x = scale * coef * gamma * exp(gamma * x) * diff_y, x <= 0;
     @endverbatim
 * @param[in] approximate
 *   Input. A Boolean value describes the implementation logic of different GELU approximation algorithms
 *   when the \p mode is \p CNNL_ACTIVATION_GELU. The default value of this parameter is true.
 *   - When the value is true, the implementation logic of GELU is as follows:
       @verbatim
        output = 0.5 * x * (1 + Tanh(sqrt(2/pi) * (x + 0.044715 * x^3)));
       @endverbatim
 *   - When the value is false, the implementation logic of GELU is as follows:
       @verbatim
        output = 0.5 * x * (1 + erf(x / sqrt(2)));
       @endverbatim
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_NOT_INITIALIZED
 *
 * @par API Dependency
 * - Before calling this function, you need to call the ::cnnlCreateActivationDescriptor function to
 *   create the activation descriptor.
 * - You need to call the ::cnnlDestroyActivationDescriptor function to destroy the descriptor.
 *
 * @note
 * - The parameter settings for multiple activation \p mode in different frameworks are as follows:
 *
 *   - When \p mode is \p CNNL_ACTIVATION_HARDSIGMOID in Activation forward and backward, the semantics of
 *     hardsigmoid in different frameworks can be achieved by setting different values for \p gamma and \p scale. Details:
 *     - PyTorch Framework: \p gamma = 1/6 and \p scale = 1/2.
 *     - TensorFlow Framework: \p gamma = 1/5 and \p scale = 1/2.
 *     - Onnx Framework: \p gamma and \p scale are set according to requirements.
 *     The implementation of HARDSIGMOID in ::cnnlActivationForward is given below:
       @verbatim
        HARDSIGMOID = max(0, min(1, gamma * x + scale));
       @endverbatim
 *
 *   - When \p mode is \p CNNL_ACTIVATION_SELU:
 *     - PyTorch: \p gamma and \p scale should be the default value,
 *       where gamma is 1.67326319217681884765625 and scale is 1.05070102214813232421875.
 *     The implementation of SELU in ::cnnlActivationForward is given below:
       @verbatim
        SELU = scale * gamma * (exp(x) - 1), x <= 0;
        SELU = scale * x, x > 0;
       @endverbatim
 *
 *   - When \p mode is \p CNNL_ACTIVATION_RELU or \p CNNL_ACTIVATION_RELU6 in ::cnnlActivationForward:
 *     - PyTorch: \p nan_prop = CNNL_PROPAGATE_NAN.
 *     - TensorFlow 2.5: \p nan_prop = CNNL_PROPAGATE_NAN.
 *     - TensorFlow 2.4 and earlier versions: \p nan_prop = CNNL_NOT_PROPAGATE_NAN.
 *
 *   - When \p mode is \p CNNL_ACTIVATION_SOFTSHRINK:
 *     - PyTorch: \p coef should be large or equal zero.
 *     The implementation of SOFTSHRINK in ::cnnlActivationForward is given below:
       @verbatim
        SOFTSHRINK = x - coef, x > coef;
        SOFTSHRINK = x + coef, x < -coef;
        SOFTSHRINK = 0, otherwise;
       @endverbatim
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlSetActivationDescAttr)
cnnlStatus_t CNNL_WIN_API cnnlSetActivationDescriptor_v6(cnnlActivationDescriptor_t activation_desc,
                                                         cnnlActivationMode_t mode,
                                                         cnnlActivationPreference_t prefer,
                                                         cnnlNanPropagation_t nan_prop,
                                                         float coef,
                                                         int sliced_dim,
                                                         float gamma,
                                                         float scale,
                                                         bool is_result,
                                                         bool approximate);

// Group:ActivationForward
/*!
 *  @brief Retrieves an activation descriptor \p activation_desc that was previously created with the
 *  ::cnnlSetActivationDescAttr function.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGetActivationDescAttr instead.
 *
 *  @param[in] activation_desc
 *    Input. The descriptor of the input activation parameters. For detailed information,
 *    see ::cnnlActivationDescriptor_t.
 *  @param[out] mode
 *    Output. Pointer to the host memory that holds information about the selected mode.
 *    For detailed information, see ::cnnlActivationMode_t.
 *  @param[out] prefer
 *    Output. Pointer to the host memory that holds information about the precision preference.
 *    For detailed information, see ::cnnlActivationPreference_t.
 *  @param[out] nan_prop
 *    Output. Pointer to the host memory that holds information about the NaN propagation.
 *    For detailed information, see ::cnnlNanPropagation_t.
 *  @param[out] coef
 *    Output. Pointer to the host memory that holds information about the scaling coefficient
 *    used in the ELU, LeakyReLU, TF_LeakyReLU, Caffe_ReLU6, HardShrink or SoftShrink.
 *    For detailed information, see ::cnnlSetActivationDescriptor function.
 *  @param[out] sliced_dim
 *    Output. Pointer to the host memory that holds information about the sliced dimension
 *    used in the GLU.
 *    For detailed information, see ::cnnlSetActivationDescriptor function.
 *  @param[out] gamma
 *    Output. Pointer to the host memory that holds information about a hyper parameter \p gamma
 *    used in the ELU, Hardsigmoid or SELU.
 *    For detailed information, see ::cnnlSetActivationDescriptor function.
 *  @param[out] scale
 *    Output. Pointer to the host memory that holds information about a hyper parameter \p scale
 *    used in the ELU, Hardsigmoid or SELU.
 *    For detailed information, see ::cnnlSetActivationDescriptor function.
 *  @param[out] is_result
 *    Output. Pointer to the host memory that holds information about a Boolean value that describes
 *    ELU back propagation implementation logic, which is only used in the ELU.
 *    For detailed information, see ::cnnlSetActivationDescriptor function.
 *  @param[out] approximate
 *    Output. Pointer to the host memory that holds information about a Boolean value that describes
 *    different GELU approximation algorithms.
 *    For detailed information, see ::cnnlSetActivationDescriptor function.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
CNNL_DEPRECATED_FOR(cnnlGetActivationDescAttr)
cnnlStatus_t CNNL_WIN_API
cnnlGetActivationDescriptor(const cnnlActivationDescriptor_t activation_desc,
                            cnnlActivationMode_t *mode,
                            cnnlActivationPreference_t *prefer,
                            cnnlNanPropagation_t *nan_prop,
                            float *coef,
                            int *sliced_dim,
                            float *gamma,
                            float *scale,
                            bool *is_result,
                            bool *approximate);

// Group:ActivationForward
/*!
 * @brief Applies a specified activation function element-wise over each input
 * value.
 *
 * You can specify the activation function to be used by the \p mode parameter
 * of ::cnnlSetActivationDescAttr.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the activation operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] activation_desc
 *   Input. The descriptor of the activation operation. For detailed information,
 *   see ::cnnlActivationDescriptor_t.
 * @param[in] alpha, beta
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor
 *   of active function in forward propagation.
 * @param[out] y_desc
 *   Output. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor
 *   of active function in forward propagation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Activation Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensors and output tensor should be the same.
 *   The supported data types of input and output tensors are as follows:
 *   - \p x: float, half, bfloat16.
 *   - \p y: float, half, bfloat16.
 *
 *   bfloat16 is only supported on MLU500 series.
 *
 * @par API Dependency
 * - Before calling this function to implement activation, you need to prepare all the
 *   parameters passed to this function. See each parameter description for details.
 * - You need to call the ::cnnlCreateActivationDescriptor and ::cnnlSetActivationDescAttr
 *   functions to create and set the activation descriptor \p activation_desc before calling this function.
 *
 * @note
 * - The input tensor \p x should be in the following range to guarantee the accuracy of output:
 *   - On MLU 200 series:
 *        - When the \p mode is \p CNNL_ACTIVATION_TANH:
 *          - If ::cnnlComputationPreference_t is set to \p CNNL_COMPUTATION_HIGH_PRECISION:
 *            - half: (-inf, -0.005] and [0.005, +inf), where inf represents infinity.
 *            - float: (-inf, -0.002] and [0.002, +inf).
 *          - If ::cnnlComputationPreference_t is set to \p CNNL_COMPUTATION_FAST:
 *            (-inf, -0.02] and [0.02, +inf).
 *        - When the \p mode is \p CNNL_ACTIVATION_SIGMOID:
 *          - If ::cnnlComputationPreference_t is set to \p CNNL_COMPUTATION_HIGH_PRECISION:
 *            - half: [-7.75, +inf).
 *            - float: [-60, 1e20].
 *            The result is 0, when the element value of input tensor \p x is less than -35.
 *          - If ::cnnlComputationPreference_t is set to \p CNNL_COMPUTATION_FAST:
 *            - half: [-1.99, +inf).
 *            - float: (-inf, -40) and [-1.99, +inf).
 *            The result is 0, when the element value of input tensor \p x is less than -7.75; the
 *            result is 1, when the element value of input tensor \p x is greater than 7.75.
 *        - When the \p mode is \p CNNL_ACTIVATION_GELU and the bool parameter \p approximate is set to true:
 *          - If ::cnnlComputationPreference_t is set to \p CNNL_COMPUTATION_HIGH_PRECISION:
 *            - half: (-3.874, -0.003) and (0.003, +inf).
 *          - If ::cnnlComputationPreference_t is set to \p CNNL_COMPUTATION_FAST:
 *            - half: (-2.4, -0.23) and [0.5, +inf).
 *            - float: (-3.874, -0.003) and (0.003, +inf).
 *            The result is -5.96e-8, when the element value of input tensor \p x is less than or equal
 *            to -3.875.
 *        - When the \p mode is \p CNNL_ACTIVATION_GELU and the bool parameter \p approximate is set to false:
 *          - If ::cnnlComputationPreference_t is set to \p CNNL_COMPUTATION_FAST:
 *            - half: (-10, -0.003) and (0.5, +inf).
 *            - float: (-10, -0.003) and (0.003, +inf).
 *        - When the \p mode is \p CNNL_ACTIVATION_ELU:
 *          - If ::cnnlComputationPreference_t is set to \p CNNL_COMPUTATION_HIGH_PRECISION:
 *            - half: (-inf, -0.0091] and [0.026, +inf).
 *            - float: (-inf, -0.09] and [0.03, +inf).
 *          - If ::cnnlComputationPreference_t is set to \p CNNL_COMPUTATION_FAST:
 *            - half: (-inf, -0.44) and [0, +inf).
 *            - float: (-inf, -0.255) and [0.026, +inf).
 *        - When the \p mode is \p CNNL_ACTIVATION_GLU:
 *            - half: [-1.99, +inf).
 *            - float: (-inf, -40) and [-1.99, +inf).
 *        - When the \p mode is \p CNNL_ACTIVATION_SILU:
 *          - If ::cnnlComputationPreference_t is set to \p CNNL_COMPUTATION_FAST:
 *            - half: [-1.99, +inf).
 *            - float: (-inf, -50) and [-1.99, +inf).
 *        - When the \p mode is \p CNNL_ACTIVATION_SELU:
 *          - If ::cnnlComputationPreference_t is set to \p CNNL_COMPUTATION_FAST:
 *            - half: (-inf, -0.05] and [0, +inf).
 *            - float: (-inf, -0.1] and [0.05, +inf).
 *          - If ::cnnlComputationPreference_t is set to \p CNNL_COMPUTATION_HIGH_PRECISION:
 *            - half: (-inf, -0.1] and [0.05, +inf).
 *            - float: (-inf, -0.01) and [0.05, +inf).
 *        - When the \p mode is \p CNNL_ACTIVATION_LOGSIGMOID:
 *            - half: (-inf, 1).
 *            - float: (-inf, 1).
 *   - On MLU300 series, MLU500 series, CE3226 or 1V series:
 *        - When the \p mode is \p CNNL_ACTIVATION_SIGMOID:
 *          - If ::cnnlComputationPreference_t is set to \p CNNL_COMPUTATION_FAST:
 *            - half: [-1.99, +inf).
 *            - float: (-inf, -40) and [-1.99, +inf).
 *            The result is 0, when the element value of input tensor \p x is less than -7.75; the
 *            result is 1, when the element value of input tensor \p x is greater than 7.75.
 *        - When the \p mode is \p CNNL_ACTIVATION_TANH:
 *          - If ::cnnlComputationPreference_t is set to \p CNNL_COMPUTATION_FAST:
 *            - half: (-inf, -0.016] and [0.016, +inf).
 *            - float: (-inf, -0.017] and [0.017, +inf).
 *        - When the \p mode is \p CNNL_ACTIVATION_GELU and the bool parameter \p approximate is set to true:
 *          - If ::cnnlComputationPreference_t is set to \p CNNL_COMPUTATION_HIGH_PRECISION:
 *            - half: (-3.874, -0.003) and (0.003, +inf).
 *          - If ::cnnlComputationPreference_t is set to \p CNNL_COMPUTATION_FAST:
 *            - half: (-2.4, -0.23) and [0.5, +inf).
 *            - float: (-3.874, -0.003) and (0.003, +inf).
 *             The result is -5.96e-8, when the element value of input tensor \p x is less than or equal
 *              to -3.875.
 *        - When the \p mode is \p CNNL_ACTIVATION_GELU and the bool parameter \p approximate is set to false:
 *          - (On 1V) If ::cnnlComputationPreference_t is set to \p CNNL_COMPUTATION_HIGH_PRECISION:
 *            - half: (-inf, -0.01) and (0.01, +inf).
 *            - float: (-inf, -0.01) and (0.01, +inf).
 *          - If ::cnnlComputationPreference_t is set to \p CNNL_COMPUTATION_FAST:
 *            - half: (-inf, -0.01) and (0.01, +inf).
 *            - float: (-inf, -0.01) and (0.01, +inf).
 *        - When the \p mode is \p CNNL_ACTIVATION_ELU:
 *          - If ::cnnlComputationPreference_t is set to \p CNNL_COMPUTATION_FAST:
 *            - half: (-inf, -0.5] and [0.1, +inf).
 *            - float: (-inf, -0.255] and [0.026, +inf).
 *        - When the \p mode is \p CNNL_ACTIVATION_SELU:
 *          - If ::cnnlComputationPreference_t is set to \p CNNL_COMPUTATION_FAST:
 *            - half: (-inf, -0.05] and [0, +inf).
 *            - float: (-inf, -0.1] and [0.1, +inf).
 *          - If ::cnnlComputationPreference_t is set to \p CNNL_COMPUTATION_HIGH_PRECISION:
 *            - half: (-inf, -1e-5] and [-1e-7, +inf).
 *            - float: (-inf, -1e-5] and [0, +inf).
 *        - When the \p mode is \p CNNL_ACTIVATION_SILU:
 *          - If ::cnnlComputationPreference_t is set to \p CNNL_COMPUTATION_FAST:
 *            - half: [-1.99, +inf).
 *            - float: (-inf, -50) and [-1.99, +inf).
 * - Activation forward operation is an element-wise operation. The dimensions of \p x and
 *   \p y must be the same.
 * - You can specify the stride of all dimensions for \p x_desc and \p y_desc with
 *   ::cnnlSetTensorDescriptorEx.
 *
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the activation forward operation is as follows:
     @verbatim
      input array by 1 * 3 * 3 * 1 -->
           x: [[[[-50], [-20], [-15]],
              [[-10], [-5], [5]],
              [[10], [15], [20]]]]

      when the mode is CNNL_ACTIVATION_SIGMOID:
        active_func: y = 1 / (1 + e^(-x))

      output array by 1 * 3 * 3 * 1 -->
           y: [[[[1.9287e-22], [2.0612e-09], [1.8554e-07]],
              [[4.5398e-05], [6.6929e-03], [9.9331e-01]],
              [[9.9995e-01], [1.0000e+00], [1.0000e+00]]]]
     @endverbatim
 *
 * @par Reference
 * - https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/activation.py.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlActivationForward(cnnlHandle_t handle,
                                                const cnnlActivationDescriptor_t activation_desc,
                                                const void *alpha,
                                                const cnnlTensorDescriptor_t x_desc,
                                                const void *x,
                                                const void *beta,
                                                const cnnlTensorDescriptor_t y_desc,
                                                void *y);

// Group:ActivationBackward
/*!
 * @brief Computes the gradient of an activation function.
 *
 * You can specify the activation function to be used by the \p mode parameter of ::cnnlSetActivationDescAttr.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the activation operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] activation_desc
 *   Input. The descriptor of the activation operation. For detailed information,
 *   see ::cnnlActivationDescriptor_t.
 * @param[in] alpha, beta
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] y_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the input tensor.
 *   The corresponding tensor represents the output of active function in forward propagation.
 *   It is also used as an input in ::cnnlActivationBackward, since the backward calculation relies
 *   on forward output.
 *   In ::cnnlActivationBackward, the input \p x and the input \p y cannot be used simultaneously
 *   because \p y can be easily calculated if \p x and forward active equation are given.
 *   That is to say, either \p x or \p y can be applied depending on different \p mode situations or
 *   frameworks requirement.
 *   The value of \p y should be NULL when the \p mode is ReLU, ReLU6, GELU,
 *   LeakyReLU, TF_LeakyReLU or LogSigmoid.
 * @param[in] diff_y_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] diff_y
 *   Input. Pointer to the MLU memory that stores the input tensor containing
 *   the gradient of \p y. It is needed due to the chain rule in backward calculation.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor of forward active function.
 *   The backward calculation is relied on it.
 *   In ::cnnlActivationBackward, the input \p x and the input \p y cannot be used simultaneously
 *   because \p y can be easily calculated if \p x and forward active equation are given.
 *   That is to say, either \p x or \p y can be applied depending on different \p mode or
 *   frameworks requirement.
 *   The value of this parameter should be NULL when the \p mode is Hardsigmoid in paddle framework, Tanh or Sigmoid.
 * @param[out] diff_x_desc
 *   Input. The descriptor of the \p diff_x tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the output tensor containing the gradient of \p x.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Activation Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensors and output tensor should be the same.
 *   The supported data types of input and output tensors are as follows:
 *   - input tensor: float, half, bfloat16.
 *   - output tensor: float, half, bfloat16.
 *
 *   bfloat16 is only supported on MLU500 series.
 *
 * @note
 * - The input tensors \p x and \p diff_y should be in the following range to
 *   guarantee the accuracy of output:
 *   - When the \p mode is \p CNNL_ACTIVATION_GELU:
 *     - Under \p CNNL_DTYPE_HALF, if ::cnnlComputationPreference_t is set to
 *       \p CNNL_COMPUTATION_HIGH_PRECISION, the data range of \p x must be
 *       [-3.5, 500].
 *     - Under \p CNNL_DTYPE_HALF, if ::cnnlComputationPreference_t is set to
 *       \p CNNL_COMPUTATION_FAST, the data range of \p x must be
 *       [-0.5, 500].
 *     - Under \p CNNL_DTYPE_FLOAT, the data range of \p x must be
 *       conditions: [-3.5, 500].
 *   - When the \p mode is \p CNNL_ACTIVATION_SILU:
 *     - On MLU 200 series:
 *       - Under \p CNNL_DTYPE_HALF, if ::cnnlComputationPreference_t is set to
 *         \p CNNL_COMPUTATION_FAST, the data range of \p x must be
 *         [-1.99, +inf).
 *       - Under \p CNNL_DTYPE_FLOAT, if ::cnnlComputationPreference_t is set to
 *         \p CNNL_COMPUTATION_FAST, the data range of \p x must be
 *          (-inf, -40) and [-1.99, +inf).
 *   - When the \p mode is \p CNNL_ACTIVATION_GLU:
 *     - On MLU 200 series:
 *       - The range of \p diff_y is recommended to be in [-50, 50] for higher precision.
 *       - Under \p CNNL_DTYPE_HALF, the data range of \p x must be
 *         [-1.99, 7.75) and [50, +inf).
 *       - Under \p CNNL_DTYPE_FLOAT, the data range of \p x must be
 *         (-inf, -40) and [-1.99, 7.75) and [50, +inf).
 *     - In the \p sliced_dim dimension of \p x, \p diff_x, and \p diff_y, the dimension size
 *       of \p x must be equal to the dimension size of \p diff_x and twice the dimension
 *       size of \p diff_y.
 *     - The data range of \p sliced_dim must be [-dim, dim - 1], where the value of dim means
 *       the dimension number of input tensors.
 *   - When the \p mode is \p CNNL_ACTIVATION_LOGSIGMOID:
 *     - On MLU 200 series:
 *       - The data range of \p x must be (-inf, 15].
 *     - The \p diff_y with NaN or inf is not supported currently.
 *   - On MLU500 series or higher, when the \p mode is \p CNNL_ACTIVATION_SELU or \p CNNL_ACTIVATION_HARDSWISH:
 *     - Under \p CNNL_DTYPE_BFLOAT16, \p CNNL_ACTIVATION_FAST is not supported.
 *   For other activation modes, there are no restrictions on input tensor.
 * - Activation backward operation is an element-wise operation. The dimensions of input and
 *   output tensors must be the same.
 * - When input data contains NAN/infinity:
 *   - When the \p mode is \p CNNL_ACTIVATION_ELU_V2 in ::cnnlActivationBackward:
 *     - On MLU300 series and CE3226:
 *       - If \p x or \p diff_y is NaN, then \p diff_x is NaN.
 *       - If \p x is positive infinity, then \p diff_x is NaN.
 *       - If \p x is negative infinity and \p diff_y is infinity, then \p diff_x is NaN.
 *       - If \p x is negative infinity and \p diff_y is not set as infinity, then \p diff_x is 0.
 *       - If \p x is not set as negative infinity and \p diff_y is positive infinity, then \p diff_x is positive infinity.
 *       - If \p x is not set as positive infinity and \p diff_y is negative infinity, then \p diff_x is negative infinity.
 *   - When the \p mode is \p CNNL_ACTIVATION_SIGMOID:
 *     - On MLU 500 series:
 *       - Supports tensor broadcasting with data type: float, half, bfloat16.
 *       - To satisfy the broadcast conditions, the length of each dimension of the two input tensors
 *         should be the same or one of them should be equal to 1, and meanwhile, each dimension of the output
 *         tensor should be equal to the larger one between corresponding dimensions of two input
 *         tensors.
 *
 * @par API Dependency
 * - Before calling this function to implement activation, you need to prepare all the
 *   parameters passed to this function. See each parameter description for details.
 * - You need to call the ::cnnlCreateActivationDescriptor and ::cnnlSetActivationDescAttr
 *   functions to create and set the activation descriptor \p activation_desc before calling this function.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the activation backward operation is as follows:
     @verbatim
       input array by 1 * 3 * 3 * 1 -->
            x: [[[[-100], [-80], [-70]],
               [[-50], [-20], [-10]],
               [[-7.75], [-5], [3]]]]

            diff_y: [[[[1], [1], [1]],
                    [[1], [1], [1]],
                    [[1], [1], [1]]]]

      when the mode is CNNL_ACTIVATION_SILU:
        active_func in forward : y = x * sigmoid(x) = x / (1 + e^(-x))
        active_func in backward: diff_x = diff_y * sigmoid(x) * (1+x* (1-sigmoid(x)))

      output array by 1 * 3 * 3 * 1 -->
           diff_x: [[[[-0.0000e+00], [-1.4258e-33], [-2.7431e-29]],
                   [[-9.4509e-21], [-3.9162e-08], [-4.0856e-04]],
                   [[-2.9048e-03], [-2.6546e-02], [1.0881e+00]]]]
     @endverbatim
 * - When the \p mode is \p CNNL_ACTIVATION_ELU_V2 in ::cnnlActivationBackward, the example is as follows:
     @verbatim
      x:      [  1, NaN, +inf, +inf, +inf, -inf, -inf, -inf,   -3,    3 ]
      diff_y: [NaN,   3,    3, +inf, -inf, +inf, -inf,    3,  +inf, -inf ]
      diff_x: [NaN, NaN,  NaN,  NaN,  NaN,  NaN,  NaN     0,  +inf, -inf ]
     @endverbatim
 *
 * @par Reference
 * - https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/activation.py.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlActivationBackward(cnnlHandle_t handle,
                                                 const cnnlActivationDescriptor_t activation_desc,
                                                 const void *alpha,
                                                 const cnnlTensorDescriptor_t y_desc,
                                                 const void *y,
                                                 const cnnlTensorDescriptor_t diff_y_desc,
                                                 const void *diff_y,
                                                 const cnnlTensorDescriptor_t x_desc,
                                                 const void *x,
                                                 const void *beta,
                                                 const cnnlTensorDescriptor_t diff_x_desc,
                                                 void *diff_x);

// Group:CustomizedActive
/*!
 * @brief Creates a descriptor pointed by \p customized_active_desc for a customized active
 *        operation, and allocates memory for holding the information about the customized
 *        active operation. The information is defined in ::cnnlCustomizedActiveDescriptor_t.
 *        For more information about descriptor, see "Cambricon CNNL User Guide".
 *
 * @param[out] customized_active_desc
 *  Output. A host pointer to the customized active descriptor that holds information
 *  about the customized active operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetCustomizedActiveDescriptor function
 *   to initialize and set the information to the customized active descriptor.
 * - You need to call the ::cnnlDestroyCustomizedActiveDescriptor function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCreateCustomizedActiveDescriptor(cnnlCustomizedActiveDescriptor_t *customized_active_desc);

// Group:CustomizedActive
/*!
 * @brief Destroys a customized active descriptor \p customized_active_desc that
 *        was previously created with the ::cnnlCreateCustomizedActiveDescriptor function.
 *
 * The customized active descriptor is defined in ::cnnlCustomizedActiveDescriptor_t
 * and holds the information about the customized active operation.
 *
 *
 * @param[in] customized_active_desc
 *   Input. The customized active descriptor to be destroyed.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - You need to call this function after calling the ::cnnlCustomizedActiveForward or
 *   ::cnnlCustomizedActiveForward_v2 function.
 *   Otherwise, ::CNNL_STATUS_BAD_PARAM is returned.
 * - This function should be called to destroy the customized active descriptor.
 *   Otherwise, the memory leak may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlDestroyCustomizedActiveDescriptor(cnnlCustomizedActiveDescriptor_t customized_active_desc);

// Group:CustomizedActive
/*!
 * @brief Initializes the customized active descriptor \p customized_active_desc that
 * was previously created with the ::cnnlCreateCustomizedActiveDescriptor function, and sets
 * the information about the customized active operation to the customized active descriptor
 * \p customized_active_desc. The information includes the number of the range of
 * independent variables of activation function \p x_begin and \p x_end, the infimum of
 * activation function within the effective range \p y_min, and the number of segment of the
 * activation function \p segment_num.
 *
 * @param[in,out] customized_active_desc
 *  Input/output. Description of customized active operation. For detailed information,
 *  see ::cnnlCustomizedActiveDescriptor_t.
 * @param[in] x_begin
 *   Input. The starting value of activation function domain.
 * @param[in] x_end
 *   Input. The end value of activation function domain.
 * @param[in] y_min
 *   Input. The infimum of activation function within the effective range.
 * @param[in] segment_num
 *   Input. The number of segment of the activation function. When implemented,
 *   segment the activation function domain linearly according to this number.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetCustomizedActiveDescriptor(cnnlCustomizedActiveDescriptor_t customized_active_desc,
                                  float x_begin,
                                  float x_end,
                                  float y_min,
                                  int segment_num);

// Group:CustomizedActive
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an
 * extra workspace to optimize the customized active operation.
 *
 * The size of the extra workspace is based on the given information of the customized
 * active operation, including the input tensor descriptor \p x,
 * customized active descriptor \p desc_t. For more information about the workspace,
 * see "Cambricon CNNL User Guide".
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the customized active operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] desc_t
 *   Input.The descriptor of the customized active operation. For detailed information,
 *   see ::cnnlCustomizedActiveDescriptor_t.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *   the customized active operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlCreateTensorDescriptor and
 *   ::cnnlSetTensorDescriptor functions to create and set the tensor
 *   descriptors \p x before calling this function.
 * - The allocated extra workspace should be passed to the ::cnnlCustomizedActiveForward or
 *   ::cnnlCustomizedActiveForward_v2 function to perform the customized active operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetCustomizedActiveForwardWorkspaceSize(cnnlHandle_t handle,
                                            const cnnlTensorDescriptor_t x_desc,
                                            const cnnlCustomizedActiveDescriptor_t desc_t,
                                            size_t *size);

// Group:CustomizedActive
/*!
 * @brief Computes an active function on input tensor \p x with the function pointer
 *        \p active_func, and returns the results in the output tensor \p y.
 *        This function is used to define your own activation function.
 *        You can call ::cnnlActivationForward to implement pre-defined activation
 *        functions. To define your own activation functions, you can get more
 *        information in ::active_func_type. Different from ::cnnlCustomizedActiveForward,
 *        ::cnnlCustomizedActiveForward_v2 needs to allocate and initialize extra input space.
 *        You can get more information in ::cnnlGetCustomizedActiveForwardExtraInputSize and
 *        ::cnnlInitCustomizedActiveForwardExtraInput.
 *
 * This function needs extra MLU memory as the workspace to work.
 * You can get the workspace size with the
 * ::cnnlGetCustomizedActiveForwardWorkspaceSize function.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the customized active operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] customized_active_desc
 *   Input. The descriptor of the customized active operation. For detailed information,
 *   see ::cnnlCustomizedActiveDescriptor_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   customized active operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the customized active operation. You can get the size of the workspace with
 *   the ::cnnlGetCustomizedActiveForwardWorkspaceSize function.
 * @param[out] y_desc
 *   Output. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] active_func
 *   Input. The active function of the customized active operation. For detailed information,
 *   see ::active_func_type.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Customized Active Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 *   This function supports the combinations of input tensor and output tensor must be
 *   half-half or float-float.
 *
 * @par Scale Limitation
 * - Customized activation operation is an element-wise operation.
 *   The dimensions of input tensor and output tensor must be the same.
 *
 * @par API Dependency
 * - Before calling this function to implement the customized active operation,
 *   you need to prepare all the parameters passed to this function.
 *   See each parameter description for details.
 *
 * @note
 * - This operation does not support NaN/infinity in input and output data.
 * - This operation is not supported on the 1V platforms.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the customized active operation is as follows:
     @verbatim
      input array by 1 * 3 * 3 * 2 -->
          input: [[[[5, 1], [-8, 1], [6, 4]],
               [[3, 8], [2,-6], [0, 6]],
               [[8, 5], [7,4], [9, -6]]]]

      param:
        active_func: y = x if (x > 0)
                     y = 0 if (x <= 0)

      output array by 1 * 3 * 3 * 2 -->
          output: [[[[5, 1], [0, 1], [6, 4]],
                [[3, 8], [2,0], [0, 6]],
                [[8, 5], [7,4], [9, 0]]]]
     @endverbatim
 *
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlCustomizedActiveForward(cnnlHandle_t handle,
                            const cnnlCustomizedActiveDescriptor_t customized_active_desc,
                            const cnnlTensorDescriptor_t x_desc,
                            const void *x,
                            const cnnlTensorDescriptor_t y_desc,
                            void *y,
                            void *workspace,
                            size_t workspace_size,
                            active_func_type active_func);

// Group:CustomizedActive
/*!
 * @brief Returns in \p extra_size the size of the MLU memory and host memory that is used as an extra
 * input data to optimize the customized active operation. You need to allocate memory both on host and
 * MLU based on the size returned in \p extra_size.
 *
 * The size of extra input data is based on the given information of the customized active operation,
 * including the input tensor descriptor \p x, customized active descriptor \p desc_t.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the customized active operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] desc_t
 *   Input. The descriptor of the customized active operation. For detailed information,
 *   see ::cnnlCustomizedActiveDescriptor_t.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra input data in bytes
 *   that is used in the customized active operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor
 *   functions to create and set the tensor descriptors \p x.
 * - After calling this function, you need to call ::cnnlInitCustomizedActiveForwardExtraInput to
 *   initialize the memory on host.
 * - The allocated extra input should be passed to the ::cnnlCustomizedActiveForward_v2 function
 *   to perform the customized active operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetCustomizedActiveForwardExtraInputSize(cnnlHandle_t handle,
                                             const cnnlTensorDescriptor_t x_desc,
                                             const cnnlCustomizedActiveDescriptor_t desc_t,
                                             size_t * size);

// Group:CustomizedActive
/*!
 * @brief Initializes the extra input data space \p extra_host_input.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the customized active operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] desc_t
 *   Input.The descriptor of the customized active operation. For detailed information,
 *   see ::cnnlCustomizedActiveDescriptor_t.
 * @param[in] active_func
 *   Input. The active function of the customized active operation.
 *   For detailed information, see ::active_func_type.
 * @param[out] extra_host_input
 *   Output. Pointer to the host memory that is used as an extra input space
 *   for the customized active operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to get the size of the extra input data with ::cnnlGetCustomizedActiveForwardExtraInputSize.
 *   The memory of the extra input data should be allocated before calling this function.
 * - This function must be called after the ::cnnlCreateTensorDescriptor and
 *   ::cnnlSetTensorDescriptor functions to create and set the tensor descriptors
 *   \p x_desc.
 * - The allocated extra input should be passed to the ::cnnlCustomizedActiveForward_v2 function
 *   to perform the customized active operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlInitCustomizedActiveForwardExtraInput(cnnlHandle_t handle,
                                          const cnnlTensorDescriptor_t x_desc,
                                          const cnnlCustomizedActiveDescriptor_t desc_t,
                                          active_func_type active_func,
                                          void * extra_host_input);

// Group:CustomizedActive
/*!
 * @brief Computes an active function on input tensor \p x with the function pointer
 *        \p active_func, and returns the results in the output tensor \p y.
 *        This function is used to define your own activation function.
 *        You can call ::cnnlActivationForward to implement Cambricon CNNL pre-defined activation
 *        functions. To define your own activation functions, you can get more
 *        information in ::active_func_type. Compared with ::cnnlCustomizedActiveForward,
 *        ::cnnlCustomizedActiveForward_v2 provides better performance with extra input space.
 *
 * This function needs extra MLU memory as the workspace to work.
 * You can get the workspace size with the
 * ::cnnlGetCustomizedActiveForwardWorkspaceSize function.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the customized active operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] customized_active_desc
 *   Input. The descriptor of the customized active operation. For detailed information,
 *   see ::cnnlCustomizedActiveDescriptor_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] extra_device_input
 *   Input. Pointer to the MLU memory that stores the extra input data. You need to copy
 *   the extra input data to MLU from the host that is initialized with
 *   ::cnnlInitCustomizedActiveForwardExtraInput. For more information
 *   about extra input data, see "Cambricon CNNL User Guide".
 * @param[in] extra_input_size
 *   Input. The size of the extra input data in bytes that needs to be used in
 *   the customized active operation. You can get the size of the extra input data with
 *   the ::cnnlGetCustomizedActiveForwardExtraInputSize function.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   customized active operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the customized active operation. You can get the size of the workspace with
 *   the ::cnnlGetCustomizedActiveForwardWorkspaceSize function.
 * @param[out] y_desc
 *   Output. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] active_func
 *   Input. The active function of the customized active operation. For detailed information,
 *   see ::active_func_type.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Customized Active Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 *   This function supports the combinations of input tensor and output tensor must be
 *   half-half or float-float.
 *
 * @par Scale Limitation
 * - Customized activation operation is an element-wise operation.
 *   The dimensions of input tensor and output tensor must be the same.
 *
 * @par API Dependency
 * - Before calling this function to implement the customized active operation,
 *   you need to prepare all the parameters passed to this function.
 *   See each parameter description for details.
 *
 * @note
 * - The operation does not support NaN/infinity in input and output data.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the customized active operation is as follows:
     @verbatim
      input array by 1 * 3 * 3 * 2 -->
          input: [[[[5, 1], [-8, 1], [6, 4]],
               [[3, 8], [2,-6], [0, 6]],
               [[8, 5], [7,4], [9, -6]]]]

      param:
        active_func: y = x if (x > 0)
                     y = 0 if (x <= 0)

      output array by 1 * 3 * 3 * 2 -->
          output: [[[[5, 1], [0, 1], [6, 4]],
                [[3, 8], [2,0], [0, 6]],
                [[8, 5], [7,4], [9, 0]]]]
     @endverbatim
 *
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlCustomizedActiveForward_v2(cnnlHandle_t handle,
                               const cnnlCustomizedActiveDescriptor_t customized_active_desc,
                               const cnnlTensorDescriptor_t x_desc,
                               const void * x,
                               const void * extra_device_input,
                               size_t extra_input_size,
                               const cnnlTensorDescriptor_t y_desc,
                               void * y,
                               void * workspace,
                               size_t workspace_size,
                               active_func_type active_func);

// Group:BboxOverlaps
/*!
 * @brief Computes the IOUs (intersection over union) or IOFs (intersection over foreground) between two sets of
 *        bounding-boxes. If \p aligned is False, then calculates the IOUs of each row between each bounding-box
 *        of \p bbox1 and \p bbox2, otherwise calculates the IOUs of the corresponding row between each aligned
 *        pair of \p bbox1 and \p bbox2. For input placed in the order of <x1, y1, x2, y2>, (x1, y1) and (x2, y2)
 *        respectively represents the top-left and bottom-right corner coordinates of bounding-box.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   bounding-box overlaps operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] mode
 *   Input. An integer value that decides to return a result IOUs or IOF.
 *   The integer 0 represents IOU and 1 represents IOF.
 * @param[in] aligned
 *   Input. A Boolean value. If it is False, then calculate the IOUs[i][j] or IOFs[i][j] between
 *   the row i of \p bbox1 and the row j of \p bbox2, otherwise the IOUs[i] or IOFs[i] between
 *   the row i of \p bbox1 and the row i of \p bbox2 are calculated. The number of row of \p bbox1
 *   and \p bbox2 must be equal for aligned is True.
 * @param[in] offset
 *   Input. An integer value determines whether to increase the length and the width of the bounding-box by 0 or 1
 *   before calculating the area.
 * @param[in] bbox1_desc
 *   Input. The descriptor of the input tensor \p bbox1. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] bbox1
 *   Input. Pointer to the MLU memory that stores the input tensor \p bbox1.
 * @param[in] bbox2_desc
 *   Input. The descriptor of the input tensor \p bbox2. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] bbox2
 *   Input. Pointer to the MLU memory that stores the input tensor \p bbox2.
 * @param[in] ious_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] ious
 *   Output. IOUs or IOFs. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "Bounding-Box Overlaps Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following combination of data types by the order of
 *   \p bbox1 - \p bbox2 - \p ious:
 *   - float - float - float.
 *
 * @par Scale Limitation
 * - The number of dimensions of \p bbox1 and \p bbox2 tensors must be 2.
 * - The length of lowest dimension of input tensor must be 4.
 *   \p bbox1 (Tensor): shape [m, 4] in <x1, y1, x2, y2> format.
 *   \p bbox2 (Tensor): shape [n, 4] in <x1, y1, x2, y2> format.
 * - Input with NaN is not supported currently. Also you need to exclude the input with (inf - inf) or (inf - (-inf)),
 *   where inf represents infinity (because the result is NaN, the actual impact is that the input has NaN).
 * - For input in type <x1, y1, x2, y2>, the coordinates must satisfy x2 > x1, y2 > y1,
 *   otherwise incorrect results will be obtained.
 * - When aligned mode is True, for input \p bbox1 and \p bbox2 with n-rows, if n is zero, the output IOUs
 *   must be a two-dimensional matrix with shape n * 1, otherwise the output IOUs must be a one-dimensional
 *   array with n-elements. When aligned mode is False, for input \p bbox1 with n-rows and
 *   \p bbox2 with m-rows, the output IOUs must be a two-dimensional matrix with shape n * m.
 *
 * @par API Dependency
 * - None.
 *
 * @note
 * - The input tensor \p x should be in the following range to guarantee the accuracy of output:
 *  - If bbox_overlaps works on (m)tp_2xx :
 *    - half : [-300, 100].
 *    - float : [-300, 100].
 * - This operation is not supported on 1V platforms.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the bounding-box overlaps operation is as follows:
     @verbatim
      input array by 3 * 4, type is float -->
          input: bbox1 = [
            [0, 0, 10, 10],
            [10, 10, 20, 20],
            [32, 32, 38, 42],
          ]
      input array by 3 * 4, type is float -->
          input: bbox2 = [
            [0, 0, 10, 20],
            [0, 10, 10, 19],
            [10, 10, 20, 20],
          ]
      param:
        mode = 0
        aligned = False
        offset = 0

      output array by 3 * 3, type is float -->
          output: [[0.5000, 0.0000, 0.0000],
                   [0.0000, 0.0000, 1.0000],
                   [0.0000, 0.0000, 0.0000]]
     @endverbatim
 *
 * @par Reference
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlBboxOverlaps(cnnlHandle_t handle,
                                           const int mode,
                                           const bool aligned,
                                           const int offset,
                                           const cnnlTensorDescriptor_t bbox1_desc,
                                           const void *bbox1,
                                           const cnnlTensorDescriptor_t bbox2_desc,
                                           const void *bbox2,
                                           const cnnlTensorDescriptor_t ious_desc,
                                           void *ious);

// Group:BoxIouRotated
/*!
 * @brief Computes the intersection-over-union (Jaccard index, IOU) of rotated
 *        bounding-boxes. If \p aligned is false, then calculate the IOUs
 *        between each rotated bounding-box of \p bbox1 and \p bbox2, otherwise
 *        the IOUs between each aligned pair of rotated bounding-box of \p bbox1
 *        and \p bbox2.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the box iou rotated operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] mode
 *   Input. An integer value that decides to return a result of
 *   IOUs (intersection over union) or IOFs (intersection over foreground).
 *   The integer 0 represents IOU and 1 represents IOF.
 * @param[in] aligned
 *   Input. A Boolean value. If it is false, then calculate the IOUs[i][j]
 *   or IOFs[i][j] between the row i of \p bbox1 and the row j of \p bbox2,
 *   otherwise calculate the IOUs[i] or IOFs[i] between the row i of \p bbox1
 *   and the row i of \p bbox2. Significantly, the number of row of \p bbox1
 *   and \p bbox2 must be equal when \p aligned is true.
 * @param[in] bbox1_desc
 *   Input. The descriptor of the input tensor \p bbox1 (rotated bounding-box).
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] bbox1
 *   Input. Pointer to the MLU memory that stores the input tensor \p bbox1.
 *   It has shape (n, 5), indicating (x, y, w, h, theta) for each row.
 *   Note that theta is in radian.
 * @param[in] bbox2_desc
 *   Input. The descriptor of the input tensor \p bbox2 (rotated bounding-box).
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] bbox2
 *   Input. Pointer to the MLU memory that stores the input tensor \p bbox2.
 *   It has shape (m, 5), indicating (x, y, w, h, theta) for each row.
 *   Note that theta is in radian.
 * @param[in] ious_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] ious
 *   Output. IOUs or IOFs of input rotated bounding-boxes. Pointer to the MLU
 *   memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "Box Iou Rotated Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - By the order of \p bbox1 - \p bbox2 - \p ious, the supported data types of
 *    \p bbox1, \p bbox2 and \p ious are as follows:
 *   - float - float - float.
 *
 * @par Scale Limitation
 * - The number of dimensions of \p bbox1 and \p bbox2 tensors must be 2.
 * - The length of lowest dimension of \p bbox1 and \p bbox2 tensors must be 5.
 * - Both sets of boxes are expected to be in
 *   (x_center, y_center, width, height, angle) format.
 *   - \p bbox1 (Tensor): shape [n, 5] in (x, y, w, h, theta) format.
 *   - \p bbox2 (Tensor): shape [m, 5] in (x, y, w, h, theta) format.
 * - When aligned mode is true, for input \p bbox1 and \p bbox2 with n-rows,
 *   the output \p ious must be a one-dimensional array with n-elements. When
 *   \p aligned is false, for input \p bbox1 with n-rows and \p bbox2 with
 *   m-rows, the output \p ious must be a two-dimensional matrix with shape n*m.
 *
 * @note
 * - When finding the point with minimum y and minimum x in convex-hull-graham,
 *   BoxIouRotated performs min-pooling operation. If the input data of pooling contains NaN:
 *   - On MLU200 series:
 *    - The \p output value is the NaN.
 *   - On MLU300 series and CE3226:
 *    - If the last value in the kernel of the pooling is NaN, the \p output value is NaN.
 *      Otherwise, the \p output value is the minimum value after the last NaN.
 * - This operation is not supported on 1V platforms.
 *
 * @par API Dependency
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Reference
 * - http://github.com/open-mmlab/mmcv/blob/master/mmcv/ops/box_iou_rotated.py
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlBoxIouRotated(cnnlHandle_t handle,
                                            const int mode,
                                            const bool aligned,
                                            const cnnlTensorDescriptor_t bbox1_desc,
                                            const void *bbox1,
                                            const cnnlTensorDescriptor_t bbox2_desc,
                                            const void *bbox2,
                                            const cnnlTensorDescriptor_t ious_desc,
                                            void *ious);
// Group:Inverse
/*!
 * @brief Computes the inverse of the square matrix \p input. \p input tensor must be a 2D square
 *        matrix, or its batches are arranged in high dimension. This function also returns a
 *        1D tensor \p infos of the same size as the number of input's batch that indicates
 *        the exception of each batch when the \p input is inverted.
 *
 * You can choose to return a row-major or column-major order result by specifying \p is_trans.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlInverse_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   inverse operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the inverse operation. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] is_trans
 *   Input. A Boolean value to decide to return a result by row-major or column-major order.
 *   When \p is_trans is true, the column-major order result will be returned; When \p is_trans is false,
 *   the row-major order result will be returned.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] infos_desc
 *   Input. The descriptor of the infos tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] infos
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *   In \p infos, the element values are all integers.
 *   - If the i'th value is 0, the i'th square matrix is invertible.
 *   - If the i'th value is 1, the i'th square matrix is not invertible.
 *   - If the i'th value is -1, the i'th square matrix has an illegal input, i.e. "NaN" or "Inf".
 *   The format of \p infos has to be a 1D tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par Formula
 * - None.
 *
 * @par Data Type
 * - Data type of the input tensor and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: float.
 *   - output tensor: float.
 *   - 1D infos tensor: int32.
 *
 * @par Scale Limitation
 * - Input tensor formats must be greater than one.
 * - The two lowest dimensions of input tensor should be equal, i.e. a square matrix.
 * - The dimension of the square matrix in input tensor should not exceed a specific value,
 *   which depends on the NRAM size.
 *
 * @par API Dependency
 * - None.
 *
 * @note
 * - If a matrix is not invertible, there is no guarantee of result correctness. It may simply return a garbage
 *   result in that batch.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the inverse operation is as follows:
     @verbatim
      input tensor x[3, 100, 156, 156], is_trans:true, output tensor y[3, 100, 156, 156], output tensor infos[3*100].
      The input tensor x will be computed the inverse and return to output tensor y through column-major order and get
      a 1D infos tensor indicating the exception of each batch when the input tensor x is inverted.
     @endverbatim
 *
 * @par Reference
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlInverse_v2)
cnnlStatus_t CNNL_WIN_API cnnlInverse(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t input_desc,
                                      const void *input,
                                      const bool is_trans,
                                      const cnnlTensorDescriptor_t output_desc,
                                      void *output,
                                      const cnnlTensorDescriptor_t infos_desc,
                                      void *infos);


// Group:BoxOverlapBev
/*!
 * @brief Computes the overlaps between each rotated bounding-box of \p boxes1 and \p boxes2.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the box overlap bev operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] boxes1_desc
 *   Input. The descriptor of the input tensor \p boxes1 (rotated bounding-box).
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] boxes1
 *   Input. Pointer to the MLU memory that stores the input tensor \p boxes1.
 *   It has shape (n, 7), with the number 7 indicating (x, y, z, dx, dy, dz, heading) for each row.
 *   Note that heading is in radian.
 * @param[in] boxes2_desc
 *   Input. The descriptor of the input tensor \p boxes2 (rotated bounding-box).
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] boxes2
 *   Input. Pointer to the MLU memory that stores the input tensor \p boxes2.
 *   It has shape (m, 7), with the number 7 indicating (x, y, z, dx, dy, dz, heading) for each row.
 *   Note that heading is in radian.
 * @param[in] overlaps_desc
 *   Input. The descriptor of the output tensor \p overlaps. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] overlaps
 *   Output. Pointer to the MLU memory that stores the output tensor \p overlaps.
 *   \p overlaps is the overlapped area of rotated bounding boxes.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "Box Overlap Bev Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - By the order of \p boxes1 - \p boxes2 - \p overlaps, the supported data types of
 *    \p boxes1, \p boxes2 and \p overlaps are as follows:
 *   - float - float - float.
 *
 * @par Scale Limitation
 * - The number of dimensions of \p boxes1 and \p boxes2 tensors must be 2.
 * - The length of lowest dimension of \p boxes1 and \p boxes2 tensors must be 7.
 * - Both sets of boxes are expected to be in
 *   - \p boxes1 (Tensor): shape [n, 7] in (x, y, z, dx, dy, dz, heading) format.
 *   - \p boxes2 (Tensor): shape [m, 7] in (x, y, z, dx, dy, dz, heading) format.
 * - For input \p boxes1 with n-rows and \p boxes2 with
 *   m-rows, the output \p overlaps must be a two-dimensional matrix with shape n*m.
 * - For input \p boxes1 with n-rows and \p boxes2 with m-rows, the n-rows and m-rows must
 *   be less than 1000.
 *
 * @note
 * - Input with NaN or Infinity is not supported currently.
 *
 * @par API Dependency
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Reference
 * - None.
 */

cnnlStatus_t CNNL_WIN_API cnnlBoxOverlapBev(cnnlHandle_t handle,
                                            const cnnlTensorDescriptor_t boxes1_desc,
                                            const void *boxes1,
                                            const cnnlTensorDescriptor_t boxes2_desc,
                                            const void *boxes2,
                                            const cnnlTensorDescriptor_t overlaps_desc,
                                            void *overlaps);

// Group:Grep
/*!
 * @brief Creates a descriptor pointed by \p grep_desc for a grep operation. The information
 *        is defined in ::cnnlGrepDescriptor_t.
 *
 * @param[out] grep_desc
 *   Output. A host pointer to the grep descriptor that holds information about the grep operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetGrepDescriptor function to initialize
 *   and set the information to the grep descriptor.
 * - You need to call the ::cnnlDestroyGrepDescriptor function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateGrepDescriptor(cnnlGrepDescriptor_t *grep_desc);

// Group:Grep
/*!
 * @brief Destroys a grep descriptor \p grep_desc that was previously created with the
 *        ::cnnlCreateGrepDescriptor function.
 *
 * The grep descriptor is defined in ::cnnlGrepDescriptor_t and holds the information
 * about the grep operation.
 *
 * @param[in] grep_desc
 *   Input. The grep descriptor to be destroyed.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - You need to call this function after calling the ::cnnlGrep function.
 *   Otherwise, ::CNNL_STATUS_BAD_PARAM is returned.
 * - This function should be called to destroy the grep descriptor.
 *   Otherwise, the memory leak may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyGrepDescriptor(cnnlGrepDescriptor_t grep_desc);

// Group:Grep
/*!
 * @brief Initializes the grep descriptor \p grep_desc that was previously created with
 * the ::cnnlCreateGrepDescriptor function, and sets the information about the grep
 * operation to the grep descriptor \p grep_desc. The information includes the beginning coordinates
 * use to crop the input tensor \p begin and \p mlu_begin, the element number to crop the input tensor
 * \p size and \p mlu_size, element number of the input tensor \p mlu_input_dim, the value to fill the
 * area when crop exceeds the boundary of input tensor \p space_number, the stride of the input tensor
 * \p mlu_mlutiplier, and the stride of the output tensor \p mlu_divisor.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlSetGrepDescriptor_v4 instead.
 *
 * @param[in,out] grep_desc
 *   Input/output. The descriptor of the grep operation. For detailed information, see ::cnnlGrepDescriptor_t.
 * @param[in] begin
 *   Input. Pointer to the host memory that saves the beginning coordinates used to crop
 *   the input tensor for each dimension.
 * @param[in] size
 *   Input. Pointer to the host memory that saves the element number used to crop the input tensor
 *   in each dimension.
 * @param[in] space_number
 *   Input. When the cropping area exceeds the boundary of the input tensor, this value will be filled in
 *   the empty space. The default value of this parameter is 0.
 * @param[in] mlu_input_dim
 *   Input. Pointer to the MLU memory that saves the element number of the input tensor in each dimension.
 * @param[in] mlu_begin
 *   Input. Pointer to the MLU memory that saves the beginning coordinates used to crop the
 *   input tensor for each dimension.
 * @param[in] mlu_size
 *   Input. Pointer to the MLU memory that saves the element number used to crop the input tensor
 *   in each dimension.
 * @param[in] mlu_mlutiplier
 *   Input. Pointer to the MLU memory that saves the stride of the input tensor in each dimensions
 *   except the lowest dimension.
 * @param[in] mlu_divisor
 *   Input. Pointer to the MLU memory that saves the stride of the output tensor in each dimensions
 *   except the lowest dimension.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS
 *
 * @note
 * - Currently, all tensor formats could be any dimension greater than one.
 * - Before calling this function, you need to call the ::cnnlCreateGrepDescriptor
 *   function to create the \p grep_desc.
 * - You need to call the ::cnnlDestroyGrepDescriptor function to destroy the descriptor.
 * - ::cnnlDestroyGrepDescriptor() needs to be called after this function.
 *
 * @par Scale Limitation
 * - The number of dimensions of the \p begin must be greater than one. The value of
 *   each dimension cannot be negative and cannot exceed the size of corresponding
 *   dimension of the input tensor.
 * - The number of dimensions of the \p size must be greater than one. The value
 *   of each dimension must be greater than zero.
 * - The number of dimensions of the \p begin must be equal to the number of dimensions
 *   of the \p size.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlSetGrepDescriptor_v4)
cnnlStatus_t CNNL_WIN_API cnnlSetGrepDescriptor(cnnlGrepDescriptor_t grep_desc,
                                                int32_t begin[],
                                                int32_t size[],
                                                float space_number,
                                                int32_t *mlu_input_dim,
                                                int32_t *mlu_begin,
                                                int32_t *mlu_size,
                                                int32_t *mlu_mlutiplier,
                                                int32_t *mlu_divisor);

// Group:Grep
/*!
 * @brief Initializes the grep descriptor \p grep_desc that was previously created with
 * the ::cnnlCreateGrepDescriptor function, and sets the information about the grep
 * operation to the grep descriptor \p grep_desc. The information includes the beginning coordinates
 * use to crop the input tensor \p begin, the element number to crop the input tensor, the value to
 * fill the area when crop exceeds the boundary of input tensor \p space_number.
 *
 * Different from ::cnnlSetGrepDescriptor, ::cnnlSetGrepDescriptor_v2 does not need to input the pointers
 * to the MLU memory.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlSetGrepDescriptor_v4 instead, which can prevent memory leakage.

 * @param[in,out] grep_desc
 *   Input/output. The descriptor of the grep operation. For detailed information, see ::cnnlGrepDescriptor_t.
 * @param[in] begin
 *   Input. Pointer to the host memory that saves the beginning coordinates used to crop
 *   the input tensor for each dimension.
 * @param[in] size
 *   Input. Pointer to the host memory that saves the element number used to crop the input tensor
 *   in each dimension.
 * @param[in] space_number
 *   Input. When the cropping area exceeds the boundary of the input tensor, this value will be filled in
 *   the empty space. The default value of this parameter is 0.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS
 *
 * @note
 * - Currently, all tensor formats could be any dimension greater than one.
 * - Before calling this function, you need to call the ::cnnlCreateGrepDescriptor
 *   function to create the \p grep_desc.
 * - You need to call the ::cnnlDestroyGrepDescriptor function to destroy the descriptor.
 * - ::cnnlDestroyGrepDescriptor() needs to be called after this function.
 *
 * @par Scale Limitation
 * - The number of dimensions of the \p begin must be greater than zero. The value of
 *   each dimension cannot be negative and cannot exceed the size of corresponding
 *   dimension of the input tensor.
 * - The number of dimensions of the \p size must be greater than zero. The value
 *   of each dimension must be greater than zero.
 * - The number of dimensions of the \p begin must be equal to the number of dimensions
 *   of the \p size.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlSetGrepDescriptor_v4)
cnnlStatus_t CNNL_WIN_API cnnlSetGrepDescriptor_v2(cnnlGrepDescriptor_t grep_desc,
                                                   int32_t *begin,
                                                   int32_t *size,
                                                   float space_number);

// Group:Grep
/*!
 * @brief Initializes the grep descriptor \p grep_desc that was previously created with
 * the ::cnnlCreateGrepDescriptor function, and sets the information about the grep
 * operation to the grep descriptor \p grep_desc. The information includes the beginning coordinates
 * \p begin to crop the input tensor, the element number \p size to crop the input tensor, the
 * value \p space_number to fill the area when crop exceeds the boundary of input tensor,
 * the dimension number \p input_dims_num of input tensor.
 *
 * Different from ::cnnlSetGrepDescriptor_v2, ::cnnlSetGrepDescriptor_v3 adds \p input_dims_num
 * that is the number of dimensions in the input tensor.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlSetGrepDescriptor_v4 instead.
 *
 * @param[in,out] grep_desc
 *   Input/output. The descriptor of the grep operation. For detailed information, see ::cnnlGrepDescriptor_t.
 * @param[in] begin
 *   Input. Pointer to the host memory that saves the beginning coordinates used to crop
 *   the input tensor for each dimension.
 * @param[in] size
 *   Input. Pointer to the host memory that saves the element number used to crop the input tensor
 *   in each dimension.
 * @param[in] space_number
 *   Input. When the cropping area exceeds the boundary of the input tensor, this value will be filled in
 *   the empty space. The default value of this parameter is 0.
 * @param[in] input_dims_num
 *   Input. The number of dimensions in the input tensor of grep operation. Currently, the value of this parameter
 *   is between [1, MAX_DIM].
 * @par Return
 * - ::CNNL_STATUS_SUCCESS
 *
 * @par API Dependency
 * - Before calling this function, you need to call the ::cnnlCreateGrepDescriptor
 *   function to create the \p grep_desc.
 * - You need to call the ::cnnlDestroyGrepDescriptor function to destroy the descriptor at the end of context.
 *
 * @par Scale Limitation
 * - The number of dimensions of the \p begin must be greater than zero. The value of
 *   each dimension cannot be negative and cannot exceed the size of corresponding
 *   dimension of the input tensor.
 * - The number of dimensions of the \p size must be greater than zero. The value
 *   of each dimension must be greater or equal to zero.
 * - The number of dimensions of the \p begin must be equal to the number of dimensions
 *   of the \p size.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlSetGrepDescriptor_v4)
cnnlStatus_t CNNL_WIN_API cnnlSetGrepDescriptor_v3(cnnlGrepDescriptor_t grep_desc,
                                                   int32_t *begin,
                                                   int32_t *size,
                                                   float space_number,
                                                   int32_t input_dims_num);

// Group:Grep
/*!
 * @brief Initializes the grep descriptor \p grep_desc that was previously created with
 * the ::cnnlCreateGrepDescriptor function, and sets the information about the grep
 * operation to the grep descriptor \p grep_desc. The information includes the beginning coordinates
 * \p begin to crop the input tensor, the element number \p size to crop the input tensor, the
 * value \p space_number to fill the area when crop exceeds the boundary of input tensor and
 * the dimension number \p input_dims_num of input tensor.
 *
 * Different from ::cnnlSetGrepDescriptor_v3, ::cnnlSetGrepDescriptor_v4 supports \p begin and \p size
 * larger than \f$2^{31}\f$.
 *
 * @param[in,out] grep_desc
 *   Input/output. The descriptor of the grep operation. For detailed information, see ::cnnlGrepDescriptor_t.
 * @param[in] begin
 *   Input. Pointer to the host memory that saves the beginning coordinates used to crop
 *   the input tensor for each dimension.
 * @param[in] size
 *   Input. Pointer to the host memory that saves the element number used to crop the input tensor
 *   in each dimension.
 * @param[in] space_number
 *   Input. When the cropping area exceeds the boundary of the input tensor, this value will be filled in
 *   the empty space. The default value of this parameter is 0.
 * @param[in] input_dims_num
 *   Input. The number of dimensions in the input tensor of grep operation. Currently, the value of this parameter
 *   is between [1, MAX_DIM].
 * @par Return
 * - ::CNNL_STATUS_SUCCESS
 *
 * @par API Dependency
 * - Before calling this function, you need to call the ::cnnlCreateGrepDescriptor
 *   function to create the \p grep_desc.
 * - You need to call the ::cnnlDestroyGrepDescriptor function to destroy the descriptor at the end of context.
 *
 * @par Scale Limitation
 * - The number of dimensions of the \p begin must be greater than zero. The value of
 *   each dimension cannot be negative and cannot exceed the size of corresponding
 *   dimension of the input tensor.
 * - The number of dimensions of the \p size must be greater than zero. The value
 *   of each dimension must be greater or equal to zero.
 * - The number of dimensions of the \p begin must be equal to the number of dimensions
 *   of the \p size.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetGrepDescriptor_v4(cnnlGrepDescriptor_t grep_desc,
                                                   int64_t *begin,
                                                   int64_t *size,
                                                   float space_number,
                                                   int32_t input_dims_num);

// Group:Grep
/*!
 * @brief Crops the input tensor \p input according to the information in the \p grep_desc,
 *        and returns the results in the output tensor \p output.
 *
 * You can continuously crop the input tensor by specifying the beginning coordinates of input
 * tensor \p begin of and \p size. The part that exceeds the size of input tensor will be filled
 * with \p space_number.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   grep operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] grep_desc
 *   Input. The descriptor of the grep operation. For detailed information, see ::cnnlGrepDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par Formula
 * - None.
 *
 * @par Data Type
 * - Data type of input tensors and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: bool, uint8, int8, uint16, int16, uint32, int32, half, float, bfloat16.
 *   - output tensor: bool, uint8, int8, uint16, int16, uint32, int32, half, float, bfloat16.
 *
 *   The bfloat16 data type is supported only on MLU500 series.
 * @par Scale Limitation
 * - All tensor formats could be any dimension that is greater than one.
 *
 * @par API Dependency
 * - Before calling this function, you need to call the ::cnnlCreateGrepDescriptor function
 *   to create the \p grep_desc and to call the ::cnnlSetGrepDescriptor function to set the
 *   information.
 * - You need to cal the ::cnnlDestroyGrepDescriptor function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the grep operation is as follows:
     @verbatim
      input tensor x[100, 256, 256, 3], begin[0, 28, 28, 0], size[100, 200, 200, 3], output tensor y[100, 200, 200, 3].
      The input tensor x will crop from [0, 28, 28, 0] to [0+100, 28+200, 28+200, 0+3] and get the output tensor y.
     @endverbatim
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGrep(cnnlHandle_t handle,
                                   const cnnlGrepDescriptor_t grep_desc,
                                   const cnnlTensorDescriptor_t input_desc,
                                   const void *input,
                                   const cnnlTensorDescriptor_t output_desc,
                                   void *output);
// Group:Reduce
/*!
 * @brief Creates a descriptor pointed by \p reduce_desc that holds the \p axis,\p reduce_op,
 *        \p tensor_type, \p nan_propagation, \p tensor_indices, \p indices_type and \p p.
 *        The information is defined in ::cnnlReduceDescriptor_t.
 *
 * @param[out] reduce_desc
 *   Output. A host pointer to the reduce descriptor that holds information about reduce.
 *   For detailed information, see ::cnnlReduceDescriptor_t.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetReorgDescriptor function to
 *   initialize and set the information to the reduce descriptor.
 * - You need to call the ::cnnlDestroyReorgDescriptor function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateReduceDescriptor(cnnlReduceDescriptor_t *reduce_desc);

// Group:Reduce
/*!
 * @brief Initializes the reduce descriptor \p reduce_desc that was previously created with
 *        the ::cnnlCreateReduceDescriptor function, and sets the information about the reduce
 *        operation. To use p-norm in this operation, call ::cnnlSetReduceDescriptor_v2.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlSetReduceDescriptor_v2 instead, which supports p-norm.
 *
 * @param[in,out] reduce_desc
 *   Input/output. The descriptor of the reduce operation.
 *   For detailed information, see ::cnnlReduceDescriptor_t.
 * @param[in] axis[]
 *   Input. The axis dimension vector of the reduce operation.
 * @param[in] axis_num
 *   Input. The size of axis vector.
 * @param[in] reduce_op
 *   Input. Enumeration to specify the reduce mode.
 *   For detailed information, see ::cnnlReduceOp_t.
 * @param[in] tensor_type
 *   Input. The data type is used in computing the reduce operation.
 *   For detailed information, see ::cnnlDataType_t.
 * @param[in] nan_propagation
 *   Input. Enumeration to specify the NaN propagation mode. Default
 *   value is NOT_PROPAGATE_NAN. Now reduce does not support this parameter.
 *   For detailed information, see ::cnnlNanPropagation_t.
 * @param[in] tensor_indices
 *   Input. Enumeration to specify the reduce indices mode.
 *   For detailed information, see ::cnnlReduceIndices_t.
 * @param[in]  indices_type
 *   Input. Enumeration to specify the bit width type of reduce indices.
 *   At present, this parameter can only be set as CNNL_32BIT_INDICES.
 *   For detailed information, see ::cnnlIndicesType_t.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, you need to call the ::cnnlCreateReduceDescriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlSetReduceDescriptor_v2)
cnnlStatus_t CNNL_WIN_API cnnlSetReduceDescriptor(cnnlReduceDescriptor_t reduce_desc,
                                                  int axis[],
                                                  int axis_num,
                                                  cnnlReduceOp_t reduce_op,
                                                  cnnlDataType_t tensor_type,
                                                  cnnlNanPropagation_t nan_propagation,
                                                  cnnlReduceIndices_t tensor_indices,
                                                  cnnlIndicesType_t indices_type);
// Group:Reduce
/*!
 * @brief Initializes the reduce descriptor \p reduce_desc that was previously created with
 * the ::cnnlCreateReduceDescriptor function, and sets the information about the reduce
 * operation. Compared with ::cnnlSetReduceDescriptor, this function supports \p
 * reduce_op == \p CNNL_REDUCE_NORMP, and includes more information such as \p p.
 *
 * @param[in,out] reduce_desc
 *   Input/output. The descriptor of the reduce operation.
 *   For detailed information, see ::cnnlReduceDescriptor_t.
 * @param[in] axis[]
 *   Input. The axis dimension vector of the reduce operation.
 * @param[in] axis_num
 *   Input. The size of axis vector.
 * @param[in] reduce_op
 *   Input. Enumeration to specify the reduce mode.
 *   For detailed information, see ::cnnlReduceOp_t.
 * @param[in] tensor_type
 *   Input. The data type is used in computing the reduce operation.
 *   Reduce is not aware of this parameter now.
 *   For detailed information, see ::cnnlDataType_t.
 * @param[in] nan_propagation
 *   Input. Enumeration to specify the NaN propagation mode. Default
 *   value is NOT_PROPAGATE_NAN. Now reduce does not support this parameter.
 *   For detailed information, see ::cnnlNanPropagation_t.
 * @param[in] tensor_indices
 *   Input. Enumeration to specify the reduce indices mode.
 *   For detailed information, see ::cnnlReduceIndices_t.
 * @param[in]  indices_type
 *   Input. Enumeration to specify the bit width type of reduce indices.
 *   Reduce is not aware of this parameter now.
 *   For detailed information, see ::cnnlIndicesType_t.
 * @param[in] p
 *   Input. The exponent value in the norm formulation. \p p cannot be 1.0 or 2.0 when \p reduce_op is \p CNNL_REDUCE_NORMP.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, you need to call the ::cnnlCreateReduceDescriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetReduceDescriptor_v2(cnnlReduceDescriptor_t reduce_desc,
                                                     int axis[],
                                                     int axis_num,
                                                     cnnlReduceOp_t reduce_op,
                                                     cnnlDataType_t tensor_type,
                                                     cnnlNanPropagation_t nan_propagation,
                                                     cnnlReduceIndices_t tensor_indices,
                                                     cnnlIndicesType_t indices_type,
                                                     float p);
// Group:Reduce
/*!
 * @brief Destroys a reduce descriptor \p reduce_desc that was previously created with
 *        the ::cnnlCreateReduceDescriptor.
 *
 * The reduce descriptor is defined in ::cnnlReduceDescriptor_t and holds the information
 * about the reduce.
 *
 * @param[in] reduce_desc
 *   Input. The reduce descriptor to be destroyed.
 *   For detailed information, see ::cnnlReduceDescriptor_t.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, you need to call the ::cnnlCreateReduceDescriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyReduceDescriptor(cnnlReduceDescriptor_t reduce_desc);

// Group:Reduce
/*!
 * @brief Applies the reduce operation to compute the sum value, mean value, norm value, maximum value,
 *         maximum index, minimum value and minimum index of tensor in the given dimension.
 *
 * @deprecated
 *   This function is deprecated and will be removed in future release.
 *   Use ::cnnlReduce_v2 instead, which can set \p indices_desc.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the reduce operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] reduce_desc
 *   Input. A struct with information of the reduce operation. For detailed information,
 *   see ::cnnlReduceDescriptor_t.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the reduce operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the reduce operation.
 *   You can get the size of the workspace with the ::cnnlGetReduceOpWorkspaceSize function.
 * @param[in] alpha
 *   Input. A host pointer to scaling factor of tensor output. The value of this parameter can be NULL.
 * @param[in] beta
 *   Input. A host pointer to bias factor of tensor output. The value of this parameter can be NULL.
 * @param[in] input_desc
 *   Input. Descriptor of input data. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] indices_size_inbytes
 *   Input. The size in bytes of indices.
 * @param[out] indices
 *   Output. Pointer to the MLU memory that stores the indices tensor.
 * @param[in] output_desc
 *   Input. Descriptor of output data. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - Data type of input tensors and output tensors must be the same
 *    except \p CNNL_REDUCE_AND, \p CNNL_REDUCE_OR and \p CNNL_REDUCE_ADD.
 * - The supported data types of input and output tensors are as follows:
 *   - When \p reduce_op == \p CNNL_REDUCE_MAX || \p reduce_op == \p CNNL_REDUCE_MIN:
 *     - input:   float, half, bfloat16, int32, bool, int64.
 *     - output:  float, half, bfloat16, int32, bool, int64.
 *     - indices: int32, int64.
 *
 *   - When \p reduce_op == \p CNNL_REDUCE_MUL:
 *     - input:   float, half, bfloat16, int32.
 *     - output:  float, half, bfloat16, int32.
 *
 *   - When \p reduce_op == \p CNNL_REDUCE_AND || \p reduce_op == \p CNNL_REDUCE_OR:
 *     - input:   float, half, bfloat16, int64, int32, int8, uint8, bool.
 *     - output:  float, half, bfloat16, int64, int32, int8, uint8, bool.
 *     - The output and input data type can be the same or different,
 *       but if they are different, the output data type must be uint8 or bool.
 *
 *   - When \p reduce_op == \p CNNL_REDUCE_ADD:
 *     - input:   float, half, bfloat16, int32, complex<float>, int64, bool.
 *     - output:  float, half, bfloat16, int32, complex<float>, int64.
 *     - The output and input data type can be the same or different,
 *       but if they are different, the input data type must be bool or int32, and the output data type must be int64.
 *
 *   - When \p reduce_op == \p CNNL_REDUCE_AVG:
 *     - input:   float, half, bfloat16, int32.
 *     - output:  float, half, bfloat16, int32.
 *
 *   - When \p reduce_op == \p CNNL_REDUCE_ASUM || \p reduce_op == \p CNNL_REDUCE_SUMSQ ||
 *          \p reduce_op == \p CNNL_REDUCE_NANSUM:
 *     - input:   float, half, bfloat16.
 *     - output:  float, half, bfloat16.
 *
 *   - When \p reduce_op == \p CNNL_REDUCE_NORM1 || \p reduce_op == \p CNNL_REDUCE_NORM2 ||
 *             \p reduce_op == \p CNNL_REDUCE_NORMP:
 *     - input:   float, half, bfloat16.
 *     - output:  float, half, bfloat16.
 *
 *   - When \p reduce_op == \p CNNL_REDUCE_MAX_LAST_INDEX ||
 *          \p reduce_op == \p CNNL_REDUCE_MIN_LAST_INDEX:
 *     - input:   float, half, int32.
 *     - output:  float, half, int32.
 *     - indices: int32.
 * - \p alpha and \p beta: If the data type of input tensor is float or half, the data type of \p alpha
 *   and \p beta should be float. If the data type of tensors is int32 or int64, the data type of \p alpha
 *   and \p beta should be int32.
 * - The bfloat16 data type is supported only on MLU500 series.
 *
 * @par Data Layout
 * - The supported layout of the input tensors and output tensors must be \p CNNL_LAYOUT_ARRAY.

 * @par API Dependency
 * - Before calling this function to implement reduce, you need to prepare all the parameters
 *   passed to this function. Call ::cnnlCreateReduceDescriptor to create the parameter \p reduce_desc,
 *   and then call ::cnnlSetReduceDescriptor_v2 to set information about the parameter \p reduce_desc and
 *   call ::cnnlGetReduceOpWorkspaceSize to get extra MLU memory size in reduce operation.
 * - After calling this function, you need to call ::cnnlDestroyReduceDescriptor to destroy the
 *   parameter \p reduce_desc.
 *
 * @note
 * - The \p axis must meet the following requirements:
 *   - When the number of \p axis is greater than 1, the values of axis vector cannot be duplicated.
 *     For example, \p axis = [1,2,3] or \p axis = [1,2,4].
 *   - The size of \p axis cannot be greater than that of input.
 * - The following reduce modes support multi-axis numbers including continuous and discontinuous axis numbers
 *   except for \p p is 0.0 when \p reduce_op == \p CNNL_REDUCE_NORMP:
 *   - \p CNNL_REDUCE_ADD, \p CNNL_REDUCE_AVG, \p CNNL_REDUCE_MUL, \p CNNL_REDUCE_OR, \p CNNL_REDUCE_AND,
 *     \p CNNL_REDUCE_NORM1, \p CNNL_REDUCE_NORM2, and \p CNNL_REDUCE_NORMP.
 * - When \p reduce_op == \p CNNL_REDUCE_MAX || \p reduce_op == \p CNNL_REDUCE_MIN:
 *   - The \p axis supports multi-axis numbers including continuous and discontinuous axis numbers,
 *     when \p tensor_indices is \p CNNL_REDUCE_NO_INDICES.
 *   - The \p axis supports single-axis number and returns the index of the first max or min value,
 *     when \p tensor_indices is \p CNNL_REDUCE_ONLY_INDICES or \p CNNL_REDUCE_FLATTENED_INDICES.
 * - When \p reduce_op == \p CNNL_REDUCE_MAX_LAST_INDEX || \p reduce_op == \p CNNL_REDUCE_MIN_LAST_INDEX:
 *   - The \p axis only supports single-axis number.
 *   - The \p tensor_indices only supports \p CNNL_REDUCE_ONLY_INDICES and \p CNNL_REDUCE_FLATTENED_INDICES,
 *     and returns the index of the last max or min value.
 * - When \p reduce_op == \p CNNL_REDUCE_ASUM || \p reduce_op == \p CNNL_REDUCE_SUMSQ:
 *   - These two modes only support Caffe framework.
 *   - The mode \p CNNL_REDUCE_ASUM refers to the cumulative reduction after taking the absolute value of
 *     a specified dimension.
 *   - The mode \p CNNL_REDUCE_SUMSQ calculates the square of the specified dimension and performs cumulative
 *     reduction.
 *   - Whether you set the \p tensor_type parameter or not, the data type used in computing the reduce operation
 *     of these two modes is fixed as float.
 * - The following modes support input with stride:
 *   - \p CNNL_REDUCE_ADD, \p CNNL_REDUCE_AVG, \p CNNL_REDUCE_MUL, \p CNNL_REDUCE_MAX, \p CNNL_REDUCE_MIN,
 *     \p CNNL_REDUCE_OR, \p CNNL_REDUCE_AND, \p CNNL_REDUCE_NORM1, \p CNNL_REDUCE_NORM2 and \p CNNL_REDUCE_NORMP.
 * - This function reduces tensor input by implementing the equation output = alpha * reduce(input) + beta,
 *   given tensors \p input and \p output and scaling factors \p alpha and \p beta.
 *   - The following modes support \p alpha and \p beta except that the input type is complex<float>:
 *     - \p CNNL_REDUCE_ADD, \p CNNL_REDUCE_AVG, \p CNNL_REDUCE_MUL, \p CNNL_REDUCE_NORM1, \p CNNL_REDUCE_NORM2,
 *       and \p CNNL_REDUCE_NORMP.
 *   - The \p alpha and \p beta can be set to NULL, or the \p alpha float value is 1.0 and the \p beta float value
 *     is 0.0 for modes that don't support \p alpha or \p beta.
 * - \p output_desc can be set to NULL when \p tensor_indices is \p CNNL_REDUCE_ONLY_INDICES.
 *
 * @par Scale Limitations
 * - When \p reduce_op == \p CNNL_REDUCE_NORMP on MLU200 series:
 *   - The sum of p power of input absolute should be in range of [7.2e-9, 507903] when data type is
 *     float and [6.1e-5,65504] when data type is half.
 *   - The p power of input absolute should be in range of [-3.4e38, 16] when data type is float and
 *     [-65504,10.25] when data type is half.
 *   - The product of 1/p and sum of p power of input absolute should be in range of [-3.4e38, 16]
 *     when data type is float and [-65504,10.25] when data type is half.
 * - When \p reduce_op == \p CNNL_REDUCE_MAX_LAST_INDEX || \p reduce_op == \p CNNL_REDUCE_MIN_LAST_INDEX:
 *   - The \p input with NaN or INFINITY is not supported.
 *   - The data range of \p input should satisfy the conditions: (-INFINITY, INFINITY).
 * - When input data contains NaN on MLU300 series and CE3226:
 *   - The CNNL_REDUCE_MIN and CNNL_REDUCE_MAX results are different with IEEE 754.
 *     - If the first operand is NaN and the second operand is finite value, then output is NaN.
 *     - If the first operand is finite value and the second operand is finite value, then output is finite value.
 *   - The \p CNNL_REDUCE_NORMP results are different with IEEE 754 when \p p is 0.0.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the reduce operation is as follows:
     @verbatim
     input dimension = [n,c,h,w,d],
     When axis = 0:
      output dimension = [1,c,h,w,d].
      (indices dimension = [1,c,h,w,d], reduce_op == CNNL_REDUCE_MAX or CNNL_REDUCE_MIN).
     When axis = 1:
      output dimension = [n,1,h,w,d].
      (indices dimension = [n,1,h,w,d], reduce_op == \CNNL_REDUCE_MAX or CNNL_REDUCE_MIN).
     When axis = 2:
      output dimension = [n,c,1,w,d].
      (indices dimension = [n,c,1,w,d], reduce_op == CNNL_REDUCE_MAX or CNNL_REDUCE_MIN).
     When axis = 3:
      output dimension = [n,c,h,1,d].
      (indices dimension = [n,c,h,1,d], reduce_op == CNNL_REDUCE_MAX or CNNL_REDUCE_MIN).
     When axis = 4:
      output dimension = [n,c,h,w,1].
      (indices dimension = [n,c,h,w,1], reduce_op == CNNL_REDUCE_MAX or CNNL_REDUCE_MIN).
     When axis = -1:
      output dimension = [1,1,1,1,1].
      (indices dimension = [1,1,1,1,1], reduce_op == CNNL_REDUCE_MAX or CNNL_REDUCE_MIN).
     @endverbatim
 *
 * @par Reference
 * - https://tensorflow.google.cn/api_docs/python/tf/math/reduce_sum
 * - https://tensorflow.google.cn/api_docs/python/tf/math/reduce_mean
 * - https://tensorflow.google.cn/api_docs/python/tf/math/reduce_prod
 * - https://tensorflow.google.cn/api_docs/python/tf/math/reduce_max
 * - https://tensorflow.google.cn/api_docs/python/tf/math/reduce_min
 */
CNNL_DEPRECATED_FOR(cnnlReduce_v2)
cnnlStatus_t CNNL_WIN_API cnnlReduce(cnnlHandle_t handle,
                                     const cnnlReduceDescriptor_t reduce_desc,
                                     void *workspace,
                                     size_t workspace_size,
                                     const void *alpha,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const void *input,
                                     const size_t indices_size_inbytes,
                                     void *indices,
                                     const void *beta,
                                     const cnnlTensorDescriptor_t output_desc,
                                     void *output);

// Group:Reduce
/*!
 * @brief Applies the reduce operation to compute the sum value, mean value, norm value, maximum value,
 *         maximum index, minimum value and minimum index of tensor in the given dimension.
 *        Compared with ::cnnlReduce, this function supports setting \p indices_desc to define indices information instead of \p indices_size_inbytes.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the reduce operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] reduce_desc
 *   Input. A struct with information of the reduce operation. For detailed information,
 *   see ::cnnlReduceDescriptor_t.
 * @param[in] input_desc
 *   Input. Descriptor of input data. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] alpha
 *   Input. A host pointer to scaling factor of tensor output. The value of this parameter can be NULL.
 * @param[in] beta
 *   Input. A host pointer to bias factor of tensor output. The value of this parameter can be NULL.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the reduce operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the reduce operation.
 *   You can get the size of the workspace with the ::cnnlGetReduceOpWorkspaceSize function.
 * @param[in] output_desc
 *   Input. Descriptor of output data. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] indices_desc
 *   Input. Descriptor of indices data. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] indices
 *   Output. Pointer to the MLU memory that stores the indices tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - Data type of input tensors and output tensors must be the same
 *    except \p CNNL_REDUCE_AND, \p CNNL_REDUCE_OR and \p CNNL_REDUCE_ADD.
 * - The supported data types of input and output tensors are as follows:
 *   - When \p reduce_op == \p CNNL_REDUCE_MAX || \p reduce_op == \p CNNL_REDUCE_MIN:
 *     - input:   float, half, bfloat16, int32, bool, int64.
 *     - output:  float, half, bfloat16, int32, bool, int64.
 *     - indices: int32, uint32, int64.
 *
 *   - When \p reduce_op == \p CNNL_REDUCE_MUL:
 *     - input:   float, half, bfloat16, int32.
 *     - output:  float, half, bfloat16, int32.
 *
 *   - When \p reduce_op == \p CNNL_REDUCE_AND || \p reduce_op == \p CNNL_REDUCE_OR:
 *     - input:   float, half, bfloat16, int64, int32, int8, uint8, bool.
 *     - output:  float, half, bfloat16, int64, int32, int8, uint8, bool.
 *     - The output and input data type can be the same or different,
 *       but if they are different, the output data type must be uint8 or bool.
 *
 *   - When \p reduce_op == \p CNNL_REDUCE_ADD:
 *     - input:   float, half, bfloat16, int32, complex<float>, int64, bool.
 *     - output:  float, half, bfloat16, int32, complex<float>, int64.
 *     - The output and input data type can be the same or different,
 *       but if they are different, the input data type must be bool or int32, and the output data type must be int64.
 *
 *   - When \p reduce_op == \p CNNL_REDUCE_AVG:
 *     - input:   float, half, bfloat16, int32.
 *     - output:  float, half, bfloat16, int32.
 *
 *   - When \p reduce_op == \p CNNL_REDUCE_ASUM || \p reduce_op == \p CNNL_REDUCE_SUMSQ ||
 *          \p reduce_op == \p CNNL_REDUCE_NANSUM:
 *     - input:   float, half, bfloat16.
 *     - output:  float, half, bfloat16.
 *
 *   - When \p reduce_op == \p CNNL_REDUCE_NORM1 || \p reduce_op == \p CNNL_REDUCE_NORM2 ||
 *             \p reduce_op == \p CNNL_REDUCE_NORMP:
 *     - input:   float, half, bfloat16.
 *     - output:  float, half, bfloat16.
 *
 *   - When \p reduce_op == \p CNNL_REDUCE_MAX_LAST_INDEX ||
 *          \p reduce_op == \p CNNL_REDUCE_MIN_LAST_INDEX:
 *     - input:   float, half, int32.
 *     - output:  float, half, int32.
 *     - indices: int32, uint32.
 * - \p alpha and \p beta: If the data type of input tensor is float or half, the data type of \p alpha
 *   and \p beta should be float. If the data type of tensors is int32 or int64, the data type of \p alpha
 *   and \p beta should be int32.
 * - The bfloat16 data type is supported only on MLU500 series.
 *
 * @par Data Layout
 * - The supported layout of the input tensors and output tensors must be \p CNNL_LAYOUT_ARRAY.

 * @par API Dependency
 * - Before calling this function to implement reduce, you need to prepare all the parameters
 *   passed to this function. Call ::cnnlCreateReduceDescriptor to create the parameter \p reduce_desc,
 *   and then call ::cnnlSetReduceDescriptor_v2 to set information about the parameter \p reduce_desc and
 *   call ::cnnlGetReduceOpWorkspaceSize_v2 to get extra MLU memory size in reduce operation.
 * - After calling this function, you need to call ::cnnlDestroyReduceDescriptor to destroy the
 *   parameter \p reduce_desc.
 *
 * @note
 * - The \p axis must meet the following requirements:
 *   - When the number of \p axis is greater than 1, the values of axis vector cannot be duplicated.
 *     For example, \p axis = [1,2,3] or \p axis = [1,2,4].
 *   - The size of \p axis cannot be greater than that of input.
 * - The following reduce modes support multi-axis numbers including continuous and discontinuous axis numbers
 *   except for \p p is 0.0 when \p reduce_op == \p CNNL_REDUCE_NORMP:
 *   - \p CNNL_REDUCE_ADD, \p CNNL_REDUCE_AVG, \p CNNL_REDUCE_MUL, \p CNNL_REDUCE_OR, \p CNNL_REDUCE_AND,
 *     \p CNNL_REDUCE_NORM1, \p CNNL_REDUCE_NORM2, and \p CNNL_REDUCE_NORMP.
 * - When \p reduce_op == \p CNNL_REDUCE_MAX || \p reduce_op == \p CNNL_REDUCE_MIN:
 *   - The \p axis supports multi-axis numbers including continuous and discontinuous axis numbers,
 *     when \p tensor_indices is \p CNNL_REDUCE_NO_INDICES.
 *   - The \p axis supports single-axis number and returns the index of the first max or min value,
 *     when \p tensor_indices is \p CNNL_REDUCE_ONLY_INDICES or \p CNNL_REDUCE_FLATTENED_INDICES.
 * - When \p reduce_op == \p CNNL_REDUCE_MAX_LAST_INDEX || \p reduce_op == \p CNNL_REDUCE_MIN_LAST_INDEX:
 *   - The \p axis only supports single-axis number.
 *   - The \p tensor_indices only supports \p CNNL_REDUCE_ONLY_INDICES and \p CNNL_REDUCE_FLATTENED_INDICES,
 *     and returns the index of the last max or min value.
 * - When \p reduce_op == \p CNNL_REDUCE_ASUM || \p reduce_op == \p CNNL_REDUCE_SUMSQ:
 *   - These two modes only support Caffe framework.
 *   - The mode \p CNNL_REDUCE_ASUM refers to the cumulative reduction after taking the absolute value of
 *     a specified dimension.
 *   - The mode \p CNNL_REDUCE_SUMSQ calculates the square of the specified dimension and performs cumulative
 *     reduction.
 *   - Whether you set the \p tensor_type parameter or not, the data type used in computing the reduce operation
 *     of these two modes is fixed as float.
 * - The following modes support input with stride:
 *   - \p CNNL_REDUCE_ADD, \p CNNL_REDUCE_AVG, \p CNNL_REDUCE_MUL, \p CNNL_REDUCE_MAX, \p CNNL_REDUCE_MIN,
 *     \p CNNL_REDUCE_OR, \p CNNL_REDUCE_AND, \p CNNL_REDUCE_NORM1, \p CNNL_REDUCE_NORM2 and \p CNNL_REDUCE_NORMP.
 * - This function reduces tensor input by implementing the equation output = alpha * reduce(input) + beta,
 *   given tensors \p input and \p output and scaling factors \p alpha and \p beta.
 *   - The following modes support \p alpha and \p beta except that the input type is complex<float>:
 *     - \p CNNL_REDUCE_ADD, \p CNNL_REDUCE_AVG, \p CNNL_REDUCE_MUL, \p CNNL_REDUCE_NORM1, \p CNNL_REDUCE_NORM2,
 *       and \p CNNL_REDUCE_NORMP.
 *   - The \p alpha and \p beta can be set to NULL, or the \p alpha float value is 1.0 and the \p beta float value
 *     is 0.0 for modes that don't support \p alpha or \p beta.
 *
 * @par Scale Limitations
 * - When \p reduce_op == \p CNNL_REDUCE_NORMP on MLU200 series:
 *   - The sum of p power of input absolute should be in range of [7.2e-9, 507903] when data type is
 *     float and [6.1e-5,65504] when data type is half.
 *   - The p power of input absolute should be in range of [-3.4e38, 16] when data type is float and
 *     [-65504,10.25] when data type is half.
 *   - The product of 1/p and sum of p power of input absolute should be in range of [-3.4e38, 16]
 *     when data type is float and [-65504,10.25] when data type is half.
 * - When \p reduce_op == \p CNNL_REDUCE_MAX_LAST_INDEX || \p reduce_op == \p CNNL_REDUCE_MIN_LAST_INDEX:
 *   - The \p input with NaN or INFINITY is not supported.
 *   - The data range of \p input should satisfy the conditions: (-INFINITY, INFINITY).
 * - When input data contains NaN on MLU300 series and CE3226:
 *   - The CNNL_REDUCE_MIN and CNNL_REDUCE_MAX results are different with IEEE 754.
 *     - If the first operand is NaN and the second operand is finite value, then output is NaN.
 *     - If the first operand is finite value and the second operand is finite value, then output is finite value.
 *   - The \p CNNL_REDUCE_NORMP results are different with IEEE 754 when \p p is 0.0.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the layer normalization forward operation is as follows:
     @verbatim
     input dimension = [n,c,h,w,d],
     When axis = 0:
      output dimension = [1,c,h,w,d].
      (indices dimension = [1,c,h,w,d], reduce_op == CNNL_REDUCE_MAX or CNNL_REDUCE_MIN).
     When axis = 1:
      output dimension = [n,1,h,w,d].
      (indices dimension = [n,1,h,w,d], reduce_op == \CNNL_REDUCE_MAX or CNNL_REDUCE_MIN).
     When axis = 2:
      output dimension = [n,c,1,w,d].
      (indices dimension = [n,c,1,w,d], reduce_op == CNNL_REDUCE_MAX or CNNL_REDUCE_MIN).
     When axis = 3:
      output dimension = [n,c,h,1,d].
      (indices dimension = [n,c,h,1,d], reduce_op == CNNL_REDUCE_MAX or CNNL_REDUCE_MIN).
     When axis = 4:
      output dimension = [n,c,h,w,1].
      (indices dimension = [n,c,h,w,1], reduce_op == CNNL_REDUCE_MAX or CNNL_REDUCE_MIN).
     When axis = -1:
      output dimension = [1,1,1,1,1].
      (indices dimension = [1,1,1,1,1], reduce_op == CNNL_REDUCE_MAX or CNNL_REDUCE_MIN).
     @endverbatim
 *
 * @par Reference
 * - https://tensorflow.google.cn/api_docs/python/tf/math/reduce_sum
 * - https://tensorflow.google.cn/api_docs/python/tf/math/reduce_mean
 * - https://tensorflow.google.cn/api_docs/python/tf/math/reduce_prod
 * - https://tensorflow.google.cn/api_docs/python/tf/math/reduce_max
 * - https://tensorflow.google.cn/api_docs/python/tf/math/reduce_min
 */
cnnlStatus_t CNNL_WIN_API cnnlReduce_v2(cnnlHandle_t handle,
                                     const cnnlReduceDescriptor_t reduce_desc,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const void *input,
                                     const void *alpha,
                                     const void *beta,
                                     void *workspace,
                                     size_t workspace_size,
                                     const cnnlTensorDescriptor_t output_desc,
                                     void *output,
                                     const cnnlTensorDescriptor_t indices_desc,
                                     void *indices);

// Group:Reduce
/*!
 * @brief Returns in \p size the size of the MLU memory that is used to get
 *        extra space size in reduce operation.
 *
 * @deprecated
 *   This function is deprecated and will be removed in future release.
 *   Use ::cnnlGetReduceOpWorkspaceSize_v2 instead, which can set \p indices_desc.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the reduce operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. A descriptor of input tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] output_desc
 *   Input. A descriptor of output tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] reduce_op
 *   Input. An operation enum, indicating a specific reduce operation.
 *   For detailed information, see ::cnnlReduceDescriptor_t.
 * @param[out] workspace_size_inbytes
 *   Output. Pointer to the returned size of the extra workspace in bytes that is
 *   used in the reduce operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlGetReduceOpWorkspaceSize_v2)
cnnlStatus_t CNNL_WIN_API cnnlGetReduceOpWorkspaceSize(cnnlHandle_t handle,
                                                       const cnnlTensorDescriptor_t input_desc,
                                                       const cnnlTensorDescriptor_t output_desc,
                                                       const cnnlReduceDescriptor_t reduce_op,
                                                       size_t *workspace_size_inbytes);

// Group:Reduce
/*!
 * @brief Returns in \p size the size of the MLU memory that is used to get
 *        extra space size in reduce operation.
 *        Compared with ::cnnlGetReduceOpWorkspaceSize, this function adds \p indices_desc to compute \p workspace_size_inbytes.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the reduce operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. A descriptor of input tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] output_desc
 *   Input. A descriptor of output tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] indices_desc
 *   Input. A descriptor of indices data.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] reduce_op
 *   Input. A specific reduce operation.
 *   For detailed information, see ::cnnlReduceDescriptor_t.
 * @param[out] workspace_size_inbytes
 *   Output. Pointer to the returned size of the extra workspace in bytes that is
 *   used in the reduce operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetReduceOpWorkspaceSize_v2(cnnlHandle_t handle,
                                                       const cnnlTensorDescriptor_t input_desc,
                                                       const cnnlTensorDescriptor_t output_desc,
                                                       const cnnlTensorDescriptor_t indices_desc,
                                                       const cnnlReduceDescriptor_t reduce_op,
                                                       size_t *workspace_size_inbytes);

// Group:InstanceNormForward
/*!
 * @brief Returns in \p workspace_size, the size of the MLU memory that is used to get
 *        extra space size in instance normalization forward operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the instance normalization forward operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] input_x_desc
 *   Input. A descriptor of input_x tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is
 *   used in the instance normalization forward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetInstanceNormForwardWorkspaceSize(
             cnnlHandle_t handle,
             const cnnlTensorDescriptor_t input_x_desc,
             size_t *workspace_size);
/******************************************************************************
 * Cambricon CNNL OP: Dynamic_Stitch
 ******************************************************************************/

// Group:Dynamic Stitch
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace
 * for the dynamic stitch operation.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGetDynamicStitchWorkspaceSize_v2 instead.
 *
 * @param[in] handle
 *  Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *  queues in the dynamic stitch operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] inputs_number
 *   Input. The number of input tensors.
 * @param[out]  workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is
 *   used in the dynamic stitch operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - \p input_size should be greater than 0.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
CNNL_DEPRECATED_FOR(cnnlGetDynamicStitchWorkspaceSize_v2)
cnnlStatus_t CNNL_WIN_API cnnlGetDynamicStitchWorkspaceSize(cnnlHandle_t handle,
                                                            const int inputs_number,
                                                            size_t *workspace_size);

// Group:Dynamic Stitch
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace
 * for the dynamic stitch operation.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGetDynamicStitchWorkspaceSize_v2 instead.
 *
 * @param[in] handle
 *  Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *  queues in the dynamic stitch operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] inputs_number
 *   Input. The number of input tensors.
 * @param[out]  workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is
 *   used in the dynamic stitch operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - \p input_size should be greater than 0.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
CNNL_DEPRECATED_FOR(cnnlGetDynamicStitchWorkspaceSize_v2)
cnnlStatus_t CNNL_WIN_API cnnlGetDynamicStitchtWorkspaceSize(cnnlHandle_t handle,
                                                             const int inputs_number,
                                                             size_t *workspace_size);

// Group:Dynamic Stitch
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace
 * for the dynamic stitch operation. Compared with the \p cnnlGetDynamicStitchWorkspaceSize,
 * the \p cnnlGetDynamicStitchWorkspaceSize_v2 can obtain the extra workspace required for kernel
 * optimization.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the dynamic stitch operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] indices_desc
 *   Input. The descriptors of the \p indices tensors. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] data_desc
 *   Input. The descriptors of the \p data tensors. The \p data tensors will be stitched into
 *   the \p output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] size
 *   Input. The size of input array.
 * @param[in] indices_dims
 *   Input. The dimensions of indices.
 * @param[out]  workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is
 *   used in the dynamic stitch operation.
 * @param[in] output_desc
 *   Output. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED.
 *
 * @par Scale Limitation
 * - \p size should be greater than 0.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetDynamicStitchWorkspaceSize_v2(cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t *indices_desc,
                                     const cnnlTensorDescriptor_t *data_desc,
                                     const int size,
                                     int *indices_dims,
                                     size_t *workspace_size,
                                     const cnnlTensorDescriptor_t output_desc);


// Group:Dynamic Stitch
/*!
 * @brief Interleaves the values from the \p data tensors into a single tensor.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the dynamic stitch operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] indices_desc
 *   Input. The descriptors of the \p indices tensors. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] indices
 *   Input. A host pointer to an array that stores the \p indices pointers on MLU devices.
 * @param[in] data_desc
 *   Input. The descriptors of the \p data tensors. The \p data tensors will be stitched into
 *   the \p output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] data
 *   Input. A host pointer to an array that stores the \p data pointers on MLU devices.
 * @param[in] size
 *   Input. The size of input array.
 * @param[in] indices_dims
 *   Input. The dimensions of indices.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   dynamic stitch operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the dynamic stitch operation. You can get the size of the workspace with
 *   the ::cnnlGetDynamicStitchWorkspaceSize_v2 function.
 * @param[in] output_desc
 *   Output. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ALLOC_FAILED,
 *   ::CNNL_STATUS_NOT_SUPPORTED, ::CNNL_STATUS_ARCH_MISMATCH, CNNL_STATUS_EXECUTION_FAILED.
 *
 * @par Data Type
 * - \p indices: int32
 * - \p data: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, bfloat16, float,
 *   compute_half, complex_float.
 * - \p output: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, bfloat16, float,
 *   compute_half, complex_float.
 * @par Scale Limitation
 * - The values of indices must include 0. There can be duplicate values but no negative values.
 *
 * @par API Dependency
 * - Before calling this function, you need to call ::cnnlGetDynamicStitchWorkspaceSize_v2.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the dynamic stitch operation is as follows:
     @verbatim
     input indices and data array
       indices[0] = 6
       indices[1] = [4, 1]
       indices[2] = [[5, 2], [0, 3]]

       data[0] = [61, 62]
       data[1] = [[41, 42], [11, 12]]
       data[2] = [[[51, 52], [21, 22]], [[1, 2], [31, 32]]]
     output stitched array
       output = [[1, 2], [11, 12], [21, 22], [31, 32], [41, 42],
                [51, 52], [61, 62]]
     @endverbatim
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlDynamicStitch(cnnlHandle_t handle,
                                            const cnnlTensorDescriptor_t *indices_desc,
                                            const int **indices,
                                            const cnnlTensorDescriptor_t *data_desc,
                                            const void **data,
                                            const int size,
                                            int *indices_dims,
                                            void *workspace,
                                            size_t workspace_size,
                                            const cnnlTensorDescriptor_t output_desc,
                                            void *output);

/******************************************************************************
 * Cambricon CNNL OP: One_Hot
 ******************************************************************************/

// Group:OneHot
/*!
 * @brief Computes the onehot code in AI networks.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in this operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] desc_indices
 *   Input. The descriptor of \p indices tensor, which is the site to be
 *   set as on_value.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] indices
 *   Input. Pointer to the MLU memory that stores the \p indices tensor.
 * @param[in] depth
 *   Input. The definition of code depth, which is number of classification.
 * @param[in] on_value
 *   Input. Pointer to the MLU memory that stores the \p on_value, the site
 *   will be set as on_value while site is in \p indices.
 * @param[in] off_value
 *   Input. Pointer to the MLU memory that stores the \p on_value, the site
 *   will be set as off_value while site is not in \p indices.
 * @param[in] axis
 *   Input. The definition of dimension which to be expanded.
 * @param[in] output_data_type
 *   Input. The data type of output.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the \p output.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 * @par Formula
 * - See "OneHot Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 *  The combinations of the data types for \p indices, \p depth, \p on_value,
 *   \p off_value, \p axis and \p output tensor are as follows:
 *    - indices: int32
 *    - depth: int32
 *    - on_value: half, float, int32
 *    - off_value: half, float, int32
 *    - axis: int32
 *    - output: half, float, int32
 *
 * @par Scale Limitation
 * - The scale of \p indices tensor, \p depth, and \p axis must meet the following requirements:
 *   - The \p depth must be greater than 0.
 *   - The \p axis must be in [-1, n], where n is the indices's dimension.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the onehot operation is as follows:
     @verbatim
     indices: a tensor of 1 --> [0, 2, -1, 1]
     depth: 3
     one_hot(indices, depth,
             on_value=5.0, off_value=0.0,
             axis=-1)
     output:
           [[5.0, 0.0, 0.0],  # one_hot(0)
            [0.0, 0.0, 5.0],  # one_hot(2)
            [0.0, 0.0, 0.0],  # one_hot(-1)
            [0.0, 5.0, 0.0]]  # one_hot(1)
     @endverbatim
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlOneHot(cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t desc_indices,
                                     const void *indices,
                                     const int depth,
                                     const void *on_value,
                                     const void *off_value,
                                     const int axis,
                                     cnnlDataType_t output_data_type,
                                     void *output);

// Group:SortedSegmentReduce
/*!
 * @brief Computes the result through the same index value of \p segment_ids.
 * The result should be a tensor that consists of the max, min, prod, sum or mean
 * value of the input tensor according to the SortedSegmentReduce mode.
 * @param[in] handle
 *   Input. Handle to a CNNL context that is used to manage MLU devices and queues in
 *   the sorted segment reduction operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] mode
 *   Input. Integer that marks a different calculation formula. For detailed information, see
 ::cnnlSortedSegmentReduceMode_t.
 * @param[in]  data_desc
 *   Input. The descriptor of data tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  data
 *   Input. Pointer to the MLU memory that stores the data tensor.
 * @param[in]  ids_desc
 *   Input. The descriptor of segment indices tensor used as an index indicating
 *   the position of the reult on output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  segment_ids
 *   Input. Pointer to the MLU memory that stores the segment indexes.
 * @param[in]  output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out]  output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED.
 *
 * @par Formula
 * - See "SortedSegmentReduce Operation" section in "Cambricon CNNL User Guide" for details.

 * @par Data Type
 * - This function supports any combinations of the following data types for data tensor \p data,
 * segment indexes tensor \p segment_ids, and output tensor \p output.
 * Note that the data type of data tensor and output tensor must be the same.
 *   - \p data: half, float, int32.
 *   - \p segment_ids: int32, int64.
 *   - \p output: half, float, int32.
 *
 * @par Scale Limitation
 * - The \p data tensor, \p segment_ids tensor, and the \p output tensor must meet the following
 requirements:
 *   - Except for the first dimension, the other dimentions of \p data should be equal to \p output.
 *   - The \p segment_ids should be a 1-D tensor whose size is equal to that of \p data's first
 dimension.
 *   - The size of \p output's first dimention should be equal to the last value of \p segment_ids plus
 one.
 *
 * @note
 * - Sorted_segment_reduce has five modes: CNNL_SEGMENT_MAX, CNNL_SEGMENT_MIN and CNNL_SEGMENT_SUM modes are supported,
 *    and CNNL_SEGMENT_MEAN and CNNL_SEGMENT_PROD modes are not supported.
 * - In the CNNL_SEGMENT_MAX mode, if max is empty for a given segment_ids i, when data type is half or float, output[i] = min(data_type);
      when data type is int32, output[i] = 0.
 * - In the CNNL_SEGMENT_MIN mode, when data type is half or float, if max is empty for a given segment_ids i, output[i] = max(data_type);
      when data type is int32, output[i] = 0.
 * - In the CNNL_SEGMENT_MEAN, CNNL_SEGMENT_PROD, or CNNL_SEGMENT_SUM mode, if max is empty for a given segment_ids i, output[i] = max(data_type).
 * - In the CNNL_SEGMENT_PROD mode, if the prod is empty for a given segment_ids i, output[i] = 1.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The examples of the sorted segment reduce operations are as follows:
     Now it supports max, min, and sum mode, and does not support mean or prod mode.
     @verbatim
      mode CNNL_SEGMENT_MAX:
      1. data: an array by 3 * 4 --> [[1, 2, 3, 4],
                                      [4, 3, 2, 1],
                                      [5, 6, 7, 8]]

         segment_ids: an array by 3 --> [0, 0, 1]

         output: an array by 2 * 4 --> [[4, 3, 3, 4],
                                        [5, 6, 7, 8]]

      2. data: an array by 3 * 4 --> [[1, 2, 3, 4],
                                      [4, 3, 2, 1],
                                      [5, 6, 7, 8]]

         segment_ids: an array by 3 --> [0, 0, 2]

         output: an array by 3 * 4 --> [[4, 3, 3, 4],
                                        [0, 0, 0, 0],
                                        [5, 6, 7, 8]]

      mode CNNL_SEGMENT_MIN:
      1. data: an array by 3 * 4 --> [[1, 2, 3, 4],
                                      [4, 3, 2, 1],
                                      [5, 6, 7, 8]]

         segment_ids: an array by 3 --> [0, 0, 1]

         output: an array by 2 * 4 --> [[1, 2, 2, 1],
                                        [5, 6, 7, 8]]

      2. data: an array by 3 * 4 --> [[1, 2, 3, 4],
                                      [4, 3, 2, 1],
                                      [5, 6, 7, 8]]

         segment_ids: an array by 3 --> [0, 0, 2]

         output: an array by 3 * 4 --> [[1, 2, 2, 1],
                                        [0, 0, 0, 0],
                                        [5, 6, 7, 8]]

      mode CNNL_SEGMENT_PROD:
      1. data: an array by 3 * 4 --> [[1, 2, 3, 4],
                                      [4, 3, 2, 1],
                                      [5, 6, 7, 8]]

         segment_ids: an array by 3 --> [0, 0, 1]

         output: an array by 2 * 4 --> [[4, 6, 6, 4],
                                        [5, 6, 7, 8]]

      2. data: an array by 3 * 4 --> [[1, 2, 3, 4],
                                      [4, 3, 2, 1],
                                      [5, 6, 7, 8]]

         segment_ids: an array by 3 --> [0, 0, 2]

         output: an array by 3 * 4 --> [[4, 6, 6, 4],
                                        [1, 1, 1, 1],
                                        [5, 6, 7, 8]]

      mode CNNL_SEGMENT_SUM:
      1. data: an array by 3 * 4 --> [[1, 2, 3, 4],
                                      [4, 3, 2, 1],
                                      [5, 6, 7, 8]]

         segment_ids: an array by 3 --> [0, 0, 0]

         output: an array by 1 * 4 --> [[5, 6, 7, 8]]

      2. data: an array by 3 * 4 --> [[1, 2, 3, 4],
                                      [4, 3, 2, 1],
                                      [5, 6, 7, 8]]

         segment_ids: an array by 3 --> [0, 0, 2]

         output: an array by 3 * 4 --> [[5, 5, 5, 5],
                                        [0, 0, 0, 0],
                                        [5, 6, 7, 8]]

      mode CNNL_SEGMENT_MEAN:
      1. data: an array by 3 * 4 --> [[1, 2, 3, 4],
                                      [4, 3, 2, 1],
                                      [5, 6, 7, 8]]

         segment_ids: an array by 3 --> [0, 0, 1]

         output: an array by 2 * 4 --> [[2.5, 2.5, 2.5, 2.5],
                                        [5,   6,   7,   8]]

      2. data: an array by 3 * 4 --> [[1, 2, 3, 4],
                                      [4, 3, 2, 1],
                                      [5, 6, 7, 8]]

         segment_ids: an array by 3 --> [0, 0, 3]

         output: an array by 4 * 4 --> [[2.5, 2.5, 2.5, 2.5],
                                        [0,   0,   0,   0],
                                        [0,   0,   0,   0],
                                        [5,   6,   7,   8]]
     @endverbatim
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/math/segment_max
 * - https://www.tensorflow.org/api_docs/python/tf/math/segment_min
 * - https://www.tensorflow.org/api_docs/python/tf/math/segment_prod
 * - https://www.tensorflow.org/api_docs/python/tf/math/segment_sum
 * - https://www.tensorflow.org/api_docs/python/tf/math/segment_mean
 */

cnnlStatus_t CNNL_WIN_API cnnlSortedSegmentReduce(cnnlHandle_t handle,
                                                  const cnnlSortedSegmentReduceMode_t mode,
                                                  const cnnlTensorDescriptor_t data_desc,
                                                  const void *data,
                                                  const cnnlTensorDescriptor_t ids_desc,
                                                  const void *segment_ids,
                                                  const cnnlTensorDescriptor_t output_desc,
                                                  void *output);

// Group:UnsortedSegmentSum
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace
 * to optimize the unsorted segment sum operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the unsorted segment sum operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  data_desc
 *   Input. The descriptor of data tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in]  output_desc
 *   Input. The descriptor of output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out]  size
 *   Output. A host pointer to the returned size of the extra workspace in bytes
 *   that is used in the unsorted segment sum operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetUnsortedSegmentSumWorkspaceSize(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t data_desc,
                                       const cnnlTensorDescriptor_t output_desc,
                                       size_t *size);
// Group:UnsortedSegmentSum
/*!
 * @brief Computes the sum along segments of a tensor.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   the unsorted segment sum operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  data_desc
 *   Input. The descriptor of data tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  data
 *   Input. Pointer to the MLU memory that stores the data tensor.
 * @param[in]  ids_desc
 *   Input. The descriptor of segment indices tensor used as an index indicating
 *   the position of summation on output. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  segment_ids
 *   Input. Pointer to the MLU memory that stores the segment indices.
 * @param[in]  workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   unsorted segment sum operation. For more information about workspace, see
 *   "Cambricon CNNL User Guide".
 * @param[in]  workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the
 *   unsorted segment sum operation. You can get the size of the workspace with the
 *   ::cnnlGetUnsortedSegmentSumWorkspaceSize function.
 * @param[in]  output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out]  output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "UnsortedSegmentSum Operation" section in "Cambricon CNNL User Guide" for details.

 * @par Data Type
 * - This function supports any combinations of the following data types for data tensor \p data,
 * segment indices tensor \p segment_ids, and output tensor \p output.
 * Note that the data type of data tensor and output tensor must be the same.
 *   - \p data: half, float, bfloat16.
 *   - \p segment_ids: int32, int64.
 *   - \p output: half, float, bfloat16.
 *
 *   The bfloat16 data type is supported only on MLU500 series.
 * @par Scale Limitation
 * - The \p data tensor, \p segment_ids tensor, and the \p output tensor must meet the following
 * requirements:
 *   - data_dim - segment_ids_dim + 1 == output_dim,
 *     where data_dim, segment_ids_dim and output_dim represent the number of
 *     dimensions of \p data, \p segment_ids and \p output, respectively.
 *   - The dimension of \p segment_ids should be less than or equal to the dimension of \p data.
 *   - The \p data will not be added to the \p output if an invalid \p segment_ids values is set.
 *     The valid value of \p segment_ids is greater than or equal to 0, or less than dims[0] of the
 *     \p output.
 *
 * @par API Dependency
 * - Before calling this function to implement unsorted segment sum, you need to call
 * ::cnnlGetUnsortedSegmentSumWorkspaceSize to get the extra space size needed in
 * unsorted segment sum operation.
 *
 * @note
 * - The int64 type of \p segment_ids is supported only on MLU500 series.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the unsorted segment sum operation is as follows:
   @verbatim
   data: an array by 3 * 4 --> [[1, 2, 3, 4],
                                [5, 6, 7, 8],
                                [4, 3, 2, 1]]

   segment_ids: an array by 3 --> [0, 1, 0]

   output: an array by 2 * 4 --> [[5, 5, 5, 5],
                                  [5, 6, 7, 8]]
   @endverbatim
 *
 * @par Reference
 * - https://tensorflow.org/api_docs/python/tf/math#Segmentation
 */
cnnlStatus_t CNNL_WIN_API cnnlUnsortedSegmentSum(cnnlHandle_t handle,
                                                 const cnnlTensorDescriptor_t data_desc,
                                                 const void *data,
                                                 const cnnlTensorDescriptor_t ids_desc,
                                                 const void *segment_ids,
                                                 void *workspace,
                                                 size_t workspace_size,
                                                 const cnnlTensorDescriptor_t output_desc,
                                                 void *output);
// Group:MatrixBandPart
/*!
 * @brief Copies a tensor setting everything outside a central band in each innermost matrix to zero.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in ::cnnlMatrixBandPart. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] data_desc
 *   Input. The descriptor of input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] num_lower
 *   Input. A int32 scalar, the number of subdiagonals to keep.
 * @param[in] num_upper
 *   Input. A int32 scalar, the number of superdiagonals to keep.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "MatrixBandPart operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types and must be the same between input
 * and output.
 *   - input: float16, float32.
 *   - num_lower: int32, int64.
 *   - num_upper: int32, int64.
 *   - output: float16, float32.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the ::cnnlMatrixBandPart function is as follows:
     @verbatim
       input tensor by 1*4*4 --> input: [[ 0,  1,  2, 3]
                                         [-1,  0,  1, 2]
                                         [-2, -1,  0, 1]
                                         [-3, -2, -1, 0]

                                 num_lower: 1

                                 num_upper: -1

       datatype: FP32

       output tensor --> output:  [ 0,  1,  2, 3]
                                  [-1,  0,  1, 2]
                                  [ 0, -1,  0, 1]
                                  [ 0,  0, -1, 0]

     @endverbatim
 *
 * @par Reference
 * - http://www.tensorflow.org/api_doc/python/tf/linalg/band_part
 */
cnnlStatus_t CNNL_WIN_API cnnlMatrixBandPart(cnnlHandle_t handle,
                                             const cnnlTensorDescriptor_t data_desc,
                                             const void *input,
                                             const int num_lower,
                                             const int num_upper,
                                             void *output);

// Group:L2Loss
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the l2loss operation.
 *
 * The size of the extra workspace is based on the given information of the l2loss operation,
 * including the input tensor descriptor \p x_desc.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the l2loss
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *    Output. Pointer to the returned size of the extra workspace in bytes that is used in the l2loss
 *    operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - This API is only used along with ::cnnlL2Loss_v2. ::cnnlL2Loss does not require this API.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetL2LossWorkspaceSize(cnnlHandle_t handle,
                                                     const cnnlTensorDescriptor_t x_desc,
                                                     size_t *workspace_size);

// Group:L2Loss
/*!
 * @brief Computes half of the sum of squares of input tensor (also referred as to half the L2 normalization without sqrt),
 *        and returns the results in the output tensor \p y.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlL2Loss_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the l2loss
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "L2Loss Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensor and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float, bfloat16.
 *   - output tensor: half, float, bfloat16.
 *
 *   The bfloat16 data type is supported only on MLU500 series or above.
 *
 * @par Data Layout
 * - The data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - ::cnnlL2Loss_v2 should be used on MLU500 series.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the l2loss operation is as follows:
 *   @verbatim
      input tensor by 2*2 --> x: [[1, 2], [3, 4]]
      output scalar  --> y: [15]
     @endverbatim
 *
 * @par Reference
 * - http://www.tensorflow.org/api_doc/python/tf/nn/l2_loss
 */
CNNL_DEPRECATED_FOR(cnnlL2Loss_v2)
cnnlStatus_t CNNL_WIN_API cnnlL2Loss(cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t x_desc,
                                     const void *x,
                                     void *y);

// Group:L2Loss
/*!
 * @brief Computes half of the sum of squares of input tensor (also referred as to half the L2 normalization without sqrt),
 *        and returns the results in the output tensor \p y.
 *
 * Compared with ::cnnlL2Loss, this function requires you to allocate some extra workspace as an input
 * parameter.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the l2loss
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in]  workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlL2Loss_v2.
 * @param[in]  workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   ::cnnlL2Loss_v2. You can get the size of the workspace with
 *   the ::cnnlGetL2LossWorkspaceSize function.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED.
 *
 * @par Data Type
 * - Data type of input tensor and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float, bfloat16.
 *   - output tensor: half, float, bfloat16.
 *
 *   The bfloat16 data type is supported only on MLU500 series or above.
 *
 * @par Data Layout
 * - The data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Formula
 * - See "L2Loss Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par API Dependency
 * - Before using this API, you need to get the size of the workspace with the ::cnnlGetL2LossWorkspaceSize
 *   function and pass the required extra workspace to the ::cnnlL2Loss_v2 function.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the l2loss operation is as follows:
 *   @verbatim
      input tensor by 2*2 --> x: [[1, 2], [3, 4]]
      output scalar  --> y: [15]
     @endverbatim
 *
 * @par Reference
 * - http://www.tensorflow.org/api_doc/python/tf/nn/l2_loss
 */
cnnlStatus_t CNNL_WIN_API cnnlL2Loss_v2(cnnlHandle_t handle,
                                        const cnnlTensorDescriptor_t x_desc,
                                        const void *x,
                                        void *workspace,
                                        size_t workspace_size,
                                        void *y);

// Group:Scale
/*!
 * @brief Applies a linear transformation to the input tensor \p x along the specified \p axis
 *        with the filter tensor \p alpha and the bias tensor \p beta, and returns the results
 *        in the output tensor \p y.
 *
 * Support several consecutive dimensions applying multiplication and addition operations
 * along the specified \p axis.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlOpTensor instead.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the scale operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] axis
 *   Input.  Integer which marks the start operating dimension of input.
 *   May be negative to index from the end (e.g., -1 for the last axis).
 * @param[in]  desc_x
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in]  desc_alpha
 *   Input. The descriptor of the \p alpha tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  alpha
 *   Input. Pointer to the MLU memory that stores the \p alpha tensor which is the scaling factor for input.
 *   Could be set to NULL which will be treated as scalar 1.
 * @param[in]  desc_beta
 *   Input. The descriptor of the \p beta tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  beta
 *    Input. Pointer to the MLU memory that stores the \p beta tensor which is the scaling factor for input.
 *    Could be set to NULL which will be treated as scalar 0.
 * @param[out]  desc_y
 *    Output. The descriptor of the output tensor. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 * @param[out]  y
 *    Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "Scale Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensor \p x, alpha tensor \p alpha, beta tensor \p beta
 *   and output tensor \p y must be the same.
 * - The supported data types of input, alpha, beta and output tensors are as follows:
 *   - input tensor: half, float.
 *   - alpha tensor: half, float.
 *   - beta tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - This function supports multi-dimensional input, alpha, and beta.
 * - The supported data layout of input, alpha, beta, output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor must have the same shape.
 * - The alpha and beta could be scalar. If they are not scalar (which means tensor has more than one element), the dimensions they represent computing need to be the same.
 *
 * @note
 * - \p desc_alpha could be set to NULL if \p alpha is NULL which will be treated as scalar 1.
 * - \p desc_beta could be set to NULL if \p beta is NULL which will be treated as scalar 0.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the scale operation is as follows:
     @verbatim

     i) About axis and supported shape.

     For input x with shape (2, 3, 5, 8, 13) which is 5D tensor, axis should be inside [-5, 4).
     If axis is 1 or -4, which marks operating from 2nd dimension, alpha could be scalar or tensor in one of the following shapes:

         - 3:        (1, 3, 1, 1, 1)  or (3,)          or (3, 1, 1, 1)
         - 3x5:      (1, 3, 5, 1, 1)  or (3, 5)        or (3, 5, 1, 1)
         - 3x5x8:    (1, 3, 5, 8, 1)  or (3, 5, 8)     or (3, 5, 8, 1)
         - 3x5x8x13: (1, 3, 5, 8, 13) or (3, 5, 8, 13)

     And for this input x, the alpha and beta may have any of the following shapes (for the given value of axis):
        - (scalar)
        - (axis == 0 == -5) 2; 2x3; 2x3x5; 2x3x5x8; 2x3x5x8x13
        - (axis == 1 == -4)      3;   3x5;   3x5x8;   3x5x8x13
        - (axis == 2 == -3)             5;     5x8;     5x8x13
        - (axis == 3 == -2)                      8;       8x13
        - (axis == 4 == -1)                                 13


     ii) About computing

     If x is 4D tensor, axis is 1, alpha/beta has two dimensions participating in computing.

     Computation could be denoted as follows:

        y[i, j, k, l] = alpha[0, j, k, 0] * x[i, j, k, l] + beta[0, j, k, 0]


     For input tensor x with shape (2,3,2,2), axis is 1, alpha/beta has shape (1,3,2,1):

           x = [[[[   0.,   1.], [   2.,    3.]],
                 [[   4.,   5.], [   6.,    7.]],
                 [[   8.,   9.], [  10.,   11.]]],

                [[[  12.,  13.], [  14.,   15.]],
                 [[  16.,  17.], [  18.,   19.]],
                 [[  20.,  21.], [  22.,   23.]]]]

       alpha = [[[[        1.0], [        -1.0]],
                 [[        0.5], [        -0.5]],
                 [[        2.0], [        -2.0]]]]

        beta = [[[[        0.0], [         0.0]],
                 [[        0.2], [        -0.2]],
                 [[        0.5], [        -0.5]]]]

     The output tensor y has shape (2,3,2,2):

           y = [[[[   0.,   1.], [  -2.,   -3.]],
                 [[  2.2,  2.7], [ -3.2,  -3.7]],
                 [[ 16.5, 18.5], [-20.5, -22.5]]],

                [[[  12.,  13.], [ -14.,  -15.]],
                 [[  8.2,  8.7], [ -9.2,  -9.7]],
                 [[ 40.5, 42.5], [-44.5, -46.5]]]]

     @endverbatim
 *
 * @par Reference
 *  - https://caffe.berkeleyvision.org/tutorial/layers/scale.html
 */
CNNL_DEPRECATED_FOR(cnnlOpTensor)
cnnlStatus_t CNNL_WIN_API cnnlScale(cnnlHandle_t handle,
                                    int axis,
                                    const cnnlTensorDescriptor_t desc_x,
                                    const void *x,
                                    const cnnlTensorDescriptor_t desc_alpha,
                                    const void *alpha,
                                    const cnnlTensorDescriptor_t desc_beta,
                                    const void *beta,
                                    const cnnlTensorDescriptor_t desc_y,
                                    void *y);

// Group:Lrn
/*!
 * @brief Calculates a local response normalization on input tensor \p input, and
 * returns the results in the output tensor \p output. For more information about quantization,
 * see "Cambricon CNNL User Guide".
 *
 * This function needs extra MLU memory as the workspace. You can get the size of
 * the workspace \p workspace_size with the ::cnnlGetLrnWorkspaceSize_v2 function.
 * This function has different calculation modes, for detailed information,
 * see ::cnnlLrnMode_t.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlLrn_v2 instead.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the cnnlLrn operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  lrn_mode
 *   Input. The computing mode of lrn operation. For detailed information, see ::cnnlLrnMode_t.
 * @param[in] lrn_n
 *   Input. The width of lrn window.
 * @param[in]  lrn_alpha
 *   Input. Value of the alpha parameter in lrn formula, as a scale factor for variance.
 * @param[in]  lrn_beta
 *   Input. Value of the beta parameter in the lrn formula, as a power parameter.
 * @param[in]  lrn_k
 *   Input. Value of the k parameter in the formula, as a hyper-parameter.
 * @param[in]  workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlLrn. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in]  workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   ::cnnlLrn. You can get the size of the workspace with
 *   the ::cnnlGetLrnWorkspaceSize_v2 function.
 * @param[in] input_desc
 *   Input. The descriptor of input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in]  output_desc
 *   Input. The descriptor of output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out]  output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_ALLOC_FAILED, ::CNNL_STATUS_INTERNAL_ERROR, ::CNNL_STATUS_EXECUTION_FAILED.
 *
 * @par API Dependency
 * - Before you call this function, you can first call the ::cnnlGetLrnWorkspaceSize_v2 function to get
 *   the size of the memory workspace in MLU device that the function uses.
 *
 * @par Formula
 * - See "Lrn operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for input tensor \p input, and
 *   output tensor \p output on all hardware platforms.
 *   - \p input data type \p dtype: int8, int16, half, float.
 *   - \p output data type \p dtype: half, float.
 * - The \p input_desc data type should be set with the following rules:
 *   - If \p dtype in \p input_desc is fix-point type, its \p onchip_dtype will be automatically set to
 *     \p dtype. In this case, invoking ::cnnlSetTensorDescriptorOnchipDataType to set \p onchip_dtype
 *     in \p input_desc is neither necessary nor effective. The corresponding quantization parameters
 *     will be used during computation.
 *   - If \p dtype in \p input_desc is float-point type:
 *      - In the \p CNNL_LRN_LOCAL_SIZE, \p CNNL_LRN_LOCAL_SIZE_ORIGINAL or \p CNNL_LRN_CROSS_CHANNEL mode:
 *        - On MLU200 series and CE3226, processing will be more efficient, if \p onchip_dtype is set to
 *          int8/int16. If not set or set to int31, online quantization would be enabled internally.
 *        - On MLU300 series, the \p onchip_dtype in \p input_desc is floating-point and cannot be set.
 *      - In the \p CNNL_LRN_WITHIN_CHANNEL mode, the \p onchip_dtype in \p input_desc is floating-point
 *        and cannot be set.
 * - You do not need to set \p onchip_dtype in \p output_desc.
 *
 * @par Data Layout
 * - The supported layouts of both the \p input_desc and \p output_desc should be as follows:
 *   - CNNL_LAYOUT_NHWC.
 *   - CNNL_LAYOUT_NCHW.
 *   - CNNL_LAYOUT_HWCN.
 *
 * @par Scale Limitation
 * - The shape of input should be the same as output's.
 * - \p lrn_n should be in range of (0, 15], and \p lrn_n % 2 must be equal to 1.
 * - \p lrn_k should be greater than 1e-5.
 * - \p lrn_beta should be greater than 0.01.
 *
 * @par Reference
 * - None.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the layout of the input tensor and
 *   output tensor to NHWC.
 *
 * @note
 * - When \p input contains NaN or infinity:
 *   - On MLU200 series, \p output is saturation value.
 *   - On CE3226, \p output is random value.
 *   - On MLU300 series, \p output contains large amounts of unexpected NaN.
 * - The int8 and int16 data types of input are deprecated and will be removed in future release.
 */
CNNL_DEPRECATED_FOR(cnnlLrn_v2)
cnnlStatus_t CNNL_WIN_API cnnlLrn(cnnlHandle_t handle,
                                  cnnlLrnMode_t lrn_mode,
                                  unsigned lrn_n,
                                  double lrn_alpha,
                                  double lrn_beta,
                                  double lrn_k,
                                  void *workspace,
                                  size_t workspace_size,
                                  const cnnlTensorDescriptor_t input_desc,
                                  const void *input,
                                  const cnnlTensorDescriptor_t output_desc,
                                  void *output);
// Group:Lrn
/*!
 * @brief Calculates a local response normalization on input tensor \p input, and
 * returns the results in the output tensor \p output. Compared with ::cnnlLrn,
 * ::cnnlLrn_v2 provides better performance with extra input space. For more information
 * about quantization, see "Cambricon CNNL User Guide".
 *
 * This function needs extra MLU memory as the workspace. You can get the size of
 * the workspace \p workspace_size with the ::cnnlGetLrnWorkspaceSize_v2 function.
 * This function has different calculation modes, for detailed information,
 * see ::cnnlLrnMode_t.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the cnnlLrn operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  lrn_mode
 *   Input. The computing mode of lrn operation. For detailed information, see ::cnnlLrnMode_t.
 * @param[in] lrn_n
 *   Input. The width of lrn window.
 * @param[in]  lrn_alpha
 *   Input. Value of the alpha parameter in lrn formula, as a scale factor for variance.
 * @param[in]  lrn_beta
 *   Input. Value of the beta parameter in the lrn formula, as a power parameter.
 * @param[in]  lrn_k
 *   Input. Value of the k parameter in the formula, as a hyper-parameter.
 * @param[in]  workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlLrn. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in]  workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   ::cnnlLrn. You can get the size of the workspace with
 *   the ::cnnlGetLrnWorkspaceSize_v2 function.
 * @param[in] input_desc
 *   Input. The descriptor of input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in]  extra_device_input
 *   Input. Pointer to the MLU memory that stores the extra input data for the extra.
 *   You need to copy the extra input data to MLU from the host that is initialized with
 *   ::cnnlInitLrnExtraInput. For more information about extra input data, see Cambricon CNNL user Guide.
 * @param[in] extra_device_input_size
 *   Input. The size of the extra input space in bytes that is used in the lrn operation.
 * @param[in]  output_desc
 *   Input. The descriptor of output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out]  output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_INTERNAL_ERROR, ::CNNL_STATUS_EXECUTION_FAILED.
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlGetLrnWorkspaceSize_v2, ::cnnlGetLrnExtraInputSize_v2 and
 *   ::cnnlInitLrnExtraInput functions. ::cnnlGetLrnExtraInputSize_v2 gets the extra host and MLU memory
 *   size \p extra_input_size that is used as an extra input data to optimize the lrn operation. You need
 *   to allocate memory both on host and MLU based on the size returned in \p extra_input_size. Then call
 *   ::cnnlInitLrnExtraInput function to initialize the host memory \p extra_host_input and copy it to MLU
 *   memory \p extra_device_input. Finally, the extra workspace allocated by ::cnnlGetLrnWorkspaceSize_v2
 *   should be passed to the ::cnnlLrn_v2 function to perform the lrn operation.
 *
 * @par Formula
 * - See "Lrn operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for input tensor \p input, and
 *   output tensor \p output on all hardware platforms.
 *   - \p input data type \p dtype: int8, int16, half, float.
 *   - \p output data type \p dtype: half, float.
 * - The \p input_desc data type should be set with the following rules:
 *   - If \p dtype in \p input_desc is fix-point type, its \p onchip_dtype will be automatically set to
 *     \p dtype. In this case, invoking ::cnnlSetTensorDescriptorOnchipDataType to set \p onchip_dtype
 *     in \p input_desc is neither necessary nor effective. The corresponding quantization parameters
 *     will be used during computation.
 *   - If \p dtype in \p input_desc is float-point type:
 *      - In the \p CNNL_LRN_LOCAL_SIZE, \p CNNL_LRN_LOCAL_SIZE_ORIGINAL or \p CNNL_LRN_CROSS_CHANNEL mode:
 *        - On MLU200 series and CE3226, processing will be more efficient, if \p onchip_dtype is set to
 *          int8/int16. If not set or set to int31, online quantization would be enabled internally.
 *        - On MLU300 series, the \p onchip_dtype in \p input_desc is floating-point and cannot be set.
 *      - In the \p CNNL_LRN_WITHIN_CHANNEL mode, the \p onchip_dtype in \p input_desc is floating-point
 *        and cannot be set.
 * - You do not need to set \p onchip_dtype in \p output_desc.
 *
 * @par Data Layout
 *   -The layout of both the \p input_desc and \p output_desc should one of follows:
 *     - CNNL_LAYOUT_NHWC.
 *     - CNNL_LAYOUT_NCHW.
 *     - CNNL_LAYOUT_HWCN.
 *
 * @par Scale Limitation
 * - The shape of input should be the same as output's.
 * - \p lrn_n should be in range of (0, 15], and \p lrn_n % 2 must be equal to 1.
 * - \p lrn_k should be greater than 1e-5.
 * - \p lrn_beta should be greater than 0.01.
 *
 * @par Reference
 * - None.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the layout of the input tensor and
 *   output tensor to NHWC.
 *
 * @note
 * - When \p input contains NaN or infinity:
 *   - On MLU200 series, \p output is saturation value.
 *   - On CE3226, \p output is random value.
 *   - On MLU300 series, \p output contains large amounts of unexpected NaN.
 * - The int8 and int16 data types of input are deprecated and will be removed in future release.
 */
cnnlStatus_t CNNL_WIN_API cnnlLrn_v2(cnnlHandle_t handle,
                                  cnnlLrnMode_t lrn_mode,
                                  unsigned lrn_n,
                                  double lrn_alpha,
                                  double lrn_beta,
                                  double lrn_k,
                                  void *workspace,
                                  size_t workspace_size,
                                  const cnnlTensorDescriptor_t input_desc,
                                  const void *input,
                                  const void * extra_device_input,
                                  size_t extra_device_input_size,
                                  const cnnlTensorDescriptor_t output_desc,
                                  void *output);

// Group:LrnGrad
/*!
 * @brief Calculates the gradient of a local response normalization on input tensor \p x,
 * diff tensor \p dy, and returns the results in the gradient tensor \p dx.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the cnnlLrnGrad operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  lrn_mode
 *   Input.  The computing mode of lrnGrad operation.
 *   For detailed information, see ::cnnlLrnMode_t.
 * @param[in] lrn_n
 *   Input. The width of lrnGrad window.
 * @param[in]  lrn_alpha
 *   Input. Value of the alpha parameter in lrnGrad formula, as a scale factor for variance.
 * @param[in]  lrn_beta
 *   Input. Value of the beta parameter in the lrnGrad formula, as a power parameter.
 * @param[in]  lrn_k
 *   Input. Value of the k parameter in the lrnGrad formula, as a hyper-parameter.
 * @param[in] x_desc
 *   Input. The descriptor of input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] dy_desc
 *   Input. The descriptor of diff tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] dy
 *   Input. Pointer to the MLU memory that stores the diff tensor.
 * @param[in] dx_desc
 *   Input. The descriptor of gradient tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] dx
 *   Output. Pointer to the MLU memory that stores the gradient tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_EXECUTION_FAILED.
 *
 * @par Formula
 * - See "LrnGrad operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The two input(x, dy) tensors should have the same data type.
 * - The data type of input and output(dx) tensors should be half or float, the LrnGrad operation
 *   only supports half and float data type.
 * - On MLU200 series, the data type of input tensor should be the same as the data type
 *   of output tensor, otherwise the precision will be uncertain.
 *
 * @par Data Layout
 * - The layout of input(x, dy) and output(dx) should be the same.
 * - The layout must be CNNL_LAYOUT_NHWC or CNNL_LAYOUT_NCHW.
 * - This operation does not support CNNL_LAYOUT_HWCN layout.
 *
 * @par Scale Limitation
 * - On MLU300 series or above:
 *   - The shape of input(\p x, \p dy) should be the same as output(\p dx).
 *   - The \p lrn_n should be in range of [3, 15], and \p lrn_n % 2 must be equal to 1.
 *   - The \p lrn_beta should be equal to or greater than 0.01.
 *   - The \p lrn_k should be equal to or greater than 1e-5.
 * - On MLU200 series, for higher precision, the value ranges of the input and parameters
 *   should satisfy the requirements described below. Otherwise, the precision will be uncertain.
 *   - The value of input tensor should be in range of [-10.0, 10.0].
 *   - The \p lrn_n should be in range of [3, 9], and \p lrn_n % 2 must be equal to 1.
 *   - The \p lrn_alpha should be in range of [1.0, 5.0].
 *   - The \p lrn_beta should be in range of [1.0, 1.5].
 *   - The \p lrn_k should be in range of [1.0, 5.0].
 *
 * @note
 * - The operation does not support NaN or infinity in input data.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlLrnGrad(cnnlHandle_t handle,
                                      cnnlLrnMode_t lrn_mode,
                                      unsigned int lrn_n,
                                      float lrn_alpha,
                                      float lrn_beta,
                                      float lrn_k,
                                      const cnnlTensorDescriptor_t x_desc,
                                      const void *x,
                                      const cnnlTensorDescriptor_t dy_desc,
                                      const void *dy,
                                      const cnnlTensorDescriptor_t dx_desc,
                                      void *dx);
// Group:RsqrtBackward
/*!
 * @brief Computes gradient of rsqrt on input tensor \p y and \p diff_y, and returns the result
 *        in the output tensor \p output.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the rsqrt backward
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] data_desc
 *   Input. The descriptor of the tensors. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] diff_y
 *   Input. Pointer to the MLU memory that stores the gradient tensor from previous operation.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Rsqrt Backward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensors and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensors: half, float, bfloat16.
 *   - output tensor: half, float, bfloat16.
 *
 *   The bfloat16 data type is supported only on MLU500 series.
 *
 * @par Data Layout
 * - The supported data layout of the input tensors and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The input tensors and output tensor must have the same shape, and the input tensor \p y must meet
 *   the following input data range:
 *   - float: [1e-10,1e6].
 *   - half: [0.01,500].
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/rsqrt_grad
 */
cnnlStatus_t CNNL_WIN_API cnnlRsqrtBackward(cnnlHandle_t handle,
                                            const cnnlTensorDescriptor_t data_desc,
                                            const void *y,
                                            const void *diff_y,
                                            void *output);

// Group:SqrtBackward
/*!
 * @brief Computes gradient of sqrt on input tensor \p y and \p diff_y, and returns the results
 *        in the output tensor \p diff_x.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the sqrt backward
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] y_dy_dx_desc
 *   Input. The descriptor of the tensors. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] diff_y
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Sqrt Backward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensors and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensors: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensors and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The input tensors and output tensor must have the same shape, and the input tensor \p y must meet
 *   the following input data range:
 *   - float: [1e-10,1e6].
 *   - half: [0.01,500].
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/sqrt_grad
 */
cnnlStatus_t CNNL_WIN_API cnnlSqrtBackward(cnnlHandle_t handle,
                                           const cnnlTensorDescriptor_t y_dy_dx_desc,
                                           const void *y,
                                           const void *diff_y,
                                           void *diff_x);

// Group:NumTrue
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace.
 *
 * The size of the extra workspace is based on the given information of numtrue operation,
 * including input tensor descriptor \p data_desc.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   numtrue operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] data_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *   the numtrue operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called before the ::cnnlNumTrue function.
 * - The allocated extra workspace should be passed to the ::cnnlNumTrue function to perform
 *   the numtrue operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetNumTrueWorkspaceSize(cnnlHandle_t handle,
                                                      const cnnlTensorDescriptor_t data_desc,
                                                      size_t *size);

// Group:NumTrue
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace.
 *
 * The size of the extra workspace is based on the given information of numtrue operation,
 * including the tensor descriptor \p num_true_desc. Compared with ::cnnlGetNumTrueWorkspaceSize, it takes the descriptor of output tensor \p num_true as the parameter instead of \p data_desc.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   numtrue operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] num_true_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *   the numtrue operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called before the ::cnnlNumTrue_v3 function.
 * - The allocated extra workspace should be passed to the ::cnnlNumTrue_v3 function to perform
 *   the numtrue operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetNumTrueWorkspaceSize_v2(cnnlHandle_t handle,
                                                         const cnnlTensorDescriptor_t num_true_desc,
                                                         size_t *workspace_size);
// Group:NumTrue
/*!
 * @brief Computes the index and the number of non-zeros or true elements in input tensor.
 *
 * It is used in transformer on TensorFlow framework.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlNumTrue_v3 instead, which does not need to compute the preliminary
 *   indices of the non-zero elements.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   numtrue operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[out] index
 *   Output. Pointer to the MLU memory that stores the coordinate of \p x which
 *   has been reshaped in one dimension.
 * @param[out] num_true
 *   Output. Pointer to the MLU memory that stores the number of non-zeros or true elements in
 *   input tensor \p x.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \p x and output tensor
 *   \p index, and \p num_true.
 *   - input tensor: half, float, int32, bool.
 *   - index tensor: uint32.
 *   - num_true tensor: uint32.
 *
 * @par Data Layout
 * - The dimension of input tensor should be less than or equal to 8.
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlGetNumTrueWorkspaceSize function.
 * - Allocated extra workspace by ::cnnlGetNumTrueWorkspaceSize function should be passed to
 *   the ::cnnlNumTrue function to perform the numtrue operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the numtrue operation is as follows:
   @verbatim
    input array by 4 * 7 -->
       input: [[0, 0, 0, 0, 5, 0, 0],
               [0, 0, 1, 0, 0, 0, 0],
               [0, 0, 0, 0, 0, 0, 3],
               [6, 0, 0, 0, 0, 0, 0]]

    output array by 1 * 32 and 1 * 1 -->
       index: [4, 0, 0, 0, 0, 0, 0, 2,
               0, 0, 0, 0, 0, 0, 6, 0,
               0, 0, 0, 0, 0, 0, 0, 0,
               0, 0, 0, 0, 1, 1, 1, 1]

   --> num_true: [4]
   @endverbatim
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/where
 */
CNNL_DEPRECATED_FOR(cnnlNumTrue_v3)
cnnlStatus_t CNNL_WIN_API cnnlNumTrue(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t x_desc,
                                      const void *x,
                                      uint32_t *index,
                                      uint32_t *num_true);


// Group:NumTrue
/*!
 * @brief Computes the number of non-zeros or true elements in input tensor.
 *
 * It is used in transformer on TensorFlow framework. Compared with ::cnnlNumTrue, it does not compute the indices of the non-zero elements.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlNumTrue_v3 instead, which does not need to compute the preliminary
 *   indices of the non-zero elements.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   numtrue operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] num_true_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] num_true
 *   Output. Pointer to the MLU memory that stores the number of non-zeros or true elements in
 *   input tensor \p x.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \p x and output tensor
 *   \p num_true.
 *   - input tensor: half, float, int32, bool.
 *   - num_true tensor: int32.
 *
 * @par Data Layout
 * - The supported data layouts of the input tensor and output tensor are as follows:
 *   - \p x: \p CNNL_LAYOUT_ARRAY.
 *   - \p num_true: \p CNNL_LAYOUT_ARRAY.
 *
 * @par API Dependency
 * - None
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the numtrue operation is as follows:
   @verbatim
    input array by 4 * 7 -->
       input: [[0, 0, 0, 0, 5, 0, 0],
               [0, 0, 1, 0, 0, 0, 0],
               [0, 0, 0, 0, 0, 0, 3],
               [6, 0, 0, 0, 0, 0, 0]]

    output array with shape of 1 * 1 -->
       num_true: [4]
   @endverbatim
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/where
 */
CNNL_DEPRECATED_FOR(cnnlNumTrue_v3)
cnnlStatus_t CNNL_WIN_API cnnlNumTrue_v2(cnnlHandle_t handle,
                                         const cnnlTensorDescriptor_t x_desc,
                                         const void *x,
                                         const cnnlTensorDescriptor_t num_true_desc,
                                         void *num_true);

// Group:NumTrue
/*!
 * @brief Computes the number of non-zeros or true elements in input tensor.
 *
 * It is used in transformer on TensorFlow framework. Compared with ::cnnlNumTrue, it does not compute the indices of the non-zero elements.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   numtrue operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the numtrue operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the numtrue
 *   operation. You can get the size of the workspace with the ::cnnlGetNumTrueWorkspaceSize_v2 function.
 * @param[in] num_true_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] num_true
 *   Output. Pointer to the MLU memory that stores the number of non-zeros or true elements in
 *   input tensor \p x.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \p x and output tensor
 *   \p num_true.
 *   - \p x: half, float, int32, bool, int8, uint8, int16, bfloat16.
 *   - \p num_true: int32, int64.
 *
 * @par Data Layout
 * - The supported data layouts of the input tensor and output tensor are as follows:
 *   - \p x: \p CNNL_LAYOUT_ARRAY.
 *   - \p num_true: \p CNNL_LAYOUT_ARRAY.
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlGetNumTrueWorkspaceSize_v2 function.
 * - The extra workspace allocated by ::cnnlGetNumTrueWorkspaceSize_v2 should be
 *   passed to this function to perform the numtrue operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the numtrue operation is as follows:
   @verbatim
    input array by 4 * 7 -->
       input: [[0, 0, 0, 0, 5, 0, 0],
               [0, 0, 1, 0, 0, 0, 0],
               [0, 0, 0, 0, 0, 0, 3],
               [6, 0, 0, 0, 0, 0, 0]]

    output array with shape of 1 * 1 -->
       num_true: [4]
   @endverbatim
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/where
 */
cnnlStatus_t CNNL_WIN_API cnnlNumTrue_v3(cnnlHandle_t handle,
                                         const cnnlTensorDescriptor_t x_desc,
                                         const void *x,
                                         void *workspace,
                                         const size_t workspace_size,
                                         const cnnlTensorDescriptor_t num_true_desc,
                                         void *num_true);

// Group:Where
/*!
 * @brief Computes index of non-zeros or true elements in input tensor.
 *
 * It is used in transformer on TensorFlow framework.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlWhere_v2 instead, which is much faster in computing the indices of the non-zero elements.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the where operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] strides
 *   Output. Pointer to the MLU memory that stores stride of each dimension in \p x.
 * @param[in] index
 *   Output. Pointer to the MLU memory that stores the coordinate of \p x which has been reshaped
 *   in one dimension.
 * @param[in] y_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor which is the coordinate of
 *   non-zeros or true elements in \p x.
 * @param[in] as_tuple
 *   Input. A scalar. When \p as_tuple is false, \p y points to a tensor containing the indices of
 *   all non-zeros or true elements of input tensor \p x. When \p as_tuple is true, \p y points to a
 *   tuple of 1D tensor, each containing the indices (in that dimension) of all
 *   non-zeros or true elements in the input tensor \p x.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \p x, \p strides,
 *   \p index and output tensor \p y.
 *   - input tensor: half, float, int32, bool.
 *   - strides tensor: uint32.
 *   - index tensor: uint32.
 *   - output tensor: int32.
 *
 * @par Data Layout
 * - The dimension of input tensor should be less than or equal to 8.
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlGetNumTrueWorkspaceSize function and
 *   ::cnnlNumTrue function.
 * - The extra workspace allocated by ::cnnlGetNumTrueWorkspaceSize should be
 *   passed to the ::cnnlNumTrue function to perform the numTrue operation. Index tensor obtained
 *   by ::cnnlNumTrue function should be passed to the ::cnnlWhere function to perform the
 *   where operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the numtrue operation is as follows:
   @verbatim
    input array by 4 * 7, 1 * 2 and 1 * 32 -->
        input: [[0, 0, 0, 0, 5, 0, 0],
                [0, 0, 1, 0, 0, 0, 0],
                [0, 0, 0, 0, 0, 0, 3],
                [6, 0, 0, 0, 0, 0, 0]]

    --> strides: [7, 1]

    --> index: [4, 0, 0, 0, 0, 0, 0,
                2, 0, 0, 0, 0, 0, 0,
                6, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0,
               1, 1, 1, 1]

   output array by 4*2 --> output: [[0, 4],
                                    [1, 2],
                                    [2, 6],
                                    [3, 0]]
   @endverbatim
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/where
 */
CNNL_DEPRECATED_FOR(cnnlWhere_v2)
cnnlStatus_t CNNL_WIN_API cnnlWhere(cnnlHandle_t handle,
                                    const cnnlTensorDescriptor_t x_desc,
                                    const void *x,
                                    const uint32_t *strides,
                                    const uint32_t *index,
                                    const cnnlTensorDescriptor_t y_desc,
                                    int *y,
                                    const bool as_tuple);


// Group:Where
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace.
 *
 * The size of the extra workspace is based on the given information of where operation,
 * including input tensor descriptor \p x_desc.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   where operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] num_true_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *   the where operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called before the ::cnnlWhere_v2 function.
 * - The allocated extra workspace should be passed to the ::cnnlWhere_v2 function to perform
 *   the where operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetWhereWorkspaceSize(cnnlHandle_t handle,
                                                    const cnnlTensorDescriptor_t num_true_desc,
                                                    size_t *workspace_size);

// Group:Where
/*!
 * @brief Computes index of non-zeros or true elements in input tensor.
 *
 * It is used in transformer on TensorFlow framework. Compared with ::cnnlWhere, this function needs
 * extra workspace and does not need the preliminary indices (instead computes the ultimate indices in a faster way).
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the where operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] num_true_desc
 *   Input. The descriptor of the \p num_true tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] num_true
 *   Input. Pointer to the MLU memory that stores the number of none-zero elements in \p x.
 * @param[in] as_tuple
 *   Input. A scalar. When \p as_tuple is false, \p y points to a tensor containing the indices of
 *   all non-zeros or true elements of input tensor \p x. When \p as_tuple is true, \p y points to a
 *   tuple of 1D tensor, each containing the indices of a specific dimension for all
 *   non-zeros or true elements in the input tensor \p x.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the where operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the where
 *   operation. You can get the size of the workspace with the ::cnnlGetWhereWorkspaceSize function.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor which is the indices of
 *   non-zeros or true elements in \p x.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \p x, \p num_true,
 *   and output tensor \p y.
 *   - \p x: half, float, int32, bool, int8, uint8, int16, bfloat16.
 *   - \p num_true: int32, int64.
 *   - \p y: int32, int64.
 *
 * @par Data Layout
 * - The dimension of input tensor should be less than or equal to CNNL_DIM_MAX.
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlGetWhereWorkspaceSize function and
 *   ::cnnlNumTrue_v2 function.
 * - The extra workspace allocated by ::cnnlGetWhereWorkspaceSize should be
 *   passed to this function to perform the where operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the where operation is as follows:
   @verbatim
    input array with shape of 4 * 7 -->
        input: [[0, 0, 0, 0, 5, 0, 0],
                [0, 0, 1, 0, 0, 0, 0],
                [0, 0, 0, 0, 0, 0, 3],
                [6, 0, 0, 0, 0, 0, 0]]


        --> num_true: 4

        --> as_tuple: false

    output array by 4*2 --> output: [[0, 4],
                                     [1, 2],
                                     [2, 6],
                                     [3, 0]]
   @endverbatim
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/where
 */
cnnlStatus_t CNNL_WIN_API cnnlWhere_v2(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t x_desc,
                                       const void *x,
                                       const cnnlTensorDescriptor_t num_true_desc,
                                       const void *num_true,
                                       const bool as_tuple,
                                       void *workspace,
                                       const size_t workspace_size,
                                       const cnnlTensorDescriptor_t y_desc,
                                       void *y);

// Group:Embedding
/*!
 * @brief Maps \p filter to \p output according to \p indices.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlEmbeddingForward_v2 instead, which supports the parameter \p padding_idx that
 *   determines which index of the embedding vector \p output should be initialized to zero.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the embedding
 *   forward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] filter_desc
 *   Input. The descriptor of the filter tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the filter tensor.
 * @param[in] indices_desc
 *   Input. The descriptor of the index tensor used to store the index of \p filter which
 *   corresponds to each row of \p output. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] indices
 *   Input. Pointer to the MLU memory that stores the index of \p filter that corresponds to each
 *   row of \p output. The value of indices should be less than the first dimension of filter.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "EmbeddingForward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This (I/O)function supports any combinations of the following data types for index tensor
 *   \p indices, filter tensor \p filter, and output tensor \p output.
 *   - filter tensor: 1-byte, 2-byte, 4-byte, 8-byte.
 *   - index tensor: int32, int64 (int64 is not supported when layout is CNNL_LAYOUT_NHWC).
 *   - output tensor: 1-byte, 2-byte, 4-byte, 8-byte.
 *
 *   The byte width of a data type can be got with the ::cnnlGetSizeOfDataType function.
 *
 *   Note that the data type of input tensor \p input and output tensor \p output must be the same.
 *
 * @par Data Layout
 * - The layout of the filter tensor, index tensor, and output tensor are as
 *   follows:
 *   - filter tensor: \p CNNL_LAYOUT_ARRAY(default) and \p CNNL_LAYOUT_NHWC.
 *   - index tensor: \p CNNL_LAYOUT_ARRAY(default) and \p CNNL_LAYOUT_NHWC.
 *   - output tensor: \p CNNL_LAYOUT_ARRAY(default) and \p CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - The filter tensor, index tensor, and output tensor must meet the following requirements:
 *   - filter tensor: The dimension should be 2.
 *   - output tensor: The element number should be no more than \p UINT_MAX on MLU300 series and CE3226.
 *
 * @note
 * - From Cambricon CNNL 1.9.O and later versions, the layout of this operation is recommended to be set
 *   as \p CNNL_LAYOUT_ARRAY. However, to be compatible with the previous layout settings,
 *   you can still set the layout of output tensor to \p CNNL_LAYOUT_NHWC, which will
 *   transpose the last two dimensions of output.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the embedding backward operation is as follows:
     @verbatim
     input two arrays by 2 * 3 and 2 * 2

     --> filter: [[0.5356,  1.5739, -0.4864], [-0.6622, -0.4790,  0.8539]]

     --> indices: [[0, 1], [1, 0]]

     output array by 2 * 2 * 3 (output array by 2 * 3 * 2 if layout is CNNL_LAYOUT_NHWC)

     --> output: [[[0.5356,  1.5739, -0.4864], [-0.6622, -0.4790,  0.8539]],
                  [[-0.6622, -0.4790,  0.8539], [0.5356,  1.5739, -0.4864]]]
     if layout is CNNL_LAYOUT_NHWC:
     --> output: [[[0.5356, -0.6622], [1.5739, -0.4790], [-0.4864, 0.8539]],
                  [[-0.6622, 0.5356], [-0.4790, 1.5739], [0.8539, -0.4864]]]

     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html
 */
CNNL_DEPRECATED_FOR(cnnlEmbeddingForward_v2)
cnnlStatus_t CNNL_WIN_API cnnlEmbeddingForward(cnnlHandle_t handle,
                                               const cnnlTensorDescriptor_t filter_desc,
                                               const void *filter,
                                               const cnnlTensorDescriptor_t indices_desc,
                                               const void *indices,
                                               const cnnlTensorDescriptor_t output_desc,
                                               void *output);

// Group:Embedding
/*!
 * @brief Maps \p filter to \p output according to \p indices.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the embedding
 *   forward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] filter_desc
 *   Input. The descriptor of the filter tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the filter tensor.
 * @param[in] indices_desc
 *   Input. The descriptor of the index tensor used to store the index of \p filter which
 *   corresponds to each row of \p output. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] indices
 *   Input. Pointer to the MLU memory that stores the index of \p filter that corresponds to each
 *   row of \p output. The value of indices should be less than the first dimension of filter.
 * @param[in] padding_idx
 *   Input. Determines which index of the embedding vector \p output should be initialized to zero.
 *   The value of padding_idx should be included in range of [-1,filter[0]).
 * @param[in] max_norm
 *   Input. Pointer to the MLU memory that stores the maximum value of the cut-off norm.
 *   Currently, \p max_norm is not supported and needs to be set as null.
 * @param[in] norm_type
 *   Input. Pointer to the MLU memory that stores the type of used p-norm.
 *   Currently, \p norm_type is not supported and needs to be set as null.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @par Formula
 * - See "EmbeddingForward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The (I/O)function supports the following data widths for \p filter and \p output tensors.
 *   - filter tensor: 1-byte, 2-byte, 4-byte, 8-byte.
 *   - index tensor: int32, int64.
 *   - output tensor: 1-byte, 2-byte, 4-byte, 8-byte.
 *
 *   The byte width of a data type can be got with the ::cnnlGetSizeOfDataType function.
 *
 *   Note that the data type of input tensor \p filter and output tensor \p output must be the same.
 *
 * @par Data Layout
 * - The layout of the filter tensor, index tensor, and output tensor are as
 *   follows:
 *   - filter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - index tensor: \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The filter tensor, index tensor, and output tensor must meet the following requirements:
 *   - filter tensor: The dimension should be 2.
 *   - output tensor: The element number should be no more than \p UINT_MAX on MLU300 series and CE3226.
 *
 * @note
 * - The \p padding_idx cannot exceed the range of int32 representation.
 * - The \p filter vector at \p padding_idx row will be modified to 0 when \p padding_idx is greater than -1.
 *
 * @par Requirements
 * - The \p padding_idx should be included in range of [-1, filter[0]).
 *
 * @par Example
 * - The example of the embedding backward operation is as follows:
     @verbatim
     input two arrays by 2 * 3 and 2 * 2

     --> filter: [[0.5356,  1.5739, -0.4864], [-0.6622, -0.4790,  0.8539]]

     --> indices: [[0, 1], [1, 0]]

     output array by 2 * 2 * 3 (output array by 2 * 3 * 2 if layout is CNNL_LAYOUT_NHWC)

     --> output: [[[0.5356,  1.5739, -0.4864], [-0.6622, -0.4790,  0.8539]],
                  [[-0.6622, -0.4790,  0.8539], [0.5356,  1.5739, -0.4864]]]

     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html
 */
cnnlStatus_t CNNL_WIN_API cnnlEmbeddingForward_v2(cnnlHandle_t handle,
                                                  const cnnlTensorDescriptor_t filter_desc,
                                                  const void *filter,
                                                  const cnnlTensorDescriptor_t indices_desc,
                                                  const void *indices,
                                                  const int padding_idx,
                                                  const float *max_norm,
                                                  const float *norm_type,
                                                  const cnnlTensorDescriptor_t output_desc,
                                                  void *output);

// Group:EmbeddingBag
/*!
 * @brief Computes sums, means or the maximum value of bags of embedding forward operation.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlEmbeddingBag_v3 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   embedding_bag operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] filter_desc
 *   Input. The descriptor of the \p filter tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores embedding matrix. The \p filter should be a
 *   two-dimensional tensor. Its first dimension represents the size of the dictionary of
 *   embeddings and its second dimension represents the size of each embedding vector.
 * @param[in] indices_desc
 *   Input. The descriptor of the \p indices tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] indices
 *   Input. Pointer to the MLU memory that stores indices of \p filter which will be reduced to
 *   bags into \p output. The value of \p indices should be less than the first dimension of
 *   \p filter. The \p indices should be a one-dimensional or two-dimensional tensor.
 * @param[in] offset_desc
 *   Input. The descriptor of the \p offset tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] offset
 *   Input. Pointer to the MLU memory that stores the starting index position of each bag in \p indices.
 *   Only used when \p indices is 1D. The value of \p offset must be less than the size of indices
 *   dimension. The element in \p offset is sorted in an ascending order and starts with 0.
 * @param[in] per_sample_filter_desc
 *   Input. The descriptor of the \p per_sample_filter tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] per_sample_filter
 *   Input. The filter of \p indices, only used when mode is ::CNNL_REDUCEMODE_SUM.
 *   The shape of \p per_sample_filter must be the same as \p indices.
 * @param[in] mode
 *   Input. The way to reduce the bag, see ::cnnlReduceMode_t for more details.
 * @param[in] max_norm
 *   Input. Pointer to the host memory that stores the maximum value of the cut-off norm.
 *   Currently, \p max_norm is not supported and needs to be set as null.
 * @param[in] norm_type
 *   Input. Pointer to the host memory that stores the type of used p-norm.
 *   Currently, \p norm_type is not supported and needs to be set as null.
 * @param[in] include_last_offset
 *   Input. A Boolean value describing whether \p offset has one additional element. If true, the last
 *   value of \p offset is equivalent to the size of \p indices. If false, there is not an
 *   additional element in \p offset. Currently, \p include_last_offset is not supported and needs to be set as false.
 * @param[in] padding_idx
 *   Input. Pointer to the host memory that determines which index of the embedding vector should be
 *   initialized to zero and excluded from the reduction. Currently, \p padding_idx is not supported and needs to be set as null.
 * @param[in] output_desc
 *   Input. The descriptor of the \p output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the \p output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "EmbeddingBag Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for filter tensor
 *   \p filter, indices tensor \p indices, offset tensor \p offset, per_sample_filter tensor
 *   \p per_sample_filter and output tensor \p output:
 *   - filter tensor: half, bfloat16, float.
 *   - indices tensor: int32, int64.
 *   - offset tensor: int32, int64.
 *   - per_sample_filter tensor: half, bfloat16, float.
 *   - output tensor: half, bfloat16, float.
 * - Note that the data types of \p filter tensor, \p per_sample_filter and \p output
 *   tensor must be the same, and bfloat16 is only supported on MLU500 series or higher.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the embedding_bag operation is as follows:
     @verbatim
     filter array by 10 * 3 -->
         filter: [[0.5860, 0.6688, 0.2687],
                 [0.4091, 0.0169, 0.6869],
                 [0.1551, 0.2168, 0.2718],
                 [0.0460, 0.1324, 0.5958],
                 [0.3568, 0.3494, 0.4879],
                 [0.5781, 0.1850, 0.0506],
                 [0.7095, 0.9444, 0.3460],
                 [0.5262, 0.7910, 0.9559],
                 [0.5842, 0.8564, 0.8038],
                 [0.1000, 0.8825, 0.9091]]
     input two arrays by 1 * 8 and 1 * 3
     indices array by 1 * 8 -->
         indices: [1, 2, 3, 4, 5, 6, 7, 8]
     offset array by 1 * 3 -->
         offset: [0, 4, 6]
     param:
       mode: CNNL_REDUCEMODE_SUM

     output array by 3 * 3 -->
         output: [[0.9671, 0.7155, 2.0425],
                 [1.2877, 1.1294, 0.3965],
                 [1.1104, 1.6474, 1.7597]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html
 */
CNNL_DEPRECATED_FOR(cnnlEmbeddingBag_v3)
cnnlStatus_t cnnlEmbeddingBag(cnnlHandle_t handle,
                              const cnnlTensorDescriptor_t filter_desc,
                              const void *filter,
                              const cnnlTensorDescriptor_t indices_desc,
                              const int *indices,
                              const cnnlTensorDescriptor_t offset_desc,
                              const int *offset,
                              const cnnlTensorDescriptor_t per_sample_filter_desc,
                              const void *per_sample_filter,
                              const cnnlReduceMode_t mode,
                              const void *max_norm,
                              const void *norm_type,
                              const bool include_last_offset,
                              const int *padding_idx,
                              const cnnlTensorDescriptor_t output_desc,
                              void *output);

/*! The descriptor of the EmbeddingBag operation that holds EmbeddingBag information
 *  including \p mode, \p max_norm, \p norm_type, \p scale_grad_by_freq, \p include_last_offset
 *  and \p padding_idx.
 *
 *  You need to call the ::cnnlCreateEmbeddingBagDescriptor function to create a descriptor,
 *  and call the ::cnnlSetEmbeddingBagDescriptor function to set the information of
 *  EmbeddingBag operation to the descriptor. Also, you need to destroy the Cambricon CNNL context
 *  at the end with the ::cnnlDestroyEmbeddingBagDescriptor function.
 */
typedef struct cnnlEmbeddingBagStruct *cnnlEmbeddingBagDescriptor_t;

/*! The descriptor of the EmbeddingBag operation that holds EmbeddingBag information
 *  including \p mode, \p max_norm, \p norm_type, \p scale_grad_by_freq, \p include_last_offset
 *  and \p padding_idx. Compared with ::cnnlEmbeddingBagDescriptor_t, this descriptor uses mode
 *  with new type cnnlEmbeddingBagReduceMode_t.
 *
 *  You need to call the ::cnnlCreateEmbeddingBagDescriptor_v2 function to create a descriptor,
 *  and call the ::cnnlSetEmbeddingBagDescriptor_v2 function to set the information of
 *  EmbeddingBag operation to the descriptor. Also, you need to destroy the Cambricon CNNL context
 *  at the end with the ::cnnlDestroyEmbeddingBagDescriptor_v2 function.
 */
typedef struct cnnlEmbeddingBagStruct_v2 *cnnlEmbeddingBagDescriptorV2_t;

// Group:EmbeddingBag
/*!
 * @brief Creates a descriptor pointed by \p desc for an EmbeddingBag operation,
 *        and allocated memory for holding the information about the EmbeddingBag operation.
 *
 * The information is defined in ::cnnlEmbeddingBagDescriptor_t. For more information
 * about descriptor, see "Cambricon CNNL user Guide".
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlCreateEmbeddingBagDescriptor_v2 instead.
 *
 * @param[out] EmbeddingBag_desc
 *   Output. A host pointer to the EmbeddingBag descriptor that holds information about
 *   the EmbeddingBag operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetEmbeddingBagDescriptor
 *   function to initialize and set information to the EmbeddingBag descriptor.
 * - You need to call the ::cnnlDestroyEmbeddingBagDescriptor function to destroy
 *   the descriptor at the end of the context.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlCreateEmbeddingBagDescriptor_v2)
cnnlStatus_t CNNL_WIN_API
cnnlCreateEmbeddingBagDescriptor(cnnlEmbeddingBagDescriptor_t *EmbeddingBag_desc);

// Group:EmbeddingBag
/*!
 * @brief Creates a descriptor pointed by \p desc for an EmbeddingBag operation,
 *        and allocated memory for holding the information about the EmbeddingBag operation.
 *
 * The information is defined in ::cnnlEmbeddingBagDescriptorV2_t. For more information
 * about descriptor, see "Cambricon CNNL user Guide".
 *
 * @param[out] EmbeddingBag_desc
 *   Output. A host pointer to the EmbeddingBag descriptor that holds information about
 *   the EmbeddingBag operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetEmbeddingBagDescriptor_v2
 *   function to initialize and set information to the EmbeddingBag descriptor.
 * - You need to call the ::cnnlDestroyEmbeddingBagDescriptor_v2 function to destroy
 *   the descriptor at the end of the context.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCreateEmbeddingBagDescriptor_v2(cnnlEmbeddingBagDescriptorV2_t *EmbeddingBag_desc);

// Group:EmbeddingBag
/*!
 * @brief Initializes the EmbeddingBag descriptor \p EmbeddingBag_desc that was previously created
 * with the ::cnnlCreateEmbeddingBagDescriptor function, and sets the information
 * about the EmbeddingBag operation to the EmbeddingBag descriptor \p desc.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlSetEmbeddingBagDescriptor_v2 instead.
 *
 * @param[in,out] EmbeddingBag_desc
 *   Input/output. The descriptor of the EmbeddingBag operation. For detailed information,
 *   see ::cnnlEmbeddingBagDescriptor_t.
 * @param[in] mode
 *   Input. The way to reduce the bag. For more details, see ::cnnlReduceMode_t.
 * @param[in] max_norm
 *   Input. Pointer to the host memory that stores the maximum value of the cut-off norm.
 *   Currently, \p max_norm is not supported and needs to be set as null.
 * @param[in] norm_type
 *   Input. Pointer to the host memory that stores the type of used p-norm.
 *   Currently, \p norm_type is not supported and needs to be set as null.
 * @param[in] padding_idx
 *   Input. Pointer to the host memory that determines which index of the embedding vector should be
 *   initialized to zero and excluded from the reduction. Currently, \p padding_idx is not supported
 and needs to be set as null.
 * @param[in] scale_grad_by_freq
 *   Input. A Boolean value that determines whether to scale output tensor \p output by the
 *   inverse of frequency of the index.
 * @param[in] include_last_offset
 *   Input. A Boolean value describing whether \p offset has one additional element. If true, the
 last
 *   value of \p offset is equivalent to the size of \p indices. If false, there is not an
 *   additional element in \p offset. Currently, \p include_last_offset is not supported and needs
 to be set as false.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlSetEmbeddingBagDescriptor_v2)
cnnlStatus_t CNNL_WIN_API
cnnlSetEmbeddingBagDescriptor(cnnlEmbeddingBagDescriptor_t EmbeddingBag_desc,
                              const cnnlReduceMode_t mode,
                              const void *max_norm,
                              const void *norm_type,
                              const void *padding_idx,
                              const bool scale_grad_by_freq,
                              const bool include_last_offset);

// Group:EmbeddingBag
/*!
 * @brief Initializes the EmbeddingBag descriptor \p EmbeddingBag_desc that was previously created
 * with the ::cnnlCreateEmbeddingBagDescriptor function, and sets the information
 * about the EmbeddingBag operation to the EmbeddingBag descriptor \p desc.
 * Compared with ::cnnlSetEmbeddingBagDescriptor, ::cnnlSetEmbeddingBagDescriptor_v2 replaces the
 * type of mode from cnnlReduceMode_t to cnnlEmbeddingBagReduceMode_t, and uses new descriptor type
 * cnnlEmbeddingBagDescriptorV2_t.
 *
 * @param[in,out] EmbeddingBag_desc
 *   Input/output. The descriptor of the EmbeddingBag operation. For detailed information,
 *   see ::cnnlEmbeddingBagDescriptorV2_t.
 * @param[in] mode
 *   Input. The way to reduce the bag. For more details, see ::cnnlEmbeddingBagReduceMode_t.
 * @param[in] max_norm
 *   Input. Pointer to the host memory that stores the maximum value of the cut-off norm.
 *   Currently, \p max_norm is not supported and needs to be set as null.
 * @param[in] norm_type
 *   Input. Pointer to the host memory that stores the type of used p-norm.
 *   Currently, \p norm_type is not supported and needs to be set as null.
 * @param[in] padding_idx
 *   Input. Pointer to the host memory that determines which index of the embedding vector should be
 *   initialized to zero and excluded from the reduction. Currently, \p padding_idx is not supported
 and needs to be set as null.
 * @param[in] scale_grad_by_freq
 *   Input. A Boolean value that determines whether to scale output tensor \p output by the
 *   inverse of frequency of the index.
 * @param[in] include_last_offset
 *   Input. A Boolean value describing whether \p offset has one additional element. If true, the
 last
 *   value of \p offset is equivalent to the size of \p indices. If false, there is not an
 *   additional element in \p offset. Currently, \p include_last_offset is not supported and needs
 to be set as false.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetEmbeddingBagDescriptor_v2(cnnlEmbeddingBagDescriptorV2_t EmbeddingBag_desc,
                                 const cnnlEmbeddingBagReduceMode_t mode,
                                 const void *max_norm,
                                 const void *norm_type,
                                 const void *padding_idx,
                                 const bool scale_grad_by_freq,
                                 const bool include_last_offset);

// Group:EmbeddingBag
/*!
 * @brief Destroys the EmbeddingBag descriptor \p desc that was previously created with the
 *        ::cnnlCreateTensorDescriptor function.
 *
 * The EmbeddingBag descriptor is defined in ::cnnlEmbeddingBagDescriptor_t and holds the information
 * about the EmbeddingBag operation.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlDestroyEmbeddingBagDescriptor_v2 instead.
 *
 * @param[in] EmbeddingBag_desc
 *   Input. The EmbeddingBag descriptor to be destroyed. For detailed information,
 *   see ::cnnlEmbeddingBagDescriptor_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlDestroyEmbeddingBagDescriptor_v2)
cnnlStatus_t CNNL_WIN_API
cnnlDestroyEmbeddingBagDescriptor(cnnlEmbeddingBagDescriptor_t EmbeddingBag_desc);

// Group:EmbeddingBag
/*!
 * @brief Destroys the EmbeddingBag descriptor \p desc that was previously created with the
 *        ::cnnlCreateTensorDescriptor function.
 *
 * The EmbeddingBag descriptor is defined in ::cnnlEmbeddingBagDescriptorV2_t and holds the
 * information about the EmbeddingBag operation.
 *
 * @param[in] EmbeddingBag_desc
 *   Input. The EmbeddingBag descriptor to be destroyed. For detailed information,
 *   see ::cnnlEmbeddingBagDescriptorV2_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlDestroyEmbeddingBagDescriptor_v2(cnnlEmbeddingBagDescriptorV2_t EmbeddingBag_desc);

// Group:EmbeddingBag
/*!
 * @brief Computes sums, means or the maximum value of bags of embedding forward operation.
 *        Compared with ::cnnlEmbeddingBag, this function adds additional three outputs offset2bag,
 *        bag_size and max_indices, where offset2bag and bag_size are supported.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlEmbeddingBag_v3 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   embedding_bag operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] EmbeddingBag_desc
 *   Input. The descriptor of the EmbeddingBag operation. For detailed information,
 *   see ::cnnlEmbeddingBagDescriptor_t.
 * @param[in] filter_desc
 *   Input. The descriptor of the \p filter tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores embedding matrix. The \p filter should be a
 *   two-dimensional tensor. Its first dimension represents the size of the dictionary of
 *   embeddings and its second dimension represents the size of each embedding vector.
 * @param[in] indices_desc
 *   Input. The descriptor of the \p indices tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] indices
 *   Input. Pointer to the MLU memory that stores indices of \p filter which will be reduced to
 *   bags into \p output. The value of \p indices should be less than the first dimension of
 *   \p filter. The \p indices should be a one-dimensional or two-dimensional tensor.
 * @param[in] offset_desc
 *   Input. The descriptor of the \p offset tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] offset
 *   Input. Pointer to the MLU memory that stores the starting index position of each bag in \p
 indices.
 *   Only used when \p indices is 1D. The value of \p offset must be less than the size of indices
 *   dimension. The element in \p offset is sorted in an ascending order and starts with 0.
 * @param[in] per_sample_filter_desc
 *   Input. The descriptor of the \p per_sample_filter tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] per_sample_filter
 *   Input. The filter of \p indices, only used when mode is ::CNNL_REDUCEMODE_SUM.
 *   The shape of \p per_sample_filter must be the same as \p indices.
 * @param[in] output_desc
 *   Input. The descriptor of the \p output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the \p output tensor. The \p output tensor
 *   should be a two-dimensional tensor.
 * @param[in] offset2bag_desc
 *   Input. The descriptor of the \p offset2bag tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] offset2bag
 *   Output. Pointer to the MLU memory that stores the index of bag each embedding vector
 *   in \p indices belongs to. The \p offset2bag tensor should be 1D with the same number
 *   of elements as \p indices.
 *   Only calculated when the pointers of \p offset2bag and \p bag_size are both not NULL.
 * @param[in] bag_size_desc
 *   Input. The descriptor of the \p bag_size tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] bag_size
 *   Output. Pointer to the MLU memory that stores the size of each bag. The \p bag_size tensor
 *   should be 1D, and when the \p indices is 1D, its size should be the same as \p offset,
 *   otherwise, it is equal to the one-dimensional size of \p indices.
 *   Only calculated when the pointers of \p offset2bag and \p bag_size are both not NULL.
 * @param[in] max_indices_desc
 *   Input. The descriptor of the \p max_indices tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] max_indices
 *   Output. Pointer to the MLU memory that stores the \p indices that each element in \p output
 *   belongs to, only calculated when mode is ::CNNL_REDUCEMODE_MAX.
 *   Currently, \p max_indices is not supported and needs to be set as null.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "EmbeddingBag Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for filter tensor
 *   \p filter, indices tensor \p indices, offset tensor \p offset, per_sample_filter tensor
 *   \p per_sample_filter and output tensor \p output:
 *   - filter tensor: half, bfloat16, float.
 *   - indices tensor: int32, int64.
 *   - offset tensor: int32, int64.
 *   - per_sample_filter tensor: half, bfloat16, float.
 *   - output tensor: half, bfloat16, float.
 *   - offset2bag tensor: int32, int64.
 *   - bag_size tensor: int32, int64.
 * - Note that the data types of \p filter tensor, \p per_sample_filter tensor and \p output
 *   tensor must be the same, the data types of \p indices tensor and \p offset tensor must be the same,
 *   and the data types of \p offset2bag tensor and \p bag_size tensor must be the same. Bfloat16 is
 *   only supported on MLU500 series or higher.
 *
 * @note
 * - The element size of the indices should not exceed the range of int32 representation on
 *   platforms lower than MLU500 series.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the embedding_bag operation is as follows:
     @verbatim
     filter array by 10 * 3 -->
         filter: [[0.5860, 0.6688, 0.2687],
                 [0.4091, 0.0169, 0.6869],
                 [0.1551, 0.2168, 0.2718],
                 [0.0460, 0.1324, 0.5958],
                 [0.3568, 0.3494, 0.4879],
                 [0.5781, 0.1850, 0.0506],
                 [0.7095, 0.9444, 0.3460],
                 [0.5262, 0.7910, 0.9559],
                 [0.5842, 0.8564, 0.8038],
                 [0.1000, 0.8825, 0.9091]]
     input two arrays by 1 * 8 and 1 * 3
     indices array by 1 * 8 -->
         indices: [1, 2, 3, 4, 5, 6, 7, 8]
     offset array by 1 * 3 -->
         offset: [0, 4, 6]
     param:
       mode: CNNL_REDUCEMODE_SUM

     output array by 3 * 3 -->
         output: [[0.9671, 0.7155, 2.0425],
                 [1.2877, 1.1294, 0.3965],
                 [1.1104, 1.6474, 1.7597]]
     output two arrays by 1 * 8 and 1 * 3
     offset2bag array by 1 * 8 -->
         indices: [0, 0, 0, 0, 1, 1, 2, 2]
     bag_size array by 1 * 3 -->
         offset: [4, 2, 2]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html
 */
CNNL_DEPRECATED_FOR(cnnlEmbeddingBag_v3)
cnnlStatus_t
cnnlEmbeddingBag_v2(cnnlHandle_t handle,
                    const cnnlEmbeddingBagDescriptor_t EmbeddingBag_desc,
                    const cnnlTensorDescriptor_t filter_desc,
                    const void *filter,
                    const cnnlTensorDescriptor_t indices_desc,
                    const void *indices,
                    const cnnlTensorDescriptor_t offset_desc,
                    const void *offset,
                    const cnnlTensorDescriptor_t per_sample_filter_desc,
                    const void *per_sample_filter,
                    const cnnlTensorDescriptor_t output_desc,
                    void *output,
                    const cnnlTensorDescriptor_t offset2bag_desc,
                    void *offset2bag,
                    const cnnlTensorDescriptor_t bag_size_desc,
                    void *bag_size,
                    const cnnlTensorDescriptor_t max_indices_desc,
                    void *max_indices);

// Group:EmbeddingBag
/*!
 * @brief Computes sums, means or the maximum value of bags of embedding forward operation.
 *        Compared with ::cnnlEmbeddingBag_v2, this function replaces type
 *        cnnlEmbeddingBagDescriptor_t with cnnlEmbeddingBagDescriptorV2_t.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   embedding_bag operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] EmbeddingBag_desc
 *   Input. The descriptor of the EmbeddingBag operation. For detailed information,
 *   see ::cnnlEmbeddingBagDescriptorV2_t.
 * @param[in] filter_desc
 *   Input. The descriptor of the \p filter tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores embedding matrix. The \p filter should be a
 *   two-dimensional tensor. Its first dimension represents the size of the dictionary of
 *   embeddings and its second dimension represents the size of each embedding vector.
 * @param[in] indices_desc
 *   Input. The descriptor of the \p indices tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] indices
 *   Input. Pointer to the MLU memory that stores indices of \p filter which will be reduced to
 *   bags into \p output. The value of \p indices should be less than the first dimension of
 *   \p filter. The \p indices should be a one-dimensional or two-dimensional tensor.
 * @param[in] offset_desc
 *   Input. The descriptor of the \p offset tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] offset
 *   Input. Pointer to the MLU memory that stores the starting index position of each bag in \p
 indices.
 *   Only used when \p indices is 1D. The value of \p offset must be less than the size of indices
 *   dimension. The element in \p offset is sorted in an ascending order and starts with 0.
 * @param[in] per_sample_filter_desc
 *   Input. The descriptor of the \p per_sample_filter tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] per_sample_filter
 *   Input. The filter of \p indices, only used when mode is ::CNNL_REDUCEMODE_SUM.
 *   The shape of \p per_sample_filter must be the same as \p indices.
 * @param[in] output_desc
 *   Input. The descriptor of the \p output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the \p output tensor. The \p output tensor
 *   should be a two-dimensional tensor.
 * @param[in] offset2bag_desc
 *   Input. The descriptor of the \p offset2bag tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] offset2bag
 *   Output. Pointer to the MLU memory that stores the index of bag each embedding vector
 *   in \p indices belongs to. The \p offset2bag tensor should be 1D with the same number
 *   of elements as \p indices.
 *   Only calculated when the pointers of \p offset2bag and \p bag_size are both not NULL.
 * @param[in] bag_size_desc
 *   Input. The descriptor of the \p bag_size tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] bag_size
 *   Output. Pointer to the MLU memory that stores the size of each bag. The \p bag_size tensor
 *   should be 1D, and when the \p indices is 1D, its size should be the same as \p offset,
 *   otherwise, it is equal to the one-dimensional size of \p indices.
 *   Only calculated when the pointers of \p offset2bag and \p bag_size are both not NULL.
 * @param[in] max_indices_desc
 *   Input. The descriptor of the \p max_indices tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] max_indices
 *   Output. Pointer to the MLU memory that stores the \p indices that each element in \p output
 *   belongs to, only calculated when mode is ::CNNL_REDUCEMODE_MAX.
 *   Currently, \p max_indices is not supported and needs to be set as null.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "EmbeddingBag Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for filter tensor
 *   \p filter, indices tensor \p indices, offset tensor \p offset, per_sample_filter tensor
 *   \p per_sample_filter and output tensor \p output:
 *   - filter tensor: half, bfloat16, float.
 *   - indices tensor: int32, int64.
 *   - offset tensor: int32, int64.
 *   - per_sample_filter tensor: half, bfloat16, float.
 *   - output tensor: half, bfloat16, float.
 *   - offset2bag tensor: int32, int64.
 *   - bag_size tensor: int32, int64.
 * - Note that the data types of \p filter tensor, \p per_sample_filter tensor and \p output
 *   tensor must be the same, the data types of \p indices tensor and \p offset tensor must be the same,
 *   and the data types of \p offset2bag tensor and \p bag_size tensor must be the same. Bfloat16 is
 *   only supported on MLU500 series or higher.
 *
 * @note
 * - The element size of the indices should not exceed the range of int32 representation on
 *   platforms lower than MLU500 series.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the embedding_bag operation is as follows:
     @verbatim
     filter array by 10 * 3 -->
         filter: [[0.5860, 0.6688, 0.2687],
                 [0.4091, 0.0169, 0.6869],
                 [0.1551, 0.2168, 0.2718],
                 [0.0460, 0.1324, 0.5958],
                 [0.3568, 0.3494, 0.4879],
                 [0.5781, 0.1850, 0.0506],
                 [0.7095, 0.9444, 0.3460],
                 [0.5262, 0.7910, 0.9559],
                 [0.5842, 0.8564, 0.8038],
                 [0.1000, 0.8825, 0.9091]]
     input two arrays by 1 * 8 and 1 * 3
     indices array by 1 * 8 -->
         indices: [1, 2, 3, 4, 5, 6, 7, 8]
     offset array by 1 * 3 -->
         offset: [0, 4, 6]
     param:
       mode: CNNL_REDUCEMODE_SUM

     output array by 3 * 3 -->
         output: [[0.9671, 0.7155, 2.0425],
                 [1.2877, 1.1294, 0.3965],
                 [1.1104, 1.6474, 1.7597]]
     output two arrays by 1 * 8 and 1 * 3
     offset2bag array by 1 * 8 -->
         indices: [0, 0, 0, 0, 1, 1, 2, 2]
     bag_size array by 1 * 3 -->
         offset: [4, 2, 2]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html
 */
cnnlStatus_t
cnnlEmbeddingBag_v3(cnnlHandle_t handle,
                    const cnnlEmbeddingBagDescriptorV2_t EmbeddingBag_desc,
                    const cnnlTensorDescriptor_t filter_desc,
                    const void *filter,
                    const cnnlTensorDescriptor_t indices_desc,
                    const void *indices,
                    const cnnlTensorDescriptor_t offset_desc,
                    const void *offset,
                    const cnnlTensorDescriptor_t per_sample_filter_desc,
                    const void *per_sample_filter,
                    const cnnlTensorDescriptor_t output_desc,
                    void *output,
                    const cnnlTensorDescriptor_t offset2bag_desc,
                    void *offset2bag,
                    const cnnlTensorDescriptor_t bag_size_desc,
                    void *bag_size,
                    const cnnlTensorDescriptor_t max_indices_desc,
                    void *max_indices);

// Group:EmbeddingBagBackward
/*!
 * @brief Computes the gradients of embeddingBagBackward.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlEmbeddingBagBackward_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   embedding_bag_backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] embeddingBag_desc
 *   Input. The descriptor of the EmbeddingBag operation. For detailed information,
 *   see ::cnnlEmbeddingBagDescriptor_t.
 *   Parameters in \p embeddingBag_desc should follow the following rules.
 *   - \p mode only supports CNNL_REDUCEMODE_SUM.
 *   - \p max_norm is not supported and needs to be set as null.
 *   - \p norm_type is not supported and needs to be set as null.
 *   - \p scale_grad_by_freq is not supported and needs to be set as false.
 *   - \p include_last_offset is not supported and needs to be set as false.
 *   - \p padding_idx is not supported and needs to be set as null.
 * @param[in] indices_desc
 *   Input. The descriptor of the \p indices tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] indices
 *   Input. Pointer to the MLU memory that stores the tensor used to store index of each row
 *   of \p output. The value of \p indices should be less than the first dimension of
 *   \p filter. The \p indices should be a one-dimensional or two-dimensional tensor.
 * @param[in] diff_desc
 *   Input. The descriptor of the \p diff tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] diff
 *   Input. Pointer to the MLU memory that stores the gradient of each tensor in \p bag.
 *   The \p diff should be a two-dimensional tensor.
 * @param[in] offset_desc
 *   Input. The descriptor of the \p offset tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] offset
 *   Input. Pointer to the MLU memory that stores the starting index position of each bag in \p
 *   indices.
 *   Only used when \p indices is 1D. The value of \p offset must be less than the size of indices
 *   dimension. The element in \p offset is sorted in an ascending order and starts with 0.
 *   When \p indices is 2D, \p offset needs to be set as null.
 * @param[in] offset2bag_desc
 *   Input. The descriptor of the \p offset2bag tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] offset2bag
 *   Input. Pointer to the MLU memory that stores the index of bag each embedding vector in \p
 *   indices belongs to.
 *   The \p offset2bag tensor should be 1D with the same number of elements as \p
 *   indices.
 * @param[in] bag_size_desc
 *   Input. The descriptor of the \p bag_size_desc tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] bag_size
 *   Input. Pointer to the MLU memory that stores the size of each bag. The \p bag_size tensor
 *   should be 1D. When the \p indices is 1D, its size should be the same as \p offsets,
 *   otherwise, its size equals the one-dimensional size of \p indices.
 * @param[in] per_sample_filter_desc
 *   Input. The descriptor of the \p per_sample_filter tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] per_sample_filter
 *   Input. The filter of \p indices, only used when mode is ::CNNL_REDUCEMODE_SUM.
 *   The shape of \p per_sample_filter must be the same as \p indices.
 *   Currently, \p per_sample_filter is not supported and needs to be set as null.
 * @param[in] max_indices_desc
 *   Input. The descriptor of the \p max_indices tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] max_indices
 *   Input. Pointer to the MLU memory that stores the \p indices that each element in \p output
 *   belongs to, only calculated when mode is ::CNNL_REDUCEMODE_MAX.
 *   Currently, \p max_indices is not supported and needs to be set as null.
 * @param[out] output_desc
 *   Input. The descriptor of the \p output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the \p output tensor.
 *   The \p output should be a two-dimensional tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "EmbeddingBagBackward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for indices tensor
 *   \p indices, diff tensor \p diff, offsets tensor \p offsets, bag_size tensor \p bag_size,
 *   offset2bag tensor \p offset2bag, per_sample_filter tensor \p per_sample_filter
 *   and output tensor \p output:
 *   - diff tensor: half, float.
 *   - indices tensor: int32, int64.
 *   - offsets tensor: int32, int64.
 *   - offset2bag tensor: int32, int64.
 *   - bag_size tensor: int32, int64.
 *   - per_sample_weights tensor: half, float.
 *   - output tensor: half, float.
 *   Note that the data types of \p diff tensor, \p per_sample_weights tensor
 *   and \p output tensor must be the same.
 *   Note that the data type of \p indices tensor, \p offset2bag tensor and \p bag_size
 *   tensor must be the same.
 *   Note that if indices has one dim, the data type of \p offsets tensor and \p indices tensor
 *   must be the same.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the embedding_bag_backward operation is as follows:
     @verbatim
     diff array by 3 * 3 -->
          diff: [[0.5860, 0.6688, 0.2687],
                 [0.4091, 0.0169, 0.6869],
                 [0.1000, 0.8825, 0.9091]]
     input two arrays by 1 * 8 and 1 * 3
     indices array by 1 * 8 -->
         indices: [1, 2, 3, 4, 5, 6, 7, 8]
     offset array by 1 * 3 -->
         offset: [0, 4, 6]
     param:
       mode: CNNL_REDUCEMODE_SUM
     output array by 10 * 3 -->
         output: [[0.0000, 0.0000, 0.0000],
                  [0.5860, 0.6688, 0.2687],
                  [0.5860, 0.6688, 0.2687],
                  [0.5860, 0.6688, 0.2687],
                  [0.4091, 0.0169, 0.6869],
                  [0.4091, 0.0169, 0.6869],
                  [0.1000, 0.8825, 0.9091],
                  [0.1000, 0.8825, 0.9091],
                  [0.1000, 0.8825, 0.9091],
                  [0.0000, 0.0000, 0.0000]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html
 */
CNNL_DEPRECATED_FOR(cnnlEmbeddingBagBackward_v2)
cnnlStatus_t CNNL_WIN_API
cnnlEmbeddingBagBackward(cnnlHandle_t handle,
                         const cnnlEmbeddingBagDescriptor_t embeddingBag_desc,
                         const cnnlTensorDescriptor_t diff_desc,
                         const void *diff,
                         const cnnlTensorDescriptor_t indices_desc,
                         const void *indices,
                         const cnnlTensorDescriptor_t offset_desc,
                         const void *offset,
                         const cnnlTensorDescriptor_t offset2bag_desc,
                         const void *offset2bag,
                         const cnnlTensorDescriptor_t bag_size_desc,
                         const void *bag_size,
                         const cnnlTensorDescriptor_t per_sample_filter_desc,
                         const void *per_sample_filter,
                         const cnnlTensorDescriptor_t max_indices_desc,
                         const void *max_indices,
                         const cnnlTensorDescriptor_t output_desc,
                         void *output);

// Group:EmbeddingBagBackward
/*!
 * @brief Computes the gradients of embeddingBagBackward. Compared with ::cnnlEmbeddingBagBackward,
 *        this function replaces type cnnlEmbeddingBagDescriptor_t with
 *        cnnlEmbeddingBagDescriptorV2_t.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   embedding_bag_backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] embeddingBag_desc
 *   Input. The descriptor of the EmbeddingBag operation. For detailed information,
 *   see ::cnnlEmbeddingBagDescriptorV2_t.
 *   Parameters in \p embeddingBag_desc should follow the following rules.
 *   - \p mode only supports CNNL_REDUCEMODE_SUM.
 *   - \p max_norm is not supported and needs to be set as null.
 *   - \p norm_type is not supported and needs to be set as null.
 *   - \p scale_grad_by_freq is not supported and needs to be set as false.
 *   - \p include_last_offset is not supported and needs to be set as false.
 *   - \p padding_idx is not supported and needs to be set as null.
 * @param[in] indices_desc
 *   Input. The descriptor of the \p indices tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] indices
 *   Input. Pointer to the MLU memory that stores the tensor used to store index of each row
 *   of \p output. The value of \p indices should be less than the first dimension of
 *   \p filter. The \p indices should be a one-dimensional or two-dimensional tensor.
 * @param[in] diff_desc
 *   Input. The descriptor of the \p diff tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] diff
 *   Input. Pointer to the MLU memory that stores the gradient of each tensor in \p bag.
 *   The \p diff should be a two-dimensional tensor.
 * @param[in] offset_desc
 *   Input. The descriptor of the \p offset tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] offset
 *   Input. Pointer to the MLU memory that stores the starting index position of each bag in \p
 *   indices.
 *   Only used when \p indices is 1D. The value of \p offset must be less than the size of indices
 *   dimension. The element in \p offset is sorted in an ascending order and starts with 0.
 *   When \p indices is 2D, \p offset needs to be set as null.
 * @param[in] offset2bag_desc
 *   Input. The descriptor of the \p offset2bag tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] offset2bag
 *   Input. Pointer to the MLU memory that stores the index of bag each embedding vector in \p
 *   indices belongs to.
 *   The \p offset2bag tensor should be 1D with the same number of elements as \p
 *   indices.
 * @param[in] bag_size_desc
 *   Input. The descriptor of the \p bag_size_desc tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] bag_size
 *   Input. Pointer to the MLU memory that stores the size of each bag. The \p bag_size tensor
 *   should be 1D. When the \p indices is 1D, its size should be the same as \p offsets,
 *   otherwise, its size equals the one-dimensional size of \p indices.
 * @param[in] per_sample_filter_desc
 *   Input. The descriptor of the \p per_sample_filter tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] per_sample_filter
 *   Input. The filter of \p indices, only used when mode is ::CNNL_REDUCEMODE_SUM.
 *   The shape of \p per_sample_filter must be the same as \p indices.
 *   Currently, \p per_sample_filter is not supported and needs to be set as null.
 * @param[in] max_indices_desc
 *   Input. The descriptor of the \p max_indices tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] max_indices
 *   Input. Pointer to the MLU memory that stores the \p indices that each element in \p output
 *   belongs to, only calculated when mode is ::CNNL_REDUCEMODE_MAX.
 *   Currently, \p max_indices is not supported and needs to be set as null.
 * @param[out] output_desc
 *   Input. The descriptor of the \p output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the \p output tensor.
 *   The \p output should be a two-dimensional tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "EmbeddingBagBackward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for indices tensor
 *   \p indices, diff tensor \p diff, offsets tensor \p offsets, bag_size tensor \p bag_size,
 *   offset2bag tensor \p offset2bag, per_sample_filter tensor \p per_sample_filter
 *   and output tensor \p output:
 *   - diff tensor: half, float.
 *   - indices tensor: int32, int64.
 *   - offsets tensor: int32, int64.
 *   - offset2bag tensor: int32, int64.
 *   - bag_size tensor: int32, int64.
 *   - per_sample_weights tensor: half, float.
 *   - output tensor: half, float.
 *   Note that the data types of \p diff tensor, \p per_sample_weights tensor
 *   and \p output tensor must be the same.
 *   Note that the data type of \p indices tensor, \p offset2bag tensor and \p bag_size
 *   tensor must be the same.
 *   Note that if indices has one dim, the data type of \p offsets tensor and \p indices tensor
 *   must be the same.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the embedding_bag_backward operation is as follows:
     @verbatim
     diff array by 3 * 3 -->
          diff: [[0.5860, 0.6688, 0.2687],
                 [0.4091, 0.0169, 0.6869],
                 [0.1000, 0.8825, 0.9091]]
     input two arrays by 1 * 8 and 1 * 3
     indices array by 1 * 8 -->
         indices: [1, 2, 3, 4, 5, 6, 7, 8]
     offset array by 1 * 3 -->
         offset: [0, 4, 6]
     param:
       mode: CNNL_REDUCEMODE_SUM
     output array by 10 * 3 -->
         output: [[0.0000, 0.0000, 0.0000],
                  [0.5860, 0.6688, 0.2687],
                  [0.5860, 0.6688, 0.2687],
                  [0.5860, 0.6688, 0.2687],
                  [0.4091, 0.0169, 0.6869],
                  [0.4091, 0.0169, 0.6869],
                  [0.1000, 0.8825, 0.9091],
                  [0.1000, 0.8825, 0.9091],
                  [0.1000, 0.8825, 0.9091],
                  [0.0000, 0.0000, 0.0000]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html
 */
cnnlStatus_t CNNL_WIN_API
cnnlEmbeddingBagBackward_v2(cnnlHandle_t handle,
                            const cnnlEmbeddingBagDescriptorV2_t embeddingBag_desc,
                            const cnnlTensorDescriptor_t diff_desc,
                            const void *diff,
                            const cnnlTensorDescriptor_t indices_desc,
                            const void *indices,
                            const cnnlTensorDescriptor_t offset_desc,
                            const void *offset,
                            const cnnlTensorDescriptor_t offset2bag_desc,
                            const void *offset2bag,
                            const cnnlTensorDescriptor_t bag_size_desc,
                            const void *bag_size,
                            const cnnlTensorDescriptor_t per_sample_filter_desc,
                            const void *per_sample_filter,
                            const cnnlTensorDescriptor_t max_indices_desc,
                            const void *max_indices,
                            const cnnlTensorDescriptor_t output_desc,
                            void *output);

// Group:MatMul
/*!
 * @brief Computes the matrix multiplication operation, then returns the results in the output
 * tensor \p c. For more information about quantization, see "Cambricon CNNL User Guide".
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlMatMul_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] is_trans_a
 *   Input. Boolean value indicating whether \p a matrix is transposed.
 * @param[in] is_trans_b
 *   Input. Boolean value indicating whether \p b matrix is transposed.
 * @param[in] alpha
 *   Input. A host pointer to scaling factor of tensor \p a. The default value is 1.0.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of the left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of the right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] beta
 *   Input. A host pointer to scaling factor of tensor \p c. The default value is 0.0.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] c
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - The supported combinations of data types are as follows in the order of
 *   \p a data type - \p b data type - \p c offchip data type - \p c onchip data type:
 *   - int8 - int8 - half - half
 *   - int8 - int8 - half - float
 *   - int8 - int8 - float - float
 *   - int16 - int16 - half - half
 *   - int16 - int16 - half - float
 *   - int16 - int16 - float - float
 *   - half - half - half - half
 *   - half - half - half - float
 *   - half - half - float - float
 *   - bfloat16 - bfloat16 - bfloat16 - float
 *   - bfloat16 - bfloat16 - float - float
 *   - float - float - float - float
 * - When the input data type is half or bfloat16, and the output is INF on MLU300 series, unexpected
 *   saturated data may be returned.
 *
 * @note
 * - Parameters \p alpha and \p beta only supports default values. To use other values for these parameters, use ::cnnlMatMul_v2 instead.
 * - On all hardware platforms, the combinations of the data types should satisfy the following rules:
 *   - The data type bitwidth of \p c onchip data type for operation computing is not shorter than \p c
 *     offchip data type.
 *
 * @par Scale Limitation
 * - The input tensors and output tensor must meet the following requirements:
 *   - \p a and \p b must be two dimensions.
 *   - The number of \p a matrix's columns must be equal to the number of \p b matrix's rows after both inputs
 *   perform transpose operations according to parameters.
 *
 * @par API Dependency
 * - Before calling this function to implement matrix multiplication, you need to prepare
 *   all the parameters passed to this function. See each parameter description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, matrix \p a does not need to transpose and matrix \p b
 *   needs to transpose.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      is_trans_a:                    false
      is_trans_b:                    false
      Dimension of input tensor a:  [99, 128]
      Dimension of input tensor b:  [128, 256]
      Dimension of output tensor c: [99, 256]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/torch.html?highlight=matmul#torch.matmul
 */
CNNL_DEPRECATED_FOR(cnnlMatMul_v2)
cnnlStatus_t CNNL_WIN_API cnnlMatMul(cnnlHandle_t handle,
                                     const bool is_trans_a,
                                     const bool is_trans_b,
                                     const void *alpha,
                                     const cnnlTensorDescriptor_t a_desc,
                                     const void *a,
                                     const cnnlTensorDescriptor_t b_desc,
                                     const void *b,
                                     const void *beta,
                                     const cnnlTensorDescriptor_t c_desc,
                                     void *c);

// Group:MatMul
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra workspace
 *        to optimize the matrix multiplication operation.
 *
 * The size of the extra workspace is based on the given information of the matrix multiplication
 * operation, including the matrix multiplication descriptor \p matmul_desc, input tensor
 * descriptor of left matrix \p a_desc, input tensor descriptor of right matrix \p b_desc, output
 * tensor  descriptor \p c_desc, and the matrix multiplication algorithm \p algo.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlGetMatMulHeuristicResult instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] matmul_desc
 *   Input. The descriptor of the matrix multiplication operations. For detail information,
 *   see ::cnnlMatMulDescriptor_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of the left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of the right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] d_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] algo
 *   Input. A host pointer to the most suitable algorithm to compute the matrix multiplication.
 *   Currently it is not supported and should be set to NULL.
 * @param[out] workspace_size
 *   Output. Pointer to the MLU memory that stores the returned size of the extra workspace in bytes.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called after ::cnnlSetMatMulDescAttr function. You also need to
 *   call the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions to create and set
 *   tensor descriptors \p a_desc, \p b_desc, \p c_desc before calling this function.
 * - The allocated extra workspace should be passed to ::cnnlMatMul_v2 function to
 *   perform the matrix multiplication operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlGetMatMulHeuristicResult)
cnnlStatus_t CNNL_WIN_API cnnlGetMatMulWorkspaceSize(cnnlHandle_t handle,
                                                     const cnnlMatMulDescriptor_t matmul_desc,
                                                     const cnnlTensorDescriptor_t a_desc,
                                                     const cnnlTensorDescriptor_t b_desc,
                                                     const cnnlTensorDescriptor_t c_desc,
                                                     const cnnlTensorDescriptor_t d_desc,
                                                     const cnnlMatMulAlgo_t algo,
                                                     size_t *workspace_size);

// Group:MatMul
/*!
 * @brief Computes the matrix multiplication operation, then returns the results in the output
 *        tensor \p d. For more information, see "Cambricon CNNL User Guide".
 *
 * Compared with ::cnnlMatMul, it supports the use of the extra workspace size, the use of \p algo
 * to pass the algorithm information and the use of \p matmul_desc to pass parameters
 * like ::CNNL_MATMUL_DESC_TRANSA.
 *
 * This function needs extra MLU memory as the workspace to improve the matrix multiplication
 * performance. You can get the workspace size with the
 * ::cnnlGetMatMulAlgoHeuristic and ::cnnlGetMatMulHeuristicResult functions in turn.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] matmul_desc
 *   Input. The descriptor of the matrix multiplication operations. For detail information,
 *   see ::cnnlMatMulDescriptor_t.
 * @param[in] algo
 *   Input. A host pointer to the most suitable algorithm to compute the matrix multiplication.
 * @param[in] alpha
 *   Input. A host pointer to scaling factor of tensor \p a. The default value is 1.0.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of the left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. Pointer to the MLU memory that stores the input tensor of the left matrix.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of the right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. Pointer to the MLU memory that stores the input tensor of the right matrix.
 * @param[in] beta
 *   Input. A host pointer to scaling factor of tensor \p c. The default value is 0.0.
 * @param[in] c_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] c
 *   Input. Pointer to the MLU memory that stores the input tensor \p c in out-of-place matrix multiplication
 *   where d = alpha * a * b + beta * c, or pointer to the MLU memory that stores the output tensor \p d in in-place
 *   matrix multiplication where c == d = alpha * a * b + beta * c.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the matrix multiplication
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the matrix multiplication
 *   operation. You can get the size of the workspace with the ::cnnlGetMatMulAlgoHeuristic and
 *   ::cnnlGetMatMulHeuristicResult functions in turn.
 * @param[in] d_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] d
 *   Output. Pointer to the MLU memory that stores the output \p d.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Data Type
 * - The supported combinations of data types are as follows in the order of
 *   \p a data type - \p b data type - \p d offchip data type - \p d onchip data type:
 *   - int8 - int8 - half - half
 *   - int8 - int8 - half - float
 *   - int8 - int8 - float - float
 *   - int16 - int16 - half - half
 *   - int16 - int16 - half - float
 *   - int16 - int16 - float - float
 *   - half - half - half - half
 *   - half - half - half - float
 *   - half - half - float - float
 *   - bfloat16 - bfloat16 - bfloat16 - float
 *   - bfloat16 - bfloat16 - float - float
 *   - float - float - float - float
 * - When the input data type is half or bfloat16, and the output is INF on MLU300 series, unexpected
 *   saturated data may be returned.
 *
 * @note
 * - The value of \p c_desc is the same as that of \p d_desc.
 * - On all hardware platforms, the combinations of the data types should satisfy the following rules:
 *   - The data type bitwidth of \p d onchip data type for operation computing is not shorter than \p d
 *     offchip data type.
 * - The leading dimensions of tensors \p a, \p b, \p c and \p d are supported. Tensors \p a and \p b's
 *   leading dimensions are lda and ldb respectively, and tensors \p c and \p d are sharing the same leading dimension ldc.
 *   You can specify the values of lda, ldb and ldc by setting the ::CNNL_MATMUL_DESC_LDA, ::CNNL_MATMUL_DESC_LDB and
 *   ::CNNL_MATMUL_DESC_LDC attributes of \p matmul_desc with ::cnnlSetMatMulDescAttr separately.
 * - The leading dimension of each tensor is not necessary for matrix multiplication on MLU. If you want to use
 *   the leading dimension feature, the strides of the tensor should be set with ::cnnlSetTensorDescriptorEx, where the
 *   lowest dimension of the strides should be 1, and the highest dimension of the strides should be equal to the leading
 *   dimension of the tensor, which should be equal to or larger than the lowest dimension of the tensor. Specially, when
 *   the dimension of tensor is 1, the corresponding strides will not be checked.
 * - The tensor descriptors of \p c and \p d are consistent.
 * - Both \p c and \p d cannot be NULL, and the desriptor values of \p c_desc and \p d_desc should be the same.
 *
 * @par Scale Limitation
 * - The input tensors and output tensor must meet the following requirements:
 *   - \p a, \p b, \p c and \p d must be two dimensions.
 *   - The number of \p a matrix's columns must be equal to the number of \p b matrix's rows after both inputs
 *   perform transpose operations according to parameters.
 *   - The maximum dimension of \p a should be less than or equal to INT_MAX.
 *   - The maximum dimension of \p b should be less than or equal to INT_MAX.
 *   - The maximum dimension of \p d should be less than or equal to INT_MAX.
 *
 * @par API Dependency
 * - Before calling this function to implement matrix multiplication operation, you need to prepare
 *   all the parameters passed to this function. See each parameter description for details.
 * - Each set of input parameters corresponds to a unique \p algo and \p workspace. If \p matmul_desc, tensor descriptors,
 *   and other parameters are changed, the associated functions ::cnnlGetMatMulAlgoHeuristic and ::cnnlGetMatMulHeuristicResult
 *   need to be re-called as well.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, matrix \p a should not be transposed and matrix \p b
 *   should be transposed.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      CNNL_MATMUL_DESC_TRANSA:      false
      CNNL_MATMUL_DESC_TRANSB:      false
      CNNL_MATMUL_USE_BETA:         false
      Dimension of input tensor a:  [99, 128]
      Dimension of input tensor b:  [128, 256]
      Dimension of input tensor c:  [99, 256]
      Dimension of output tensor d: [99, 256]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/torch.html?highlight=matmul#torch.matmul
 */
cnnlStatus_t CNNL_WIN_API cnnlMatMul_v2(cnnlHandle_t handle,
                                        const cnnlMatMulDescriptor_t matmul_desc,
                                        const cnnlMatMulAlgo_t algo,
                                        const void *alpha,
                                        const cnnlTensorDescriptor_t a_desc,
                                        const void *a,
                                        const cnnlTensorDescriptor_t b_desc,
                                        const void *b,
                                        const void *beta,
                                        const cnnlTensorDescriptor_t c_desc,
                                        void *c,
                                        void *workspace,
                                        size_t workspace_size,
                                        const cnnlTensorDescriptor_t d_desc,
                                        void *d);

// Group:MatMul
/*!
 * @brief Creates a descriptor pointed by \p result for a matrix multiplication heuristic result,
 *        and allocates memory for the result. The result is defined in ::cnnlMatMulHeuristicResult_t.
 *
 * @param[out] result
 *   Output. A host pointer to the struct of matrix multiplication heuristic result.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - You need to call the ::cnnlDestroyMatMulHeuristicResult function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t cnnlCreateMatMulHeuristicResult(cnnlMatMulHeuristicResult_t *result);

// Group:MatMul
/*!
 * @brief Destroys a matrix multiplication heuristic result that was previously created with
 *        ::cnnlCreateMatMulHeuristicResult.
 *
 * @param[in] result
 *   Input. The matrix multiplication heuristic result to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t cnnlDestroyMatMulHeuristicResult(cnnlMatMulHeuristicResult_t result);

// Group:MatMul
/*!
 * @brief Gets the matrix multiplication algorithm and workspace size from heuristic result,
 *        that was previously selected with ::cnnlGetMatMulAlgoHeuristic.
 *
 * @param[in] result
 *   Input. The matrix multiplication heuristic result obtained by ::cnnlGetMatMulAlgoHeuristic.
 *
 * @param[out] algo
 *   Output. The matrix multiplication algorithm.
 *
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in
 *   the matrix multiplication operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t cnnlGetMatMulHeuristicResult(const cnnlMatMulHeuristicResult_t result,
                                          cnnlMatMulAlgo_t algo,
                                          size_t *workspace_size);

// Group:MatMul
/*!
 * @brief Retrieves the possible algorithms that can be used in the matrix multiplication.
 *        The output is placed in result_array[] in the order of increasing estimated compute time.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] matmul_desc
 *   Input. The descriptor of the matrix multiplication operations. For detail information,
 *   see ::cnnlMatMulDescriptor_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of the left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of the right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] d_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] preference
 *   Input. The descriptor of the matrix multiplication that holds the preferences for cnnlMatMulHeuristicResult_t
 *   configuration. Currently it is not supported and should be set to NULL.
 * @param[in] requested_algo_count
 *   Input. The number of requested algorithms. The maximum number of algorithms to be returned.
 *   Currently this value only supports 1.
 * @param[out] result_array
 *   Output. Array containing the algorithm heuristics and associated runtime characteristics, returned by this function,
 *   in the order of increasing estimated compute time.
 * @param[out] return_algo_count
 *   Output. A host pointer to the number of algorithms returned by this function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - Currently the maximum number of algorithms \p requested_algo_count only supports 1.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t cnnlGetMatMulAlgoHeuristic(cnnlHandle_t handle,
                                        const cnnlMatMulDescriptor_t matmul_desc,
                                        const cnnlTensorDescriptor_t a_desc,
                                        const cnnlTensorDescriptor_t b_desc,
                                        const cnnlTensorDescriptor_t c_desc,
                                        const cnnlTensorDescriptor_t d_desc,
                                        const cnnlMatMulPrefer_t preference,
                                        int requested_algo_count,
                                        cnnlMatMulHeuristicResult_t result_array[],
                                        int *return_algo_count);

// Group:MatMul
/*!
 * @brief Retrieves the IDs of all the matrix multiply algorithms that are valid,
 *        for given tensors described by \p a_desc, \p b_desc, \p c_desc and \p d_desc.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] matmul_desc
 *   Input. The descriptor of the matrix multiplication operations. For detail information,
 *   see ::cnnlMatMulDescriptor_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of the left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of the right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] d_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] requestedAlgoCount
 *   Input. The number of requested algorithms, which is also the maximum number of algorithms to be returned.
 *   The value must be greater than or equal to 0.
 * @param[out] algoIdsArray
 *   Output. Array containing the algorithm IDs returned by this function.
 * @param[out] returnAlgoCount
 *   Output. A host pointer to the number of algorithms returned by this function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - Currently the maximum number of algorithms \p requested_algo_count only supports 1.
 * - The length of algoIdsArray must be greater than \p requestedAlgoCount.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t cnnlGetMatMulAlgoIds(cnnlHandle_t handle,
                                  const cnnlMatMulDescriptor_t matmul_desc,
                                  const cnnlTensorDescriptor_t a_desc,
                                  const cnnlTensorDescriptor_t b_desc,
                                  const cnnlTensorDescriptor_t c_desc,
                                  const cnnlTensorDescriptor_t d_desc,
                                  int requestedAlgoCount,
                                  int algoIdsArray[],
                                  int *returnAlgoCount);

// Group:MatMul
/*!
 * @brief Initializes the matrix multiply algorithm structure for a specified algorithm ID,
 * which can be run by the ::cnnlMatMul_v2 function.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlInitialMatMulAlgo instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] matmul_desc
 *   Input. The descriptor of the matrix multiplication operations. For detail information,
 *   see ::cnnlMatMulDescriptor_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of the left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of the right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] d_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] prefer
 *   Input. The preference options for initializing the matrix multiplication algorithm. The function will return
 *   ::CNNL_STATUS_EXECUTION_FAILED when initialization failed as the preference uses ::CNNL_MATMUL_ALGO_INIT_LIMITED_BY_ALGO_ID.
 *   When initialization failed, the function will return a best fit algorithm as the preference uses ::CNNL_MATMUL_ALGO_INIT_ALTER_BEST_FIT,
 *   and it will return the default algorithm as the preference uses ::CNNL_MATMUL_ALGO_INIT_ALTER_DEFAULT.
 * @param[in] algoId
 *   Input. The algorithm ID. It can be an algoId returned by the
 *   ::cnnlGetMatMulAlgoIds() function, or you can traverse the value from 0 to 11 until it returns ::CNNL_STATUS_SUCCESS.
 * @param[out] algo
 *   Output. The matrix multiplication algorithm to be initialized.
 @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the matrix multiplication operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - \p algoId should be greater than or equal to 0, and less than 11. It only supports 9 on 1V.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlInitialMatMulAlgo)
cnnlStatus_t cnnlMatMulAlgoInitial(cnnlHandle_t handle,
                                   const cnnlMatMulDescriptor_t matmul_desc,
                                   const cnnlTensorDescriptor_t a_desc,
                                   const cnnlTensorDescriptor_t b_desc,
                                   const cnnlTensorDescriptor_t c_desc,
                                   const cnnlTensorDescriptor_t d_desc,
                                   const cnnlMatMulAlgoInitPrefer_t prefer,
                                   int algoId,
                                   cnnlMatMulAlgo_t algo,
                                   size_t *workspace_size);

// Group:MatMul
/*!
 * @brief Initializes the matrix multiply algorithm structure for a specified algorithm ID,
 * which can be run by the ::cnnlMatMul_v2 function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] matmul_desc
 *   Input. The descriptor of the matrix multiplication operations. For detail information,
 *   see ::cnnlMatMulDescriptor_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of the left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of the right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] d_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] prefer
 *   Input. The preference options for initializing the matrix multiplication algorithm. The function will return
 *   ::CNNL_STATUS_EXECUTION_FAILED when initialization failed as the preference uses ::CNNL_MATMUL_ALGO_INIT_LIMITED_BY_ALGO_ID.
 *   When initialization failed, the function will return a best fit algorithm as the preference uses ::CNNL_MATMUL_ALGO_INIT_ALTER_BEST_FIT,
 *   and it will return the default algorithm as the preference uses ::CNNL_MATMUL_ALGO_INIT_ALTER_DEFAULT.
 * @param[in] algoId
 *   Input. The algorithm ID. It can be an algoId returned by the
 *   ::cnnlGetMatMulAlgoIds() function, or you can traverse the value from 0 to 11 until it returns ::CNNL_STATUS_SUCCESS.
 * @param[out] algo
 *   Output. The matrix multiplication algorithm to be initialized.
 @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the matrix multiplication operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - \p algoId should be greater than or equal to 0, and less than 11. It only supports 9 on 1V.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t cnnlInitialMatMulAlgo(cnnlHandle_t handle,
                                   const cnnlMatMulDescriptor_t matmul_desc,
                                   const cnnlTensorDescriptor_t a_desc,
                                   const cnnlTensorDescriptor_t b_desc,
                                   const cnnlTensorDescriptor_t c_desc,
                                   const cnnlTensorDescriptor_t d_desc,
                                   const cnnlMatMulAlgoInitPrefer_t prefer,
                                   int algoId,
                                   cnnlMatMulAlgo_t algo,
                                   size_t *workspace_size);


// Group:MatMul
/*!
 * @brief Creates a descriptor pointed by \p matmul_desc for a matrix multiplication operation,
 *        and allocates memory for holding the information about the matrix multiplication operation.
 *        The information is defined in ::cnnlMatMulDescriptor_t.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlCreateMatMulDescriptor instead.
 *
 * @param[out] matmul_desc
 *   Output. A host pointer to the matrix multiplication descriptor that holds information about the matrix
 *   multiplication operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetMatMulDescAttr function to initialize
 *   and set the information to the matrix multiplication descriptor.
 * - You need to call the ::cnnlMatMulDescDestroy function to destroy the descriptor.
 *
 * @note
 * - The default compute data type of c is c_desc->dtype, use cnnlSetTensorDescriptorOnchipDataType() to
 *   set onchip data type if high accuracy of c is needed.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlCreateMatMulDescriptor)
cnnlStatus_t CNNL_WIN_API cnnlMatMulDescCreate(cnnlMatMulDescriptor_t *matmul_desc);

// Group:MatMul
/*!
 * @brief Creates a descriptor pointed by \p matmul_desc for a matrix multiplication operation,
 *        and allocates memory for holding the information about the matrix multiplication operation.
 *        The information is defined in ::cnnlMatMulDescriptor_t.
 *
 * @param[out] matmul_desc
 *   Output. A host pointer to the matrix multiplication descriptor that holds information about the matrix
 *   multiplication operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetMatMulDescAttr function to initialize
 *   and set the information to the matrix multiplication descriptor.
 * - You need to call the ::cnnlDestroyMatMulDescriptor function to destroy the descriptor.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateMatMulDescriptor(cnnlMatMulDescriptor_t *matmul_desc);

// Group:MatMul
/*!
 * @brief Destroys a matrix multiplication descriptor \p matmul_desc
 *        that was previously created with ::cnnlMatMulDescCreate.
 *
 * The matrix multiplication descriptor is defined in ::cnnlMatMulDescriptor_t
 * and holds the information about the matrix multiplication operation.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlDestroyMatMulDescriptor instead.
 *
 * @param[in] matmul_desc
 *   Input. The matrix multiplication descriptor to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - When ::CNNL_MATMUL_A_QUANT and ::CNNL_MATMUL_B_QUANT are set, ::cnnlQuantizeExDescriptor_t should
 *   be prepared.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlDestroyMatMulDescriptor)
cnnlStatus_t CNNL_WIN_API cnnlMatMulDescDestroy(cnnlMatMulDescriptor_t matmul_desc);

// Group:MatMul
/*!
 * @brief Destroys a matrix multiplication descriptor \p matmul_desc
 *        that was previously created with ::cnnlCreateMatMulDescriptor.
 *
 * The matrix multiplication descriptor is defined in ::cnnlMatMulDescriptor_t
 * and holds the information about the matrix multiplication operation.
 *
 * @param[in] matmul_desc
 *   Input. The matrix multiplication descriptor to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyMatMulDescriptor(cnnlMatMulDescriptor_t matmul_desc);

// Group:MatMul
/*!
 * @brief Initializes the matrix multiplication descriptor \p matmul_desc
 * that was previously created with the ::cnnlCreateMatMulDescriptor function, and sets
 * the information about the matrix multiplication operation to the matrix multiplication
 * descriptor \p matmul_desc. The information includes the attribute \p attr defined in
 * ::cnnlMatMulDescAttribute_t, the host pointer \p buf to the attribute value, and
 * the size of buffer for verification.
 *
 * @param[in,out] matmul_desc
 *   Input/output. The descriptor of the matrix multiplication operation. For detailed
 *   information, see ::cnnlMatMulDescriptor_t.
 * @param[in] attr
 *   Input. Attribute of matrix multiplication descriptor to be set. For detailed
 *   information, see ::cnnlMatMulDescAttribute_t.
 * @param[in] buf
 *   Input. A host pointer to the attribute value set by this function.
 * @param[in] size_in_bytes
 *   Input. Buffer in bytes for verification.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetMatMulDescAttr(cnnlMatMulDescriptor_t matmul_desc,
                                                cnnlMatMulDescAttribute_t attr,
                                                const void *buf,
                                                size_t size_in_bytes);

// Group:MatMul
/*!
 * @brief Returns the pointer to the \p buf and size of the buffer \p size_written of the attribute
 * retrieved with the given matmul multiplication descriptor \p matmul_desc, attribute \p attr.
 *
 * You can set the attribute in the matrix multiplication descriptor based on the return value
 * of this function.
 *
 * @param[in] matmul_desc
 *   Input. The descriptor of the matrix multiplication operation. For detailed
 *   information, see ::cnnlMatMulDescriptor_t.
 * @param[in] attr
 *   Input. Attribute of matrix multiplication descriptor to be retrieved.
 * @param[out] buf
 *   Output. A host pointer to the attribute value to be retrieved by this function.
 * @param[in] size_in_bytes
 *   Input. Buffer in bytes for verification, which is used to check whether the memory size is the same as \p size_written.
 * @param[out] size_written
 *   Output. A host pointer to the number of bytes actually written to the buffer.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetMatMulDescAttr(const cnnlMatMulDescriptor_t matmul_desc,
                                                const cnnlMatMulDescAttribute_t attr,
                                                void *buf,
                                                size_t size_in_bytes,
                                                size_t *size_written);

// Group:MatMul
/*!
 * @brief Creates a descriptor pointed by \p algo for a matrix multiplication algorithm,
 *        and allocates memory for holding the information about the algorithm.
 *        The information is defined in ::cnnlMatMulAlgo_t.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlCreateMatMulAlgo instead.
 *
 * @param[out] algo
 *   Output. A host pointer to the matrix multiplication algorithm that holds information about the matrix
 *   multiplication algorithm.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlGetQuantizeMatMulAlgorithm function to initialize
 *   and set the information to the matrix multiplication algorithm.
 * - You need to call the ::cnnlMatMulAlgoDestroy function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlCreateMatMulAlgo)
cnnlStatus_t CNNL_WIN_API cnnlMatMulAlgoCreate(cnnlMatMulAlgo_t *algo);

// Group:MatMul
/*!
 * @brief Creates a descriptor pointed by \p algo for a matrix multiplication algorithm,
 *        and allocates memory for holding the information about the algorithm.
 *        The information is defined in ::cnnlMatMulAlgo_t.
 *
 * @param[out] algo
 *   Output. A host pointer to the matrix multiplication algorithm that holds information about the matrix
 *   multiplication algorithm.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlGetMatMulAlgoHeuristic function to initialize
 *   and set the information to the matrix multiplication algorithm.
 * - You need to call the ::cnnlDestroyMatMulAlgo function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateMatMulAlgo(cnnlMatMulAlgo_t *algo);

// Group:MatMul
/*!
 * @brief Destroys a matrix multiplication algorithm descriptor \p algo
 *        that was previously created by ::cnnlMatMulAlgoCreate.
 *
 * The matrix multiplication descriptor is defined in ::cnnlMatMulAlgo_t
 * and holds the information about the matrix multiplication algorithm.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlDestroyMatMulAlgo instead.
 *
 * @param[in] algo
 *   Input. The matrix multiplication algorithm descriptor to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlDestroyMatMulAlgo)
cnnlStatus_t CNNL_WIN_API cnnlMatMulAlgoDestroy(cnnlMatMulAlgo_t algo);

// Group:MatMul
/*!
 * @brief Destroys a matrix multiplication algorithm descriptor \p algo
 *        that was previously created by ::cnnlCreateMatMulAlgo.
 *
 * The matrix multiplication descriptor is defined in ::cnnlMatMulAlgo_t
 * and holds the information about the matrix multiplication algorithm.
 *
 * @param[in] algo
 *   Input. The matrix multiplication algorithm descriptor to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyMatMulAlgo(cnnlMatMulAlgo_t algo);


// Group:MatMul
/*!
 * @brief Returns the most suited matrix multiplication algorithm that can be used
 * in the operation.
 *
 * The returned matrix multiplication is chosen from all supported matrix
 * algorithms by Cambricon CNNL defined in ::cnnlMatMulAlgo_t and is based on the given matrix
 * multiplication descriptor \p matmul_desc, tensor descriptor of left matrix \p a_desc, tensor
 * descriptor of right matrix \p b_desc, tensor descriptor of output matrix \p c_desc, and
 * matrix multiplication algorithm \p preference.
 *
 * The computing performance options \p preference is defined in ::cnnlMatMulPreference_t,
 * only supports the high speed mode.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGetMatMulExAlgoHeuristic instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] matmul_desc
 *  Input. The descriptor of the matrix multiplication operation. For detailed
 *  information, see ::cnnlMatMulDescriptor_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor descriptor of the left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor descriptor of the right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the input tensor descriptor of output matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] preference
 *   Input. The options for implementing the matrix multiplication operation to
 * get better performance. This parameter only supports CNNL_MATMUL_FASTEST now.
 * @param[out] algo
 *   Output. A host pointer to the returned algorithm that is best suited for computing the matrix
 *   multiplication. The algorithms are defined in the ::cnnlMatMulAlgo_t enum.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlGetMatMulExAlgoHeuristic)
cnnlStatus_t CNNL_WIN_API cnnlGetQuantizeMatMulAlgorithm(cnnlHandle_t handle,
                                                         const cnnlMatMulDescriptor_t matmul_desc,
                                                         const cnnlTensorDescriptor_t a_desc,
                                                         const cnnlTensorDescriptor_t b_desc,
                                                         const cnnlTensorDescriptor_t c_desc,
                                                         const cnnlMatMulPreference_t preference,
                                                         cnnlMatMulAlgo_t *algo);

// Group:MatMul
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra workspace
 * to optimize the matrix multiplication operation.
 *
 * The size of the extra workspace is based on the given information of the matrix multiplication
 * operation, including the matrix multiplication descriptor \p matmul_desc, input tensor
 * descriptor of left matrix \p a_desc, input tensor descriptor of right matrix \p b_desc, output
 * tensor descriptor \p c_desc, and the matrix multiplication algorithm \p algo. For more
 * information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGetMatMulExAlgoHeuristic instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] matmul_desc
 *   Input. The descriptor of the matrix multiplication operations. For detail information,
 *   see ::cnnlMatMulDescriptor_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of the left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of the right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the input tensor of output matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] algo
 *   Input. The algorithm used to compute the matrix multiplication. The algorithms are defined
 *   in the ::cnnlMatMulAlgo_t enum. You can get the best suited algorithm with the
 *   ::cnnlGetQuantizeMatMulAlgorithm function.
 * @param[out] workspace_size
 *   Output. Pointer to the MLU memory that stores the returned size of the extra workspace in bytes.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called after ::cnnlGetQuantizeMatMulAlgorithm function. You also need to
 *   call the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions to create and set
 *   tensor descriptors \p a_desc, \p b_desc, \p c_desc before calling this function.
 * - The allocated extra workspace should be passed to the ::cnnlQuantizeMatMul function to
 *   performs the matrix multiplication operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlGetMatMulExAlgoHeuristic)
cnnlStatus_t CNNL_WIN_API cnnlGetQuantizeMatMulWorkspaceSize(cnnlHandle_t handle,
                                                          const cnnlMatMulDescriptor_t matmul_desc,
                                                          const cnnlTensorDescriptor_t a_desc,
                                                          const cnnlTensorDescriptor_t b_desc,
                                                          const cnnlTensorDescriptor_t c_desc,
                                                          const cnnlMatMulAlgo_t algo,
                                                          size_t *workspace_size);

// Group:MatMul
/*!
 * @brief Quantizes data type of input tensor \p a and \p b, and computes the matrix
 * multiplication, then returns the results in the output tensor \p c. For more
 * information about quantization, see "Cambricon CNNL User Guide".
 *
 * The matrix multiplication is computed based on the matrix multiplication algorithm
 * set in \p algo. You can call the ::cnnlGetQuantizeMatMulAlgorithm function to get
 * the most suitable algorithm. This function needs extra MLU memory as the workspace to
 * improve the matrix multiplication performance. You can get the size of the workspace
 * \p workspace_size_in_bytes with the ::cnnlGetQuantizeMatMulWorkspaceSize function.
 *
 * The scaling factors \p alpha and \p beta are not supported currently.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlMatMulEx instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] matmul_desc
 *   Input. The descriptor of the matrix multiplication operation. For detail information,
 *   see ::cnnlMatMulDescriptor_t.
 * @param[in] alpha, beta
 *   Input. Reserved for future use. Set the value of alpha to 1.0 and the value of beta to 0.0 now.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of the left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. Pointer to the MLU memory that stores left matrix.
 * @param[in] a_position
 *   Input. Pointer to the MLU memory associated tensor \p a quantization parameter \p position.
 * @param[in] a_scale
 *   Input. Pointer to the MLU memory associated tensor \p a quantization parameter \p scale.
 *   The value of this parameter can be NULL.
 * @param[in] a_offset
 *   Input. Pointer to the MLU memory associated tensor \p a quantization parameter \p offset.
 *   The value of this parameter can be NULL.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of the right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. Pointer to the MLU memory that stores right matrix.
 * @param[in] b_position
 *   Input. Pointer to the MLU memory associated tensor \p b quantization parameter \p position.
 * @param[in] b_scale
 *   Input. Pointer to the MLU memory associated tensor \p b quantization parameter \p scale.
 *   The value of this parameter can be NULL.
 * @param[in] b_offset
 *   Input. Pointer to the MLU memory associated tensor \p b quantization parameter \p offset.
 *   The value of this parameter can be NULL.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] c
 *   Output. Pointer to the MLU memory that stores the output matrix.
 * @param[in] algo
 *   Input. The algorithm used to compute the matrix multiplication. The algorithm detail is stored
 *   in an opaque structure ::cnnlMatMulAlgo_t points to. You can get the best suitable algorithm with
 *   ::cnnlGetQuantizeMatMulAlgorithm function.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the matrix multiplication
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size_in_bytes
 *   Input. The size of the extra workspace in bytes that needs to be used in the matrix multiplication
 *   operation. You can get the size of the workspace with the ::cnnlGetQuantizeMatMulWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for input tensor \p a, \p b and
 *   output tensor \p c.
 *   - \p a offchip data type: half, float, int8, int16, int31.
 *   - \p a onchip data type: int8, int16, int31.
 *   - \p b offchip data type: half, float, int8, int16, int31.
 *   - \p b onchip data type: int8, int16, int31.
 *   - \p c offchip data type: half, float.
 *   - \p c onchip data type: half, float.
 * - \p a offchip data type should be the same as \p b offchip data type when \p a and \p offchip data type is
 *   floating point.
 * - If both \p a and \p b offchip data type is integer, you do not need to set the onchip data type of \a and \p b.
 *   If you set the onchip data type of \p a and \p b, the offchip and onchip data type of \p a must be the same,
 *   and the offchip and onchip data type of \p b must be the same too.
 *
 * @note
 * - The combinations of the data types should satisfy the following rules:
 *   - The data type bitwidth for \p c onchip data type should not be shorter than \p c offchip data type.
 *
 * - This function does not support offline asymmetric quantization currently.
 * - The combinations of the data types should satisfy the following extra rules on CE3226 platform:
 *   - The onchip data type of \p a and \p b must be fixed-point type.
 *   - The \p b data type must be int8 if \p a data type is int8.
 *   - The data type bitwidth of \p c onchip data type for operation computing is not shorter than \p c
 *     offchip data type.
 *
 * @par Scale Limitation
 * - The input tensors and output tensor must meet the following requirements:
 *   1. \p a and \p b must be two dimensions.
 *   2. The number of \p a matrix's columns must be equal to the number of \p b matrix's rows after both inputs
 *   perform transpose operations according to parameters.
 *
 * @par API Dependency
 * - Before calling this function to implement matrix multiplication, you need to prepare
 *   all the parameters passed to this function. See each parameter description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, matrix \p a does not need to transpose and matrix \p b
 *   needs to transpose.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      CNNL_MATMUL_DESC_TRANSA:      false
      CNNL_MATMUL_DESC_TRANSB:      false
      Dimension of input tensor a:  [99, 128]
      Dimension of input tensor b:  [128, 256]
      Dimension of output tensor c: [99, 256]
     @endverbatim
 * - CNNL_MATMUL_DESC_TRANSA, CNNL_MATMUL_DESC_TRANSB attributes of \p matmul_desc can be set by
 *   ::cnnlSetMatMulDescAttr function.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/torch.html?highlight=matmul#torch.matmul
 */
CNNL_DEPRECATED_FOR(cnnlMatMulEx)
cnnlStatus_t CNNL_WIN_API cnnlQuantizeMatMul(cnnlHandle_t handle,
                                             const cnnlMatMulDescriptor_t matmul_desc,
                                             const void *alpha,
                                             const cnnlTensorDescriptor_t a_desc,
                                             const void *a,
                                             const void *a_position,
                                             const void *a_scale,
                                             const void *a_offset,
                                             const cnnlTensorDescriptor_t b_desc,
                                             const void *b,
                                             const void *b_position,
                                             const void *b_scale,
                                             const void *b_offset,
                                             const void *beta,
                                             const cnnlTensorDescriptor_t c_desc,
                                             void *c,
                                             cnnlMatMulAlgo_t algo,
                                             void *workspace,
                                             size_t workspace_size_in_bytes);

// Group:MatMulInference
/*!
 * @brief Returns the most suited matrix multiplication algorithm that can be used
 * in the matrix multiplication operation with quantization.
 *
 * The returned matrix multiplication is chosen from all the CNNL supported matrix
 * algorithms defined in ::cnnlMatMulAlgo_t and is based on the given matrix
 * multiplication descriptor \p matmul_desc, tensor descriptor of left matrix \p a_desc, tensor
 * descriptor of right matrix \p b_desc, tensor descriptor of output matrix \p c_desc, and
 * matrix multiplication algorithm \p preference.
 *
 * The computing performance options \p preference is defined in the ::cnnlMatMulPreference_t
 * enum, and only supports the high speed mode.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGetMatMulExAlgoHeuristic instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] matmul_desc
 *  Input. The descriptor of the matrix multiplication operation. For detailed
 *  information, see ::cnnlMatMulDescriptor_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor descriptor of the left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor descriptor of the right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the input tensor descriptor of output matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] preference
 *   Input. The options for implementing the matrix multiplication operation to get better performance.
 *   This parameter only supports CNNL_MATMUL_FASTEST now.
 * @param[out] algo
 *   Output. A host pointer to the most suitable algorithm to compute the matrix multiplication.
 *   Attributes of The algorithms are defined in an opaque structure pointed by ::cnnlMatMulAlgo_t.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlGetMatMulExAlgoHeuristic)
cnnlStatus_t CNNL_WIN_API cnnlGetMatMulInferenceAlgorithm(cnnlHandle_t handle,
                                                          cnnlMatMulDescriptor_t matmul_desc,
                                                          cnnlTensorDescriptor_t a_desc,
                                                          cnnlTensorDescriptor_t b_desc,
                                                          cnnlTensorDescriptor_t c_desc,
                                                          cnnlMatMulPreference_t preference,
                                                          cnnlMatMulAlgo_t *algo);

// Group:MatMulInference
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra workspace
 * to optimize the matrix multiplication operation after quantization.
 *
 * The size of the extra workspace is based on the given information of the matrix multiplication
 * operation, including the matrix multiplication descriptor \p matmul_desc, input tensor
 * descriptor of left matrix \p a_desc, input tensor descriptor of right matrix \p b_desc, output
 * tensor descriptor \p c_desc, and the matrix multiplication algorithm \p algo. For more
 * information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGetMatMulExAlgoHeuristic instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] matmul_desc
 *   Input. The descriptor of the matrix multiplication operations. For detail information,
 *   see ::cnnlMatMulDescriptor_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of the left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of the right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the input tensor of output matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] algo
 *   Input. A host pointer to the most suitable algorithm to compute the matrix multiplication.
 *   Attributes of The algorithms are defined in an opaque structure pointed by ::cnnlMatMulAlgo_t.
 *   You can get the best suited algorithm with the ::cnnlGetMatMulInferenceAlgorithm function.
 * @param[out] workspace_size
 *   Output. Pointer to the MLU memory that stores the returned size of the extra workspace in bytes.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called after ::cnnlGetMatMulInferenceAlgorithm function. You also need to
 *   call the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions to create and set
 *   tensor descriptors \p a_desc, \p b_desc, \p c_desc before calling this function.
 * - The allocated extra workspace should be passed to the ::cnnlMatMulInference function to
 *   perform the matrix multiplication operation with quantization.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlGetMatMulExAlgoHeuristic)
cnnlStatus_t CNNL_WIN_API
cnnlGetMatMulInferenceWorkspaceSize(cnnlHandle_t handle,
                                    cnnlMatMulDescriptor_t matmul_desc,
                                    cnnlTensorDescriptor_t a_desc,
                                    cnnlTensorDescriptor_t b_desc,
                                    cnnlTensorDescriptor_t c_desc,
                                    cnnlMatMulAlgo_t algo,
                                    size_t *workspace_size);

// Group:MatMulInference
 /*!
 * @brief Converts the data type of input \p x to integer when the offchip data type of input \p x is
 * floating point, then computes the matrix multiplication operation. And converts the data type of
 * result to integer when the offchip data type of \p c is integer. For more information about
 * quantization, see "Cambricon CNNL User Guide".
 *
 * This function needs extra MLU memory as the workspace to improve the matrix multiplication
 * performance. You can get the workspace size with the
 * ::cnnlGetMatMulInferenceWorkspaceSize function. The matrix multiplication is computed based
 * on the matrix multiplication algorithm set in \p algo. You can call the
 * ::cnnlGetMatMulInferenceAlgorithm function to get the most suited algorithm.
 *
 * The scaling factors \p alpha and \p beta are not supported currently.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlMatMulEx_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] matmul_desc
 *   Input. The descriptor of the matrix multiplication operations. For detail information,
 *   see ::cnnlMatMulDescriptor_t.
 * @param[in] alpha, beta
 *   Input. Reserved for future use. Set the value of alpha to 1.0 and the value of beta to 0.0 now.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of the left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. Pointer to the MLU memory that stores the input tensor of the left matrix.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of the right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. Pointer to the MLU memory that stores the input tensor of the right matrix.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] c
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] algo
 *   Input. A host pointer to the most suitable algorithm to compute the matrix multiplication.
 *   Attributes of The algorithms are defined in an opaque structure pointed by ::cnnlMatMulAlgo_t.
 *   You can get the best suited algorithm with the ::cnnlGetMatMulInferenceAlgorithm function.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the matrix multiplication
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size_in_bytes
 *   Input. The size of the extra workspace in bytes that needs to be used in the matrix multiplication
 *   operation. You can get the size of the workspace with the ::cnnlGetMatMulInferenceWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for input tensor \p a, \p b and
 *   output tensor \p c on all hardware platforms.
 *   - The offchip data type of \p a: half, float, int8, int16.
 *   - The onchip data type of \p a: int8, int16.
 *   - The offchip data type and onchip data type of \p b: int8, int16.
 *   - The offchip data type of \p c: half, float, int8, int16.
 *   - The onchip data type of \p c: half, float.
 * - This function supports the combinations of the following data types for input tensor \p a, \p b and
 *   output tensor \p c on MLU300 series or above.
 *   - The offchip data type and onchip data type of \p a, \p b, \p c : half, half, half, half, half, half.
 *   - The offchip data type and onchip data type of \p a, \p b, \p c : half, half, half, half, half, float.
 *   - The offchip data type and onchip data type of \p a, \p b, \p c : float, float, float, float, float, float.
 * - The combinations of the data types should satisfy the following rules:
 *   - If \p a offchip data type is integer type, it should be the same as \p a onchip data type.
 *   - The bitwidth of \p a onchip data type must be greater than or equal to the bitwidth of \p b onchip data type.
 *   - If \p a offchip data type and \p c offchip data type are both integer types, they should be the same integer type.
 *   - The \p c offchip data type should be equal to \p a onchip data type or \p c onchip data type.
 *   - The floating point data types of \p a \p b and \p c under the same combination need to be consistent.
 *   - When the offchip data type of \p c is floating point, the data type of \p bias should be the same
 *     with the offchip data type of \p c, otherwise the data type of \p bias should be the same as the
 *     onchip data type of \p c.
 *
 * @note
 * - The function supports adding bias to matrix multiplication on all platforms. You can
 *   set the pointer of \p bias in the \p matmul_desc with ::cnnlSetMatMulDescAttr.
 * - The combinations of the data types should satisfy the following extra rules on CE3226 platform:
 *   - The onchip data type of \p a and \p b must be fixed-point type.
 *   - The \p b data type must be int8 if \p a data type is int8.
 *   - The data type bitwidth of \p c onchip data type for operation computing is not shorter than \p c
 *     offchip data type.
 *
 * @par Scale Limitation
 * - The input tensors and output tensor must meet the following requirements:
 *   1. \p a and \p b must be two dimensions.
 *   2. The number of columns in matrix \p a must be equal to the number of rows in matrix \p b after both inputs
 *   perform transpose operations according to parameters.
 *   3. The number of \p bias must be equal to the number of columns in matrix \p b after both inputs perform transpose
 *   operations according to parameters.
 *
 * @par API Dependency
 * - Before calling this function to implement matrix multiplication, you need to prepare
 *   all the parameters passed to this function. See each parameter description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, matrix \p a should not be transposed and matrix \p b
 *   should be transposed.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      CNNL_MATMUL_DESC_TRANSA:      false
      CNNL_MATMUL_DESC_TRANSB:      false
      Dimension of input tensor a:  [99, 128]
      Dimension of input tensor b:  [128, 256]
      Dimension of output tensor c: [99, 256]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/torch.html?highlight=matmul#torch.matmul
 */
CNNL_DEPRECATED_FOR(cnnlMatMulEx_v2)
cnnlStatus_t CNNL_WIN_API cnnlMatMulInference(cnnlHandle_t handle,
                                              cnnlMatMulDescriptor_t matmul_desc,
                                              const void *alpha,
                                              cnnlTensorDescriptor_t a_desc,
                                              const void *a,
                                              cnnlTensorDescriptor_t b_desc,
                                              const void *b,
                                              const void *beta,
                                              cnnlTensorDescriptor_t c_desc,
                                              void *c,
                                              cnnlMatMulAlgo_t algo,
                                              void *workspace,
                                              size_t workspace_size_in_bytes);

// Group:BatchMatMul
/*!
 * @brief Creates a descriptor pointed by \p bmm_desc for a batch matrix multiplication operation,
 *        and allocates memory for holding the information about the batch matrix multiplication operation.
 *        The information is defined in ::cnnlBatchMatMulDescriptor_t.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[out] bmm_desc
 *   Output. A host pointer to the batch matrix multiplication descriptor that holds information about the
 *   batch matrix multiplication operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetBatchMatMulDescAttr function to initialize
 *   and set the information to the batch matrix multiplication descriptor.
 * - You need to call the ::cnnlBatchMatMulDescDestroy function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlBatchMatMulDescCreate(cnnlBatchMatMulDescriptor_t *bmm_desc);

/*! The descriptor of the fusion bias info EpilogueBias used by cnnlMatMulInferenceDesc_t. This descriptor should be set
 *  with cnnlSetMatmulInferenceDesc.
 *
 *  You need to call the ::cnnlCreateEpilogueBias function to
 *  create a descriptor, and call the ::cnnlSetEpilogueBias function to set the information to the descriptor.
 *  Also, you need to destroy the descriptor at the end with the
 *  ::cnnlDestroyEpilogueBias function.
 *
 *  It is deprecated and will be removed in future release.
 */
typedef struct EpilogueBias *cnnlEpilogueBias_t;

// Group:MatMulInference
/*!
 * @brief Creates a descriptor pointed by fusion bias info \p bias_epilogue used by cnnlMatMulInferenceDesc_t,
 *        and allocates memory for holding the information about the operation. The information is defined in
 *        ::cnnlEpilogueBias_t.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[out] bias_epilogue
 *   Output. A host pointer to the struct of matrix multiplication inference fused operation with bias.
 *
 * @param[out] size
 *   Output. A host pointer to the size of struct of matrix multiplication inference fused operation with bias.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetEpilogueBias function to initialize
 *   and set the information to the matrix multiplication inference fused operation with bias.
 * - You need to call the ::cnnlDestroyEpilogueBias function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlCreateEpilogueBias(cnnlEpilogueBias_t *bias_epilogue, size_t *size);

// Group:MatMulInference
/*!
 * @brief Destroys a fusion bias info descriptor \p bias_epilogue that was previously created with the
 *  ::cnnlCreateEpilogueBias.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] bias_epilogue
 *   Input. The descriptor of matrix multiplication inference fused operation information to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlDestroyEpilogueBias(cnnlEpilogueBias_t bias_epilogue);

// Group:MatMulInference
/*!
 * @brief Initializes the fusion bias info descriptor \p bias_epilogue that was previously created with the
 *  ::cnnlCreateEpilogueBias function, and sets the information of descriptor \p bias_epilogue.
 *  The information includes the shape of bias, and the device pointer to the bias value \p bias_ptr.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlSetMatMulExBias instead.
 *
 * @param[in,out] bias_epilogue
 *   Input/output. The descriptor of the matrix multiplication inference fused operation with bias. For detailed
 *   information, see ::cnnlEpilogueBias_t.
 * @param[in] bias_desc
 *   Input. The descriptor of the bias tensor to be set. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] bias_ptr
 *   Output. Pointer to the MLU memory that stores the bias tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - Currently 4 types of bias are supported. If the matrix multiplication output shape is M * N,
     the supported shapes of bias are as follows:
 *   - 1 * 1
 *   - 1 * N
 *   - M * N
 *   - M * 1
 * - The \p dim of \p bias_desc should be 1 or 2. The bias is scalar when \p dim = 1, and bias is vector or
 *   matrix when \p dim = 2.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlSetMatMulExBias)
cnnlStatus_t CNNL_WIN_API cnnlSetEpilogueBias(cnnlEpilogueBias_t bias_epilogue,
                                              cnnlTensorDescriptor_t bias_desc,
                                              void *bias_ptr);

/*! The descriptor of the matrix multiplication inference that holds the fused operation information.
 *
 *  It is deprecated and will be removed in future release.
 *
 *  You need to call the ::cnnlCreateMatMulInferenceDesc function to create a descriptor, and call the
 *  ::cnnlSetMatmulInferenceDesc function to set the information to the descriptor. Also, you need to
 *  destroy the Cambricon CNNL context at the end with the ::cnnlDestroyMatMulInferenceDesc function.
 */
typedef struct cnnlMatMulInferenceDesc *cnnlMatMulInferenceDesc_t;

/*! The descriptor of the matrix multiplication inference that holds the configured matrix multiplication
 *  algorithm descriptor and its runtime properties.
 *
 *  You need to call the ::cnnlCreateMatMulInferHeuristicResult function to create a descriptor.
 *  Also, you need to destroy the Cambricon CNNL context at the end with the ::cnnlDestroyMatMulInferHeuristicResult function.
 *
 *  It is deprecated and will be removed in future release.
 */
typedef struct cnnlMatMulInferHeuristicResult *cnnlMatMulInferHeuristicResult_t;

/*! The descriptor of the matrix multiplication that holds the preferences for cnnlMatMulInferHeuristicResult_t
 *  configuration.
 *
 *  It is deprecated and will be removed in future release.
 */
typedef struct cnnlMatMulInferPrefer *cnnlMatMulInferPrefer_t;

// Group:MatMulInference
/*!
 * @brief Creates a descriptor pointed by \p algo for a matrix multiplication inference fused operation,
 *        and allocates memory for holding the information about the operation. The information is defined in
 *        ::cnnlMatMulInferenceAlgo_t.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlCreateMatMulExAlgo instead.
 *
 * @param[out] algo
 *   Output. A host pointer to the struct of matrix multiplication inference algorithm.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - You need to call the ::cnnlDestroyMatMulInferenceAlgo function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlCreateMatMulExAlgo)
cnnlStatus_t cnnlCreateMatMulInferenceAlgo(cnnlMatMulInferenceAlgo_t *algo);

// Group:MatMulInference
/*!
 * @brief Destroys a matrix multiplication inference algorithm with descriptor \p algo,
 *        that was previously created with ::cnnlCreateMatMulInferenceAlgo.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlDestroyMatMulExAlgo instead.
 *
 * @param[in] algo
 *   Input. The matrix multiplication inference algorithm descriptor to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlDestroyMatMulExAlgo)
cnnlStatus_t cnnlDestroyMatMulInferenceAlgo(cnnlMatMulInferenceAlgo_t algo);
// Group:MatMulInference
/*!
 * @brief Creates a descriptor pointed by \p result for a matrix multiplication inference heuristic result,
 *        and allocates memory for the result. The result is defined in ::cnnlMatMulInferHeuristicResult_t.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlCreateMatMulExHeuristicResult instead.
 *
 * @param[out] result
 *   Output. A host pointer to the struct of matrix multiplication inference heuristic result.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - You need to call the ::cnnlDestroyMatMulInferHeuristicResult function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlCreateMatMulExHeuristicResult)
cnnlStatus_t cnnlCreateMatMulInferHeuristicResult(cnnlMatMulInferHeuristicResult_t *result);

// Group:MatMulInference
/*!
 * @brief Destroys a matrix multiplication inference heuristic result,
 *        that was previously created with ::cnnlCreateMatMulInferenceDesc.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlDestroyMatMulExHeuristicResult instead.
 *
 * @param[in] result
 *   Input. The matrix multiplication inference heuristic result to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlDestroyMatMulExHeuristicResult)
cnnlStatus_t cnnlDestroyMatMulInferHeuristicResult(cnnlMatMulInferHeuristicResult_t result);

// Group:MatMulInference
/*!
 * @brief Gets matrix multiplication inference algorithm and workspace size from heuristic result
 *  that was previously selected with ::cnnlMatMulInferGetAlgoHeuristic.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGetMatMulExHeuristicResult instead.
 *
 * @param[in] result
 *   Input. The matrix multiplication inference heuristic result created with ::cnnlCreateMatMulInferHeuristicResult.

 * @param[in] algo
 *   Output. The matrix multiplication inference algorithm.

 * @param[in] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in
 *   the matrix multiplication inference operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlGetMatMulExHeuristicResult)
cnnlStatus_t cnnlGetMatMulInferHeuristicResult(cnnlMatMulInferHeuristicResult_t result,
                                          cnnlMatMulInferenceAlgo_t algo,
                                          size_t *workspace_size);

// Group:MatMulInference
/*!
 * @brief Sets the \p inference_desc with fusion bias information. The information includes the shape of bias,
 *  the data type of bias, and the device pointer to the bias value \p bias_ptr.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlSetMatMulExBias instead.
 *
 * @param[out] inference_desc
 *   Output. A host pointer to the struct of matrix multiplication inference fused operation.
 *
 * @param[in] bias_desc
 *   Input. The descriptor of the bias tensor to be set. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] bias_ptr
 *   Output. Pointer to the MLU memory that stores the bias tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - Currently 4 types of bias are supported. If the matrix multiplication output shape is M * N,
     the supported shapes of bias are as follows:
 *   - 1 * 1
 *   - 1 * N
 *   - M * N
 *   - M * 1
 * - The \p dim of \p bias_desc should be 1 or 2. The bias is scalar when \p dim = 1, and bias is vector or
 *   matrix when \p dim = 2.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlSetMatMulExBias)
cnnlStatus_t cnnlSetMatMulEpilogueBias(cnnlMatMulInferenceDesc_t inference_desc,
                                       cnnlTensorDescriptor_t bias_desc,
                                       void *bias_ptr);

// Group:MatMulInference
/*!
 * @brief Sets the \p inference_desc with fused operations in the following order:
 *  bias, scale, batch normalization and activation with following formula:
 *
 *  bn_alpha * ((scale_alpha * (matmul + bias) + scale_beta) - bn_mean) / rsqrt(bn_var + epsilon)) + bn_beta
 *
 *  Batch normalization operation will be supported in future release.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlSetMatMulExBiasScaleBNActive instead.
 *
 * @param[in,out] inference_desc
 *   Input/output. A host pointer to the struct of matrix multiplication inference fused operation.
 * @param[in] bias_desc
 *   Input. The descriptor of the bias tensor to be set. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the bias tensor.
 * @param[in] scale_alpha
 *   Input. Pointer to the MLU memory that stores the scale_alpha scalar tensor.
 * @param[in] scale_beta
 *   Input. Pointer to the MLU memory that stores the scale_beta tensor.
 * @param[in] bn_mean
 *   Input. Pointer to the MLU memory that stores the bn_mean tensor. Currently it is not supported and should be set to NULL.
 * @param[in] bn_var
 *   Input. Pointer to the MLU memory that stores the bn_var tensor. Currently it is not supported and should be set to NULL.
 * @param[in] bn_filter
 *   Input. Pointer to the MLU memory that stores the bn_filter tensor. Currently it is not supported and should be set to NULL.
 * @param[in] bn_beta
 *   Input. Pointer to the MLU memory that stores the bn_beta tensor. Currently it is not supported and should be set to NULL.
 * @param[in] epsilon
 *   Input. A float value added to the denominator for numerical stability, used in batch norm operation. Currently not
 *   supported and should be set to 0.
 * @param[in] scale_alpha_num
 *   Input. The number of scale_alpha tensor elements, used in scale operation.
 * @param[in] scale_beta_num
 *   Input. The number of scale_beta tensor elements, used in scale operation.
 * @param[in] bn_filter_num
 *   Input. The number of bn_filter tensor elements, used in batch norm operation. Currently not supported and
 *   should be set to NULL.
 * @param[in] bn_beta_num
 *   Input. The number of bn_beta tensor elements, used in batch norm operation. Currently it is not supported and should be set to NULL.
 * @param[in] active_desc
 *   Input. The descriptor of the activation operation. For detailed information,
 *   see ::cnnlActivationDescriptor_t.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - Currently only supports fused operation of combination of matmul, bias, scale, and activation.
 *   BN operation will be supported in future release.
 * - The activation mode only supports relu.
 * - If the matrix multiplication output shape is M * N, the supported shape of bias is 1 * N. The length of
 *   scale_alpha_num and scale_beta_num should be 1 or N if has scale operation.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */

CNNL_DEPRECATED_FOR(cnnlSetMatMulExBiasScaleBNActive)
cnnlStatus_t cnnlSetEpilogueBiasScaleBNActive(cnnlMatMulInferenceDesc_t inference_desc,
                                              cnnlTensorDescriptor_t bias_desc,
                                              const void *bias,
                                              const void *scale_alpha,
                                              const void *scale_beta,
                                              const void *bn_mean,
                                              const void *bn_var,
                                              const void *bn_filter,
                                              const void *bn_beta,
                                              const float epsilon,
                                              const int scale_alpha_num,
                                              const int scale_beta_num,
                                              const int bn_filter_num,
                                              const int bn_beta_num,
                                              cnnlActivationDescriptor_t active_desc);
// Group:MatMulInference
/*!
 * @brief Retrieves the possible algorithms can be used in the matrix multiplication inference.
 *  The output is placed in resultArray[] in the order of increasing estimated compute time.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGetMatMulExAlgoHeuristic instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] inference_desc
 *   Output. A host pointer to the struct of matrix multiplication inference fused descriptor.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of the left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of the right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] d_desc
 *   Input. The descriptor of the output tensor. Used in out-of-place matrix multiplication.
 * @param[in] preference
 *   Input. The descriptor of the matrix multiplication that holds the preferences for ::cnnlMatMulInferHeuristicResult_t
 *   configuration. Currently it is not supported and should be set to NULL.
 * @param[in] requestedAlgoCount
 *   Input. The size of requested algorithm. This is the requested maximum number of algorithms to be returned.
 *   Currently this value only supports 1.
 * @param[in] resultArray
 *   Input. Array containing the algorithm heuristics and associated runtime characteristic, returned by this function,
 *   in the order of increasing estimated compute time.
 * @param[in] returnAlgoCount
 *   Input. The number of algorithms returned by this function.

 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - Currently the maximum number of algorithms only supports 1.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlGetMatMulExAlgoHeuristic)
cnnlStatus_t cnnlMatMulInferGetAlgoHeuristic(cnnlHandle_t handle,
                                             cnnlMatMulInferenceDesc_t inference_desc,
                                             cnnlTensorDescriptor_t a_desc,
                                             cnnlTensorDescriptor_t b_desc,
                                             cnnlTensorDescriptor_t c_desc,
                                             cnnlTensorDescriptor_t d_desc,
                                             cnnlMatMulInferPrefer_t preference,
                                             int requestedAlgoCount,
                                             cnnlMatMulInferHeuristicResult_t resultArray[],
                                             int *returnAlgoCount);

// Group:MatMulInference
/*!
 * @brief Creates a descriptor pointed by \p inference_desc for a matrix multiplication inference fused operation,
 *        and allocates memory for holding the information about the operation. The information is defined in
 *        ::cnnlMatMulInferenceDesc_t.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlCreateMatMulExDescriptor instead.
 *
 * @param[out] inference_desc
 *   Output. A host pointer to the struct of matrix multiplication inference fused operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetMatmulInferenceDesc function to initialize
 *   and set the information to the matrix multiplication inference fused operation.
 * - You need to call the ::cnnlDestroyMatMulInferenceDesc function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlCreateMatMulExDescriptor)
cnnlStatus_t CNNL_WIN_API cnnlCreateMatMulInferenceDesc(cnnlMatMulInferenceDesc_t *inference_desc);

// Group:MatMulInference
/*!
 * @brief Destroys a matrix multiplication inference fused operation with descriptor \p inference_desc,
 *        that was previously created with ::cnnlCreateMatMulInferenceDesc.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlDestroyMatMulExDescriptor instead.
 *
 * @param[in] inference_desc
 *   Input. The matrix multiplication inference fused operation descriptor to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlDestroyMatMulExDescriptor)
cnnlStatus_t CNNL_WIN_API cnnlDestroyMatMulInferenceDesc(cnnlMatMulInferenceDesc_t inference_desc);

// Group:MatMulInference
/*!
 * @brief Initializes the matrix multiplication inference descriptor \p inference_desc
 * that was previously created with the ::cnnlCreateMatMulInferenceDesc function, and sets
 * the information about the matrix multiplication inference operation to the matrix multiplication
 *  inference descriptor \p inference_desc. The information includes the attribute \p attr defined in
 * ::cnnlMatMulDescAttribute_t, the host pointer \p buf to the attribute value, and
 * the size of buffer for verification.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlSetMatMulExDescAttr instead.
 *
 * @param[in,out] inference_desc
 *   Input/output. The descriptor of the matrix multiplication inference operation. For detailed
 *   information, see ::cnnlMatMulInferenceDesc_t.
 * @param[in] attr
 *   Input. Attribute of matrix multiplication descriptor to be set. For detailed
 *   information, see ::cnnlMatMulDescAttribute_t.
 * @param[in] buf
 *   Input. A host pointer to the attribute value set by this function.
 * @param[in] size_in_bytes
 *   Input. Buffer in bytes for verification.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - You should set CNNL_MATMUL_DESC_EPILOGUE_TYPE attribute before set CNNL_MATMUL_DESC_EPILOGUE_OPERAND
 *   attribute.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlSetMatMulExDescAttr)
cnnlStatus_t CNNL_WIN_API cnnlSetMatmulInferenceDesc(cnnlMatMulInferenceDesc_t inference_desc,
                                                     cnnlMatMulDescAttribute_t attr,
                                                     const void *buf,
                                                     size_t size_in_bytes);
// Group:MatMulInference
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra workspace
 * to optimize the matrix multiplication inference operation.
 *
 * The size of the extra workspace is based on the given information of the matrix multiplication inference
 * operation, including the matrix multiplication inference descriptor \p matmul_desc, input tensor
 * descriptor of left matrix \p a_desc, input tensor descriptor of right matrix \p b_desc, output
 * tensor  descriptor \p c_desc, and the matrix multiplication algorithm \p algo. For more
 * information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGetMatMulExHeuristicResult instead.
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] inference_desc
 *   Input. The descriptor of the matrix multiplication inference operations. For detail information,
 *   see ::cnnlMatMulInferenceDesc_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of the left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of the right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the input tensor of output matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] d_desc
 *   Input. The descriptor of the input tensor of output matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] algo
 *   Input. A host pointer to the most suitable algorithm to compute the matrix multiplication.
 * @param[out] workspace_size
 *   Output. Pointer to the MLU memory that stores the returned size of the extra workspace in bytes.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called after ::cnnlSetMatmulInferenceDesc function. You also need to
 *   call the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions to create and set
 *   tensor descriptors \p a_desc, \p b_desc, \p c_desc before calling this function.
 * - The allocated extra workspace should be passed to the ::cnnlMatMulInference_v2 function to
 *   perform the matrix multiplication inference operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlGetMatMulExHeuristicResult)
cnnlStatus_t CNNL_WIN_API
cnnlGetMatMulInferenceV2WorkspaceSize(cnnlHandle_t handle,
                                      cnnlMatMulInferenceDesc_t inference_desc,
                                      cnnlTensorDescriptor_t a_desc,
                                      cnnlTensorDescriptor_t b_desc,
                                      cnnlTensorDescriptor_t c_desc,
                                      cnnlTensorDescriptor_t d_desc,
                                      cnnlMatMulInferenceAlgo_t algo,
                                      size_t *workspace_size);
// Group:MatMulInference
/*!
 * @brief Compute the matrix multiplication inference operation. Supports to convert the data type of
 * \p a or \p b to integer when the offchip type of \p a or \p b is float. Also supports different
 * fused operation after matrix multiplication.
 *
 * Compared with ::cnnlMatMulInference, it supports 4 types of bias.
 *
 * This function needs extra MLU memory as the workspace to improve the matrix multiplication
 * performance. You can get the workspace size with the
 * ::cnnlGetMatMulInferenceV2WorkspaceSize function.

 * The matrix multiplication algorithm \p algo is not supported currently.
 *
 * The scaling factors \p alpha and \p beta are not supported currently.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlMatMulEx_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   matrix multiplication inference operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] inference_desc
 *   Input. The descriptor of the matrix multiplication operations. For detail information,
 *   see ::cnnlMatMulInferenceDesc_t.
 * @param[in] alpha
 *   Input. Reserved for future use. Set the value of alpha to 1.0 now.
 * @param[in] beta
 *   Input. Reserved for future use. Set the value of beta to 0.0 now.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of the left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. Pointer to the MLU memory that stores the input tensor of the left matrix.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of the right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. Pointer to the MLU memory that stores the input tensor of the right matrix.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] c
 *   Output. Pointer to the MLU memory that stores the output tensor \p c, or to the MLU memory that stores the
     bias \p c tensor when c = alpha * a * b + beta * c. Used in in-place matrix multiplication.
 * @param[in] d_desc
 *   Input. The descriptor of the output tensor. Used in out-of-place matrix multiplication.
     Currently not support and should be set to NULL.
 * @param[out] d
 *   Output. Pointer to the MLU memory that stores the output \p d when d = alpha * a * b + beta * c.
 *   Used in out-of-place matrix multiplication. Currently it is not supported and should be set to NULL.
 * @param[in] algo
 *   Input. A host pointer to the most suitable algorithm to compute the matrix multiplication. Currently not
 *   supported.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the matrix multiplication
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the matrix multiplication
 *   operation. You can get the size of the workspace with the ::cnnlGetMatMulInferenceV2WorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for input tensor \p a, \p b and
 *   output tensor \p c on all hardware platforms.
 *   - The offchip data type of \p a: half, float, int8, int16.
 *   - The onchip data type of \p a: int8, int16.
 *   - The offchip data type of \p b: half, float, int8, int16.
 *   - The onchip data type of \p b: int8, int16.
 *   - The offchip data type of \p c: half, float.
 *   - The onchip data type of \p c: half, float.
 *   - The onchip data type of \p d: half, float.
 * - This function supports the combinations of the following data types for input tensor \p a, \p b and
 *   output tensor \p c on MLU300 series or above.
 *   - The offchip data type and onchip data type of \p a, \p b, \p c: half, half, half, half, half, half.
 *   - The offchip data type and onchip data type of \p a, \p b, \p c: half, half, half, half, half, float.
 *   - The offchip data type and onchip data type of \p a, \p b, \p c: float, float, float, float, float, float.
 * - The combinations of the data types should satisfy the following rules:
 *   - If \p a offchip data type is integer type, it should be the same as \p a onchip data type.
 *   - The floating point data types of \p a \p b and \p c under the same combination need to be consistent.
 *   - The data type of \p bias should be the same as the offchip data type of \p c.
 *
 * @note
 * - The function supports adding bias to matrix multiplication on all platforms. You can
 *   set the pointer of \p bias in the \p inference_desc with ::cnnlSetMatMulDescAttr.
 * - The combinations of the data types should satisfy the following rules on CE3226 platform:
 *   - The onchip data type of \p a and \p b must be fixed-point type.
 *   - The \p b data type must be int8 if \p a data type is int8.
 *   - The data type bitwidth of \p c onchip data type for operation computing is not shorter than \p c
 *     offchip data type.
 * - When the fuse type is not ::CNNL_MATMUL_EPI_BIAS, the combinations of the data types should
 *   satisfy the following rules:
 *   - The \p a offchip data type should be the same as \p c onchip data type when \p a offchip data type is
 *     floating point type on MLU200 series.
 *   - The \p c offchip data type should be the same as \p c onchip data type when \p c offchip data type is
 *     floating point type on MLU200 series.
 * - The function supports fused operation after matrix multiplication. The fused information should be set with
 *   ::cnnlMatMulInferenceDesc_t.
 * - Currently this function does not supports zero element input.
 *
 * @par Scale Limitation
 * - \p a and \p b must be two dimensional.
 *
 * @par API Dependency
 * - Before calling this function to implement matrix multiplication inference operation, you need to prepare
 *   all the parameters passed to this function. See each parameter description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, matrix \p a should not be transposed and matrix \p b
 *   should be transposed.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      CNNL_MATMUL_DESC_TRANSA:      false
      CNNL_MATMUL_DESC_TRANSB:      false
      Dimension of input tensor a:  [99, 128]
      Dimension of input tensor b:  [128, 256]
      Dimension of output tensor c: [99, 256]
      Dimension of bias tensor: [1, 256]
     @endverbatim
 */
CNNL_DEPRECATED_FOR(cnnlMatMulEx_v2)
cnnlStatus_t CNNL_WIN_API cnnlMatMulInference_v2(cnnlHandle_t handle,
                                                 cnnlMatMulInferenceDesc_t inference_desc,
                                                 const void *alpha,
                                                 const cnnlTensorDescriptor_t a_desc,
                                                 const void *a,
                                                 const cnnlTensorDescriptor_t b_desc,
                                                 const void *b,
                                                 const void *beta,
                                                 const cnnlTensorDescriptor_t c_desc,
                                                 void *c,
                                                 const cnnlTensorDescriptor_t d_desc,
                                                 void *d,
                                                 cnnlMatMulInferenceAlgo_t algo,
                                                 void *workspace,
                                                 size_t workspace_size);


/*! The descriptor of the matrix multiplication extension operation that holds the preferences for cnnlMatMulExHeuristicResult_t
 *  configuration.
 */
typedef struct cnnlMatMulExPrefer *cnnlMatMulExPrefer_t;

/*! The descriptor of the matrix multiplication extension algorithm. */
typedef struct cnnlMatMulExAlgo *cnnlMatMulExAlgo_t;

/*! The descriptor of the matrix multiplication that holds the configured matrix multiplication extension
 *  algorithm and its runtime properties. You need to call the ::cnnlCreateMatMulExHeuristicResult
 *  function to create a descriptor. Also, you need to destroy the Cambricon CNNL context at the end with the
 *  ::cnnlDestroyMatMulExHeuristicResult function.
 */
typedef struct cnnlMatMulExHeuristicResult *cnnlMatMulExHeuristicResult_t;

/*! The descriptor of the matrix multiplication extension operation. You need to call the ::cnnlCreateMatMulExDescriptor
 *  function to create a descriptor. Also, you need to destroy the Cambricon CNNL context at the end with the
 *  ::cnnlDestroyMatMulExDescriptor function.
 */
typedef struct cnnlMatMulExDesc *cnnlMatMulExDescriptor_t;

// Group:MatMulEx
/*!
 * @brief Creates a descriptor pointed by \p algo for a matrix multiplication extension operation,
 * and allocates memory for holding the information about the operation. The information is defined in
 * ::cnnlMatMulExAlgo_t.
 *
 * @param[out] algo
 *   Output. A host pointer to the struct of matrix multiplication extension algorithm.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - You need to call the ::cnnlDestroyMatMulExAlgo function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateMatMulExAlgo(cnnlMatMulExAlgo_t *algo);

// Group:MatMulEx
/*!
 * @brief Destroys a matrix multiplication extension algorithm with descriptor \p algo,
 *        which was previously created with ::cnnlCreateMatMulExAlgo.
 *
 * @param[in] algo
 *   Input. The matrix multiplication extension algorithm descriptor to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyMatMulExAlgo(cnnlMatMulExAlgo_t algo);

// Group:MatMulEx
/*!
 * @brief Creates a descriptor pointed by \p result for a matrix multiplication extension heuristic result,
 *        and allocates memory for the result. The result is defined in ::cnnlMatMulExHeuristicResult_t.
 *
 * @param[out] result
 *   Output. A host pointer to the struct of matrix multiplication extension heuristic result.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - You need to call the ::cnnlDestroyMatMulExHeuristicResult function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateMatMulExHeuristicResult(cnnlMatMulExHeuristicResult_t *result);

// Group:MatMulEx
/*!
 * @brief Destroys a matrix multiplication extension heuristic result,
 *        which was previously created with ::cnnlCreateMatMulExHeuristicResult.
 *
 * @param[in] result
 *   Input. The matrix multiplication extension heuristic result to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyMatMulExHeuristicResult(cnnlMatMulExHeuristicResult_t result);

// Group:MatMulEx
/*!
 * @brief Gets matrix multiplication extension algorithm and workspace size from heuristic result
 *  that was previously selected with ::cnnlGetMatMulExAlgoHeuristic.
 *
 * @param[in] result
 *   Input. The matrix multiplication extension heuristic result created with ::cnnlCreateMatMulExHeuristicResult.

 * @param[out] algo
 *   Output. The matrix multiplication extension algorithm.

 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in
 *   the matrix multiplication extension operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetMatMulExHeuristicResult(const cnnlMatMulExHeuristicResult_t result,
                                                         cnnlMatMulExAlgo_t algo,
                                                         size_t *workspace_size);

// Group:MatMulEx
/*!
 * @brief Sets the matrix multiplication extension descriptor \p matmulex_desc with fused bias information.
 *  The information includes the shape of bias, the data type of bias, and the device pointer to the bias value \p bias_ptr.
 *
 * @param[in,out] matmulex_desc
 *   Input/output. A host pointer to the struct of matrix multiplication extension fused operation.
 * @param[in] bias_desc
 *   Input. The descriptor of the bias tensor to be set. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] bias_ptr
 *   Input. Pointer to the MLU memory that stores the bias tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - If the matrix multiplication output shape is [M, N], the supported shapes of \p bias are as follows:
 *   - [1]
 *   - [1, 1]
 *   - [1, N]
 *   - [M, N]
 *   - [M, 1]
 * - The dimension of \p bias_desc should be 1 or 2. The \p bias is scalar when dimension = 1, and \p bias is vector or
 *   matrix when dimension = 2.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetMatMulExBias(cnnlMatMulExDescriptor_t matmulex_desc,
                                              cnnlTensorDescriptor_t bias_desc,
                                              void *bias_ptr);

// Group:MatMulEx
/*!
 * @brief Sets the matrix multiplication extension descriptor \p matmulex_desc with fused operations in the following order:
 *  bias, scale, batch normalization and activation with following formula:
 *
 *  bn_alpha * ((scale_alpha * (matmul + bias) + scale_beta) - bn_mean) / rsqrt(bn_var + epsilon)) + bn_beta
 *
 *  Batch normalization operation will be supported in future release.
 *
 * @param[in,out] matmulex_desc
 *   Input/output. A host pointer to the struct of matrix multiplication extension descriptor.
 * @param[in] bias_desc
 *   Input. The descriptor of the bias tensor to be set. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the bias tensor.
 * @param[in] scale_alpha
 *   Input. Pointer to the MLU memory that stores the scale_alpha scalar tensor.
 * @param[in] scale_beta
 *   Input. Pointer to the MLU memory that stores the scale_beta tensor.
 * @param[in] bn_mean
 *   Input. Pointer to the MLU memory that stores the bn_mean tensor. Currently it is not supported and should be set to NULL.
 * @param[in] bn_var
 *   Input. Pointer to the MLU memory that stores the bn_var tensor. Currently it is not supported and should be set to NULL.
 * @param[in] bn_weight
 *   Input. Pointer to the MLU memory that stores the bn_weight tensor. Currently it is not supported and should be set to NULL.
 * @param[in] bn_beta
 *   Input. Pointer to the MLU memory that stores the bn_beta tensor. Currently it is not supported and should be set to NULL.
 * @param[in] epsilon
 *   Input. A float value added to the denominator for numerical stability, used in batch norm operation. Currently not
 *   supported and should be set to 0.
 * @param[in] scale_alpha_num
 *   Input. The number of scale_alpha tensor elements, used in scale operation.
 * @param[in] scale_beta_num
 *   Input. The number of scale_beta tensor elements, used in scale operation.
 * @param[in] bn_weight_num
 *   Input. The number of bn_weight tensor elements, used in batch norm operation. Currently it is not supported and
 *   should be set to 0.
 * @param[in] bn_beta_num
 *   Input. The number of bn_beta tensor elements, used in batch norm operation. Currently it is not supported and should be set to 0.
 * @param[in] active_desc
 *   Input. The descriptor of the activation operation. For detailed information,
 *   see ::cnnlActivationDescriptor_t.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - Currently only supports fused operation of combinations of matmul, bias, scale and activation.
 *   Batch normalization operation will be supported in future release.
 * - The activation \p mode only supports ::CNNL_ACTIVATION_RELU, ::CNNL_ACTIVATION_LEAKYRELU and
 *   ::CNNL_ACTIVATION_GELU, and CNNL_ACTIVATION_SILU.
 * - When the activation \p mode is CNNL_ACTIVATION_GELU, \p approximate should be set to describe the
 *   implementation logic of different GELU approximation algorithms.
     - When \p approximate is true, the
 *     implementation logic of GELU is as follows:
       @verbatim
        output = 0.5 * x * (1 + Tanh(sqrt(2/pi) * (x + 0.044715 * x^3)));
       @endverbatim
 *   - When \p approximate is false, the implementation logic of GELU is as follows:
       @verbatim
        output = 0.5 * x * (1 + erf(x / sqrt(2)));
       @endverbatim
 * - The activation preference \p mode for ::CNNL_ACTIVATION_GELU can be ::CNNL_COMPUTATION_FAST or
 *   ::CNNL_COMPUTATION_HIGH_PRECISION. The \p preference modes are defined in ::cnnlComputationPreference_t.
 * - If the matrix multiplication output shape is [M, N], the supported shape of \p bias is [1, N]. The length of
 *   \p scale_alpha_num and \p scale_beta_num should be 1 or N. When the length is 1, \p scale_alpha and \p scale_beta should be [1].
 *   When the length is N, \p scale_alpha and \p scale_beta should be [1, N].
 * - When activation \p mode is CNNL_ACTIVATION_SILU, the activation algorithm runs fast but may result in low accuracy.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetMatMulExBiasScaleBNActive(cnnlMatMulExDescriptor_t matmulex_desc,
                                                           cnnlTensorDescriptor_t bias_desc,
                                                           const void *bias,
                                                           const void *scale_alpha,
                                                           const void *scale_beta,
                                                           const void *bn_mean,
                                                           const void *bn_var,
                                                           const void *bn_weight,
                                                           const void *bn_beta,
                                                           const float epsilon,
                                                           const int scale_alpha_num,
                                                           const int scale_beta_num,
                                                           const int bn_weight_num,
                                                           const int bn_beta_num,
                                                           cnnlActivationDescriptor_t active_desc);

// Group:MatMulEx
/*!
 * @brief Retrieves the possible algorithms that can be used in the matrix multiplication extension operation.
 *  The output is placed in resultArray[] in the order of increasing estimated compute time.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   matrix multiplication extension operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] matmulex_desc
 *   Input. A host pointer to the struct of matrix multiplication extension descriptor.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of the left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of the right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] d_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] preference
 *   Input. The descriptor of the matrix multiplication that holds the preferences for ::cnnlMatMulExHeuristicResult_t
 *   configuration. Currently it is not supported and should be set to NULL.
 * @param[in] requestedAlgoCount
 *   Input. The size of requested algorithm. This is the requested maximum number of algorithms to be returned.
 *   Currently this value only supports 1.
 * @param[out] resultArray
 *   Output. Array containing the algorithm heuristics and associated runtime characteristic, returned by this function,
 *   in the order of increasing estimated compute time.
 * @param[out] returnAlgoCount
 *   Output. The number of algorithms returned by this function.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - Currently the maximum number of algorithms only supports 1.
 * - \p c_desc and \p d_desc cannot be NULL at the same time.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetMatMulExAlgoHeuristic(cnnlHandle_t handle,
                                                       const cnnlMatMulExDescriptor_t matmulex_desc,
                                                       const cnnlTensorDescriptor_t a_desc,
                                                       const cnnlTensorDescriptor_t b_desc,
                                                       const cnnlTensorDescriptor_t c_desc,
                                                       const cnnlTensorDescriptor_t d_desc,
                                                       const cnnlMatMulExPrefer_t preference,
                                                       int requestedAlgoCount,
                                                       cnnlMatMulExHeuristicResult_t resultArray[],
                                                       int *returnAlgoCount);

// Group:MatMulEx
/*!
 * @brief Initializes an algorithm \p algo that was previously created with the ::cnnlCreateMatMulExAlgo
 * function and \p workspace_size for auto-tuning mode in ::cnnlMatMulEx_v2, and sets the information
 * about the tile size, warp scheduler, and split k number of the algorithm.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   matrix multiplication extension operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] matmulex_desc
 *   Input. A host pointer to the struct of matrix multiplication extension descriptor.
 *   For detailed information, see ::cnnlMatMulExDescriptor_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of the left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of the right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] d_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] block_m
 *   Input. Pointer to the host memory that holds information about the block size of \p M, which should
 *   be a positive number.
 * @param[in] block_n
 *   Input. Pointer to the host memory that holds information about the block size of \p N, which should
 *   be a positive number.
 * @param[in] block_k
 *   Input. Pointer to the host memory that holds information about the block size of \p K, which should
 *   be a positive number.
 * @param[in] warp_scheduler
 *   Input. Pointer to the host memory that holds information about the warp scheduler in one cluster.
 *   The value can only be 1, 2 or 4.
 * @param[in] split_k_num
 *   Input. Pointer to the host memory that holds information about the split number of \p K, which should
 *   be a positive number and small than or equal to the cluster number of the device.
 * @param[in,out] algo
 *   Input/output. The algorithm of the matrix multiplication extension operation. For detailed information,
 *   see ::cnnlMatMulExAlgo_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the matrix multiplication operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par API Dependency
 * - This function is only used for auto-tuning mode in ::cnnlMatMulEx_v2.
 * - Before calling this function to set algorithm \p algo and workspace size \p workspace_size, you need to prepare
 *   all the input parameters that need to be passed to this function. See each parameter description for details.
 * - After calling this function, you need to call ::cnnlMatMulEx_v2 to execute the matrix multiplication
 *   extension operation.
 * - Each set of input parameters corresponds to a unique \p algo and \p workspace_size.
 *   If input parameters \p matmulex_desc, \p a_desc, \p b_desc, \p c_desc, \p d_desc, \p block_m, \p block_n, \p block_k,
 *   \p warp_scheduler or \p split_k_num are changed, the function ::cnnlSetMatMulExAlgo need to be re-called.
 *
 * @par Data Type
 * - The supported combinations of data types are as follows in the order of
 *   \p a data type - \p a onchip data type - \p b data type - \p b onchip data type - \p d onchip data type - \p d data type:
 *   - float - float - float - float - float - float
 *   - half - half - half - half - float - half
 *   - bfloat16 - bfloat16 - bfloat16 - bfloat16 - float - bfloat16
 *   - int8 - int8 - int8 - int8 - float - float
 *   - int8 - int8 - int8 - int8 - float - half
 * - If the onchip data type and data type are consistent, the onchip data type can be defaulted.
 *
 * @par Performance Optimization
 * - For better practices, to have a better performance, the input data of \p a and \p b is suggested to be filled with zeros, where
 *   we can get much more accurate tiles in ::cnnlMatMulEx_v2.
 *
 * @note
 * - The function supports fused operation after matrix multiplication. The fused information should be set with
 *   ::cnnlMatMulExDescriptor_t.
 * - When ::CNNL_MATMUL_EX_DESC_EPILOGUE_TYPE is ::CNNL_MATMUL_EPI_BIAS_SCALE_BN_ACTIVATION, only bias with
 *   shape of [\p 1, \p N] and ::CNNL_ACTIVATION_IDENTITY activation mode are supported. Other fused operations,
 *   such as \p BatchNorm and other activation modes are not supported.
 * - When \p d is set to NULL, \p c is output. Use \p c as input and \p d as output. The tensor
 *   descriptors of \p c and \p d are consistent.
 * - The leading dimensions of tensors \p a, \p b, \p c and \p d are supported. Tensors \p a and \p b's
 *   leading dimensions are lda and ldb respectively, and tensors \p c and \p d are sharing the same leading dimension ldc.
 *   You can specify the values of lda, ldb and ldc by setting ::CNNL_MATMUL_EX_DESC_LDA, ::CNNL_MATMUL_EX_DESC_LDB and
 *   ::CNNL_MATMUL_EX_DESC_LDC attributes of \p matmul_desc with ::cnnlSetMatMulExDescAttr separately.
 * - The leading dimension of each tensor is not necessary for matrix multiplication on MLU devices. If you want to use
 *   the leading dimension feature, the strides of the tensors should be set with ::cnnlSetTensorDescriptorEx, where the
 *   lowest dimension of the strides should be 1, and the highest dimension of the strides should be equal to the leading
 *   dimension of the tensors, which should be equal to or larger than the lowest dimension of the tensors. Specially, when
 *   the dimension of each tensor is 1, the corresponding strides will not be checked.
 * - When \p beta value is 0, \p c can be NULL. When \p c is not NULL, the descriptor values of \p c_desc and \p d_desc
 *   should be the same.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetMatMulExAlgo(cnnlHandle_t handle,
                                              const cnnlMatMulExDescriptor_t matmulex_desc,
                                              const cnnlTensorDescriptor_t a_desc,
                                              const cnnlTensorDescriptor_t b_desc,
                                              const cnnlTensorDescriptor_t c_desc,
                                              const cnnlTensorDescriptor_t d_desc,
                                              const int32_t block_m,
                                              const int32_t block_n,
                                              const int32_t block_k,
                                              const int32_t warp_scheduler,
                                              const int32_t split_k_num,
                                              cnnlMatMulExAlgo_t algo,
                                              size_t *workspace_size);
// Group:MatMulEx
/*!
 * @brief Retrieves an algorithm \p algo that was previously created with the
 * ::cnnlCreateMatMulExAlgo function for auto-tuning mode in ::cnnlMatMulEx_v2, and gets the
 * information about the tile size, warp scheduler, and split k number of the algorithm.
 *
 * @param[in] algo
 *   Input. The algorithm of the matrix multiplication extension operation. For detailed information,
 *   see ::cnnlMatMulExAlgo_t.
 * @param[out] block_m
 *   Output. Pointer to the host memory that holds information about the block size of \p M.
 * @param[out] block_n
 *   Output. Pointer to the host memory that holds information about the block size of \p N.
 * @param[out] block_k
 *   Output. Pointer to the host memory that holds information about the block size of \p K.
 * @param[out] warp_scheduler
 *   Output. Pointer to the host memory that holds information about the warp scheduler in one cluster.
 *   The value can only be 1, 2 or 4.
 * @param[out] split_k_num
 *   Output. Pointer to the host memory that holds information about the split number of \p K.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function is only used for auto-tuning mode in ::cnnlMatMulEx_v2.
 * - Before calling this function, you should call ::cnnlGetMatMulExAlgo to set algorithm information first.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetMatMulExAlgo(cnnlMatMulExAlgo_t algo,
                                              int *block_m,
                                              int *block_n,
                                              int *block_k,
                                              int *warp_scheduler,
                                              int *split_k_num);

// Group:MatMulEx
/*!
 * @brief Retrieves the IDs of all the matrix multiply extension algorithms that are valid,
 *        for given tensors described by \p a_desc, \p b_desc, \p c_desc and \p d_desc.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] matmulex_desc
 *   Input. The descriptor of the matrix multiplication extension operations. For detail information,
 *   see ::cnnlMatMulExDescriptor_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of the left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of the right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] d_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] requestedAlgoCount
 *   Input. The number of requested algorithms, which is also the maximum number of algorithms to be returned.
 *   The value must be greater than or equal to 0. Currently it supports only 1.
 * @param[out] algoIdsArray
 *   Output. An array containing the algorithm IDs returned by this function.
 * @param[out] returnAlgoCount
 *   Output. A host pointer to the number of algorithms returned by this function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - The length of \p algoIdsArray must be greater than \p requestedAlgoCount.
 * - \p c_desc and \p d_desc cannot be NULL at the same time.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetMatMulExAlgoIds(cnnlHandle_t handle,
                                                 cnnlMatMulExDescriptor_t matmulex_desc,
                                                 cnnlTensorDescriptor_t a_desc,
                                                 cnnlTensorDescriptor_t b_desc,
                                                 cnnlTensorDescriptor_t c_desc,
                                                 cnnlTensorDescriptor_t d_desc,
                                                 int requestedAlgoCount,
                                                 int algoIdsArray[],
                                                 int *returnAlgoCount);

// Group:MatMulEx
/*!
 * @brief Initializes the matrix multiply algorithm struct for a specified algorithm ID.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlInitialMatMulExAlgo instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] matmul_desc
 *   Input. A host pointer to the struct of matrix multiplication extension descriptor.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of the left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of the right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] d_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] prefer_algo
 *   Input. The descriptor of the matrix multiplication that holds the preferences for ::cnnlMatMulExHeuristicResult_t
 *   configuration. Currently it is not supported and should be set to NULL.
 * @param[in] algoId
 *   Input. The algorithm ID. You can traverse the value from 32 to 41 until it returns ::CNNL_STATUS_SUCCESS.
 * @param[out] inference_algo
 *   Output. The matrix multiplication algorithm to be initialized.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the matrix multiplication operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - \p algoId should be greater than or equal to 32, and less than 41. It is not supported on 1V.
 * - \p c_desc and \p d_desc cannot be NULL at the same time.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlInitialMatMulExAlgo)
cnnlStatus_t CNNL_WIN_API cnnlMatMulExAlgoInitial(cnnlHandle_t handle,
                                                  const cnnlMatMulExDescriptor_t matmul_desc,
                                                  const cnnlTensorDescriptor_t a_desc,
                                                  const cnnlTensorDescriptor_t b_desc,
                                                  const cnnlTensorDescriptor_t c_desc,
                                                  const cnnlTensorDescriptor_t d_desc,
                                                  const cnnlMatMulAlgoInitPrefer_t prefer_algo,
                                                  int algoId,
                                                  cnnlMatMulExAlgo_t inference_algo,
                                                  size_t *workspace_size);

// Group:MatMulEx
/*!
 * @brief Initializes the matrix multiply algorithm struct for a specified algorithm ID,
 * which can be run by the ::cnnlMatMulEx function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] matmulex_desc
 *   Input. A host pointer to the struct of matrix multiplication extension descriptor.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of the left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of the right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] d_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] prefer
 *   Input. The descriptor of the matrix multiplication that holds the preferences for ::cnnlMatMulExHeuristicResult_t
 *   configuration. Currently it is not supported and should be set to NULL.
 * @param[in] algoId
 *   Input. The algorithm ID. You can traverse the value from 32 to 41 until it returns ::CNNL_STATUS_SUCCESS.
 * @param[out] algo
 *   Output. The matrix multiplication algorithm to be initialized.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the matrix multiplication operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - \p algoId should be greater than or equal to 32, and less than 42. It is not supported on 1V.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlInitialMatMulExAlgo(cnnlHandle_t handle,
                                                  const cnnlMatMulExDescriptor_t matmulex_desc,
                                                  const cnnlTensorDescriptor_t a_desc,
                                                  const cnnlTensorDescriptor_t b_desc,
                                                  const cnnlTensorDescriptor_t c_desc,
                                                  const cnnlTensorDescriptor_t d_desc,
                                                  const cnnlMatMulAlgoInitPrefer_t prefer,
                                                  int algoId,
                                                  cnnlMatMulExAlgo_t algo,
                                                  size_t *workspace_size);

// Group:MatMulEx
/*!
 * @brief Creates a descriptor pointed by \p matmulex_desc for a matrix multiplication extension operation,
 *        and allocates memory for holding the information about the operation. The information is defined in
 *        ::cnnlMatMulExDescriptor_t.
 *
 * @param[out] matmulex_desc
 *   Output. A host pointer to the struct of matrix multiplication extension operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetMatMulExDescAttr function to initialize
 *   and set the attribute information to the matrix multiplication extension operation.
 * - You need to call the ::cnnlDestroyMatMulExDescriptor function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateMatMulExDescriptor(cnnlMatMulExDescriptor_t *matmulex_desc);

// Group:MatMulEx
/*!
 * @brief Sets the matrix multiplication extension descriptor \p matmulex_desc that was previously created with
 *        ::cnnlCreateMatMulExDescriptor. The descriptor information includes basic attributes
 *        ::CNNL_MATMUL_DESC_TRANSA, ::CNNL_MATMUL_EX_DESC_TRANSB, ::CNNL_MATMUL_EX_USE_BETA and
 *        ::CNNL_MATMUL_EX_DESC_EPILOGUE_TYPE defined in ::cnnlMatMulExDescAttribute_t.
 *
 * @param[in,out] matmulex_desc
 *   Input/output. The descriptor of the matrix multiplication extension. For detailed
 *   information, see ::cnnlMatMulExDescriptor_t.
 * @param[in] trans_a
 *   Input. Value indicating whether \p a matrix is transposed. For detailed
 *   information, see ::cnnlMatrixOperation_t.
 * @param[in] trans_b
 *   Input. Value indicating whether \p b matrix is transposed. For detailed
 *   information, see ::cnnlMatrixOperation_t.
 * @param[in] use_beta
 *   Input. Boolean value indicating whether to use \p beta on \p c matrix.
 * @param[in] epilogue_type
 *   Input. Specifies the fused operation after matrix multiplication.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - To set attributes other than the basic attributes, call ::cnnlSetMatMulExDescAttr.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetMatMulExDescAttrBase(cnnlMatMulExDescriptor_t matmulex_desc,
                                                      cnnlMatrixOperation_t trans_a,
                                                      cnnlMatrixOperation_t trans_b,
                                                      bool use_beta,
                                                      cnnlMatMulEpilogueType_t epilogue_type);

// Group:MatMulEx
/*!
 * @brief Sets the matrix multiplication extension descriptor \p matmulex_desc that was previously created with
 *        ::cnnlCreateMatMulExDescriptor. The information includes the attribute \p attr defined in
 *        ::cnnlMatMulExDescAttribute_t, the host pointer \p buf to the attribute
 *        value, and the size of buffer for verification.
 *
 * @param[in,out] matmulex_desc
 *   Input/output. The descriptor of the matrix multiplication extension. For detailed
 *   information, see ::cnnlMatMulExDescriptor_t.
 * @param[in] attr
 *   Input. Attribute of matrix multiplication extension descriptor to be set. For detailed
 *   information, see ::cnnlMatMulExDescAttribute_t.
 * @param[in] buf
 *   Input. A host pointer to the attribute value set by this function.
 * @param[in] size_in_bytes
 *   Input. Buffer in bytes for verification.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - You should set ::CNNL_MATMUL_EX_DESC_EPILOGUE_TYPE attribute before set ::CNNL_MATMUL_EX_DESC_EPILOGUE_OPERAND
 *   attribute.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetMatMulExDescAttr(cnnlMatMulExDescriptor_t matmulex_desc,
                                                  cnnlMatMulExDescAttribute_t attr,
                                                  const void *buf,
                                                  size_t size_in_bytes);

// Group:MatMulEx
/*!
 * @brief Destroys a matrix multiplication extension operation with descriptor \p matmulex_desc,
 *        which was previously created with ::cnnlCreateMatMulExDescriptor.
 *
 * @param[in] matmulex_desc
 *   Input. The matrix multiplication extension operation descriptor to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyMatMulExDescriptor(cnnlMatMulExDescriptor_t matmulex_desc);

// Group:MatMulEx
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra workspace
 * to optimize the matrix multiplication extension operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   matrix multiplication extension operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] matmulex_desc
 *   Input. The descriptor of the matrix multiplication extension operation. For detail information,
 *   see ::cnnlMatMulExDescriptor_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of the left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of the right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] d_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] algo
 *   Input. The suitable algorithm to compute the matrix multiplication extension operation.
 *   The value of this parameter can be NULL.
 * @param[out] workspace_size
 *   Output. Pointer to the MLU memory that stores the returned size of the extra workspace in bytes.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - This function is an alternative to ::cnnlGetMatMulExAlgoHeuristic to get the workspace size.
 *   In comparison, ::cnnlGetMatMulExWorkspaceSize will get the maximum workspace size and ::cnnlGetMatMulExAlgoHeuristic
 *   will get a relatively suitable workspace size based on the retrieved algorithm.
 * - \p c_desc and \p d_desc cannot be NULL at the same time.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetMatMulExWorkspaceSize(cnnlHandle_t handle,
                             const cnnlMatMulExDescriptor_t matmulex_desc,
                             const cnnlTensorDescriptor_t a_desc,
                             const cnnlTensorDescriptor_t b_desc,
                             const cnnlTensorDescriptor_t c_desc,
                             const cnnlTensorDescriptor_t d_desc,
                             const cnnlMatMulExAlgo_t algo,
                             size_t *workspace_size);

// Group:MatMulEx
/*!
 * @brief Computes the matrix multiplication extension operation. It supports quantization operations
 * of input and output matrices. It also supports different fused operations after matrix multiplication.
 *
 *  The matrix multiplication is computed based on the matrix multiplication algorithm set in \p algo.
 *  You can call ::cnnlGetMatMulExAlgoHeuristic to get the most suited algorithm.
 *
 *  This function needs extra MLU memory as the workspace to improve the performance. You can get
 *  the workspace size with ::cnnlGetMatMulExHeuristicResult.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlMatMulEx_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   matrix multiplication extension operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] matmulex_desc
 *   Input. The descriptor of the matrix multiplication extension operations. For detail information,
 *   see ::cnnlMatMulExDescriptor_t.
 * @param[in] alpha
 *   Input. A host pointer to the scaling factor of tensor \p a. The default value is 1.0.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of the left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. Pointer to the MLU memory that stores the input tensor of the left matrix.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of the right matrix. For detailed information,
 * @param[in] b
 *   Input. Pointer to the MLU memory that stores the input tensor of the right matrix.
 * @param[in] beta
 *   Input. A host pointer to the scaling factor of tensor \p c. The default value is 0.0.
 * @param[in] c_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] c
 *   Input. Pointer to the MLU memory that stores the input tensor \p c in out-of-place matrix multiplication
 *   where d = alpha * a * b + beta * c, or pointer to the MLU memory that stores the output tensor \p d in in-place
 *   matrix multiplication where c == d = alpha * a * b + beta * c.
 * @param[in] d_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] d
 *   Output. Pointer to the MLU memory that stores the output \p d.
 * @param[in] algo
 *   Input. A host pointer to the most suitable algorithm to compute the matrix multiplication. Currently not
 *   supported.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the matrix multiplication
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the matrix multiplication
 *   operation. You can get the size of the workspace with ::cnnlGetMatMulExHeuristicResult.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Data Type
 * - The supported combinations of data types are as follows in the order of
 *   \p a data type - \p a onchip data type - \p b data type - \p b onchip data type - \p d onchip data type - \p d data type:
 *   - float - float - float - float - float - float
 *   - half - half - half - half - half - half
 *   - half - half - half - half - float - half
 *   - bfloat16 - bfloat16 - bfloat16 - bfloat16 - float - bfloat16
 *   - int8 - int8 - int8 - int8 - float - float
 *   - int8 - int8 - int8 - int8 - half - half
 *   - int8 - int8 - int8 - int8 - float - half
 *   - float - int8 - int8 - int8 - float - float
 *   - half - int8 - int8 - int8 - half - half
 *   - half - int8 - int8 - int8 - float - half
 *   - int8 - int8 - float - int8 - float - float
 *   - int8 - int8 - half - int8 - half - half
 *   - int8 - int8 - half - int8 - float - half
 *   - float - int8 - float - int8 - float - float
 *   - half - int8 - half - int8 - half - half
 *   - half - int8 - half - int8 - float - half
 *   - int8 - int8 - int8 - int8 - float - int8
 *   - int8 - int8 - int8 - int8 - half - int8
 *   - float - int8 - int8 - int8 - float - int8
 *   - half - int8 - int8 - int8 - half - int8
 *   - half - int8 - int8 - int8 - float - int8
 *   - int8 - int8 - float - int8 - float - int8
 *   - int8 - int8 - half - int8 - half - int8
 *   - int8 - int8 - half - int8 - float - int8
 *   - float - int8 - float - int8 - float - int8
 *   - half - int8 - half - int8 - half - int8
 *   - half - int8 - half - int8 - float - int8
 * - The data type combinations of int16 are the same as those of int8.
 * - If the onchip data type and data type are consistent, the onchip data type can be defaulted.
 * - When the d data type is fixed-point, the data type of \p bias, \p scale_alpha, \p scale_beta and other fusion
 *   parameters should be the same as the \p d onchip data type. Otherwise, it should be the same as the \p d data type.
 * - When ::CNNL_MATMUL_EX_DESC_EPILOGUE_TYPE is ::CNNL_MATMUL_EPI_BIAS_SCALE_BN_ACTIVATION, the following
 *   data type combinations are not supported:
 *   - int8 - int8 - int8 - int8 - float - half
 *   - half - int8 - int8 - int8 - float - half
 *   - int8 - int8 - half - int8 - float - half
 *   - half - int8 - half - int8 - float - half
 *   - half - int8 - int8 - int8 - float - int8
 *   - half - int8 - half - int8 - float - int8
 *
 * @note
 * - When ::CNNL_MATMUL_EX_DESC_EPILOGUE_TYPE is ::CNNL_MATMUL_EPI_BIAS_SCALE_BN_ACTIVATION and there are
 *   scale, activation or other fusions, the shape of \p bias only supports [1, N].
 * - The function supports fused operation after matrix multiplication. The fused information should be set with
 *   ::cnnlMatMulExDescriptor_t.
 * - When \p d is set to NULL, \p c is output. Use \p c as input and \p d as output. The tensor
 *   descriptors of \p c and \p d are consistent.
 * - When the onchip data type of \p a and \p b is bfloat16, the following limitations apply:
 *   - When ::CNNL_MATMUL_EX_DESC_EPILOGUE_TYPE is ::CNNL_MATMUL_EPI_BIAS_SCALE_BN_ACTIVATION, batch norm and scale
 *     fusions are not supported.
 *   - When ::CNNL_MATMUL_EX_DESC_EPILOGUE_TYPE is ::CNNL_MATMUL_EPI_BIAS, bias fusions except for [1, N] are not
 *     supported, and bias fusion [1, N] with the following data type combinations are not supported:
 *     - \p a onchip data type is fixed-point, \p d onchip data type is float and \p a data type is half
 *     - \p a onchip data type is fixed-point, \p d onchip data type is float and \p d data type is half.
 * - The leading dimensions of tensors \p a, \p b, \p c and \p d are supported. Tensors \p a and \p b's
 *   leading dimensions are lda and ldb respectively, and tensors \p c and \p d are sharing the same leading dimension ldc.
 *   You can specify the values of lda, ldb and ldc by setting ::CNNL_MATMUL_EX_DESC_LDA, ::CNNL_MATMUL_EX_DESC_LDB and
 *   ::CNNL_MATMUL_EX_DESC_LDC attributes of \p matmul_desc with ::cnnlSetMatMulExDescAttr separately.
 * - The leading dimension of each tensor is not necessary for matrix multiplication on MLU devices. If you want to use
 *   the leading dimension feature, the strides of the tensors should be set with ::cnnlSetTensorDescriptorEx, where the
 *   lowest dimension of the strides should be 1, and the highest dimension of the strides should be equal to the leading
 *   dimension of the tensors, which should be equal to or larger than the lowest dimension of the tensors. Specially, when
 *   the dimension of tensor is 1, the corresponding strides will not be checked.
 * - When \p beta value is not 0, both \p c and \p d cannot be NULL. When \p beta value is 0, at most one of \p c and \p d
 *   can be NULL. When both \p c and \p d are not NULL, the descriptor values of \p c_desc and \p d_desc should be the same.
 *
 * @par Scale Limitation
 * - \p a, \p b, \p c and \p d must be two dimensions.
 * - The number of \p a matrix's columns must be equal to the number of \p b matrix's rows after both inputs
 *   perform transpose operations according to parameters.
 * - The maximum dimension of \p a should be less than or equal to INT_MAX.
 * - The maximum dimension of \p b should be less than or equal to INT_MAX.
 * - The maximum dimension of \p d should be less than or equal to INT_MAX.
 *
 * @par API Dependency
 * - Before calling this function to implement matrix multiplication extension operation, you need to prepare
 *   all the parameters passed to this function. See each parameter description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, matrix \p a should not be transposed and matrix \p b
 *   should be transposed.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      CNNL_MATMUL_DESC_TRANSA:      false
      CNNL_MATMUL_DESC_TRANSB:      false
      Dimension of input tensor a:  [99, 128]
      Dimension of input tensor b:  [128, 256]
      Dimension of output tensor d: [99, 256]
      Dimension of bias tensor: [1, 256]
     @endverbatim
 */
CNNL_DEPRECATED_FOR(cnnlMatMulEx_v2)
cnnlStatus_t CNNL_WIN_API cnnlMatMulEx(cnnlHandle_t handle,
                                       cnnlMatMulExDescriptor_t matmulex_desc,
                                       const void *alpha,
                                       const cnnlTensorDescriptor_t a_desc,
                                       const void *a,
                                       const cnnlTensorDescriptor_t b_desc,
                                       const void *b,
                                       const void *beta,
                                       const cnnlTensorDescriptor_t c_desc,
                                       void *c,
                                       const cnnlTensorDescriptor_t d_desc,
                                       void *d,
                                       cnnlMatMulExAlgo_t algo,
                                       void *workspace,
                                       size_t workspace_size);
// Group:MatMulEx
/*!
 * @brief Computes the matrix multiplication extension operation. It supports quantization operations
 *        of input and output matrices. It also supports different fused operations after matrix multiplication.
 *
 *  Compared with ::cnnlMatMulEx, it supports a more standardized order of interface parameters and clarifies
 *  the output tensor.
 *
 *  The matrix multiplication is computed based on the matrix multiplication algorithm set in \p algo.
 *  You can call ::cnnlGetMatMulExAlgoHeuristic to get the most suited algorithm.
 *
 *  This function needs extra MLU memory as the workspace to improve the performance. You can get
 *  the workspace size with ::cnnlGetMatMulExHeuristicResult.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   matrix multiplication extension operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] matmulex_desc
 *   Input. The descriptor of the matrix multiplication extension operations. For detail information,
 *   see ::cnnlMatMulExDescriptor_t.
 * @param[in] algo
 *   Input. A host pointer to the most suitable algorithm to compute the matrix multiplication. Currently not
 *   supported.
 * @param[in] alpha
 *   Input. A host pointer to the scaling factor of tensor \p a. The default value is 1.0.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of the left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. Pointer to the MLU memory that stores the input tensor of the left matrix.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of the right matrix. For detailed information,
 * @param[in] b
 *   Input. Pointer to the MLU memory that stores the input tensor of the right matrix.
 * @param[in] beta
 *   Input. A host pointer to the scaling factor of tensor \p c. The default value is 0.0.
 * @param[in] c_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] c
 *   Input. Pointer to the MLU memory that stores the input tensor \p c in out-of-place matrix multiplication
 *   where d = alpha * a * b + beta * c, or pointer to the MLU memory that stores the output tensor \p d in in-place
 *   matrix multiplication where c == d = alpha * a * b + beta * c.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the matrix multiplication
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the matrix multiplication
 *   operation. You can get the size of the workspace with ::cnnlGetMatMulExHeuristicResult.
 * @param[in] d_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] d
 *   Output. Pointer to the MLU memory that stores the output \p d.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Data Type
 * - The supported combinations of data types are as follows in the order of
 *   \p a data type - \p a onchip data type - \p b data type - \p b onchip data type - \p d onchip data type - \p d data type:
 *   - float - float - float - float - float - float
 *   - half - half - half - half - half - half
 *   - half - half - half - half - float - half
 *   - bfloat16 - bfloat16 - bfloat16 - bfloat16 - float - bfloat16
 *   - float8_e4m3fn - float8_e4m3fn - float8_e4m3fn - float8_e4m3fn - float - float
 *   - float8_e4m3fn - float8_e4m3fn - float8_e4m3fn - float8_e4m3fn - float - half
 *   - float8_e4m3fn - float8_e4m3fn - float8_e4m3fn - float8_e4m3fn - float - bfloat16
 *   - float8_e5m2 - float8_e5m2 - float8_e5m2 - float8_e5m2 - float - float
 *   - float8_e5m2 - float8_e5m2 - float8_e5m2 - float8_e5m2 - float - half
 *   - float8_e5m2 - float8_e5m2 - float8_e5m2 - float8_e5m2 - float - bfloat16
 *   - float8_e4m3fn - float8_e4m3fn - float8_e5m2 - float8_e5m2 - float - float
 *   - float8_e4m3fn - float8_e4m3fn - float8_e5m2 - float8_e5m2 - float - half
 *   - float8_e4m3fn - float8_e4m3fn - float8_e5m2 - float8_e5m2 - float - bfloat16
 *   - float8_e5m2 - float8_e5m2 - float8_e4m3fn - float8_e4m3fn - float - float
 *   - float8_e5m2 - float8_e5m2 - float8_e4m3fn - float8_e4m3fn - float - half
 *   - float8_e5m2 - float8_e5m2 - float8_e4m3fn - float8_e4m3fn - float - bfloat16
 *   - int8 - int8 - int8 - int8 - float - float
 *   - int8 - int8 - int8 - int8 - half - half
 *   - int8 - int8 - int8 - int8 - float - half
 *   - float - int8 - int8 - int8 - float - float
 *   - half - int8 - int8 - int8 - half - half
 *   - half - int8 - int8 - int8 - float - half
 *   - int8 - int8 - float - int8 - float - float
 *   - int8 - int8 - half - int8 - half - half
 *   - int8 - int8 - half - int8 - float - half
 *   - float - int8 - float - int8 - float - float
 *   - half - int8 - half - int8 - half - half
 *   - half - int8 - half - int8 - float - half
 *   - int8 - int8 - int8 - int8 - float - int8
 *   - int8 - int8 - int8 - int8 - half - int8
 *   - float - int8 - int8 - int8 - float - int8
 *   - half - int8 - int8 - int8 - half - int8
 *   - half - int8 - int8 - int8 - float - int8
 *   - int8 - int8 - float - int8 - float - int8
 *   - int8 - int8 - half - int8 - half - int8
 *   - int8 - int8 - half - int8 - float - int8
 *   - float - int8 - float - int8 - float - int8
 *   - half - int8 - half - int8 - half - int8
 *   - half - int8 - half - int8 - float - int8
 * - The following data type combinations are deprecated:
 *   - \p a data type is half and \p a onchip data type is int16.
 *   - \p b data type is half and \p b onchip data type is int16.
 *   - \p d onchip data type is half and \p d data type is int16.
 *
 *   The rest data type combinations of int16 are the same as those of int8.
 * - If the onchip data type and data type are consistent, the onchip data type can be defaulted.
 * - When the d data type is fixed-point, the data type of \p bias, \p scale_alpha, \p scale_beta and other fusion
 *   parameters should be the same as the \p d onchip data type. Otherwise, it should be the same as the \p d data type.
 * - When ::CNNL_MATMUL_EX_DESC_EPILOGUE_TYPE is ::CNNL_MATMUL_EPI_BIAS_SCALE_BN_ACTIVATION, the following
 *   data type combinations are not supported:
 *   - int8 - int8 - int8 - int8 - float - half
 *   - half - int8 - int8 - int8 - float - half
 *   - int8 - int8 - half - int8 - float - half
 *   - half - int8 - half - int8 - float - half
 *   - half - int8 - int8 - int8 - float - int8
 *   - half - int8 - half - int8 - float - int8
 *
 * @note
 * - When ::CNNL_MATMUL_EX_DESC_EPILOGUE_TYPE is ::CNNL_MATMUL_EPI_BIAS_SCALE_BN_ACTIVATION and there are
 *   scale, activation or other fusions, the shape of \p bias only supports [1, N].
 * - The function supports fused operation after matrix multiplication. The fused information should be set with
 *   ::cnnlMatMulExDescriptor_t.
 * - When \p d is set to NULL, \p c is output. Use \p c as input and \p d as output. The tensor
 *   descriptors of \p c and \p d are consistent.
 * - When the onchip data type of \p a and \p b is bfloat16, the following limitations apply:
 *   - When ::CNNL_MATMUL_EX_DESC_EPILOGUE_TYPE is ::CNNL_MATMUL_EPI_BIAS_SCALE_BN_ACTIVATION, batch norm and scale
 *     fusions are not supported.
 *   - When ::CNNL_MATMUL_EX_DESC_EPILOGUE_TYPE is ::CNNL_MATMUL_EPI_BIAS, bias fusions except for [1, N] are not
 *     supported, and bias fusion [1, N] with the following data type combinations are not supported:
 *     - \p a onchip data type is fixed-point, \p d onchip data type is float and \p a data type is half
 *     - \p a onchip data type is fixed-point, \p d onchip data type is float and \p d data type is half.
 * - The leading dimensions of tensors \p a, \p b, \p c and \p d are supported. Tensors \p a and \p b's
 *   leading dimensions are lda and ldb respectively, and tensors \p c and \p d are sharing the same leading dimension ldc.
 *   You can specify the values of lda, ldb and ldc by setting ::CNNL_MATMUL_EX_DESC_LDA, ::CNNL_MATMUL_EX_DESC_LDB and
 *   ::CNNL_MATMUL_EX_DESC_LDC attributes of \p matmul_desc with ::cnnlSetMatMulExDescAttr separately.
 * - The leading dimension of each tensor is not necessary for matrix multiplication on MLU devices. If you want to use
 *   the leading dimension feature, the strides of the tensors should be set with ::cnnlSetTensorDescriptorEx, where the
 *   lowest dimension of the strides should be 1, and the highest dimension of the strides should be equal to the leading
 *   dimension of the tensors, which should be equal to or larger than the lowest dimension of the tensors. Specially, when
 *   the dimension of each tensor is 1, the corresponding strides will not be checked.
 * - When \p beta value is 0, \p c can be NULL. When \p c is not NULL, the descriptor values of \p c_desc and \p d_desc
 *   should be the same.
 *
 * @par Scale Limitation
 * - \p a, \p b, \p c and \p d must be two dimensions.
 * - The number of \p a matrix's columns must be equal to the number of \p b matrix's rows after both inputs
 *   perform transpose operations according to parameters.
 * - The maximum dimension of \p a should be less than or equal to INT_MAX.
 * - The maximum dimension of \p b should be less than or equal to INT_MAX.
 * - The maximum dimension of \p d should be less than or equal to INT_MAX.
 *
 * @par API Dependency
 * - Before calling this function to implement matrix multiplication extension operation, you need to prepare
 *   all the parameters passed to this function. See each parameter description for details.
 * - When using the auto-tuning mode, you should call ::cnnlSetMatMulExAlgo to set algorithm tile \p block_m, \p block_n,
 *   \p block_k, \p warp_scheduler and \p split_k_num first.
 *
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, matrix \p a should not be transposed and matrix \p b
 *   should be transposed.
 * - During the searching processes in auto-tuning mode, the input data of \p a and \p b is suggested to be filled with zeros, where
 *   we can get much more accurate tiles.
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      CNNL_MATMUL_DESC_TRANSA:      false
      CNNL_MATMUL_DESC_TRANSB:      false
      Dimension of input tensor a:  [99, 128]
      Dimension of input tensor b:  [128, 256]
      Dimension of output tensor d: [99, 256]
      Dimension of bias tensor: [1, 256]
     @endverbatim
 */
cnnlStatus_t CNNL_WIN_API cnnlMatMulEx_v2(cnnlHandle_t handle,
                                          cnnlMatMulExDescriptor_t matmulex_desc,
                                          cnnlMatMulExAlgo_t algo,
                                          const void *alpha,
                                          const cnnlTensorDescriptor_t a_desc,
                                          const void *a,
                                          const cnnlTensorDescriptor_t b_desc,
                                          const void *b,
                                          const void *beta,
                                          const cnnlTensorDescriptor_t c_desc,
                                          void *c,
                                          void *workspace,
                                          size_t workspace_size,
                                          const cnnlTensorDescriptor_t d_desc,
                                          void *d);

// Group:BatchMatMul
/*!
 * @brief Destroys a batch matrix multiplication descriptor \p bmm_desc
 *        that was previously created with ::cnnlBatchMatMulDescCreate.
 *
 * The batch matrix multiplication descriptor is defined in ::cnnlBatchMatMulDescriptor_t
 * and holds the information about the batch matrix multiplication operation.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] bmm_desc
 *   Input. The batch matrix multiplication descriptor to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlBatchMatMulDescDestroy(cnnlBatchMatMulDescriptor_t bmm_desc);

// Group:BatchMatMul
/*!
 * @brief Initializes the batch matrix multiplication descriptor \p bmm_desc
 * that was previously created with the ::cnnlBatchMatMulDescCreate function, and sets
 * the information about the batch matrix multiplication operation to the batch matrix multiplication
 * descriptor \p bmm_desc. The information includes the attribute \p attr defined in
 * ::cnnlBatchMatMulDescAttribute_t, the host pointer \p buf to the attribute value, and
 * the size of buffer for verification.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in,out] bmm_desc
 *   Input/output. The descriptor of the batch matrix multiplication operation. For detailed
 *   information, see ::cnnlBatchMatMulDescriptor_t.
 * @param[in] attr
 *   Input. Attribute of batch matrix multiplication descriptor to be set. For detailed
 *   information, see ::cnnlBatchMatMulDescAttribute_t.
 * @param[in] buf
 *   Input. A host pointer to the attribute value set by this function.
 * @param[in] size_in_bytes
 *   Input. Buffer in bytes for verification.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlSetBatchMatMulDescAttr(cnnlBatchMatMulDescriptor_t bmm_desc,
                                                     cnnlBatchMatMulDescAttribute_t attr,
                                                     const void *buf,
                                                     size_t size_in_bytes);

// Group:BatchMatMul
/*!
 * @brief Returns the pointer to the \p buf and size of the buffer \p size_written of the attribute
 * retrieved with the given batch matmul multiplication descriptor \p matmul_desc, attribute \p attr.
 *
 * You can set the attribute in the batch matrix multiplication descriptor based on the return value
 * of this function.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] bmm_desc
 *   Input. The descriptor of the batch matrix multiplication operation. For detailed
 *   information, see ::cnnlBatchMatMulDescriptor_t.
 * @param[in] attr
 *   Input. Attribute of batch matrix multiplication descriptor to be retrieved.
 * @param[out] buf
 *   Output. A host pointer to the attribute value to be retrieved by this function.
 * @param[in] size_in_bytes
 *   Input. Buffer in bytes for verification, which is used to check whether the memory size is the same as \p size_written.
 * @param[out] size_written
 *   Output. The number of bytes actually written to the buffer.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlGetBatchMatMulDescAttr(const cnnlBatchMatMulDescriptor_t bmm_desc,
                                                     const cnnlBatchMatMulDescAttribute_t attr,
                                                     void *buf,
                                                     size_t size_in_bytes,
                                                     size_t *size_written);

// Group:BatchMatMul
/*!
 * @brief Creates a descriptor pointed by \p algo for a batch matrix multiplication algorithm,
 *        and allocates memory for holding the information about the algorithm.
 *        The information is defined in ::cnnlBatchMatMulAlgo_t.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[out] algo
 *   Output. A host pointer to the batch matrix multiplication algorithm that holds information about
 *   the batch matrix multiplication algorithm.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlGetQuantizeBatchMatMulAlgorithm function to initialize
 *   and set the information to the batch matrix multiplication algorithm.
 * - You need to call the ::cnnlBatchMatMulAlgoDestroy function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlBatchMatMulAlgoCreate(cnnlBatchMatMulAlgo_t *algo);

// Group:BatchMatMul
/*!
 * @brief Destroys a batch matrix multiplication algorithm descriptor \p algo
 *        that was previously created with ::cnnlBatchMatMulAlgoCreate.
 *
 * The batch matrix multiplication descriptor is defined in ::cnnlBatchMatMulAlgo_t
 * and holds the information about the batch matrix multiplication algorithm.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] algo
 *   Input. The batch matrix multiplication algorithm descriptor to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlBatchMatMulAlgoDestroy(cnnlBatchMatMulAlgo_t algo);

// Group:BatchMatMul
/*!
 * @brief Returns the most suited batch matrix multiplication algorithm that can be used
 * in the operation.
 *
 * The returned batch matrix multiplication is chosen from all the supported batch matrix
 * algorithms defined in ::cnnlBatchMatMulAlgo_t and is based on the given batch matrix
 * multiplication descriptor \p bmm_desc, tensor descriptor of left matrix \p a_desc, tensor
 * descriptor of right matrix \p b_desc, tensor descriptor of output matrix \p c_desc, and batch
 * matrix multiplication algorithm \p preference.
 *
 * The computing performance options \p preference is defined in the ::cnnlBatchMatMulPreference_t
 * enum, and only supports the high speed mode.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the batch
 *   matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] bmm_desc
 *  Input. The descriptor of the batch matrix multiplication operation. For detailed
 *  information, see ::cnnlBatchMatMulDescriptor_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor descriptor of the left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor descriptor of the right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the input tensor descriptor of output matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] preference
 *   Input. The options for implementing the batch matrix multiplication operation to
 * get better performance. This parameter only supports CNNL_BMM_FASTEST now.
 * @param[out] algo
 *   Output. A host pointer to the returned algorithm that is best suited for computing the batch matrix
 *   multiplication. The algorithms are defined in the ::cnnlBatchMatMulAlgo_t enum.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlGetQuantizeBatchMatMulAlgorithm(cnnlHandle_t handle,
                                    const cnnlBatchMatMulDescriptor_t bmm_desc,
                                    const cnnlTensorDescriptor_t a_desc,
                                    const cnnlTensorDescriptor_t b_desc,
                                    const cnnlTensorDescriptor_t c_desc,
                                    cnnlBatchMatMulPreference_t preference,
                                    cnnlBatchMatMulAlgo_t *algo);

// Group:BatchMatMul
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra workspace
 * to optimize the batch matrix multiplication operation.
 *
 * The size of the extra workspace is based on the given information of the batch matrix multiplication
 * operation, including the batch matrix multiplication descriptor \p bmm_desc, input tensor
 * descriptor of left matrix \p a_desc, input tensor descriptor of right matrix \p b_desc, output
 * tensor descriptor \p c_desc, and the batch matrix multiplication algorithm \p algo. For more
 * information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the batch
 *   matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] bmm_desc
 *   Input. The descriptor of the batch matrix multiplication operations. For detail information,
 *   see ::cnnlBatchMatMulDescriptor_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of the left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of the right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the input tensor of output matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] algo
 *   Input. The algorithm used to compute the batch matrix multiplication. The algorithms are defined
 *   in the ::cnnlBatchMatMulAlgo_t enum. You can get the best suited algorithm with the
 *   ::cnnlGetQuantizeBatchMatMulAlgorithm function.
 * @param[out] workspace_size
 *   Output. Pointer to the MLU memory that stores the returned size of the extra workspace in bytes.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called after ::cnnlGetQuantizeBatchMatMulAlgorithm function. You also need to
 *   call the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions to create and set
 *   tensor descriptors \p a_desc, \p b_desc, \p c_desc before calling this function.
 * - The allocated extra workspace should be passed to the ::cnnlQuantizeBatchMatMul function to
 *   performs the batch matrix multiplication operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlGetQuantizeBatchMatMulWorkspaceSize(cnnlHandle_t handle,
                                        const cnnlBatchMatMulDescriptor_t bmm_desc,
                                        const cnnlTensorDescriptor_t a_desc,
                                        const cnnlTensorDescriptor_t b_desc,
                                        const cnnlTensorDescriptor_t c_desc,
                                        cnnlBatchMatMulAlgo_t algo,
                                        size_t *workspace_size);

// Group:BatchMatMul
 /*!
 * @brief Quantizes data type of input tensor \p a and \p b, and computes the batch matrices
 * multiplication operation, then returns the results in the output tensor \p c.
 * For more information about quantization, see "Cambricon CNNL User Guide".
 *
 * This function needs extra MLU memory as the workspace to improve the batch matrix multiplication
 * performance. You can get the workspace size with the
 * ::cnnlGetQuantizeBatchMatMulWorkspaceSize function. The batch matrix multiplication is computed based
 * on the batch matrix multiplication algorithm set in \p algo. You can call the
 * ::cnnlGetQuantizeBatchMatMulAlgorithm function to get the most suited algorithm.
 *
 * The scaling factors \p alpha and \p beta are not supported currently.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlBatchMatMulEx instead.
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the batch
 *   matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] bmm_desc
 *   Input. The descriptor of the batch matrix multiplication operations. For detail information,
 *   see ::cnnlBatchMatMulDescriptor_t.
 * @param[in] alpha, beta
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of the left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] a_position
 *   Input. Pointer to the MLU memory associated tensor \p a quantization parameter \p position.
 * @param[in] a_scale
 *   Input. Pointer to the MLU memory associated tensor \p a quantization parameter \p scale.
 *   The value of this parameter can be NULL.
 * @param[in] a_offset
 *   Input. Pointer to the MLU memory associated tensor \p a quantization parameter \p offset.
 *   The value of this parameter can be NULL.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of the right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] b_position
 *   Input. Pointer to the MLU memory associated tensor \p b quantization parameter \p position.
 * @param[in] b_scale
 *   Input. Pointer to the MLU memory associated tensor \p b quantization parameter \p scale.
 *   The value of this parameter can be NULL.
 * @param[in] b_offset
 *   Input. Pointer to the MLU memory associated tensor \p b quantization parameter \p offset.
 *   The value of this parameter can be NULL.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] c
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] algo
 *   Input. The algorithm used to compute the batch matrix multiplication. The algorithms are defined
 *   in the ::cnnlBatchMatMulAlgo_t enum. You can get the best suited algorithm with the
 *   ::cnnlGetQuantizeBatchMatMulAlgorithm function.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the batch matrix multiplication
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size_in_bytes
 *   Input. The size of the extra workspace in bytes that needs to be used in the batch matrix multiplication
 *   operation. You can get the size of the workspace with the ::cnnlGetQuantizeBatchMatMulWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for input tensor \p a, \p b and
 *   output tensor \p c.
 *   - \p a offchip data type: half, float.
 *   - \p a onchip data type: int8, int16, int31.
 *   - \p b offchip data type: half, float.
 *   - \p b onchip data type: int8, int16, int31.
 *   - \p c offchip data type: half, float.
 *   - The data type for operation computing: half, float.
 * - \p a offchip data type should be the same as \p b offchip data type.
 * - \p a offchip data type can be combined with any onchip data type.
 * - \p b offchip data type can be combined with any onchip data type.
 *
 * @note
 * - The combinations of the data types should satisfy the following rules:
 *   - The data type bitwidth for operation computing is not shorter than \p c offchip data type.
 *   - The data type for operation computing must be float when onchip data type is int31.
 *   - The number of dimensions is no more than \p CNNL_DIM_MAX.
 * - This function does not support offline asymmetric quantization currently.
 *
 * @par Scale Limitation
 * - The inputs \p a and \p b are multi-dimensional arrays, the shape must be no less than
 *   two dimensions and no more than \p CNNL_DIM_MAX dimensions.
 * - The last two dimensions of the \p a and \p b must be the number of rows and the number
 *   of columns for matrix multiplication respectively.
 * - The number of columns of \p a matrix must be equal to the number of rows of \p b matrix
 *   after both inputs have performed the transpose operations according to parameters.
 *
 * @par API Dependency
 * - Before calling this function to implement batch matrix multiplication, you need to prepare
 *   all the parameters passed to this function. See each parameter description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, matrix \p a does not need to transpose and matrix \p b
 *   needs to transpose.
 * - If \p a and \p b are two-dimensional tensors, for best practices, it is recommended to call
 *   ::cnnlQuantizeMatMul.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      transa:                    false
      transb:                    false
      Dimension of input tensor a:  [64, 99, 128]
      Dimension of input tensor b:  [64, 128, 256]
      Dimension of output tensor c: [64, 99, 256]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/torch.html?highlight=bmm#torch.bmm
 */
CNNL_DEPRECATED_FOR(cnnlBatchMatMulEx)
cnnlStatus_t CNNL_WIN_API cnnlQuantizeBatchMatMul(cnnlHandle_t handle,
                                                  const cnnlBatchMatMulDescriptor_t bmm_desc,
                                                  const void *alpha,
                                                  const cnnlTensorDescriptor_t a_desc,
                                                  const void *a,
                                                  const void *a_position,
                                                  const void *a_scale,
                                                  const void *a_offset,
                                                  const cnnlTensorDescriptor_t b_desc,
                                                  const void *b,
                                                  const void *b_position,
                                                  const void *b_scale,
                                                  const void *b_offset,
                                                  const void *beta,
                                                  const cnnlTensorDescriptor_t c_desc,
                                                  void *c,
                                                  cnnlBatchMatMulAlgo_t algo,
                                                  void *workspace,
                                                  size_t workspace_size_in_bytes);

// Group:StridedSlice
/*!
 * @brief Slices on input tensor \p input with the begin \p begin, end \p end,
 * stride \p stride, and returns the results in the output tensor \p output.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlStridedSlice_v2 instead, which supports the int64 type parameters of \p begin, end
 *   and stride.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the strided_slice operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in]  begin
 *   Input. A host pointer to the \p begin data that holds starting indices of each dimension of the input tensor.
 * @param[in]  end
 *   Input. A host pointer to the \p end data that holds endding indices of each dimension of the input tensor.
 * @param[in]  stride
 *   Input. A host pointer to the \p stride data that holds strides of each dimension of the input tensor.
 * @param[in]  output_desc
 *   Output. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out]  output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_INTERNAL_ERROR
 *
 * @par Formula
 * - See "StridedSlice Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The (I/O)function supports the following data widths for \p input and \p output tensors.
 *   - input tensor: 1-byte, 2-byte, 4-byte, 8-byte.
 *   - output tensor: 1-byte, 2-byte, 4-byte, 8-byte.
 *
 *   The byte width of a data type can be got with the ::cnnlGetSizeOfDataType function.
 *
 *   Note that the data type of input tensor \p input and output tensor \p output must be the same.
 *
 * @par Scale Limitation
 * - For each dimension:
 *   - \p stride cannot be 0.
 *   - Convert negative \p begin and \p end to positive values:
 *     - If \p begin < 0: \p begin = \p begin + input_dims.
 *     - If \p end < 0: \p end = \p end + input_dims.
 *   - output_dims must meet the following requirements:
 *     - If (\p stride > 0) and (\p begin < \p end) and (\p end > 0) and (\p begin < input_dims):
 *       output_dims = ceil_div(min(\p end, input_dims) - max(\p begin, 0), \p stride).
 *     - If (\p stride < 0) and (\p begin > \p end) and (\p begin >= 0) and (\p end < input_dims - 1):
 *       output_dims = ceil_div(min(\p begin, input_dims - 1) - max(\p end, -1), abs(\p stride)).
 *     - Otherwise, output_dims = 0.
 *
 * @par Example
 * - The example of strided_slice operation is as follows:
    @verbatim
    input: an array by 4*7 -->   [[1,2,3,4,5,6,7],
                                 [8,9,10,11,12,13,14],
                                 [15,16,17,18,19,20,21],
                                 [22,23,24,25,26,27,28]]

    begin:  an array by 1*2  --> [0,0]

    end:    an array by 1*2  --> [4,7]

    stride: an array by 1*2  --> [2,3]

    Then we will get the output:

    output: an array by 2*3 --> [[1,4,7],
                                 [15,18,21]]
    @endverbatim
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/strided_slice
 * - https://github.com/tensorflow/tfjs/blob/master/tfjs-core/src/ops/strided_slice.ts
 *
 */
CNNL_DEPRECATED_FOR(cnnlStridedSlice_v2)
cnnlStatus_t CNNL_WIN_API cnnlStridedSlice(cnnlHandle_t handle,
                                           const cnnlTensorDescriptor_t input_desc,
                                           const void *input,
                                           const int32_t begin[],
                                           const int32_t end[],
                                           const int32_t stride[],
                                           const cnnlTensorDescriptor_t output_desc,
                                           void *output);

// Group:StridedSlice
/*!
 * @brief Slices on input tensor \p input with the begin \p begin, end \p end,
 * stride \p stride, and returns the results in the output tensor \p output.
 *
 * Compared with ::cnnlStridedSlice, this function support int64 type \p begin, end and
 * stride parameters.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the strided_slice operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in]  begin
 *   Input. A host pointer to the \p begin data that holds starting indices of each dimension of the input tensor.
 * @param[in]  end
 *   Input. A host pointer to the \p end data that holds ending indices of each dimension of the input tensor.
 * @param[in]  stride
 *   Input. A host pointer to the \p stride data that holds strides of each dimension of the input tensor.
 * @param[in]  output_desc
 *   Output. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out]  output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_INTERNAL_ERROR
 *
 * @par Formula
 * - See "StridedSlice Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The (I/O)function supports the following data widths for \p input and \p output tensors.
 *   - input tensor: bool, int8, uint8, int16, uint16, int32, uint32, int64, uint64, half, float,
 *                   double, complex_half, complex_float, bfloat16.
 *   - output tensor: bool, int8, uint8, int16, uint16, int32, uint32, int64, uint64, half, float,
 *                    double, complex_half, complex_float, bfloat16.
 *
 *   The byte width of a data type can be got with the ::cnnlGetSizeOfDataType function.
 *
 *   Note that the data type of input tensor \p input and output tensor \p output must be the same.
 *
 * @par Scale Limitation
 * - For each dimension:
 *   - \p stride cannot be 0.
 *   - Convert negative \p begin and \p end to positive values:
 *     - If \p begin < 0: \p begin = \p begin + input_dims.
 *     - If \p end < 0: \p end = \p end + input_dims.
 *   - Output_dims must meet the following requirements:
 *     - If (\p stride > 0) and (\p begin < \p end) and (\p end > 0) and (\p begin < input_dims):
 *       output_dims = ceil_div(min(\p end, input_dims) - max(\p begin, 0), \p stride).
 *     - If (\p stride < 0) and (\p begin > \p end) and (\p begin >= 0) and (\p end < input_dims - 1):
 *       output_dims = ceil_div(min(\p begin, input_dims - 1) - max(\p end, -1), abs(\p stride)).
 *     - Otherwise, output_dims = 0.
 *
 * @par Example
 * - The example of strided_slice operation is as follows:
    @verbatim
    input: an array by 4*7 -->   [[1,2,3,4,5,6,7],
                                 [8,9,10,11,12,13,14],
                                 [15,16,17,18,19,20,21],
                                 [22,23,24,25,26,27,28]]

    begin:  an array by 1*2  --> [0,0]

    end:    an array by 1*2  --> [4,7]

    stride: an array by 1*2  --> [2,3]

    Then we will get the output:

    output: an array by 2*3 --> [[1,4,7],
                                 [15,18,21]]
    @endverbatim
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/strided_slice
 * - https://github.com/tensorflow/tfjs/blob/master/tfjs-core/src/ops/strided_slice.ts
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlStridedSlice_v2(cnnlHandle_t handle,
                                              const cnnlTensorDescriptor_t input_desc,
                                              const void *input,
                                              const int64_t begin[],
                                              const int64_t end[],
                                              const int64_t stride[],
                                              const cnnlTensorDescriptor_t output_desc,
                                              void *output);

// Group:LogicOp
/*!
 * @brief Computes the element-wise truth value (true or false) based on
 *        the logical relationship \p logic_op between two input tensors \p a
 *        and \p b, and returns the results in the output tensor \p c.
 *
 * When the input tensor \p a and the output tensor \p c have the same data type dwidth,
 * this function supports in-place operation, which means that the first input
 * tensor \p a and the output tensor \p c can be the same one.
 *
 * This function also supports tensor broadcasting as long as \p a, \p b, and \p c satisfy the
 * broadcast conditions. For more details about tensor broadcasting, see Limitations section.
 *
 * Specially, the two input tensors \p a and \p b should be the same one when
 * the logic operation is ::CNNL_LOGIC_OP_NOT.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlLogicOpNot instead for logical NOT operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the logic operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] logic_op
 *   Input. The specific logic operation performed in the function. The operations
 *   are defined in the ::cnnlLogicOp_t enum.
 * @param[in] a_desc
 *   Input. The descriptor of the first input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. Pointer to the MLU memory that stores the first input tensor.
 * @param[in] b_desc
 *   Input. The descriptor of the second input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. Pointer to the MLU memory that stores the second input tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   logic operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the logic operation. You can get the size of the workspace with
 *   the ::cnnlGetLogicOpWorkspaceSize function.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] c
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - None.
 *
 * @par Data Type
 * - By the order of \p a - \p b - \p c, the supported combinations of data types are:
 *   - complex_float-complex_float-bool.
 *   - float-float-float.
 *   - float-float-bool.
 *   - float-int32-bool.
 *   - bfloat16-bfloat16-bfloat16 (only on MLU500 series and 1V).
 *   - bfloat16-bfloat16-bool (only on MLU500 series and 1V).
 *   - half-half-half.
 *   - half-half-bool.
 *   - int64-int64-int64.
 *   - int64-int64-bool.
 *   - int32-int32-int32.
 *   - int32-int32-bool.
 *   - int32-float-bool.
 *   - int16-int16-int16.
 *   - int16-int16-bool.
 *   - int8-int8-int8.
 *   - int8-int8-bool.
 *   - uint8-uint8-uint8.
 *   - uint8-uint8-bool.
 *   - bool-bool-bool.
 * - complex_float-complex_float-bool data type combination is valid only when the logic operation
 *   is ::CNNL_LOGIC_OP_EQ.
 * - int64-int64-int64 and int64-int64-bool data type combinations are valid only when the logic
 *   operation is ::CNNL_LOGIC_OP_EQ, ::CNNL_LOGIC_OP_NE, ::CNNL_LOGIC_OP_LE, ::CNNL_LOGIC_OP_LT,
 *   ::CNNL_LOGIC_OP_GE or ::CNNL_LOGIC_OP_GT.
 *
 * @par Limitations
 * - To satisfy the broadcast conditions, the length of each dimension of the two input tensors
 *   should be the same or one of them equal to 1, and meanwhile, each dimension of the output
 *   tensor should be equal to the larger one between corresponding dimensions of two input
 *   tensors.
 * - If the input data type is int64 and one of the input tensors is scalar data on CPU, the scalar
 *   data value should be in range of [\f$-2^{47}\f$, \f$2^{47}-1\f$].
 * - If the input data type is int64 and only one input tensor has 1 data number, the data value of the
 *   tensor with 1 data number should be in range of [\f$-2^{47}\f$, \f$2^{47}-1\f$].
 * - If the input data type is complex_float, it is not supported to set the scalar data on CPU.
 *
 * @par API Dependency
 * - Before calling this function to perform logic operation, you need to get the
 *   size of workspace by the ::cnnlGetLogicOpWorkspaceSize function.
 *
 * @par Performance Optimization
 * - To get better performance, set the data type of the output tensor to one bool,
 *   uint8, or int8.
 *
 * @note
 * - You can specify the stride of all dimensions for a_desc, b_desc and c_desc with
 *   ::cnnlSetTensorDescriptorEx.
 * - Either tensor \p a or \p b can be scalar data on CPU.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlLogicOp(cnnlHandle_t handle,
                                      const cnnlLogicOp_t logic_op,
                                      const cnnlTensorDescriptor_t a_desc,
                                      const void *a,
                                      const cnnlTensorDescriptor_t b_desc,
                                      const void *b,
                                      void *workspace,
                                      size_t workspace_size,
                                      const cnnlTensorDescriptor_t c_desc,
                                      void *c);

// Group:LogicOp
/*!
 * @brief Computes the element-wise truth value (true or false) based on the logical NOT
 *        relationship by input tensor \p a and returns the results in output tensor \p c.
 *
 * When the input tensor \p a and the output tensor \p c have the same data type dwidth,
 * this function supports in-place operation, which means that the input
 * tensor \p a and the output tensor \p c can be the same one.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the logic operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] c
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - None.
 *
 * @par Data Type
 * - By the order of \p a - \p c, the supported combinations of data types are:
 *   - float-float.
 *   - float-bool.
 *   - bfloat16-bfloat16 (only on MLU500 series and 1V).
 *   - bfloat16-bool (only on MLU500 series and 1V).
 *   - half-half.
 *   - half-bool.
 *   - int32-int32.
 *   - int32-bool.
 *   - int16-int16.
 *   - int16-bool.
 *   - int8-int8.
 *   - int8-bool.
 *   - uint8-uint8.
 *   - uint8-bool.
 *   - bool-bool.
 *
 * @par Limitations
 * - The tensor shape between input and output tensors should be the same.
 *
 * @par API Dependency
 * - None.
 *
 * @par Performance Optimization
 * - None.
 *
 * @note
 * - You can specify the stride of all dimensions for \p a_desc and \p c_desc with
 *   ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlLogicOpNot(cnnlHandle_t handle,
                                         const cnnlTensorDescriptor_t a_desc,
                                         const void *a,
                                         const cnnlTensorDescriptor_t c_desc,
                                         void *c);

// Group:LogicOp
/*!
 * @brief Returns in \p size the size of the MLU memory in bytes that is used as
 *        an extra workspace to optimize the logic operation.
 *
 * The size of the extra workspace is based on the given information of the input
 * and output tensor descriptors, \p a_desc, \p b_desc, and \p c_desc. For more
 * information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the logic operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] a_desc
 *   Input. The descriptor of the first input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the second input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is
 *   used in the logic operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlLogicOp function
 *   to perform the logic operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetLogicOpWorkspaceSize(cnnlHandle_t handle,
                                                      const cnnlTensorDescriptor_t a_desc,
                                                      const cnnlTensorDescriptor_t b_desc,
                                                      const cnnlTensorDescriptor_t c_desc,
                                                      size_t *size);

// Group:SoftmaxCrossEntropyWithLogits
/*!
 * @brief Computes the softmax cross entropy loss and back propagation gradients between input
 *        tensor \p x and \p p based on the given \p mode defined in ::cnnlSoftmaxMode_t,
 *        where \p x is the features and \p p is the label, and returns the results in the
 *        output tensors \p y and \p diff_y.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   softmax_corss_entropy_with_logits operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] mode
 *   Input. The reduction dimension in the computation procedure, defined in ::cnnlSoftmaxMode_t.
 * @param[in] x_desc
 *   Input. The descriptor of input feature tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the feature tensor, which is the output of last layer of
 *   AI network.
 * @param[in] p_desc
 *   Input. The descriptor of input label tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] p
 *   Input. Pointer to the MLU memory that stores the input label tensor, which is a valid
 *   probability distribution in reduction dimension.
 * @param[in] y_desc
 *   Input. The descriptor of output loss tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output loss tensor, which is the loss of per example.
 * @param[in] diff_y_desc
 *   Input. The descriptor of output back propagation gradients tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] diff_y
 *   Output. Pointer to the MLU memory that stores the output back propagation gradients tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "SoftmaxCrossEntropyWithLogits Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of inputs and outputs must be the same.
 * - The supported data type of inputs and outputs are as follows:
 *   - feature tensor: half, float.
 *   - label tensor: half, float.
 *   - loss tensor: half, float.
 *   - backpropagation tensor: half, float.
 *
 * @par Scale Limitation
 * - When the data type is half, the sum of exp(xk - x_max) should be in range of [-65504.0, 65504.0],
 *   where xk and x_max mean each element and the maximum value in the reduction dimension of the input tensor
 *   respectively.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, reshape the reduction dimension to the lowest dimension and set
 *   the \p mode to \p CNNL_SOFTMAX_MODE_LOW_DIMENSION.
 *
 * @note
 * - Only supports 3D input tensor and label tensor.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the softmax_cross_entropy_with_logits operation is as follows:
     @verbatim
     input two array by 1 * 1 * 4
     --> feature: [[[1, 2, 3, 4]]]

     --> label: [[[0.2, 0.4, 0.3, 0.1]]]

     mode: CNNL_SOFTMAX_MODE_LOW_DIMENSION

     output two array by 1 * 1 * 1 and 1 * 1 * 4
     --> loss: [[[2.14019]]]

     --> backprop: [[[-0.167914, -0.312856, -0.0631172, 0.543914]]]
     @endverbatim
 *
 * @par Reference
 * - http://www.tensorflow.org/api_doc/python/tf/nn/softmax_cross_entropy_with_logits
 * - http://www.tensorflow.org/api_doc/cc/class/tensorflow/ops/softmax_cross_entropy_with_logits
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlSoftmaxCrossEntropyWithLogits(cnnlHandle_t handle,
                                  cnnlSoftmaxMode_t mode,
                                  const cnnlTensorDescriptor_t x_desc,
                                  const void *x,
                                  const cnnlTensorDescriptor_t p_desc,
                                  const void *p,
                                  const cnnlTensorDescriptor_t y_desc,
                                  void *y,
                                  const cnnlTensorDescriptor_t diff_y_desc,
                                  void *diff_y);

// Group:SoftmaxCrossEntropyWithLogits
/*!
 * @brief Computes the softmax cross entropy loss and back propagation gradients between input
 *        tensor \p x and \p p based on the given \p mode defined in ::cnnlSoftmaxMode_t,
 *        where \p x is the features and \p p is the label, and returns the results in the
 *        output tensors \p y and \p diff_y. The difference between this function and
 *        ::cnnlSoftmaxCrossEntropyWithLogits is that this function needs the enumeration
 *        parameter ::cnnlComputationPreference_t to choose the best suited algorithm used for
 *        implementation of this function.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *  Use ::cnnlSoftmaxCrossEntropyWithLogits instead, which does not need the enumeration
 *  parameter ::cnnlComputationPreference_t.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   softmax_corss_entropy_with_logits operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] mode
 *   Input. The reduction dimension in the computation procedure, defined in ::cnnlSoftmaxMode_t.
 * @param[in] prefer
 *  Input. The algorithm used to compute the output. For detailed information,
 *  see ::cnnlComputationPreference_t.
 * @param[in] x_desc
 *   Input. The descriptor of input feature tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the feature tensor, which is the output of last layer of
 *   AI network.
 * @param[in] p_desc
 *   Input. The descriptor of input label tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] p
 *   Input. Pointer to the MLU memory that stores the input label tensor, which is a valid
 *   probability distribution in reduction dimension.
 * @param[in] y_desc
 *   Input. The descriptor of output loss tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output loss tensor, which is the loss of per example.
 * @param[in] diff_y_desc
 *   Input. The descriptor of output back propagation gradients tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] diff_y
 *   Output. Pointer to the MLU memory that stores the output back propagation gradients tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "SoftmaxCrossEntropyWithLogits Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of inputs and outputs must be the same.
 * - The supported data type of inputs and outputs are as follows:
 *   - feature tensor: half, float.
 *   - label tensor: half, float.
 *   - loss tensor: half, float.
 *   - backpropagation tensor: half, float.
 *
 * @par Scale Limitation
 * - When the data type is half, the sum of exp(xk - x_max) should be in range of [-65504.0, 65504.0],
 *   where xk and x_max mean each element and the maximum value in the reduction dimension of the input tensor
 *   respectively.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, reshape the reduction dimension to the lowest dimension and set
 *   the \p mode to \p CNNL_SOFTMAX_MODE_LOW_DIMENSION.
 *
 * @par Note
 * - Only supports 3D input tensor and label tensor.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the softmax_cross_entropy_with_logits operation is as follows:
     @verbatim
     input two array by 1 * 1 * 4
     --> feature: [[[1, 2, 3, 4]]]

     --> label: [[[0.2, 0.4, 0.3, 0.1]]]

     mode: CNNL_SOFTMAX_MODE_LOW_DIMENSION

     output two array by 1 * 1 * 1 and 1 * 1 * 4
     --> loss: [[[2.14019]]]

     --> backprop: [[[-0.167914, -0.312856, -0.0631172, 0.543914]]]
     @endverbatim
 *
 * @par Reference
 * - http://www.tensorflow.org/api_doc/python/tf/nn/softmax_cross_entropy_with_logits
 * - http://www.tensorflow.org/api_doc/cc/class/tensorflow/ops/softmax_cross_entropy_with_logits
 *
 */
CNNL_DEPRECATED_FOR(cnnlSoftmaxCrossEntropyWithLogits)
cnnlStatus_t CNNL_WIN_API
cnnlSoftmaxCrossEntropyWithLogits_v2(cnnlHandle_t handle,
                                     cnnlSoftmaxMode_t mode,
                                     cnnlComputationPreference_t prefer,
                                     const cnnlTensorDescriptor_t x_desc,
                                     const void *x,
                                     const cnnlTensorDescriptor_t p_desc,
                                     const void *p,
                                     const cnnlTensorDescriptor_t y_desc,
                                     void *y,
                                     const cnnlTensorDescriptor_t diff_y_desc,
                                     void *diff_y);

// Group:ListDiff
/*!
 * @brief Returns in \p size_out the size of the MLU memory that is used as an extra workspace
 * to optimize the listdiff operation.
 *
 * The size of the extra workspace is determined based on the given information of the listdiff
 * operation, including the number of elements in \p input1 tensor. For more information about
 * the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   the listdiff operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] len_input1
 *   Input. The number of elements in \p input1 tensor.
 * @param[in] len_input2
 *   Input. The number of elements in \p input2 tensor.
 * @param[out] size_len
 *   Output. Pointer to the MLU memory that stores the returned size of the extra
 *   workspace in bytes that is used in ::cnnlListDiffGetOutLen.
 * @param[out] size_out
 *   Output. Pointer to the MLU memory that stores the returned size of the extra
 *   workspace in bytes that is used in the listdiff operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Scale Limitation
 * - \p len_input1 and \p len_input2 should be greater than 0.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetListDiffWorkSpace(cnnlHandle_t handle,
                                                   const int len_input1,
                                                   const int len_input2,
                                                   int *size_len,
                                                   int *size_out);
// Group:ListDiff
/*!
 * @brief Computes the numbers of elements in the \p input1 but not in the \p input2
 * with the input data type \p data_type.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   the listdiff operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] data_type
 *   Input. Data type of \p input1 and \p input2.
 * @param[in] input1
 *   Input. Pointer to the MLU memory that stores the \p input1 tensor.
 * @param[in] input2
 *   Input. Pointer to the MLU memory that stores the \p input2 tensor.
 * @param[in] len_input1
 *   Input. The number of elements in \p input1 tensor.
 * @param[in] len_input2
 *   Input. The number of elements in \p input2 tensor.
 * @param[out] gsh_len
 *   Output. Pointer to the MLU memory that stores the number of elements in the \p input1
 *   but not in the \p input2 in each MLU core.
 * @param[out] output_len
 *   Output. Pointer to the MLU memory that stores the number of elements in the \p input1
 *   but not in the \p input2.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - Data types of tensor \p input1 and \p input2 must be the same.
 * - The supported data types of input and output are as follows:
 *   - \p data_type: \p CNNL_DTYPE_INT32, \p CNNL_DTYPE_FLOAT
 *   - \p input1: float, int32
 *   - \p input2: float, int32
 *   - \p len_input1: int32
 *   - \p len_input2: int32
 *   - \p gsh_len: int32
 *   - \p output_len: int32
 *
 * @par Scale Limitation
 * - \p len_input1 and \p len_input2 should be greater than 0.
 * - The number of dimensions of \p input1 and \p input2 must be 1.
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlGetListDiffWorkSpace function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlListDiffGetOutLen(cnnlHandle_t handle,
                                                cnnlDataType_t data_type,
                                                const void *input1,
                                                const void *input2,
                                                const int len_input1,
                                                const int len_input2,
                                                int *gsh_len,
                                                int *output_len);
// Group:ListDiff
/*!
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @brief Retrieves elements in the \p input1 tensor but not in the \p input2 tensor.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   the listdiff operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] data_type
 *   Input. Data type of \p input1, \p input2 and \p output_data.
 * @param[in] input1
 *   Input. Pointer to the MLU memory that stores the \p input1 tensor.
 * @param[in] input2
 *   Input. Pointer to the MLU memory that stores the \p input2 tensor.
 * @param[in] len_input1
 *   Input. The number of elements in \p input1 tensor.
 * @param[in] len_input2
 *   Input. The number of elements in \p input2 tensor.
 * @param[out] gsh_out
 *   Output. Pointer to the MLU memory that stores the number of elements which is
 *   in the \p input1 tensor but not in the \p input2 tensor.
 * @param[out] output_data
 *   Output. Pointer to the MLU memory that stores the output tensor \p output_data.
 * @param[out] output_index
 *   Output. Pointer to the MLU memory that stores the index of input1 that corresponds
 *   to each elements of \p output_data.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - Data types of tensor \p input1, \p input2 and \p output_data must be the same.
 * - The supported data types of input and output are as follows:
 *   - \p data_type: \p CNNL_DTYPE_INT32, \p CNNL_DTYPE_FLOAT
 *   - \p input1: float, int32
 *   - \p input2: float, int32
 *   - \p len_input1: int32
 *   - \p len_input2: int32
 *   - \p gsh_out: int32
 *   - \p output_data: float, int32
 *   - \p output_index: int32
 *
 * @par Scale Limitation
 * - \p len_input1 and \p len_input2 should be greater than 0.
 * - \p output_data cannot be NULL.
 * - \p output_index cannot be NULL.
 * - The number of dimensions of \p input1, \p input2, \p output_data and \p output_index must be 1.
 *
 * @par API Dependency
 * - You need to call the ::cnnlListDiffGetOutLen function and the ::cnnlGetListDiffWorkSpace
 *   function before calling this function.
 *
 * @note
 * - This operation is not supported on the 1V platforms.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the listdiff operation is as follows:
     @verbatim
       data_type: CNNL_DTYPE_INT32
       input1 array: [0, 9, 11, 4, 9, 1, 3, 5, 5, 10, 13, 8, 2, 2, 4]
       input2 array: [0, 9, 11, 4, 9, 1, 3, 5, 5, 10]
       len_input1: 15
       len_input2: 10
       gsh_out: 4
       output_data array: [13, 8, 2, 2]
       output_index array: [10, 11, 12, 13]
     @endverbatim
 *
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlListDiff(cnnlHandle_t handle,
                                       cnnlDataType_t data_type,
                                       const void *input1,
                                       const void *input2,
                                       const int len_input1,
                                       const int len_input2,
                                       int *gsh_out,
                                       void *output_data,
                                       int *output_index);

/******************************************************************************
 * Cambricon CNNL OP: InvertPermutation
 ******************************************************************************/

// Group:InvertPermutation
/*!
 * @brief Computes the inverse permutation of input tensor \p input, and returns
 *        the results in the output tensor \p output.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the invert_permutation operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "InvertPermutation Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported data type of the input tensor and output tensor are as follows:
 *   - input tensor: int32.
 *   - output tensor: int32.
 *
 * @par Data Layout
 * - The supported data layouts of the input tensor and output tensor are as follows:
 *   - input tensor: \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - Suppose the shape of \p input is [N], the value of \p input tensor must meet the following requirements:
 *   - It is in range of [0, N-1].
 *   - There are no duplicate values.
 *
 * @par API Dependency
 * - Before calling this function to implement invert_permutation, you need to prepare
 *   all the parameters passed to this function. See each parameter description
 *   for details.
 *
 * @note
 * - The dimension of input tensor must be 1D.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the invert_permutation operation is as follows:
     @verbatim
     input array by 5 --> input: [3, 4, 0, 2, 1]
     output array by 5 --> output: [2, 4, 3, 0, 1]
     @endverbatim
 */
cnnlStatus_t CNNL_WIN_API cnnlInvertPermutation(cnnlHandle_t handle,
                                                const cnnlTensorDescriptor_t input_desc,
                                                const void *input,
                                                void *output);
// Group:Unique
/*!
 * @brief Creates a descriptor pointer by \p unique_desc for a unique operator, and allocates
 *        memory for holding the information about the unique operator. The information is
 *        defined in ::cnnlUniqueDescriptor_t. For more information about descriptor,
 *        see "Cambricon CNNL User Guide".
 *
 * @param[out] unique_desc
 *   Output. Pointer to the unique descriptor that holds information about the unique operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetUniqueDescriptor function to initialize
 *   and set the information to the descriptor.
 * - You need to call the ::cnnlDestroyUniqueDescriptor function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCreateUniqueDescriptor(cnnlUniqueDescriptor_t *unique_desc);

// Group:Unique
/*!
 *  @brief Destroys a unique descriptor \p unique_desc that was previously created with the
 *         ::cnnlCreateUniqueDescriptor function.
 *
 *  The unique descriptor is defined in ::cnnlUniqueDescriptor_t and holds the information
 *  about the unique operator.
 *
 * @param[in] unique_desc
 *   Input. The unique descriptor to be destroyed.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - You need to call this function after calling ::cnnlUnique.
 * - This function should be called to destroy the unique descriptor. Otherwise, the memory
 *   leak may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlDestroyUniqueDescriptor(cnnlUniqueDescriptor_t unique_desc);

// Group:Unique
/*!
 * @brief Initializes the unique descriptor \p unique_desc that was previously created
 *        with the ::cnnlCreateUniqueDescriptor function, and sets the information about the
 *        unique operation to the unique descriptor \p unique_desc. The information includes
 *        the sorted mode of the unique \p mode, the number of the unique dimensions \p dim,
 *        whether to output index \p return_inverse, and whether to output counts \p return_counts.
 *
 * @param[in,out] unique_desc
 *   Input/output. The descriptor of the unique operation. For detailed information,
 *   see ::cnnlUniqueDescriptor_t.
 * @param[in] mode
 *   Input. The sorted mode of unique operation. The sorted modes are define in the
 *   ::cnnlUniqueSort_t enum.
 * @param[in] dim
 *   Input. The number of dimensions in the input tensor of the unique operation.
 * @param[in] return_inverse
 *   Input. A Boolean value that specifies whether to return the index of input elements that
 *   are in the returned unique elements.
 * @param[in] return_counts
 *   Input. A Boolean value that specifies whether to return the number of duplicate values
 *   for each unique element.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - The \p dim must meet the following requirements:
 *   - If \p dim is set to -1, the unique of the flattened input is to apply.
 *   - When the \p mode is CNNL_UNSORT_FORWARD or CNNL_UNSORT_REVERSE, the unique operation only
 *     support the unique of the flattened input.
 *   - When the \p mode is CNNL_SORT_ASCEND, the \p dim can be applied to unique, the size of \p dim
 *     should be greater than or equal to 0 and less than the size of input dimension.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetUniqueDescriptor(cnnlUniqueDescriptor_t unique_desc,
                                                  cnnlUniqueSort_t mode,
                                                  int dim,
                                                  bool return_inverse,
                                                  bool return_counts);
// Group:Unique
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace to store
 *        unique data.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGetUniqueWorkspaceSize instead, which supports better performance to unique.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the unique
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] unique_desc
 *   Input. The descriptor of the unique operation. For detailed information,
 *   see ::cnnlUniqueDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   unique operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions
 *   to create and set the tensor descriptor \p input_desc before calling this function, and
 *   call the ::cnnlCreateUniqueDescriptor and ::cnnlSetUniqueDescriptor functions to create
 *   and set the unique descriptor \p unique_desc.
 * - The allocated extra workspace should be passed to the ::cnnlUnique function to perform the
 *   unique operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlGetUniqueWorkspaceSize)
cnnlStatus_t CNNL_WIN_API cnnlGetUniqueWorkSpace(cnnlHandle_t handle,
                                                 const cnnlUniqueDescriptor_t unique_desc,
                                                 const cnnlTensorDescriptor_t input_desc,
                                                 size_t *size);
// Group:Unique
/*!
 * @brief Computes the length of unique data of input tensor, and returns the results
 *        in \p output_len.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlUnique_v2 instead, which supports better performance to unique.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the unique
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] unique_desc
 *   Input. The descriptor of the unique operation. For detailed information,
 *   see ::cnnlUniqueDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[out] unique_data
 *   Output. Pointer to the MLU memory that is used as an extra workspace for the unique operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[out] output_len
 *   Output. Pointer to the MLU memory that stores the length of unique data.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par API Dependency
 * - You need to call the ::cnnlGetUniqueWorkSpace function to allocate extra workspace for
 *   \p unique_data.
 *
 * @par Data Type
 * - Data types of input tensor \p input and output tensor \p unique_data must be the same.
 * - The supported data types of input tensor \p input and output tensor \p unique_data are as follows:
 *   - input tensor: float, int32
 *   - output tensor: float, int32
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlUnique_v2)
cnnlStatus_t CNNL_WIN_API cnnlUniqueGetOutLen(cnnlHandle_t handle,
                                              const cnnlUniqueDescriptor_t unique_desc,
                                              const cnnlTensorDescriptor_t input_desc,
                                              const void *input,
                                              void *unique_data,
                                              int *output_len);
// Group:Unique
/*!
 * @brief Retrieves unique elements in the input tensor.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlUnique_v2 instead, which supports better performance to unique.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   the unique operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] unique_desc
 *   Input. The descriptor of the unique operation. For detailed information,
 *   see ::cnnlUniqueDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_len
 *   Input. An integer value that is the length of unique data of input tensor.
 * @param[in] unique_data
 *   Input. Pointer to the MLU memory that is used as an extra workspace to store unique
 *   data. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[out] output_data
 *   Output. Pointer to the MLU memory that stores the output tensor \p output_data.
 * @param[out] output_index
 *   Output. Pointer to the MLU memory that stores the index of input elements that are in
 *   the returned unique elements \p output_data. This parameter only returns meaningful
 *   value when \p return_inverse is set to true. If \p return_inverse is to false, this
 *   parameter returns meaningless value. It is recommended to set this parameter to NULL
 *   if \p return_inverse is to false.
 * @param[out] output_counts
 *   Output. Pointer to the MLU memory that stores the number of duplicate values for each
 *   unique element \p output_data. This parameter only returns meaningful value when
 *   \p return_counts is set to true. If \p return_counts is to false, this parameter
 *   returns meaningless value. It is recommended to set this parameter to NULL if
 *   \p return_counts is set to false.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par API Dependency
 * - You need to call the ::cnnlUniqueGetOutLen function to get the length of unique data
 *   of input tensor \p output_len and the unique data \p unique_data.
 *
 * @par Formula
 * - See "Unique Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor \p input and output tensor \p output_data must be the same.
 * - The supported data types of input tensor \p input and output tensors are as follows:
 *   - input tensor: float, int32
 *   - \p output_data: float, int32
 *   - \p output_index: int32
 *   - \p output_counts: int32
 *
 * @par Scale Limitation
 * - The input tensor \p input must meet the following requirement:
 *   - When the \p mode is set to \p CNNL_UNSORT_FORWARD, the dimension of \p input must be
 *     one dimensional.
 * - Currently, the dimension \p dim do not support to apply unique, and the \p output is the unique
 *   of the flattened \p input.
 *
 * @note
 * - The \p input with NaN is not supported currently, and the data range of \p input should
 *   satisfy the following conditions:
 *   - (-inf, +inf), where inf represents infinity.
 * - You need to call the ::cnnlUniqueGetOutLen function to get the scale \p output_len and
 *   the tensor \p unique_data before calling this function.
 * - The tensor \p output_index is the same shape as input tensor \p input, and the tensor
 *   \p output_counts is the same shape as \p output_data.
 * - When the \p mode is set to \p CNNL_UNSORT_FORWARD, the output \p output_counts is not
 *   supported yet.
 * - Not supported on 520 platforms.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the unique operation is as follows:
 *   @verbatim
 *     Example 1:
 *     input array:
 *       input: [1, 1, 2, 4, 4, 9, 7, 8, 8]
 *     param:
 *       mode: CNNL_UNSORT_FORWARD
 *     output array:
 *       output_data: [1, 2, 4, 9, 7, 8]
 *       output_index: [0, 0, 1, 2, 2, 3, 4, 5, 5]

 *     Example 2:
 *     input array:
 *       input: [1, 1, 2, 4, 4, 9, 7, 8, 8]
 *     param:
 *       mode: CNNL_SORT_ASCEND, return_inverse: true, return_counts: true,
 *     output array:
 *       output_data: [1, 2, 4, 7, 8, 9]
 *       output_index: [0, 0, 1, 2, 2, 5, 3, 4, 4]
 *       output_counts: [2, 1, 2, 1, 2, 1]

 *     Example 3:
 *     input array:
 *       input: [1, 1, 2, 4, 4, 9, 7, 8, 8]
 *     param:
 *       mode: CNNL_SORT_REVERSE, return_inverse: true, return_counts: true,
 *     output array:
 *       output_data: [8, 7, 9, 4, 2, 1]
 *       output_index: [5, 5, 4, 3, 3, 2, 1, 0, 0]
 *       output_counts: [2, 1, 1, 2, 1, 2]
 *   @endverbatim
 *
 * @par Reference
 * - http://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/Unique.cpp
 *
 */
CNNL_DEPRECATED_FOR(cnnlUnique_v2)
cnnlStatus_t CNNL_WIN_API cnnlUnique(cnnlHandle_t handle,
                                     const cnnlUniqueDescriptor_t unique_desc,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const void *input,
                                     const int output_len,
                                     void *unique_data,
                                     void *output_data,
                                     int *output_index,
                                     int *output_counts);

// Group:Unique
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra workspace to
 *        optimize the unique operation.
 *
 * Compared with ::cnnlGetUniqueWorkSpace, this function have a better performance for unique operation.
 *
 * The size of the extra workspace is based on the given information of the unique operation,
 * including the input tensor descriptors \p input_desc, and the unique operation
 * descriptor \p unique_desc.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the unique
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] unique_desc
 *   Input. The descriptor of the unique operation. For detailed information,
 *   see ::cnnlUniqueDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   unique operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions
 *   to create and set the tensor descriptor \p input_desc before calling this function, and
 *   call the ::cnnlCreateUniqueDescriptor and ::cnnlSetUniqueDescriptor functions to create
 *   and set the unique descriptor \p unique_desc.
 * - The allocated extra workspace should be passed to the ::cnnlUnique_v2 function to perform the
 *   unique operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetUniqueWorkspaceSize(cnnlHandle_t handle,
                                                     const cnnlUniqueDescriptor_t unique_desc,
                                                     const cnnlTensorDescriptor_t input_desc,
                                                     size_t *workspace_size);

// Group:Unique
/*!
 *  @brief Retrieves unique elements in the input tensor.
 *
 * Compared with ::cnnlUniqueGetOutLen and ::cnnlUnique, this function have a better performance.
 *
 * This function needs extra MLU memory as the workspace to improve the unique
 * performance. You can get the workspace size with the
 * ::cnnlGetUniqueWorkspaceSize function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   the unique operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] unique_desc
 *   Input. The descriptor of the unique operation. For detailed information,
 *   see ::cnnlUniqueDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor \p input.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the unique
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the unique
 *   operation. You can get the size of the workspace with the
 *   ::cnnlGetUniqueWorkspaceSize function.
 * @param[out] output_num
 *   Output. Pointer to the MLU memory that stores the number of output unique data.
 *   When the unique of the flattened input is to apply, \p output_num is the total number
 *   of \p output. When the dimension \p dim to apply unique, \p output_num is the dimension
 *   size of \p output in \p dim.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor \p output.
 * @param[in] indices_desc
 *   Input. The descriptor of the inverse indices tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] inverse_indices
 *   Output. Pointer to the MLU memory that stores the index of input elements that are in
 *   the returned unique elements \p output. This parameter returns meaningful
 *   value only when \p return_inverse is set to true, or \p mode is set to \p CNNL_UNSORT_FORWARD.
 *   If \p return_inverse is to false and \p mode is not \p CNNL_UNSORT_FORWARD,
 *   this parameter returns meaningless value, and it is recommended to set this parameter to NULL.
 * @param[in] counts_desc
 *   Input. The descriptor of the \p counts tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] counts
 *   Output. Pointer to the MLU memory that stores the number of duplicate values for each
 *   unique element \p output. This parameter returns meaningful value only when
 *   \p return_counts is set to true. If \p return_counts is to false, this parameter
 *   returns meaningless value. It is recommended to set this parameter to NULL if
 *   \p return_counts is set to false.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH,
 *   ::CNNL_STATUS_NOT_SUPPORTED, ::CNNL_STATUS_INTERNAL_ERROR, CNNL_STATUS_EXECUTION_FAILED
 *
 * @par API Dependency
 * - You need to call the ::cnnlGetUniqueWorkspaceSize function to allocate extra
 *   workspace for \p workspace.
 *
 * @par Formula
 * - See "Unique Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor \p input and output tensor \p output must be the same.
 * - The supported data types of input tensor \p input and output tensors are as follows:
 *   - \p input: float, int32
 *   - \p output_num: int32
 *   - \p output: float, int32
 *   - \p inverse_indices: int32
 *   - \p counts: int32
 * - When \p mode is \p CNNL_UNSORT_FORWARD or \p CNNL_SORT_ASCEND on MLU300 series and MLU500 series,
 *   the additional supported data types of input tensor \p input and output tensors are as follows:
 *   - \p input: int64, uint64
 *   - \p output_num: int32
 *   - \p output: int64, uint64
 *   - \p inverse_indices: int32
 * - When \p mode is \p CNNL_SORT_ASCEND on MLU300 series and MLU500 series, the additional supported
 *   data types of input tensor \p input and output tensors are as follows:
 *   - \p input: half
 *   - \p output_num: int32
 *   - \p output: half
 *   - \p inverse_indices: int32
 *   - \p counts: int32
 * - When \p mode is \p CNNL_SORT_ASCEND, and the input is flatten on MLU300 series and MLU500 series,
 *   the additional supported data type of output tensor \p inverse_indices is as follows:
 *   - \p inverse_indices: int64
 *
 * @par Data Layout
 * - The supported layout of the input tensors and output tensors must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The \p input with NaN is not supported currently, and the data range of \p input should
 *   satisfy the following condition:
 *   - (-inf, +inf), where inf represents infinity.
 * - The input tensor \p input must meet the following requirement:
 *   - When the \p mode is set to \p CNNL_UNSORT_FORWARD, the dimension of \p input must be
 *     one dimensional.
 * - The \p dim in \p unique_desc should meet the following requirements:
 *   - If \p dim is set to -1, the unique of the flattened input is to apply.
 *   - When the \p mode is \p CNNL_UNSORT_FORWARD or \p CNNL_UNSORT_REVERSE, the unique operation only
 *     supports the unique of the flattened input.
 *   - When the \p mode is \p CNNL_SORT_ASCEND and input data type is float or int32, the \p dim can be
 *     applied to unique, the size of \p dim should be greater than or equal to 0 and less than
 *     the size of \p input dimension.
 *   - When the \p mode is \p CNNL_SORT_ASCEND and input data type is uint64 or int64, the unique operation
 *     only supports the unique of the flattened input, the \p dim is set to -1, or \p dim is set to 0 if
 *     the \p input is one dimensional.
 *
 * @note
 * - The mode \p CNNL_UNSORT_REVERSE is deprecated and will be removed in future release.
 * - When the input type is uint64_t, the input value must be in range of [0, MAX_INT64 - 1].
 * - The number of dimensions is no more than \p CNNL_DIM_MAX.
 * - When the \p dim in \p unique_desc is set to -1, the tensor \p inverse_indices has the same
 *   shape as input tensor \p input, and the tensor \p counts has the same shape as \p output.
 * - When the \p dim in \p unique_desc to apply unique, the tensor \p inverse_indices is
 *   one-dimensional and the size of dimension is equal to the size of input tensor \p input
 *   in \p dim, and the tensor \p counts is also one dimensional.
 * - When the \p mode is set to \p CNNL_UNSORT_FORWARD, the output \p counts is not
 *   supported yet.
 * - When the \p mode is set to \p CNNL_UNSORT_FORWARD, and the input data type is uint64 or int64,
 *   the input number should be less than or equal to (UINT23_T / 4).
 * - Not supported on 520 platforms.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the unique operation is as follows:
 *  @verbatim
 *    Example 1:
 *    input array:
 *      input: [1, 1, 2, 4, 4, 9, 7, 8, 8]
 *    param:
 *      mode: CNNL_UNSORT_FORWARD
 *    output array:
 *      output_num: 6
 *      output: [1, 2, 4, 9, 7, 8]
 *      inverse_indices: [0, 0, 1, 2, 2, 3, 4, 5, 5]
 *    Example 2:
 *    input array:
 *      input: [1, 1, 2, 4, 4, 9, 7, 8, 8]
 *    param:
 *      mode: CNNL_SORT_ASCEND, return_inverse: true, return_counts: true
 *    output array:
 *      output_num: 6
 *      output: [1, 2, 4, 7, 8, 9]
 *      inverse_indices: [0, 0, 1, 2, 2, 5, 3, 4, 4]
 *      counts: [2, 1, 2, 1, 2, 1]
 *
 *    Example 3:
 *    input array:
 *      input: [[1, 1, 5, 4], [4, 4, 7, 8]]
 *    param:
 *      mode: CNNL_SORT_ASCEND, return_inverse: true, return_counts: true, dim: 1
 *    output array:
 *      output_num: 3
 *      output: [[1, 4, 5], [4, 8, 7]]
 *      inverse_indices: [0, 0, 2, 1]
 *      counts: [2, 1, 1]
 *
 *    Example 4:
 *    input array:
 *      input: [1, 1, 2, 4, 4, 9, 7, 8, 8]
 *    param:
 *      mode: CNNL_SORT_REVERSE, return_inverse: true, return_counts: true
 *    output array:
 *      output_num: 6
 *      output: [8, 7, 9, 4, 2, 1]
 *      inverse_indices: [5, 5, 4, 3, 3, 2, 1, 0, 0]
 *      counts: [2, 1, 1, 2, 1, 2]
 * @endverbatim
 *
 * @par Reference
 * - http://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/Unique.cpp
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlUnique_v2(cnnlHandle_t handle,
                                        const cnnlUniqueDescriptor_t unique_desc,
                                        const cnnlTensorDescriptor_t input_desc,
                                        const void *input,
                                        void *workspace,
                                        const size_t workspace_size,
                                        int *output_num,
                                        const cnnlTensorDescriptor_t output_desc,
                                        void *output,
                                        const cnnlTensorDescriptor_t indices_desc,
                                        void *inverse_indices,
                                        const cnnlTensorDescriptor_t counts_desc,
                                        void *counts);
// Group:UniqueConsecutive
/*!
 * @brief Creates a descriptor pointer by \p unique_consecutive_desc for a unique_consecutive
 *        operator, and allocates memory for holding the information about the
 *        unique_consecutive operator. The information is defined in
 *        ::cnnlUniqueConsecutiveDescriptor_t. For more information about this descriptor,
 *        see "Cambricon CNNL User Guide".
 *
 * @param[in] unique_consecutive_desc
 *   Input. Pointer to the unique_consecutive descriptor that holds information about
 *   the unique_consecutive operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetUniqueConsecutiveDescriptor
 *   function to initialize and set the information to the descriptor.
 * - You need to call the ::cnnlDestroyUniqueConsecutiveDescriptor
 *   function to destroy a descriptor when you done with it.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCreateUniqueConsecutiveDescriptor(cnnlUniqueConsecutiveDescriptor_t *unique_consecutive_desc);

// Group:UniqueConsecutive
/*!
 * @brief Destroys unique_consecutive descriptor \p unique_consecutive_desc
 *         that was previously created with the ::cnnlCreateUniqueConsecutiveDescriptor function.
 *
 *  The unique_consecutive descriptor is defined in ::cnnlUniqueConsecutiveDescriptor_t
 *  and holds the information about the unique_consecutive operator.
 *
 * @param[in] unique_consecutive_desc
 *   Input. The unique_consecutive descriptor to be destroyed.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - You need to call this function after calling ::cnnlUniqueConsecutive.
 * - This function should be called to destroy the unique_consecutive descriptor.
 *   Otherwise, the memory leak may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlDestroyUniqueConsecutiveDescriptor(cnnlUniqueConsecutiveDescriptor_t unique_consecutive_desc);

// Group:UniqueConsecutive
/*!
 * @brief Initializes the unique_consecutive descriptor \p unique_consecutive_desc
 *        that was previously created with the ::cnnlCreateUniqueConsecutiveDescriptor function,
 *        and sets the information about the unique_consecutive operation to this descriptor.
 *        The information includes the sorted mode of the unique_consecutive \p mode,
 *        the number of the unique_consecutive dimensions \p dim,
 *        whether to output index \p return_inverse, and whether to output counts \p return_counts.
 *
 * @param[in] unique_consecutive_desc
 *   Input. The descriptor of the unique_consecutive operation. For detailed information,
 *   see ::cnnlUniqueConsecutiveDescriptor_t.
 * @param[in] dim
 *   Input. The number of dimensions in the input tensor of the unique_consecutive operation.
 *   - If \p dim is set to input.shape.size(dim),
 *     the unique_consecutive of the flattened input should be applied.
 *   - When the \p dim is to apply unique_consecutive, the value of \p dim
 *     should be [-input.shape.size(dim) ~ input.shape.size(dim) - 1].
 * @param[in] return_inverse
 *   Input. A boolean value that specifies whether to return the index of input elements that
 *   are in the returned unique_consecutive elements.
 * @param[in] return_counts
 *   Input. A boolean value that specifies whether to return the number of duplicate values
 *   for each unique_consecutive element.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetUniqueConsecutiveDescriptor(
                            cnnlUniqueConsecutiveDescriptor_t unique_consecutive_desc,
                            const int dim,
                            const bool return_inverse,
                            const bool return_counts);
// Group:UniqueConsecutive
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as
 *        an extra workspace to optimize the unique_consecutive operation.
 *
 * The size of the extra workspace is based on the given information of the
 * unique_consecutive operation, including the input tensor descriptors \p input_desc,
 * and the unique_consecutive operation descriptor \p unique_consecutive_desc.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage
 *   MLU devices and queues in the unique_consecutive
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] unique_consecutive_desc
 *   Input. The descriptor of the unique_consecutive operation. For detailed information,
 *   see ::cnnlUniqueConsecutiveDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   unique_consecutive operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, call the ::cnnlCreateTensorDescriptor and
 *   ::cnnlSetTensorDescriptor functions to create and set the tensor descriptor \p input_desc,
 *   and call the ::cnnlCreateUniqueConsecutiveDescriptor and
 *   ::cnnlSetUniqueConsecutiveDescriptor functions to create
 *   and set the unique_consecutive descriptor \p unique_consecutive_desc.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetUniqueConsecutiveWorkspaceSize(
                            cnnlHandle_t handle,
                            const cnnlUniqueConsecutiveDescriptor_t unique_consecutive_desc,
                            const cnnlTensorDescriptor_t input_desc,
                            size_t *workspace_size);

// Group:UniqueConsecutive
/*!
 * @brief Eliminates all but the first element from each equivalent group of elements.
 *
 * This function needs extra MLU memory as the workspace to improve the unique_consecutive
 * performance. You can get the workspace size with the
 * ::cnnlGetUniqueConsecutiveWorkspaceSize function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   the unique operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] unique_consecutive_desc
 *   Input. The descriptor of the unique_consecutive operation. For detailed information,
 *   see ::cnnlUniqueConsecutiveDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor \p input.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the unique_consecutive
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the
 *   unique_consecutive operation. You can get the size of the workspace with the
 *   ::cnnlGetUniqueConsecutiveWorkspaceSize function.
 * @param[out] output_num
 *   Output. Pointer to the MLU memory that stores the number of output unique_consecutive data.
 *   Output_num is the total number of \p output.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor \p output.
 * @param[in] indices_desc
 *   Input. The descriptor of the inverse indices tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] inverse_indices
 *   Output. Pointer to the MLU memory that stores the index of input elements that are in
 *   the returned unique_consecutive elements \p output. This parameter only returns meaningful
 *   value when \p return_inverse is set to true. It is recommended to set this parameter to NULL
 *   if \p return_inverse is to set to false.
 * @param[in] counts_desc
 *   Input. The descriptor of the counts tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] counts
 *   Output. Pointer to the MLU memory that stores the number of duplicate values for each
 *   unique_consecutive element \p output. This parameter only returns meaningful value when
 *   \p return_counts is set to true. It is recommended to set this parameter to NULL
 *   if \p return_counts is to set to false.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par API Dependency
 * - You need to call the ::cnnlGetUniqueConsecutiveWorkspaceSize function to allocate extra
 *   workspace for \p workspace.
 *
 * @par Formula
 * - See "UniqueConsecutive Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor \p input and output tensor \p output must be the same.
 * - The supported data types of input tensor \p input and output tensors are as follows:
 *   - \p input: float, half, int16, int32, int64
 *   - \p output_num: int64
 *   - \p output: float, half, int16, int32, int64
 *   - \p inverse_indices: int32, int64
 *   - \p counts: int32, int64
 *
 * @par Data Layout
 * - The supported layout of the input tensors and output tensors must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The \p dim in \p unique_consecutive_desc should meet the following requirements:
 *   - If \p dim is set to input.shape.size(dim),
 *     the unique_consecutive of the flattened input should be applied.
 *   - When the \p dim is to apply unique_consecutive, the value of \p dim
 *     should be [-input.shape.size(dim) ~ input.shape.size(dim) - 1].
 *
 * @note
 * - The unique_consecutive atcion is the same as torch action
 *   when The \p input contain Nan/-Nan/Inf/-Inf.
 * - When the \p dim in \p unique_consecutive_desc is set to input.shape.size(dim),
 *   the tensor \p inverse_indices and counts have the same shape as input tensor \p input.
 * - When the \p dim in \p unique_consecutive_desc is applied to unique_consecutive,
 *   the tensor \p inverse_indices is one-dimensional and the dimension size is eqaul to
 *   that of input tensor \p input in \p dim, and the tensor \p counts is also one dimensional.
 * - Not supported on 520 platforms.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 *  @verbatim
 *    Example 1:
 *    input array:
 *      input: [1, 1, 2, 2, 3, 1, 1, 2]
 *    param:
 *      return_inverse: true, return_counts: true
 *    output array:
 *      output_num: 5
 *      output: [1, 2, 3, 1, 2]
 *      inverse_indices: [0, 0, 1, 1, 2, 3, 3, 4]
 *      counts: [2, 2, 1, 2, 1]
 *
 *    Example 2:
 *    input array[2 1 2 15]:
 *       tensor([[[[0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0],
 *                 [0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1]]],
 *               [[[0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0],
 *                 [1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0]]]])
 *    param:
 *      return_inverse: true, return_counts: true, dim=None
 *    output array:
 *      output_num: 33
 *      output: [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
 *               0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]
 *      inverse_indices[2 1 2 15]:
 *         tensor([[[[ 0,  0,  0,  1,  2,  2,  2,  2,  3,  4,  5,  6,  6,  6,  6],
 *                   [ 6,  6,  6,  7,  7,  8,  9,  9, 10, 11, 12, 13, 13, 14, 15]]],
 *                 [[[16, 16, 16, 16, 16, 17, 17, 18, 19, 20, 21, 22, 22, 23, 24],
 *                   [25, 25, 25, 26, 26, 26, 26, 27, 28, 29, 30, 30, 31, 32, 32]]]])
 *      counts: [3, 1, 4, 1, 1, 1, 7, 2, 1, 2, 1, 1, 1, 2, 1, 1,
 *               5, 2, 1, 1, 1, 1, 2, 1, 1, 3, 4, 1, 1, 1, 2, 1, 2]
 *    Example 3:
 *    input array[2 5 1 2]:
 *       tensor([[[[1, 1]],
 *                [[0, 0]],
 *                [[1, 0]],
 *                [[1, 0]],
 *                [[0, 0]]],
 *
 *               [[[0, 0]],
 *                [[0, 1]],
 *                [[0, 1]],
 *                [[0, 1]],
 *                [[1, 0]]]])
 *    param:
 *      return_inverse: true, return_counts: true, dim: 1
 *    output array[2 4 1 2]:
 *      output_num: 4
 *      output:
 *         tensor([[[[1, 1]],
 *                  [[0, 0]],
 *                  [[1, 0]],
 *                  [[0, 0]]],
 *
 *                 [[[0, 0]],
 *                  [[0, 1]],
 *                  [[0, 1]],
 *                  [[1, 0]]]])
 *      inverse_indices: [0, 1, 2, 2, 3]
 *      counts: [1, 1, 2, 1]
 * @endverbatim
 *
 * @par Reference
 * - http://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/Unique.cpp
 * - http://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/Unique.cu
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlUniqueConsecutive(
                            cnnlHandle_t handle,
                            const cnnlUniqueConsecutiveDescriptor_t unique_consecutive_desc,
                            const cnnlTensorDescriptor_t input_desc,
                            const void *input,
                            void *workspace,
                            const size_t workspace_size,
                            int64_t *output_num,
                            const cnnlTensorDescriptor_t output_desc,
                            void *output,
                            const cnnlTensorDescriptor_t indices_desc,
                            void *inverse_indices,
                            const cnnlTensorDescriptor_t counts_desc,
                            void *counts);

// Group:ConvolutionForward
/*!
 * @brief Initializes the convolution descriptor \p desc that was previously created
 * with the ::cnnlCreateConvolutionDescriptor function, and sets the information
 * about the convolution forward and backward operation to the convolution descriptor
 * \p desc. The information includes the number of the convolution dimensions \p dimNb,
 * the padding size for each dimension \p pad, the stride of the sliding window for
 * each dimension \p stride, the dilation
 * factor for each dimension \p dilation, and the number of groups to be split into
 * by channel \p group_count.
 *
 * @param[in,out] desc
 *   Input/output. The descriptor of the convolution operation. For detailed information,
 *   see ::cnnlConvolutionDescriptor_t.
 * @param[in] dimNb
 *   Input. The number of dimensions in the input tensor of the convolution operation.
 *   Currently, the value of this parameter can only be set to 4 or 5. The value of this parameter
 *   should be the same as the one you set in the input tensor descriptor.
 * @param[in] pad
 *   Input. An array that stores the zero-padding size for each dimension of the input tensor
 *   used in the convolution operation.
 *   For each dimension, the padding size represents the number of zeros to be concatenated at the
 *   start and end of that dimension. If \p dimNb is set to 4, the padding is on top, bottom, left,
 *   and right. If \p dimNb is set to 5, the padding is on front, back, top, bottom, left,
 *   and right. The value of this parameter should be greater than or equal to 0.
 * @param[in] stride
 *   Input. An array that stores the filter stride for each dimension of the input tensor
 *   used in the convolution operation. For each dimension, the filter stride represents
 *   the number of elements to slide over the input tensor. If \p dimNb is set to 4,
 *   the stride is in height and width.  If \p dimNb is set to 5,
 *   the stride is in depth_stride, height and width.
 *   The value of this parameter should be greater than or equal
 *   to 1.
 * @param[in] dilation
 *   Input. An array that stores the dilation factor for each dimension of the filter tensor
 *   used in the convolution operation. For each dimension, the dilation factor represents
 *   the spacing between the kernel points. If \p dimNb is set to 4, the dilation should be set in
 *   height and width dimension. The value of this parameter
 *   should be greater than or equal to 1. If \p dimNb is set to 5, the dilation should be set in
 *   depth, height and width dimension. The value of this parameter should be greater than or equal to 1.
 * @param[in] group_count
 *   Input. The number of groups that the input data is split by the number of channels
 *   in the input tensor. Each group is convolved separately. The filter used for each group is
 *   the filter tensor divides \p group_count. The result of
 *   the convolution operation is the concatenation of all the group convolution results by the
 *   number of channels in the input tensor. Make sure that the number of channels in the input tensor
 *   and the output tensor are divisible by \p group_count. The value of this parameter should
 *   be greater than or equal to 1 and less than or equal to the number of channels in the input tensor,
 *   and input channels and output channels must both be divisible by group_count.
 *   - If \p group_count is set to 1, the input tensor is convolved without splitting into groups.
 *   - If \p group_count is set to the number of channels in the input tensor, the depthwise convolution
 *     is performed.
 *   - If the value of \p group_count is between 1 and the number of channels of input tensor,
 *     the operator becomes equivalent to group_count numbers of convolution operations side by side.
 *     Each convolution operation is (input_channel/group_count) input channels,
 *     and (output_channel/group_count) output channels,
 *     concat the output of each convolution operations subsequently.
 * @param[in] compute_type
 *   Input. The data type of temporary result in convolution operation, only supports
 *   floating-point type.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *
 * @note
 * - Currently, only supports 4D and 5D input tensor for convolution
 *   forward or backward operation.
 *
 * @par Requirements
 * - The data width of compute_type cannot be less than output tensor's data type.
 *
 * @par Example
 * - See the Cambricon CNNL sample code in ``samples/conv_sample/conv_sample.cc``
 */
cnnlStatus_t CNNL_WIN_API cnnlSetConvolutionDescriptor(cnnlConvolutionDescriptor_t desc,
                                                       int dimNb,
                                                       const int pad[],
                                                       const int stride[],
                                                       const int dilation[],
                                                       const int group_count,
                                                       const cnnlDataType_t compute_type);

// Group:ConvolutionForward
/*!
 * @brief Initializes the convolution descriptor \p desc that was previously created
 * with the ::cnnlCreateConvolutionDescriptor function, and sets the information
 * about the convolution forward and backward operation to the convolution descriptor
 * \p desc. This function also includes the \p allow_tf32 parameter that is used to
 * control whether to enable TensorFloat-32.
 *
 * @param[in] desc
 *   Input. The descriptor of the convolution operation. For detailed information,
 *   see ::cnnlConvolutionDescriptor_t.
 * @param[in] allow_tf32
 *   Input. An integer value that determines whether to enable TensorFloat-32.
 *   TensorFloat-32 is enabled by default.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - Currently, only supports 4D and 5D input tensor for convolution
 *   forward or backward operation.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetConvolutionDescriptorAllowTF32(cnnlConvolutionDescriptor_t desc,
                                                                const int allow_tf32);

// Group:ConvolutionForward
/*!
 * @brief Sets the algorithm search strategy used in ::cnnlFindConvolutionForwardAlgo.
 *
 * @param[in] desc
 *   Input. The descriptor of the convolution operation. For detailed information,
 *   see ::cnnlConvolutionDescriptor_t.
 * @param[in] search_mode
 *   Input. Search strategy used in ::cnnlFindConvolutionForwardAlgo. For more
 *   information, see ::cnnlConvolutionFwdAlgoSearchMode_t.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - Currently, only supports 4D and 5D input tensor for convolution
 *   forward or backward operation.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetConvolutionDescriptorAlgoSearchMode(cnnlConvolutionDescriptor_t desc,
                                           const cnnlConvolutionFwdAlgoSearchMode_t search_mode);

// Group:Convolution
/*!
 * @brief Sets the reorder type of the filter and bias tensors used in the convolution operation.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] desc
 *   Input. The descriptor of the convolution operation. For detailed information,
 *   see ::cnnlConvolutionDescriptor_t.
 * @param[in] filter_reorder_type
 *   Input. The reorder type of the convolution filter. For detailed information,
 *   see ::cnnlReorderType_t
 * @param[in] bias_reorder_type
 *   Input. The reorder type of the convolution bias. For detailed information,
 *   see ::cnnlReorderType_t
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - After calling this function, you need to call ::cnnlGetReorderConvDataSize and
 *   ::cnnlHostReorderConvData functions to reorder data for convolution operation.
 *
 * @note
 * - You must set filter_reorder_type to CNNL_REORDER.
 * - Due to the limitation of cnnlHostReorderConvData, only MLU Edge devices can use
 *   cnnlHostReorderConvData when you use the cluster num optional function.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlSetConvolutionDescriptorReorderType(
                                                    cnnlConvolutionDescriptor_t desc,
                                                    const cnnlReorderType_t filter_reorder_type,
                                                    const cnnlReorderType_t bias_reorder_type);
// Group:Convolution
/*!
 * @brief Sets the quantization information to the convolution descriptor \p desc that was
 *  previously created with ::cnnlCreateConvolutionDescriptor.
 *
 * @param[in] desc
 *   Input. The descriptor of the convolution operation. For detailed information,
 *   see ::cnnlConvolutionDescriptor_t.
 * @param[in] input0_quant_desc
 *   Input. The quantization descriptor that holds quantization information of the input0.
 *   - For cnnlConvolutionForward, it means x.
 *   - For CnnlConvolutionBackwardFilter, it means x.
 *   - For cnnlConvolutionBackwardData, it means diff_y.
 *   - For cnnlDeconvolution, it means input.
 * @param[in] input1_quant_desc
 *   Input. The quantization descriptor that holds quantization information of the input1.
 *   - For cnnlConvolutionForward, it means w.
 *   - For CnnlConvolutionBackwardFilter, it means diff_y.
 *   - For cnnlConvolutionBackwardData, it means filter.
 *   - For cnnlDeconvolution, it means filter.
 * @param[in] output_quant_desc
 *   Input. The quantization descriptor that holds quantization information of the output.
 *   - For cnnlConvolutionForward, it means y.
 *   - For CnnlConvolutionBackwardFilter, it means diff_w.
 *   - For cnnlConvolutionBackwardData, it means diff_x.
 *   - For cnnlDeconvolution, it means output.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - None
 * @note
 * - None
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetConvolutionDescriptorQuant(
                                               cnnlConvolutionDescriptor_t desc,
                                               const cnnlQuantizeExDescriptor_t input0_quant_desc,
                                               const cnnlQuantizeExDescriptor_t input1_quant_desc,
                                               const cnnlQuantizeExDescriptor_t output_quant_desc);

// Group:ConvolutionForward
/*!
 * @brief Creates a descriptor pointed by \p desc for a convolution forward or backward
 *        operation, and allocates memory for holding the information about the convolution
 *        operation. The information is defined in ::cnnlConvolutionDescriptor_t. For more
 *        information about descriptor, see "Cambricon CNNL User Guide".
 *
 * @param[out] desc
 *  Output. A host pointer to the convolution descriptor that holds information about the convolution operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_NOT_INITIALIZED
 *
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetConvolutionDescriptor function to initialize
 *   and set the information to the convolution descriptor.
 * - You need to call the ::cnnlDestroyConvolutionDescriptor function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - See the Cambricon CNNL sample code in ``samples/conv_sample/conv_sample.cc``
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateConvolutionDescriptor(cnnlConvolutionDescriptor_t *desc);

// Group:ConvolutionForward
/*!
 * @brief Destroys a convolution descriptor \p desc that was previously created with the
 *        ::cnnlCreateConvolutionDescriptor function.
 *
 * The convolution descriptor is defined in ::cnnlConvolutionDescriptor_t
 * and holds the information about the convolution forward or backward operation.
 *
 *
 * @param[in] desc
 *   Input. The convolution descriptor to be destroyed.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - You need to call this function after calling the ::cnnlConvolutionForward,
 *   ::cnnlConvolutionBackwardData, or ::cnnlConvolutionBackwardFilter function.
 *   Otherwise, ::CNNL_STATUS_BAD_PARAM is returned.
 * - This function should be called to destroy the convolution descriptor.
 *   Otherwise, the memory leak may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - See the Cambricon CNNL sample code in ``samples/conv_sample/conv_sample.cc``
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyConvolutionDescriptor(cnnlConvolutionDescriptor_t desc);

// Group:ConvolutionForward
/*!
 * @brief Returns the shape \p dimSize of the output tensor of a convolution forward
 * operation with the given convolution descriptor \p desc, input tensor \p x, filter tensor
 * \p w, and the number of dimensions of the input tensor \p dimNb.
 *
 * You can set the shape of the output tensor in the output tensor descriptor based on
 * the return value of this function.
 *
 * @param[in] desc
 *    Input. The descriptor of the convolution operation. For detailed information,
 *   see ::cnnlConvolutionDescriptor_t.
 * @param[in] x
 *    Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] w
 *    Input. The descriptor of the filter tensor used as a filter in the
 *   convolution operation. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] dimNb
 *    Input. The number of dimensions in the input tensor of the convolution forward operation.
 *    Currently, the value of this parameter can only be set to 4 or 5. The value of this parameter
 *    should be the same as the one you set in the input tensor descriptor.
 * @param[out] dimSize
 *    Output. An array that stores the shape of the output tensor of the convolution forward
 *    operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *
 * @par Formula
 * \n
 * The shape of the output is based on the input tensor, padding, and dilation you set with
 * the ::cnnlSetConvolutionDescriptor function, and the filter you set in the filter tensor
 * descriptor.
 * - The height dimension of the output tensor is as follows:
 *
 *   \p output_height =
 *
 *   1 + (\p input_height + \p pad_top + \p pad_bottom -
 *   (((\p filter_height - 1) * \p dilation_height) + 1)) / \p stride_height;
 * - The width dimension of the output tensor is as follows:
 *
 *   \p output_width =
 *
 *   1 + (\p input_width + \p pad_top + \p pad_bottom -
 *   (((\p filter_width - 1) * \p dilation_width) + 1)) / \p stride_width;
 *
 *
 * @note
 * - You need to call the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions
 *   to create and set the tensor descriptors \p x and \p w before calling this function.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 *
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - See the Cambricon CNNL sample code in ``samples/conv_sample/conv_sample.cc``
 */
cnnlStatus_t CNNL_WIN_API cnnlGetConvolutionForwardOutputDim(const cnnlConvolutionDescriptor_t desc,
                                                             const cnnlTensorDescriptor_t x,
                                                             const cnnlTensorDescriptor_t w,
                                                             int dimNb,
                                                             int dimSize[]);

// Group:ConvolutionForward
/*!
 * @brief Returns the most suited convolution algorithm that can be used
 * in the convolution forward operation.
 *
 * The returned convolution algorithm is chosen from all the supported convolution
 * algorithms defined in ::cnnlConvolutionForwardAlgo_t and is based on the given
 * convolution descriptor \p desc, input tensor \p x, filter tensor \p w, output tensor \p y,
 * and the computing performance preferences \p preference.
 *
 * The computing performance option \p preference only supports high speed mode.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlFindConvolutionForwardAlgo instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the convolution operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] conv_desc
 *   Input. The descriptor of the convolution operation. For detailed information,
 *   see ::cnnlConvolutionDescriptor_t.
 * @param[in] x
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] w
 *   Input. The descriptor of the filter tensor used as a filter in the
 *   convolution operation. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. The descriptor of the output tensor. You can get the shape of the
 *   tensor with the ::cnnlGetConvolutionForwardOutputDim function.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] preference
 *   Input. The options for implementing the convolution operation to get a better performance.
 *   This parameter only supports CNNL_CONVOLUTION_FWD_FASTEST.
 * @param[out] algo
 *   Output. The returned algorithm that is best suited for computing the convolution. The
 *   algorithms are defined in the ::cnnlConvolutionForwardAlgo_t enum.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlFindConvolutionForwardAlgo)
cnnlStatus_t CNNL_WIN_API
cnnlGetConvolutionForwardAlgorithm(cnnlHandle_t handle,
                                   const cnnlConvolutionDescriptor_t conv_desc,
                                   const cnnlTensorDescriptor_t x,
                                   const cnnlTensorDescriptor_t w,
                                   const cnnlTensorDescriptor_t y,
                                   const cnnlConvolutionFwdPreference_t preference,
                                   cnnlConvolutionForwardAlgo_t *algo);

// Group:ConvolutionForward
/*!
 * @brief Returns the most suited convolution algorithm that can be used
 * in the convolution forward operation.
 *
 * The returned convolution algorithm is chosen from all the supported convolution
 * algorithms defined in ::cnnlConvolutionForwardAlgo_t and is based on the given
 * convolution descriptor \p desc, input tensor \p x, filter tensor \p w, output tensor \p y,
 * and the computing performance preferences \p preference.
 *
 * The computing performance option \p preference only supports high speed mode.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the convolution operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] conv_desc
 *   Input. The descriptor of the convolution operation. For detailed information,
 *   see ::cnnlConvolutionDescriptor_t.
 * @param[in] x
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] w
 *   Input. The descriptor of the filter tensor used as a filter in the
 *   convolution operation. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. The descriptor of the output tensor. You can get the shape of the
 *   tensor with the ::cnnlGetConvolutionForwardOutputDim function.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] preference
 *   Input. The options for implementing the convolution operation to get a better performance.
 *   This parameter only supports CNNL_CONVOLUTION_FWD_FASTEST.
 * @param[out] algo
 *   Output. The returned algorithm that is best suited for computing the convolution. The
 *   algorithms are defined in ::cnnlConvolutionForwardAlgo_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlFindConvolutionForwardAlgo)
cnnlStatus_t CNNL_WIN_API
cnnlGetConvolutionForwardAlgo(cnnlHandle_t handle,
                              const cnnlConvolutionDescriptor_t conv_desc,
                              const cnnlTensorDescriptor_t x,
                              const cnnlTensorDescriptor_t w,
                              const cnnlTensorDescriptor_t y,
                              const cnnlConvolutionFwdPreference_t preference,
                              cnnlConvolutionForwardAlgo_t *algo);

// Group:ConvolutionForward
/*!
 * @brief Collects a set of convolution algorithm candidates, attempts all
 * candidates, and then summarizes and returns hardware time. This API requires workspace
 * size and algorithm determinism information for every candidate.
 *
 * The algorithm candidates are defined in ::cnnlConvolutionForwardAlgo_t. The
 * candidate collection strategy is specified by
 * ::cnnlSetConvolutionDescriptorAlgoSearchMode. All available strategies are
 * defined in ::cnnlConvolutionFwdAlgoSearchMode_t.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlFindConvolutionForwardAlgo instead.
 *
 * @param[in] handle
 *   Input. Handle to the Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the convolution operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] conv_desc
 *   Input. The descriptor of the convolution operation. For detailed information,
 *   see ::cnnlConvolutionDescriptor_t.
 * @param[in] cast_mode
 *   Input. The descriptor of the cast mode. For detailed information,
 *   see ::cnnlConvolutionCastMode_t.
 * @param[in] alpha
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] w_desc
 *   Input. The descriptor of the weight tensor used as a filter in the
 *   convolution operation. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] w
 *   Input. Pointer to the MLU memory that stores the weight tensor.
 * @param[in] bias_desc
 *   Input. The descriptor of the bias tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the bias tensor.
 * @param[in] beta
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. You can get the shape of the
 *   tensor with the ::cnnlGetConvolutionForwardOutputDim function.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the output tensor.
 * @param[in] requested_algo_count
 *   Input. Number of desired algorithms. This number should be equal to the length
 *   of \p perf_results array.
 * @param[out] returned_algo_count
 *   Output. Number of actual returned algorithms. According to execution result, this
 *   number will be equal to or less than \p requested_algo_count.
 * @param[out] perf_results
 *   Output. An array stores multiple ::cnnlConvolutionFwdAlgoPerf_t structs. Each
 *   struct stores an algorithm id \p algo and execution result of it, including
 *   returned status \p status, hardware time \p time, required workspace size \p memory,
 *   and the algorithm determinism \p determinism of it.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   convolution operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the convolution operation. You can get the size of the workspace with
 *   the ::cnnlGetConvolutionForwardWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_INTERNAL_ERROR,
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Data Type
 * - This function supports data type combinations below:
  *
 *   (Notation: x - input tensor data type, w - weight tensor data type,
 *   y - output tensor data type, xo - input tensor onchp data type,
 *   wo - weight tensor onchip data type, yo - output tensor onchip data type)
 *   - x, xo, w, wo, y, yo = half/int8, int8, int8, int8, half, half/int8
 *   - x, xo, w, wo, y, yo = half/int16, int16, int16, int16, half, half/int16
 *   - x, xo, w, wo, y, yo = float/int8, int8, int8, int8, float, float/int8
 *   - x, xo, w, wo, y, yo = float/int16, int16, int16, int16, float, float/int16
 * - Only on MLU300 series and MLU500 series:
 *   - x, xo, w, wo, y, yo = half, half, half, half, half, half
 *   - x, xo, w, wo, y, yo = half, half, half, half, half, float
 *   - x, xo, w, wo, y, yo = float, float, float, float, float, float
 *
 * @par Data Layout
 * - The supported data layouts of the input tensor, weight tensor, bias tensor, and
 *   output tensor are as follows:
 *   - If \p dimNb is set to 4:
  *   - input tensor: \p CNNL_LAYOUT_NHWC.
  *   - weight tensor: \p CNNL_LAYOUT_NHWC and \p CNNL_LAYOUT_HWCN.
  *   - bias tensor: \p CNNL_LAYOUT_NHWC or \p CNNL_LAYOUT_ARRAY.
  *   - output tensor: \p CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - The input tensor, output tensor, and the convolution descriptor
 *   (including pad, stride, dilation, and group_count) must meet the following
 *   requirements:
 *   - \p pad >= 0
 *   - \p stride >= 1
 *   - \p dilation >= 1
 *   - \p group_count should be greater than or equal to 1, and less than or equal to
 *     the number of channels in the input tensor. Input channels and output channels
 *     must both be divisible by group_count.
 *   - input tensor: \p batch > 0, \p height > 0, \p width > 0, \p channel > 0
 *   - output tensor: \p height > 0, \p width > 0, \p channel > 0
 *   - onchip_dtype of y_desc must be the same as \p compute_type of \p conv_desc.
 *
 * @par API Dependency
 * - Before calling this function to execute algorithm performance, you need to
 *   set your preference of search strategy. See
 *   ::cnnlSetConvolutionDescriptorAlgoSearchMode for more information.
 * - After that, you need to call ::cnnlGetConvolutionForwardWorkspaceSize
 *   to compute an appropriate workspace size for algorithm search.
 *
 * @note
 * - The parameter \p cast_mode must be set to \p CNNL_NO_QUANTIZE, when input
 *   onchip type is float or half.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - See the Cambricon CNNL sample code in ``samples/conv_sample/conv_sample.cc``
 */
CNNL_DEPRECATED_FOR(cnnlFindConvolutionForwardAlgo)
cnnlStatus_t CNNL_WIN_API
cnnlFindConvolutionForwardAlgorithm(cnnlHandle_t handle,
                                    const cnnlConvolutionDescriptor_t conv_desc,
                                    const cnnlConvolutionCastMode_t cast_mode,
                                    const void *alpha,
                                    const cnnlTensorDescriptor_t x_desc,
                                    const void *x,
                                    const cnnlTensorDescriptor_t w_desc,
                                    const void *w,
                                    const cnnlTensorDescriptor_t bias_desc,
                                    const void *bias,
                                    const void *beta,
                                    const cnnlTensorDescriptor_t y_desc,
                                    void *y,
                                    const int requested_algo_count,
                                    int *returned_algo_count,
                                    cnnlConvolutionFwdAlgoPerf_t *perf_results,
                                    void *workspace,
                                    const size_t workspace_size);

// Group:ConvolutionForward
/*!
 * @brief Collects a set of convolution algorithm candidates, attempts all
 * candidates, and then summarizes and returns hardware time. This API requires workspace
 * size and algorithm determinism information for every candidate.
 *
 * The algorithm candidates are defined in ::cnnlConvolutionForwardAlgo_t. The
 * candidate collection strategy is specified by
 * ::cnnlSetConvolutionDescriptorAlgoSearchMode. All available strategies are
 * defined in ::cnnlConvolutionFwdAlgoSearchMode_t.
 *
 * @param[in] handle
 *   Input. Handle to the Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the convolution operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] conv_desc
 *   Input. The descriptor of the convolution operation. For detailed information,
 *   see ::cnnlConvolutionDescriptor_t.
 * @param[in] alpha
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] w_desc
 *   Input. The descriptor of the weight tensor used as a filter in the
 *   convolution operation. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] w
 *   Input. Pointer to the MLU memory that stores the weight tensor.
 * @param[in] bias_desc
 *   Input. The descriptor of the bias tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the bias tensor.
 * @param[in] beta
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. You can get the shape of the
 *   tensor with the ::cnnlGetConvolutionForwardOutputDim function.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the output tensor.
 * @param[in] requested_algo_count
 *   Input. Number of desired algorithms. This number should be equal to the length
 *   of \p perf_results array.
 * @param[out] returned_algo_count
 *   Output. Number of actual returned algorithms. According to execution result, this
 *   number will be equal to or less than \p requested_algo_count.
 * @param[out] perf_results
 *   Output. An array stores multiple ::cnnlConvolutionFwdAlgoPerf_t structs. Each
 *   struct stores an algorithm id \p algo and execution result of it, including
 *   returned status \p status, hardware time \p time, required workspace size \p memory,
 *   and the algorithm determinism \p determinism of it.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   convolution operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the convolution operation. You can get the size of the workspace with
 *   the ::cnnlGetConvolutionForwardWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_INTERNAL_ERROR,
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Data Type
 * - This function supports data type combinations below:
  *
 *   (Notation: x - input tensor data type, w - weight tensor data type,
 *   y - output tensor data type, xo - input tensor onchp data type,
 *   wo - weight tensor onchip data type, yo - output tensor onchip data type)
 *   - x, xo, w, wo, y, yo = half/int8, int8, int8, int8, half, half/int8
 *   - x, xo, w, wo, y, yo = half/int16, int16, int16, int16, half, half/int16
 *   - x, xo, w, wo, y, yo = float/int8, int8, int8, int8, float, float/int8
 *   - x, xo, w, wo, y, yo = float/int16, int16, int16, int16, float, float/int16
 * - Only on MLU300 series and MLU500 series:
 *   - x, xo, w, wo, y, yo = half, half, half, half, half, half
 *   - x, xo, w, wo, y, yo = half, half, half, half, half, float
 *   - x, xo, w, wo, y, yo = float, float, float, float, float, float
 *
 * @par Data Layout
 * - The supported data layouts of the input tensor, weight tensor, bias tensor and
 *   output tensor are as follows:
 *   - If \p dimNb is set to 4:
  *   - input tensor: \p CNNL_LAYOUT_NHWC.
  *   - weight tensor: \p CNNL_LAYOUT_NHWC and \p CNNL_LAYOUT_HWCN.
  *   - bias tensor: \p CNNL_LAYOUT_NHWC or \p CNNL_LAYOUT_ARRAY.
  *   - output tensor: \p CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - The input tensor, output tensor, and the convolution descriptor
 *   (including pad, stride, dilation, and group_count) must meet the following
 *   requirements:
 *   - \p pad >= 0
 *   - \p stride >= 1
 *   - \p dilation >= 1
 *   - \p group_count should be greater than or equal to 1, and less than or equal to
 *     the number of channels in the input tensor. Input channels and output channels
 *     must both be divisible by group_count.
 *   - input tensor: \p batch > 0, \p height > 0, \p width > 0, \p channel > 0
 *   - output tensor: \p height > 0, \p width > 0, \p channel > 0
 *   - onchip_dtype of y_desc must be the same as \p compute_type of \p conv_desc.
 *
 * @par API Dependency
 * - Before calling this function to execute algorithm performance, you need to
 *   set your preference of search strategy. See
 *   ::cnnlSetConvolutionDescriptorAlgoSearchMode for more information.
 * - After that, you need to call ::cnnlGetConvolutionForwardWorkspaceSize
 *   to compute an appropriate workspace size for algorithm search.
 *
 * @note
 * - The parameter \p cast_mode must be set to \p CNNL_NO_QUANTIZE, when input
 *   onchip type is float or half.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - See the Cambricon CNNL sample code in ``samples/conv_sample/conv_sample.cc``.
 */
cnnlStatus_t CNNL_WIN_API
cnnlFindConvolutionForwardAlgo(cnnlHandle_t handle,
                               const cnnlConvolutionDescriptor_t conv_desc,
                               const void *alpha,
                               const cnnlTensorDescriptor_t x_desc,
                               const void *x,
                               const cnnlTensorDescriptor_t w_desc,
                               const void *w,
                               const cnnlTensorDescriptor_t bias_desc,
                               const void *bias,
                               const void *beta,
                               const cnnlTensorDescriptor_t y_desc,
                               void *y,
                               const int requested_algo_count,
                               int *returned_algo_count,
                               cnnlConvolutionFwdAlgoPerf_t *perf_results,
                               void *workspace,
                               const size_t workspace_size);

// Group:Convolution
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace
 * to optimize the convolution forward operation.
 *
 * The size of the extra workspace is based on the given information of the convolution
 * forward operation, including the input tensor descriptor \p x, filter tensor descriptor \p w,
 * output tensor descriptor \p y, convolution descriptor \p conv_desc, and the convolution
 * algorithm \p algo. For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @deprecated
 * This function is deprecated and will be removed in
 *  future release. Use ::cnnlGetConvolutionForwardWorkspaceSize instead, which supports
 *   ::cnnlSetConvolutionDescriptorQuant to set the quantization descriptor.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the convolution operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  x
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] w
 *   Input. The descriptor of the filter tensor used as a filter in the
 *   convolution operation. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. The descriptor of the output tensor. You can get the shape of the
 *   tensor with the ::cnnlGetConvolutionForwardOutputDim function.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] bias
 *   Input. The descriptor of the bias tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] conv_desc
 *   Input.The descriptor of the convolution operation. For detailed information,
 *   see ::cnnlConvolutionDescriptor_t.
 * @param[in] algo
 *   Input. The algorithm used to compute the convolution. The algorithms are defined in the
 *  ::cnnlConvolutionForwardAlgo_t enum. You can get the best suited algorithm
 *  with the ::cnnlGetConvolutionForwardAlgo function.
 * @param[out] size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in
 *   the convolution forward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlGetConvolutionForwardAlgo function.
 *   You also need to call
 *   the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions
 *   to create and set the tensor descriptors \p x, \p w, \p y before calling this function.
 * - The allocated extra workspace should be passed to the ::cnnlConvolutionForward function
 *   to perform the convolution forward operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - See the Cambricon CNNL sample code in ``samples/conv_sample/conv_sample.cc``
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetConvolutionForwardWorkspaceSize(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t x,
                                       const cnnlTensorDescriptor_t w,
                                       const cnnlTensorDescriptor_t y,
                                       const cnnlTensorDescriptor_t bias,
                                       const cnnlConvolutionDescriptor_t conv_desc,
                                       const cnnlConvolutionForwardAlgo_t algo,
                                       size_t *size);

// Group:ConvolutionForward
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace
 * to optimize the convolution forward inference operation.
 *
 * The size of the extra workspace is based on the given information of the convolution forward
 * inference operation, including the input tensor descriptor \p x, filter tensor descriptor \p w,
 * output tensor descriptor \p y, convolution descriptor \p conv_desc, and the convolution
 * algorithm \p algo. For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the convolution operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  x
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] w
 *   Input. The descriptor of the filter tensor used as a filter in the
 *   convolution operation. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. The descriptor of the output tensor. You can get the shape of the
 *   tensor with the ::cnnlGetConvolutionForwardOutputDim function.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] bias
 *   Input. The descriptor of the bias tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] conv_desc
 *   Input.The descriptor of the convolution operation. For detailed information,
 *   see ::cnnlConvolutionDescriptor_t.
 * @param[in] cast_mode
 *   Input. The descriptor of the cast mode. For detailed information,
 *   see ::cnnlConvolutionCastMode_t.
 * @param[in] algo
 *   Input. The algorithm used to compute the convolution. The algorithms are defined in the
 *  ::cnnlConvolutionForwardAlgo_t enum. You can get the best suited algorithm
 *  with the ::cnnlGetConvolutionForwardAlgo function.
 * @param[out] size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in
 *   the convolution forward inference operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlGetConvolutionForwardAlgo function.
 *   You also need to call
 *   the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions
 *   to create and set the tensor descriptors \p x, \p w, \p y before calling this function.
 * - The allocated extra workspace should be passed to the ::cnnlConvolutionForwardInference
 *   function to perform the convolution forward inference operation.
 *
 * @note
 * - The parameter cast_mode must be set to CNNL_NO_QUANTIZE, when input onchip type is float or half.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - See the Cambricon CNNL sample code in ``samples/conv_sample/conv_sample.cc``.
 */
CNNL_DEPRECATED_FOR(cnnlGetConvolutionForwardWorkspaceSize)
cnnlStatus_t CNNL_WIN_API
cnnlGetConvolutionForwardInferenceWorkspaceSize(cnnlHandle_t handle,
                                                const cnnlTensorDescriptor_t x,
                                                const cnnlTensorDescriptor_t w,
                                                const cnnlTensorDescriptor_t y,
                                                const cnnlTensorDescriptor_t bias,
                                                const cnnlConvolutionDescriptor_t conv_desc,
                                                const cnnlConvolutionCastMode_t cast_mode,
                                                const cnnlConvolutionForwardAlgo_t algo,
                                                size_t *size);

// Group:ConvolutionForward
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace
 * to optimize the convolution forward operation.
 *
 * The size of the extra workspace is based on the given information of the quantization convolution
 * forward operation, including the input tensor descriptor \p x, filter tensor descriptor \p w,
 * output tensor descriptor \p y, convolution descriptor \p conv_desc, and the convolution
 * algorithm \p algo. For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @deprecated
 * This function is deprecated and will be removed in future
 *   release. Use ::cnnlGetConvolutionForwardWorkspaceSize instead, which supports
 *   ::cnnlSetConvolutionDescriptorQuant to set the quantization descriptor.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the quantization convolution operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  x
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] w
 *   Input. The descriptor of the filter tensor used as a filter in the
 *   convolution operation. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. The descriptor of the output tensor. You can get the shape of the
 *   tensor with the ::cnnlGetConvolutionForwardOutputDim function.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] bias
 *   Input. The descriptor of the bias tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] conv_desc
 *   Input.The descriptor of the convolution operation. For detailed information,
 *   see ::cnnlConvolutionDescriptor_t.
 * @param[in] algo
 *   Input. The algorithm used to compute the convolution. The algorithms are defined in the
 *  ::cnnlConvolutionForwardAlgo_t enum. You can get the best suited algorithm
 *  with the ::cnnlGetConvolutionForwardAlgo function.
 * @param[out] size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in
 *   the quantization convolution forward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlGetConvolutionForwardAlgo function.
 *   You also need to call
 *   the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions
 *   to create and set the tensor descriptors \p x, \p w, \p y before calling this function.
 * - The allocated extra workspace should be passed to the ::cnnlQuantizeConvolutionForward
 *   function to perform the quantization convolution forward operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - See the Cambricon CNNL sample code in ``samples/conv_sample/conv_sample.cc``.
 */
CNNL_DEPRECATED_FOR(cnnlGetConvolutionForwardWorkspaceSize)
cnnlStatus_t CNNL_WIN_API
cnnlGetQuantizeConvolutionForwardWorkspaceSize(cnnlHandle_t handle,
                                               const cnnlTensorDescriptor_t x,
                                               const cnnlTensorDescriptor_t w,
                                               const cnnlTensorDescriptor_t y,
                                               const cnnlTensorDescriptor_t bias,
                                               const cnnlConvolutionDescriptor_t conv_desc,
                                               const cnnlConvolutionForwardAlgo_t algo,
                                               size_t *size);

// Group:ConvolutionForward
/*!
 * @brief Computes a 2D or 3D cross-correlation on input tensor \p x_ptr with the filter
 * \p w_ptr, and returns the results in the output tensor \p y_ptr. For more information
 * about quantization, see "Cambricon CNNL User Guide".
 *
 * This function needs extra MLU memory as the workspace to improve the convolution
 * performance. You can get the workspace size
 * with the ::cnnlGetConvolutionForwardWorkspaceSize function. The convolution
 * operation is computed based on the convolution algorithm set in \p algo.
 * You can call the ::cnnlGetConvolutionForwardAlgo function to get the most
 * suited algorithm.
 *
 * Depthwise convolution operation is performed when the \p group_count parameter of the convolution
 * descriptor \p conv_desc is set to the number of channels of the input tensor \p x_desc.
 *
 * The scaling factors \p alpha and \p beta are not supported currently.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the convolution operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] conv_desc
 *   Input. The descriptor of the convolution operation. For detailed information,
 *   see ::cnnlConvolutionDescriptor_t.
 * @param[in] algo
 *   Input. The algorithm used to compute the convolution. The algorithms are
 *   defined in the ::cnnlConvolutionForwardAlgo_t enum. You can get the best
 *   suited algorithm with the ::cnnlGetConvolutionForwardAlgo function.
 * @param[in] alpha, beta
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x_ptr
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] w_desc
 *   Input. The descriptor of the filter tensor used as a filter in the
 *   convolution operation. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] w_ptr
 *   Input. Pointer to the MLU memory that stores the filter tensor.
 * @param[in] bias_desc
 *   Input. The descriptor of the bias tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] bias_ptr
 *   Input. Pointer to the MLU memory that stores the bias tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   convolution operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the convolution operation. You can get the size of the workspace with
 *   the ::cnnlGetConvolutionForwardWorkspaceSize function.
 * @param[out] y_desc
 *   Output. The descriptor of the output tensor. You can get the shape of the
 *   tensor with the ::cnnlGetConvolutionForwardOutputDim function.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y_ptr
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 *
 * @par Formula
 * - See "Convolution Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for
 *   input tensor \p x_ptr, filter tensor \p w_ptr, bias tensor \p bias_ptr
 *   and output tensor \p y_ptr.
 *   Note that the combinations of bias tensor and output tensor must be half-half
 *   or float-float.
 *   - input tensor:
 *     - int8, int16 (on MLU200 series and CE3226).
 *     - int8, int16, half, float (on MLU300 series and MLU500 series).
 *     - int8, int16 (on 1V).
 *   - filter tensor:
 *     - int8, int16 (on MLU200 series and CE3226).
 *     - int8, int16, half, float (on MLU300 series and MLU500 series).
 *     - int8, int16 (on 1V).
 *   - bias tensor: half, float.
 *   - output tensor: half, float.
 * - If the group_count is equal to the number of input channels,
 *   the input tensor and filter tensor can also be half or float.
 *   The combinations of data types are shown below with the following order:
 *   \p input - \p filter - \p output.
 *   - half-half-half.
 *   - float-float-float.
 * - This function also supports the combinations of the following data types
 *   for input tensor, filter tensor and output tensor on MLU300 series or above.
 *   - input tensor, filter tensor, output tensor: half, half, half.
 *
 *     In some scenarios requiring higher precision, the computation type in
 *     \p conv_desc can be set to float to enhance computational accuracy.
 *   - input tensor, filter tensor, output tensor: float, float, float.
 * - On MLU500 series, this function additionally supports the following data type combinations:
 *   - The offchip data type of input tensor, filter tensor, bias tensor
 *     and output tensor: bfloat16_t.
 *   - The onchip data type of input tensor, filter tensor and bias tensor: bfloat16_t.
 *   - The onchip data type of onput tensor: float.
 *   - The compute_type of conv_desc: float.
 *
 * @par Data Layout
 * - The supported data layouts of the input tensor, filter tensor, bias tensor, and
 *   output tensor are as follows:
 * - If \p dimNb is set to 4:
 *   - input tensor: \p CNNL_LAYOUT_NHWC.
 *   - filter tensor: \p CNNL_LAYOUT_NHWC and \p CNNL_LAYOUT_HWCN.
 *   - bias tensor: \p CNNL_LAYOUT_NHWC or \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_NHWC.
 * - If \p dimNb is set to 5:
 *   - input tensor: \p CNNL_LAYOUT_NDHWC.
 *   - filter tensor: \p CNNL_LAYOUT_NDHWC.
 *   - bias tensor: \p CNNL_LAYOUT_NDHWC or \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_NDHWC.
 *
 * @par Scale Limitation
 * - The input tensor, output tensor, and the convolution descriptor
 *   (including pad, stride, dilation, and group_count) must meet the following
 *   requirements:
 *   - \p pad >= 0
 *   - \p stride >= 1
 *   - \p dilation >= 1
 *   - \p group_count should be greater than or equal to 1 and less than or equal to
 *     the number of channels in the input tensor, input channels and output channels
 *     must both be divisible by group_count.
 *   - input tensor: \p batch > 0, \p height > 0, \p width > 0, \p channel > 0
 *   - filter tensor: If \p dimNb is set to 5, the following limitations must meet:
 *     - Only supports \p depth <= 7, \p height <=7, \p width <= 7.
 *     - Size of filter tensor should not be too large, must meet limitation below:
 *       \p kh * \p kw * max(128 / sizeof(input_dtype), 64 / sizeof(filter_dtype)) *
 *       sizeof(filter_dtype) <= 8192 on MLU220 and MLU290; <= 16384 on MLU270.
 *   - output tensor: \p height > 0, \p width > 0, \p channel > 0
 *   - onchip_dtype of y_desc must be the same as compute_type of conv_desc.
 *   - dtype of y_desc must be the same as compute_type of conv_desc
 *     when \p dimNb is set to 5.
 *
 * @par API Dependency
 * - Before calling this function to implement convolution, you need to prepare
 *   all the parameters passed to this function. See each parameter description
 *   for details.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, when \p dimNb is set to 4, set the layout of
 *   the input tensor, output tensor and bias tensor to NHWC,
 *   and if the operation is depthwise, set the layout of the filter tensor to HWCN, otherwise set
 *   the layout of the filter tensor to NHWC.
 *
 * @note
 * - When data type of output tensor is CNNL_DTYPE_HALF,
 *   you can set compute_type of conv_desc to CNNL_DTYPE_FLOAT to get higher precision.
 *   In this case, the type of the parameter \p bias can only be CNNL_DTYPE_HALF in 2D convolution.
 * - After CNNL 2.0, the combination of half input data type and int16 input onchip data type
 *   will be deprecated.
 * - After CNNL 2.0, the combination of half filter data type and int16 filter onchip data type
 *   will be deprecated.
 * - After CNNL 2.0, the combination of int16 output data type and half output onchip data type
 *   will be deprecated.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the convolution forward operation is as follows:
     @verbatim
      input three arrays by 1 * 3 * 3 * 2, 1 * 1 * 1 * 1 and 1 * 3 * 3 * 2 -->
          input: [[[[5, 1], [8, 1], [6, 4]],
               [[3, 8], [2,6], [0, 6]],
               [[8, 5], [7,4], [9, 6]]]]

      --> bias: [[[[1]]]]

      --> filter: [[[[1, 5], [7,5], [4, 2]],
                    [[1, 8], [3,8], [6, 2]],
                    [[4, 8], [5,0], [9, 5]]]]

      param:
        pad: (1, 1, 1, 1), stride: (2, 2), dilation: (1, 1),

      output array by 1 * 2 * 2 * 1 --> output: [[[[137], [123]],
                                                  [[196], [177]]]]
     @endverbatim
 * - For the example of how to program with the convolution functions,
 *   see the Cambricon CNNL sample code in ``samples/conv_sample/conv_sample.cc``
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/nn/conv1d.
 * - https://www.tensorflow.org/api_docs/python/tf/nn/conv2d.
 * - Gradient-Based Learning Applied to Document Recognition, Yann Lecun, 1998.
 * - Multi-scale Context Aggregation by Dilated Convolution, Fisher Yu, 2016.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlConvolutionForward(cnnlHandle_t handle,
                                                 const cnnlConvolutionDescriptor_t conv_desc,
                                                 cnnlConvolutionForwardAlgo_t algo,
                                                 const void *alpha,
                                                 const cnnlTensorDescriptor_t x_desc,
                                                 const void *x_ptr,
                                                 const cnnlTensorDescriptor_t w_desc,
                                                 const void *w_ptr,
                                                 const cnnlTensorDescriptor_t bias_desc,
                                                 const void *bias_ptr,
                                                 void *workspace,
                                                 size_t workspace_size,
                                                 const void *beta,
                                                 const cnnlTensorDescriptor_t y_desc,
                                                 void *y_ptr);
// Group:ConvolutionForward
/*!
 * @brief Converts the floating-point data of input tensor \p x_ptr and
 * filter \p w_ptr into fixed-point numbers according to the quantization parameters,
 * then computes a 2D or 3D cross-correlation on them,
 * and returns the results in the output tensor \p y_ptr. For more information about quantization,
 * see "Cambricon CNNL User Guide".
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *  Use ::cnnlConvolutionForward instead, which supports
 *   ::cnnlConvolutionForward to set the quantization descriptor.
 *
 * This function needs extra MLU memory as the workspace to improve the convolution
 * performance. You can get the workspace size
 * with the ::cnnlGetConvolutionForwardWorkspaceSize function. The convolution
 * operation is computed based on the convolution algorithm set in \p algo.
 * You can call the ::cnnlGetConvolutionForwardAlgorithm function to get the most
 * suited algorithm.
 *
 * Depthwise convolution operation is performed when the \p group_count parameter of the convolution
 * descriptor \p conv_desc is set to the number of channels of the input tensor \p x_desc.
 *
 * The scaling factors \p alpha and \p beta are not supported currently.
 *
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the convolution operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] conv_desc
 *   Input. The descriptor of the convolution operation. For detailed information,
 *   see ::cnnlConvolutionDescriptor_t.
 * @param[in] algo
 *   Input. The algorithm used to compute the convolution. The algorithms are
 *   defined in the ::cnnlConvolutionForwardAlgo_t enum. You can get the best
 *   suited algorithm with the ::cnnlGetConvolutionForwardAlgo function.
 * @param[in] alpha, beta
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x_ptr
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] x_position
 *   Input. Pointer to the MLU memory that stores the position factor
 *   for quantizing the input tensor.
 * @param[in] x_scale
 *   Input. Pointer to the MLU memory that stores the scale factor
 *   for quantizing the input tensor.
 * @param[in] x_offset
 *   Input. Pointer to the MLU memory that stores the offset factor
 *   for quantizing the input tensor.
 * @param[in] w_desc
 *   Input. The descriptor of the filter tensor used as a filter in the
 *   convolution operation. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] w_ptr
 *   Input. Pointer to the MLU memory that stores the filter tensor.
 * @param[in] w_position
 *   Input. Pointer to the MLU memory that stores the position factor
 *   for quantizing the filter tensor.
 * @param[in] w_scale
 *   Input. Pointer to the MLU memory that stores the scale factor
 *   for quantizing the filter tensor.
 * @param[in] w_offset
 *   Input. Pointer to the MLU memory that stores the offset factor
 *   for quantizing the filter tensor.
 * @param[in] bias_desc
 *   Input. The descriptor of the bias tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] bias_ptr
 *   Input. Pointer to the MLU memory that stores the bias tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   convolution operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the convolution operation. You can get the size of the workspace with
 *   the ::cnnlGetConvolutionForwardWorkspaceSize function.
 * @param[in] y_desc
 *   Output. The descriptor of the output tensor. You can get the shape of the
 *   tensor with the ::cnnlGetConvolutionForwardOutputDim function.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y_ptr
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 *
 * @par Formula
 * - See "Convolution Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The offchip data types should be set by following rules:
 *   - If the offchip data types of input tensor and filter tensor are fix-point data types,
 *     this function supports any combinations of the following data types.
 *     - input tensor:
         - int8, int16 (on MLU200 series, MLU300 series, CE3226 and MLU500 series).
         - int8, int16 (on 1V).
 *     - filter tensor:
         - int8, int16 (on MLU200 series, MLU300 series, CE3226 and MLU500 series).
         - int8, int16 (on 1V).
 *   - If the offchip data types of input tensor or filter tensor are floating-point data types,
 *     they should be set by following rules:
 *     - Offchip data types of input tensor and filter tensor must be the same.
 *       - input tensor: half; filter tensor: half
 *       - input tensor: float; filter tensor: float
 *     - Floating-point convolution only supports on MLU300 series and MLU500 series. On other platforms,
 *       if offchip data types of input tensor and filter tensor are floating-point data types,
 *       their onchip data types must be specified, and their onchip data types must be fix-point
 *       data types.
 *       Onchip data types of input tensor and filter tensor supports any combinations below:
 *       - input tensor:
 *         - int8, int16 (on MLU200 series, MLU300 series, CE3226 and MLU500 series).
 *         - int8, int16 (on 1V).
 *       - filter tensor:
 *         - int8, int16 (on MLU200 series, MLU300 series and CE3226 and MLU500 series).
 *         - int8, int16 (on 1V).
 *   - The offchip data types of output tensor and bias tensor should be set by following rules:
 *     - Offchip data types of output tensor and bias tensor must be floating-point data types
 *       and must be the same.
 *       - output tensor: half; bias tensor: half
 *       - output tensor: float; bias tensor: float
 *     - If offchip data types of input tensor and filter tensor are floating-point data types,
 *       offchip data types of output tensor must be the same as input tensor and filter tensor.
 *   - You need to call ::cnnlSetTensorDescriptor to set \p offchip_dtype in \p x_desc and \p w_desc.
 *
 * - The onchip data types should be set by following rules:
 *   - If the onchip data types of input tensor and filter tensor are fix-point, this function supports
 *     any combinations of the following data types.
 *     - input tensor:
 *       - int8, int16 (on MLU200 series, MLU300 series, CE3226 and MLU500 series).
 *       - int8, int16 on 1V.
 *     - filter tensor:
 *       - int8, int16 (on MLU200 series, MLU300 series, CE3226 and MLU500 series).
 *       - int8, int16 (on 1V).
 *   - Floating-point convolution only supports on MLU300 series and MLU500 series. Onchip data types of input tensor,
 *     filter tensor and output tensor must be the combinations below:
 *     - input tensor: half; filter tensor: half; output tensor: half
 *     - input tensor: float; filter tensor: float; output tensor: float
 *   - You need to call ::cnnlSetTensorDescriptorOnchipDataType to set \p onchip_dtype in
 *     \p x_desc and \p w_desc.
 *     And you do not need to call ::cnnlSetTensorDescriptorOnchipDataType to set \p onchip_dtype
 *     in \p y_desc. If you call ::cnnlSetTensorDescriptorOnchipDataType to set \p onchip_dtype in
 *     \p y_desc, it must be the same as \p compute_type in \p conv_desc.
 *
 * @par Data Layout
 * - The supported data layouts of the input tensor, filter tensor, bias tensor, and
 *   output tensor are as follows:
 * - If \p dimNb is set to 4:
 *   - input tensor: \p CNNL_LAYOUT_NHWC.
 *   - filter tensor: \p CNNL_LAYOUT_NHWC and \p CNNL_LAYOUT_HWCN.
 *   - bias tensor: \p CNNL_LAYOUT_NHWC or \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_NHWC.
 * - If \p dimNb is set to 5:
 *   - input tensor: \p CNNL_LAYOUT_NDHWC.
 *   - filter tensor: \p CNNL_LAYOUT_NDHWC.
 *   - bias tensor: \p CNNL_LAYOUT_NDHWC or \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_NDHWC.
 *
 * @par Scale Limitation
 * - The input tensor, output tensor, and the convolution descriptor
 *   (including pad, stride, dilation, and group_count) must meet the following
 *   requirements:
 *   - \p pad >= 0
 *   - \p stride >= 1
 *   - \p dilation >= 1
 *   - \p group_count should be greater than or equal to 1 and less than or equal to
 *     the number of channels in the input tensor, input channels and output channels
 *     must both be divisible by group_count.
 *   - input tensor: \p batch > 0, \p height > 0, \p width > 0, \p channel > 0
 *   - output tensor: \p height > 0, \p width > 0, \p channel > 0
 *   - onchip_dtype of y_desc must be the same as compute_type of conv_desc.
 *   - dtype of y_desc must be the same as compute_type of conv_desc
 *     when \p dimNb is set to 5.
 *
 * @par API Dependency
 * - Before calling this function to implement convolution, you need to prepare
 *   all the parameters passed to this function. See each parameter description
 *   for details.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, when \p dimNb is set to 4, set the layout of
 *   the input tensor, output tensor and bias tensor to NHWC,
 *   and if the operation is depthwise, set the layout of the filter tensor to HWCN, otherwise set
 *   the layout of the filter tensor to NHWC.
 *
 * @note
 * - This function does not support offline asymmetric quantization currently.
 * - When data type of output tensor is CNNL_DTYPE_HALF,
 *   you can set compute_type of conv_desc to CNNL_DTYPE_FLOAT to get higher precision.
 *   In this case, the type of the parameter \p bias can only be CNNL_DTYPE_HALF in 2D convolution.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the convolution forward operation is as follows:
     @verbatim
      input three arrays by 1 * 3 * 3 * 2, 1 * 1 * 1 * 1 and 1 * 3 * 3 * 2 -->
          input: [[[[5, 1], [8, 1], [6, 4]],
               [[3, 8], [2,6], [0, 6]],
               [[8, 5], [7,4], [9, 6]]]]

      --> bias: [[[[1]]]]

      --> filter: [[[[1, 5], [7,5], [4, 2]],
                    [[1, 8], [3,8], [6, 2]],
                    [[4, 8], [5,0], [9, 5]]]]

      param:
        pad: (1, 1, 1, 1), stride: (2, 2), dilation: (1, 1),

      output array by 1 * 2 * 2 * 1 --> output: [[[[137], [123]],
                                                  [[196], [177]]]]
     @endverbatim
 * - For the example of how to program with the convolution functions,
 *   see the Cambricon CNNL sample code in ``samples/conv_sample/conv_sample.cc``
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/nn/conv1d
 * - https://www.tensorflow.org/api_docs/python/tf/nn/conv2d
 * - Gradient-Based Learning Applied to Document Recognition, Yann Lecun, 1998.
 * - Multi-scale Context Aggregation by Dilated Convolution, Fisher Yu, 2016.
 *
 */
CNNL_DEPRECATED_FOR(cnnlConvolutionForward)
cnnlStatus_t CNNL_WIN_API cnnlQuantizeConvolutionForward(cnnlHandle_t handle,
                                   const cnnlConvolutionDescriptor_t conv_desc,
                                   cnnlConvolutionForwardAlgo_t algo,
                                   const void *alpha,
                                   const cnnlTensorDescriptor_t x_desc,
                                   const void *x_ptr,
                                   const void *x_position,
                                   const void *x_scale,
                                   const void *x_offset,
                                   const cnnlTensorDescriptor_t w_desc,
                                   const void *w_ptr,
                                   const void *w_position,
                                   const void *w_scale,
                                   const void *w_offset,
                                   const cnnlTensorDescriptor_t bias_desc,
                                   const void *bias_ptr,
                                   void *workspace,
                                   size_t workspace_size,
                                   const void *beta,
                                   const cnnlTensorDescriptor_t y_desc,
                                   void *y_ptr);
// Group:ConvolutionForward
/*!
 * @brief Converts the floating-point data of input tensor \p x_ptr into
 * fixed-point numbers, and computes a 2D or 3D cross-correction on the
 * fixed-point input tensor with the filter \p w_ptr. Then converts the
 * floating-point computing results into fixed-point, and returns the
 * fixed-point results in the output tensor \p y_ptr. This function is only
 * used for inference.
 *
 * This function needs extra MLU memory as the workspace to improve the convolution
 * performance. You can get the workspace size
 * with the ::cnnlGetConvolutionForwardWorkspaceSize function. The convolution
 * operation is computed based on the convolution algorithm set in \p algo.
 * You can call the ::cnnlGetConvolutionForwardAlgo function to get the most
 * suited algorithm.
 *
 * Depthwise convolution operation is performed when the \p group_count parameter of the convolution
 * descriptor \p conv_desc is set to the number of channels of the input tensor \p x_desc.
 *
 * To set the factors for quantization:
 * - If offline symmetric quantization with position was used, you need to call the
 *   ::cnnlSetTensorDescriptorPosition function to set the position factor used in the fixed-point
 *   quantization.
 * - If offline symmetric quantization with position and scale factors was used,
 *   you need to call the ::cnnlSetTensorDescriptorPositionAndScale function to set the position
 *   and scale factors used in fixed-point quantization.
 * - If offline asymmetric quantization was used, you need to call the
 *   ::cnnlSetTensorDescriptorPositionScaleAndOffset function to set the position, scale and offset
 *   factors used in fixed-point quantization.
 * - For more information about quantization, see "Cambricon CNNL User Guide".
 *
 * The scaling factors \p alpha and \p beta are not supported currently.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *  Use ::cnnlConvolutionForward instead, which supports
 *   ::cnnlConvolutionForward to set quantization descriptor.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the convolution operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] conv_desc
 *   Input. The descriptor of the convolution operation. For detailed information,
 *   see ::cnnlConvolutionDescriptor_t.
 * @param[in] cast_mode
 *   Input. The descriptor of the cast mode. For detailed information,
 *   see ::cnnlConvolutionCastMode_t.
 * @param[in] algo
 *   Input. The algorithm used to compute the convolution. The algorithms are
 *   defined in the ::cnnlConvolutionForwardAlgo_t enumeration. You can get the best
 *   suited algorithm with the ::cnnlGetConvolutionForwardAlgo function.
 * @param[in] alpha, beta
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x_ptr
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] w_desc
 *   Input. The descriptor of the filter tensor used as a filter in the
 *   convolution operation. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] w_ptr
 *   Input. Pointer to the MLU memory that stores the filter tensor.
 * @param[in] bias_desc
 *   Input. The descriptor of the bias tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] bias_ptr
 *   Input. Pointer to the MLU memory that stores the bias tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   convolution operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the convolution operation. You can get the size of the workspace with
 *   the ::cnnlGetConvolutionForwardWorkspaceSize function.
 * @param[in] y_desc
 *   Output. The descriptor of the output tensor. You can get the shape of the
 *   tensor with the ::cnnlGetConvolutionForwardOutputDim function.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y_ptr
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 *
 * @par Formula
 * - See "Convolution Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The \p x_desc data type should be set with the following rules:
 *   - If \p dtype in \p x_desc is fix-point type, the x_desc onchip_dtype will be automatically set
 *     to x_desc dtype, and you do not need to call ::cnnlSetTensorDescriptorOnchipDataType to set
 *     onchip_dtype in x_desc.
 *   - If \p dtype in \p x_desc is float-point type, it must be the same as \p compute_type in \p
 *     conv_desc, and
 *     - On MLU200 series/CE3226/CE3255/1V, you need to call
 *       ::cnnlSetTensorDescriptorOnchipDataType to set onchip_dtype in \p x_desc to fix-point type.
 *     - On MLU300 series and MLU500 series,
 *       - If \p x_desc onchip_dtype is float-point type, the x_desc onchip_dtype will be
 *         automatically set.
 *       - If \p x_desc onchip_dtype is not float-point type, you need to call
 *         ::cnnlSetTensorDescriptorOnchipDataType to set onchip_dtype in \p x_desc to fix-point
 *         type.
 * - The \p w_desc data type should be set with the following rules:
 *   - If \p dtype in \p w_desc is fix-point type, the w_desc onchip_dtype will be automatically set
 *     to w_desc dtype, and you do not need to call ::cnnlSetTensorDescriptorOnchipDataType to set
 *     onchip_dtype in w_desc.
 *   - If \p dtype in \p w_desc is float-point type,
 *     - On MLU200 series/CE3226/CE3255/1V, you need to call
 *       ::cnnlSetTensorDescriptorOnchipDataType to set \p onchip_dtype in \p w_desc to fix-point
 *       type and ::cnnlHostReorderConvData to reorder filter data on host.
 *     - On MLU300 series and MLU500 series,
 *       - If \p w_desc onchip_dtype is float-point type, the w_desc onchip_dtype will be
 *         automatically set.
 *       - If \p w_desc onchip_dtype is not float-point type, you need to call
 *         ::cnnlSetTensorDescriptorOnchipDataType to set onchip_dtype in \p w_desc to fix-point
 *         type and ::cnnlHostReorderConvData to reorder filter data on host.
 *   - The \p width of \p onchip_dtype in \p w_desc must be less than or the same as \p width of \p
 *     onchip_dtype in \p x_desc.
 * - The \p bias_desc data type should be set with the following rules:
 *   - If \p dtype in \p bias_desc is the same as \p compute_type in \p conv_desc, the \p
 *     onchip_dtype in \p bias_desc will be automatically set \p dtype in \p bias_desc, and you do
 *     not need to call ::cnnlSetTensorDescriptorOnchipDataType to set \p onchip_dtype in \p bias_desc.
 *   - If \p dtype in \p bias_desc is not same as \p compute_type in \p conv_desc, you need to call
 *     ::cnnlSetTensorDescriptorOnchipDataType to set \p onchip_dtype in \p bias_desc to \p
 *     compute_type in \p conv_desc and ::cnnlHostReorderConvData to reorder bias data on host.
 * - You do not need to set \p onchip_dtype in \p y_desc. When \p dtype in \p y_desc is fix-point
 *   type, it must be the same as \p onchip_dtype in \p x_desc. When \p dtype in \p y_desc is
 *   float-point type, it must be the same as \p compute_type in \p conv_desc.
 * - On MLU300 series and MLU500 series, \p x_desc onchip_dtype, w_desc onchip_dtype and
 *   y_desc dtype are half.
 *
 *   In some scenarios requiring higher precision, the computation type in \p conv_desc
 *    and \p y_desc onchip_dtype type can be set to float to enhance computational accuracy.
 * - On MLU500 series, this function additionally supports the following data type combinations:
 *   - The offchip data type of x_desc, w_desc, bias_desc and y_desc: bfloat16_t.
 *   - The onchip data type of x_desc, w_desc and bias_desc: bfloat16_t.
 *   - The onchip data type of y_desc: float.
 *   - The compute_type of conv_desc: float.
 *
 * @par Data Layout
 * - The supported data layouts of the input tensor, filter tensor, bias tensor, and
 *   output tensor are as follows:
 * - If \p dimNb is set to 4:
 *   - input tensor: \p CNNL_LAYOUT_NHWC.
 *   - filter tensor: \p CNNL_LAYOUT_NHWC and \p CNNL_LAYOUT_HWCN.
 *   - bias tensor: \p CNNL_LAYOUT_NHWC or \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_NHWC.
 * - If \p dimNb is set to 5:
 *   - input tensor: \p CNNL_LAYOUT_NDHWC.
 *   - filter tensor: \p CNNL_LAYOUT_NDHWC.
 *   - bias tensor: \p CNNL_LAYOUT_NDHWC or \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_NDHWC.
 *
 * @par Scale Limitation
 * - The input tensor, output tensor, and the convolution descriptor
 *   (including pad, stride, dilation, and group_count) must meet the following
 *   requirements:
 *   - \p pad >= 0
 *   - \p stride >= 1
 *   - \p dilation >= 1
 *   - \p group_count should be greater than or equal to 1 and less than or equal to
 *     the number of channels in the input tensor, input channels and output channels
 *     must both be divisible by group_count.
 *   - input tensor: \p batch > 0, \p height > 0, \p width > 0, \p channel > 0
 *   - output tensor: \p height > 0, \p width > 0, \p channel > 0
 *   - onchip_dtype of y_desc must be the same as compute_type of conv_desc.
 *   - dtype of y_desc must be the same as compute_type of conv_desc
 *     when \p dimNb is set to 5.
 *
 * @par API Dependency
 * - Before calling this function to implement convolution, you need to prepare
 *   all the parameters passed to this function. See each parameter description
 *   for details.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, when \p dimNb is set to 4, set the layout of
 *   the input tensor, output tensor and bias tensor to NHWC,
 *   and if the operation is depthwise, set the layout of the filter tensor to HWCN, otherwise set
 *   the layout of the filter tensor to NHWC.
 *
 * @note
 * - The parameter cast_mode must be set to CNNL_NO_QUANTIZE, when input onchip type is float or half.
 * - When data type of output tensor is CNNL_DTYPE_HALF,
 *   you can set compute_type of conv_desc to CNNL_DTYPE_FLOAT to get higher precision.
 *   In this case, this feature only supports 3D convolution.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the convolution forward operation is as follows:
     @verbatim
      input three arrays by 1 * 3 * 3 * 2, 1 * 1 * 1 * 1 and 1 * 3 * 3 * 2 -->
          input: [[[[5, 1], [8, 1], [6, 4]],
               [[3, 8], [2,6], [0, 6]],
               [[8, 5], [7,4], [9, 6]]]]

      --> bias: [[[[1]]]]

      --> filter: [[[[1, 5], [7,5], [4, 2]],
                    [[1, 8], [3,8], [6, 2]],
                    [[4, 8], [5,0], [9, 5]]]]

      param:
        pad: (1, 1, 1, 1), stride: (2, 2), dilation: (1, 1),

      output array by 1 * 2 * 2 * 1 --> output: [[[[137], [123]],
                                                  [[196], [177]]]]
     @endverbatim
 * - For the example of how to program with the convolution functions,
 *   see the Cambricon CNNL sample code in ``samples/conv_sample/conv_sample.cc``
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/nn/conv1d
 * - https://www.tensorflow.org/api_docs/python/tf/nn/conv2d
 * - Gradient-Based Learning Applied to Document Recognition, Yann Lecun, 1998.
 * - Multi-scale Context Aggregation by Dilated Convolution, Fisher Yu, 2016.
 *
 */
CNNL_DEPRECATED_FOR(cnnlConvolutionForward)
cnnlStatus_t CNNL_WIN_API
cnnlConvolutionForwardInference(cnnlHandle_t handle,
                                const cnnlConvolutionDescriptor_t conv_desc,
                                const cnnlConvolutionCastMode_t cast_mode,
                                cnnlConvolutionForwardAlgo_t algo,
                                const void *alpha,
                                const cnnlTensorDescriptor_t x_desc,
                                const void *x_ptr,
                                const cnnlTensorDescriptor_t w_desc,
                                const void *w_ptr,
                                const cnnlTensorDescriptor_t bias_desc,
                                const void *bias_ptr,
                                void *workspace,
                                size_t workspace_size,
                                const void *beta,
                                const cnnlTensorDescriptor_t y_desc,
                                void *y_ptr);

// Group:StridedSliceBackward
/*!
 * @brief Computes gradients of strided slice. For detailed information, see ::cnnlStridedSlice.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the strided
 *   slice backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] begin
 *   Input. A host pointer to the \p begin data that holds starting indices of each dimension of the output tensor.
 * @param[in] end
 *   Input. A host pointer to the \p end data that holds ending indices of each dimension of the output tensor.
 * @param[in] stride
 *   Input. A host pointer to the \p stride data that holds strides of each dimension of the output tensor.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "StridedSliceBackward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \p input and output tensor
 *   \p output. Data type of both tensors should be the same.
 *   - input tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float, bfloat16.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float, bfloat16.
 *
 *   The bfloat16 data type is supported only on MLU500 series.
 * @par Scale Limitation
 * - For each dimension:
 *   - \p stride cannot be 0.
 *   - Convert negative \p begin and \p end to positive values:
 *     - If \p begin < 0: \p begin = \p begin + output_dims.
 *     - If \p end < 0: \p end = \p end + output_dims.
 *   - input_dims must meet the following requirements:
 *     - If (\p stride > 0) and (\p begin < \p end) and (\p end > 0) and (\p begin < output_dims):
 *       input_dims = ceil_div(min(\p end, output_dims) - max(\p begin, 0), \p stride).
 *     - If (\p stride < 0) and (\p begin > \p end) and (\p begin >= 0) and (\p end < output_dims - 1):
 *       input_dims = ceil_div(min(\p begin, output_dims - 1) - max(\p end, -1), abs(\p stride)).
 *     - Otherwise, input_dims = 0.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the strided slice backward operation is as follows:
     @verbatim
     input array by 4 * 4 --> input: [[11, 12, 13, 14],
                                      [21, 22, 23, 24],
                                      [31, 32, 33, 34],
                                      [41, 42, 43, 44]]

     param:
       begin: [1,2], end: [4,8], stride: [1,2]

     output array by 5 * 9 --> output:[[0, 0,  0, 0,  0, 0,  0, 0,  0],
                                       [0, 0, 11, 0, 12, 0, 13, 0, 14]
                                       [0, 0, 21, 0, 22, 0, 23, 0, 24]
                                       [0, 0, 31, 0, 32, 0, 33, 0, 34]
                                       [0, 0, 41, 0, 42, 0, 43, 0, 44]]
     @endverbatim
 *
 * @par Reference
 * - https://tensorflow.org/api_docs/python/tf/raw_ops/StridedSliceGrad
 */
cnnlStatus_t CNNL_WIN_API cnnlStridedSliceBackward(cnnlHandle_t handle,
                                                   const int begin[],
                                                   const int end[],
                                                   const int stride[],
                                                   const cnnlTensorDescriptor_t input_desc,
                                                   const void *input,
                                                   const cnnlTensorDescriptor_t output_desc,
                                                   void *output);

// Group:StdForward
/*!
 * @brief Calculates the standard deviation for each row of the input tensor in a given
 * dimension.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlStdVarMean instead, which supports multiple axes, std and mean output.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the std forward operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] dim
 *   Input. The dimension of \p input to reduce.
 * @param[in] unbiased
 *   Input. Whether to use the unbiased estimation or not. If unbiased is false, then the
 *   standard-deviation will be calculated via the biased estimator.
 *   Otherwise, Bessel's correction will be used.
 * @param[in] input_desc
 *   Input. The descriptors of the \p input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_desc
 *   Input. The descriptors of the \p output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - This function supports the following data types for \p unbiased, \p dim,
 *   input tensor \p input and output tensor \p output. Data type of both tensors should be the
 *   same, and the inputs and outputs message are as follows:
 *   - \p unbiased: bool
 *   - \p dim: int32
 *   - \p input: float, half, bfloat16
 *   - \p output: float, half, bfloat16
 *
 *   The bfloat16 data type is supported only on MLU500 series.
 *
 * @par Reference
 * - https://pytorch.org/docs/1.0.0/torch.html?highlight=std#torch.std
 *
 * @par API Dependency
 * - None.
 *
 * @par Example
 * The example of the std forward operation is as follows:
   @verbatim
   input: a tensor which shape is 4 * 4  --> [[0.2035, 1.2959, 1.8101, -0.4644],
                                               [1.5027, -0.3270, 0.5905, 0.6538],
                                               [-1.5745, 1.3330, -0.5596, -0.6548],
                                               [0.1264, -0.5080, 1.6420, 0.1992]]
    param: dim = 1, unbiased = True

    Then we will get the output:

    output: a tensor by 4 * 1             --> [1.0311, 0.7477, 1.2204, 0.9087]
   @endverbatim
 *
 */

CNNL_DEPRECATED_FOR(cnnlStdVarMean)
cnnlStatus_t CNNL_WIN_API cnnlStdForward(cnnlHandle_t handle,
                                         int dim,
                                         bool unbiased,
                                         const cnnlTensorDescriptor_t input_desc,
                                         const void *input,
                                         const cnnlTensorDescriptor_t output_desc,
                                         const void *output);

// Group:StdBackward
/*!
 * @brief Calculates the inverse gradient of the standard deviation.
 * The corresponding forward calculation of the function is ::cnnlStdForward.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the std backward operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] dim
 *   Input. The dimension of \p input to reduce.
 * @param[in] unbiased
 *   Input. Whether to use the unbiased estimation or not. If unbiased is false, then the
 *   standard-deviation will be calculated via the biased estimator.
 *   Otherwise, Bessel's correction will be used.
 * @param[in] input_desc
 *   Input. The descriptors of the \p input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_desc
 *   Input. The descriptors of the \p output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] output
 *   Input. Pointer to the MLU memory that stores the output tensor.
 * @param[in] diff_grad_desc
 *   Input. The descriptor of the \p diff_grad tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] diff_grad
 *   Input. Pointer to the MLU memeory that stores the gradient tensor.
 * @param[in] diff_output_desc
 *   Input. The descriptor of the \p diff_output tensor. For detailed information.
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] diff_output
 *   Output. Pointer to the MLU memory that stores the output gradient tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \p input, output tensor
 *   \p output, gradient tensor \p diff_grad and output gradient tensor \p diff_output.
 *   Data type of these input tensors should be the same.
 * - \p dim: int32
 * - \p unbiased: bool
 * - \p input: float, half
 * - \p output: float, half
 * - \p diff_grad: float, half.
 * - \p diff_output: float, half.
 *
 * @par Scale Limitation
 * - The value of \p output should be in range of [0.01, 500].
 *
 * @par Reference
 * - https://pytorch.org/docs/1.0.0/torch.html?highlight=std#torch.std
 *
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlStdBackward(cnnlHandle_t handle,
                                          int dim,
                                          bool unbiased,
                                          const cnnlTensorDescriptor_t input_desc,
                                          const void *input,
                                          const cnnlTensorDescriptor_t output_desc,
                                          const void *output,
                                          const cnnlTensorDescriptor_t diff_grad_desc,
                                          const void *diff_grad,
                                          const cnnlTensorDescriptor_t diff_output_desc,
                                          const void *diff_output);

/*! The descriptor of the std_var_mean operation.
 *
 *  You need to call the ::cnnlCreateStdVarMeanDescriptor function to create a descriptor,
 *  and call the ::cnnlSetStdVarMeanDescriptor function to set the information of
 *  std_var_mean operation to the descriptor. Also, you need to destroy the Cambricon CNNL context
 *  at the end with the ::cnnlDestroyStdVarMeanDescriptor function.
 */
typedef struct cnnlStdVarMeanStruct *cnnlStdVarMeanDescriptor_t;

// Group:StdVarMean
/*!
 * @brief Creates a descriptor pointed by \p desc for a std_var_mean operation,
 *        and allocates memory for holding the information about the std_var_mean operation.
 *
 * The information is defined in ::cnnlStdVarMeanDescriptor_t. For more information
 * about descriptor, see "Cambricon CNNL user Guide".
 *
 * @param[out] desc
 *   Output. A host pointer to the std_var_mean descriptor that holds information about
 *   the std_var_mean operation.
 * @par Return
 *   ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetStdVarMeanDescriptor
 *   function to initialize and set information to the std_var_mean descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateStdVarMeanDescriptor(cnnlStdVarMeanDescriptor_t *desc);

// Group:StdMean
/*!
 * @brief Destroys a std_var_mean descriptor \p desc that was previously created with the
 *        ::cnnlCreateStdVarMeanDescriptor function.
 *
 * The std_var_mean descriptor is defined in ::cnnlStdVarMeanDescriptor_t and holds the information
 * about the std_var_mean operation.
 *
 * @param[in] desc
 *   Input. The std_var_mean descriptor to be destroyed. For detailed information,
 *   see ::cnnlStdVarMeanDescriptor_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyStdVarMeanDescriptor(cnnlStdVarMeanDescriptor_t desc);

// Group:StdVarMean
/*!
 * @brief Initializes the std_var_mean descriptor \p desc that was previously created
 *        with the ::cnnlCreateStdVarMeanDescriptor function, and set the information
 *        about the std_var_mean operation to the std_var_mean descriptor \p desc.
 *        The information includes the operator \p mode, axis number \p axis_num,
 *        \p axis to reduce and \p unbiased to specify whether to use Bessel's correction.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlSetStdVarMeanDescriptor_v2 instead, which supports correction.
 *
 * @param[in,out] desc
 *   Input/output. The descriptor of the std_var_mean operation. For detailed information,
 *   see ::cnnlStdVarMeanDescriptor_t.
 * @param[in] mode
 *   Input. Enumeration to specify the std_var_mean mode.
 *   For detailed information, see ::cnnlStdVarMeanOp_t.
 * @param[in] axis_num
 *   Input. The size of axis vector.
 * @param[in] axis
 *   Input. The dimension to reduce. Currently, for each dimension, the value of axis
 *   should be in range of [0,...,dims -1], and should not be the same.
 * @param[in] unbiased
 *   Input. If unbiased is true, Bessel's correction will be used to calculate the variance
 *   and standard deviation. Otherwise Bessel's correction will not be used.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, :: CNNL_STATUS_NOT_SUPPORTED
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlSetStdVarMeanDescriptor_v2)
cnnlStatus_t CNNL_WIN_API cnnlSetStdVarMeanDescriptor(cnnlStdVarMeanDescriptor_t desc,
                                                      const cnnlStdVarMeanOp_t mode,
                                                      const int axis_num,
                                                      const int axis[],
                                                      const bool unbiased);

// Group:StdVarMean
/*!
 * @brief Initializes the std_var_mean descriptor \p desc that was previously created
 *        with the ::cnnlCreateStdVarMeanDescriptor function, and sets the information
 *        about the std_var_mean operation to the std_var_mean descriptor \p desc.
 *        The information includes the operator \p mode, axis number \p axis_num,
 *        \p axis to reduce and \p correction.
 *
 * @param[in,out] desc
 *   Input/output. The descriptor of the std_var_mean operation. For detailed information,
 *   see ::cnnlStdVarMeanDescriptor_t.
 * @param[in] mode
 *   Input. The std_var_mean mode to be specified.
 *   For detailed information, see ::cnnlStdVarMeanOp_t.
 * @param[in] axis_num
 *   Input. The size of the axis vector.
 * @param[in] axis
 *   Input. The dimension to reduce. Currently, for each dimension, the value of axis
 *   should be in range of [0,...,dims -1], and should not be the same.
 * @param[in] correction
 *   Input. The parameter is used to specify the correction for the standard deviation calculation.
 *   If \p correction is 1, it is equal to Bessel's correction
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetStdVarMeanDescriptor_v2(cnnlStdVarMeanDescriptor_t desc,
                                                         const cnnlStdVarMeanOp_t mode,
                                                         const int axis_num,
                                                         const int axis[],
                                                         const double correction);

// Group:StdVarMean
/*!
 * @brief Returns in \p size of the MLU memory that is used as an extra workspace to optimize
 * the std_var_mean operation.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   std_var_mean operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  desc
 *   Input. The descriptor of the std_var_mean operator. For detailed information,
 *   see ::cnnlStdVarMeanDescriptor_t.
 * @param[in]  input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out]  size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used
 *   in the std_var_mean operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None
 */
cnnlStatus_t CNNL_WIN_API cnnlGetStdVarMeanWorkspaceSize(cnnlHandle_t handle,
                                                         const cnnlStdVarMeanDescriptor_t desc,
                                                         const cnnlTensorDescriptor_t input_desc,
                                                         size_t *size);

// Group:StdVarMean
/*!
 *  @brief Calculates the standard deviation, variance and mean of tensor in the given dimension.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the std_var_mean operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] desc
 *   Input. The descriptor of the std_var_mean operation. For detailed information,
 *   see ::cnnlStdVarMeanDescriptor_t.
 * @param[in] input_desc
 *   Input. Descriptor of input data. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] std_desc
 *   Input. Descriptor of the \p std tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] std
 *   Output. Pointer to the MLU memory that stores the \p std tensor.
 * @param[in] var_desc
 *   Input. Descriptor of the \p var tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] var
 *   Output. Pointer to the MLU memory that stores the \p var tensor.
* @param[in] mean_desc
 *   Input. Descriptor of the \p mean tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] mean
 *   Output. Pointer to the MLU memory that stores the \p mean tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the std_var_mean operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] size
 *   Input. The size of the extra workspace in bytes that needs to be used in the std_var_mean operation.
 *   You can get the size of the workspace with the ::cnnlGetStdVarMeanWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH,
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Data Type
 * - Data type of input, std, var and mean should be the same,
 *   and the input and outputs message are as follows:
 *   - \p input: float, half, bfloat16
 *   - \p std: float, half, bfloat16
 *   - \p var: float, half, bfloat16
 *   - \p mean: float, half, bfloat16
 *
 *   The bfloat16 data type is supported only on MLU500 series.
 *
 * @par Data Layout
 * - The supported layout of the input, std, var, and mean tensors must be \p CNNL_LAYOUT_ARRAY.

 * @par API Dependency
 * - Before calling this function to implement std_var_mean, you need to call
 *   ::cnnlCreateStdVarMeanDescriptor and ::cnnlSetStdVarMeanDescriptor to create and set the descriptor
 *   and call ::cnnlGetStdVarMeanWorkspaceSize to get extra MLU memory size in std_var_mean operation.
 * - After calling this function, ::cnnlDestroyStdVarMeanDescriptor needs to be called to
 *   destroy the parameter \p desc.
 *
 * @note
 * - When \p axis_num is greater than 1, the values of axis vector cannot be duplicated.
 *   For example, \p axis can be [1,2,3] but cannot be \p axis = [1,2,2].
 * - The number of \p axis cannot be greater than the size of input.
 *
 * @par Requirements
 * - None.
 *
 * @par Formula
 * - See "StdVarMean" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.std.html?highlight=std#torch.std
 * - https://pytorch.org/docs/stable/generated/torch.std_mean.html?highlight=std_mean#torch.std_mean
 * - https://pytorch.org/docs/stable/generated/torch.var.html?highlight=var#torch.var
 * - https://pytorch.org/docs/stable/generated/torch.var_mean.html?highlight=var_mean#torch.var_mean
 */
cnnlStatus_t CNNL_WIN_API cnnlStdVarMean(cnnlHandle_t handle,
                                         const cnnlStdVarMeanDescriptor_t desc,
                                         const cnnlTensorDescriptor_t input_desc,
                                         const void *input,
                                         void* workspace,
                                         const size_t size,
                                         const cnnlTensorDescriptor_t std_desc,
                                         void *std,
                                         const cnnlTensorDescriptor_t var_desc,
                                         void *var,
                                         const cnnlTensorDescriptor_t mean_desc,
                                         void *mean);


// Group:Nllloss
/*!
 * @brief Returns in \p size of the MLU memory that is used as an extra workspace to optimize
 * the nllloss forward operation.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   nllloss forward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out]  size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used
 *   in the nllloss forward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None
 */
cnnlStatus_t CNNL_WIN_API cnnlGetNlllossWorkspaceSize(cnnlHandle_t handle,
                                                      const cnnlTensorDescriptor_t x_desc,
                                                      size_t *size);
// Group:Nllloss
/*!
 * @brief Computes a Negative-Log-Likelihood Loss, which is used to train a
 * classification problem with C class, on input tensor \p x with \p target
 * and \p filter, and returns the results in the output tensor \p y.
 *
 * This function needs extra MLU memory as the workspace to improve the nllloss forward
 * performance. You can get the workspace size with the
 * ::cnnlGetNlllossWorkspaceSize function. The nllloss operation is computed based
 * on the nllloss algorithm set in \p algorithm.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlNlllossForward_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   nllloss forward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] algorithm
 *   Input. The algorithm used to compute the nllloss. The algorithms are defined in the
 *   ::cnnlNlllossAlgorithm_t enum.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   nllloss forward operation.
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the nllloss forward operation. You can get the size of the workspace with the
 *   ::cnnlGetNlllossWorkspaceSize function.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] t_desc
 *   Input. The descriptor of the target tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] target
 *   Input. Pointer to the MLU memory that stores the target tensor.
 * @param[in] ignore_index
 *   Input. An index that specifies a target value that is ignored and does not
 *   contribute to the input gradient.
 * @param[in] w_desc
 *   Input. The descriptor of the filter tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the filter tensor.
 * @param[in] tf_desc
 *   Input. The descriptor of the total_filter tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] total_filter
 *   Output. Pointer to the MLU memory that stores the total_filter tensor, which
 *   means the sum of the filter.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
  * @par Data Type
 * - Data types of input tensors \p x, \p filter and output tensors \p y, \p total_filter
 *   must be the same while data type of target must be int32. If the data type of target
 *   is int64, see ::cnnlNlllossForward_v2.
 * - The supported data types are as follows:
 *   - x: float, half, bfloat16.
 *   - target: int32.
 *   - filter: float, half, bfloat16.
 *   - y: float, half, bfloat16.
 *   - total_filter: float, half, bfloat16.
 *
 * @par Limitations
 * - The dimension of the input tensor, target tensor, filter tensor and output tensor
 *   must meet the following requirements:
 *   - x: [N, C]
 *   - target: [N]
 *   - filter: [C]
 *   - y: [N] in NONE mode or [1] in SUM and MEAN mode
 * - The value of the target tensor should be in range of [0, C-1] or equal to \p ignore_index.
 * - \p filter and \p w_desc must be NULL or not NULL at the same time.
 * - When \p algorithm is CNNL_REDUCTION_NONE, \p total_filter and \p tf_desc can be NULL.
 * - When \p algorithm is not CNNL_REDUCTION_NONE, \p total_filter and \p tf_desc should not be NULL.
 *
 * @par API Dependency
 * - Before calling this function, you need to call ::cnnlGetNlllossWorkspaceSize
 *   to get the extra workspace size needed in nllloss forward operation.
 *
 * @note
 * - This function only supports 2D input tensor currently.
 *
 * @par Requirements
 * - None.
 *
 * @par Formula
 * - See "Nllloss Forward" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Example
 * - The example of the nllloss forward operation is as follows:
     @verbatim
       input three arrays by 2 * 3, 2 and 3
       --> x: [[1,2,3], [4,5,6]]

       --> target: [0,1]

       --> filter: [1,2,3]

       param:
         ignore_index: 0, algorithm: CNNL_REDUCTION_NONE

       output two arrays by 2 and 1
       --> y: [0,-10]
       --> total_filter: [5]
     @endverbatim
 *
 */
CNNL_DEPRECATED_FOR(cnnlNlllossForward_v2)
cnnlStatus_t CNNL_WIN_API cnnlNlllossForward(cnnlHandle_t handle,
                                             cnnlNlllossAlgorithm_t algorithm,
                                             void *workspace,
                                             size_t workspace_size,
                                             const cnnlTensorDescriptor_t x_desc,
                                             const void *x,
                                             const cnnlTensorDescriptor_t t_desc,
                                             const void *target,
                                             const int ignore_index,
                                             const cnnlTensorDescriptor_t w_desc,
                                             const void *filter,
                                             const cnnlTensorDescriptor_t tf_desc,
                                             void *total_filter,
                                             const cnnlTensorDescriptor_t y_desc,
                                             void *y);
// Group:Nllloss
/*!
 * @brief Computes a NLLLoss (Negative-Log-Likelihood Loss), which is used to train a
 * classification problem with C class, on input tensor \p x with \p target
 * and \p weight, and returns the results in the output tensor \p y. Compared with
 * ::cnnlNlllossForward, this function additionally supports int64 data type for \p target.
 *
 * This function needs extra MLU memory as the workspace to improve the NLLLoss forward
 * performance. You can get the workspace size with the
 * ::cnnlGetNlllossWorkspaceSize function. The NLLLoss operation is computed based
 * on the NLLLoss algorithm set in \p algorithm.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   NLLLoss forward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] algorithm
 *   Input. The algorithm used to compute the NLLLoss. The algorithms are defined in the
 *   ::cnnlNlllossAlgorithm_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] t_desc
 *   Input. The descriptor of the target tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] target
 *   Input. Pointer to the MLU memory that stores the target tensor.
 * @param[in] ignore_index
 *   Input. An index that specifies a target value that is ignored and does not
 *   contribute to the input gradient.
 * @param[in] w_desc
 *   Input. The descriptor of the filter tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] weight
 *   Input. Pointer to the MLU memory that stores the weight tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   NLLLoss forward operation.
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the NLLLoss forward operation. You can get the size of the workspace with the
 *   ::cnnlGetNlllossWorkspaceSize function.
 * @param[in] tw_desc
 *   Input. The descriptor of the total_weight tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] total_weight
 *   Output. Pointer to the MLU memory that stores the total_weight tensor, which
 *   means the sum of the weight.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
  * @par Data Type
 * - Data types of input tensors \p x, \p filter and output tensors \p y, \p total_filter
 *   must be the same while data type of target can be int32 or int64.
 * - The supported data types are as follows:
 *   - x: float, half, bfloat16.
 *   - target: int32, int64.
 *   - filter: float, half, bfloat16.
 *   - y: float, half, bfloat16.
 *   - total_filter: float, half, bfloat16.
 *
 * @par Limitations
 * - The dimension of the input tensor, target tensor, filter tensor and output tensor
 *   must meet the following requirements:
 *   - x: [N, C]
 *   - target: [N]
 *   - filter: [C]
 *   - y: [N] in NONE mode or [1] in SUM and MEAN mode
 * - The value of the target tensor should be in range of [0, C-1] or equal to \p ignore_index.
 * - \p filter and \p w_desc must be NULL or not NULL at the same time.
 * - When \p algorithm is CNNL_REDUCTION_NONE, \p total_filter and \p tf_desc can be NULL.
 * - When \p algorithm is not CNNL_REDUCTION_NONE, \p total_filter and \p tf_desc should not be NULL.
 *
 * @par API Dependency
 * - Before calling this function, you need to call ::cnnlGetNlllossWorkspaceSize
 *   to get the extra workspace size needed in NLLLoss forward operation.
 *
 * @note
 * - This function only supports 2D input tensor currently.
 *
 * @par Requirements
 * - None.
 *
 * @par Formula
 * - See "NLLLoss Forward" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Example
 * - The example of the NLLLoss forward operation is as follows:
     @verbatim
       input three arrays by 2 * 3, 2 and 3
       --> x: [[1,2,3], [4,5,6]]

       --> target: [0,1]

       --> filter: [1,2,3]

       param:
         ignore_index: 0, algorithm: CNNL_REDUCTION_NONE

       output two arrays by 2 and 1
       --> y: [0,-10]
       --> total_filter: [5]
     @endverbatim
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlNlllossForward_v2(cnnlHandle_t handle,
                                                const cnnlNlllossAlgorithm_t algorithm,
                                                const cnnlTensorDescriptor_t x_desc,
                                                const void *x,
                                                const cnnlTensorDescriptor_t t_desc,
                                                const void *target,
                                                const int64_t ignore_index,
                                                const cnnlTensorDescriptor_t w_desc,
                                                const void *weight,
                                                const void *workspace,
                                                const size_t workspace_size,
                                                const cnnlTensorDescriptor_t tw_desc,
                                                void *total_weight,
                                                const cnnlTensorDescriptor_t y_desc,
                                                void *y);

// Group:NlllossBackward
/*!
 * @brief Computes an nllloss backward on input tensor \p diff_y with \p target, \p filter
 * and \p total_filter, and returns the results in the output tensor \p diff_x.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlNlllossBackward_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   nllloss backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] algorithm
 *   Input. The algorithm used to compute the nllloss backward. The algorithms are defined
 *   in the ::cnnlNlllossAlgorithm_t enum.
 * @param[in] diff_y_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] diff_y
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] t_desc
 *   Input. The descriptor of the target tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] target
 *   Input. Pointer to the MLU memory that stores the target tensor.
 * @param[in] ignore_index
 *   Input. An index that specifies a target value that is ignored and does not
 *   contribute to the input gradient.
 * @param[in] w_desc
 *   Input. The descriptor of the filter tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the filter tensor.
 * @param[in] tw_desc
 *   Input. The descriptor of the total_filter tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] total_filter
 *   Input. Pointer to the MLU memory that stores the total_filter tensor, which means
 *   the sum of the filter.
 * @param[in] diff_x_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 * @par Data Type
 * - Data types of input tensors \p diff_y, \p filter, \p total_filter and output tensor \p diff_x
 *   must be the same while data type of target must be int32. If the data type of target is int64,
 *   see ::cnnlNlllossBackward_v2.
 * - The supported data types are as follows:
 *   - diff_y: float, half, bfloat16.
 *   - target: int32.
 *   - filter: float, half, bfloat16.
 *   - total_filter: float, half, bfloat16.
 *   - diff_x: float, half, bfloat16.
 *
 * @par Limitations
 * - The dimension of the input tensor, target tensor, filter tensor and output tensor
 *   must meet the following requirements:
 *   - diff_y: [N] in NONE mode or [1] in SUM and MEAN mode
 *   - target: [N]
 *   - filter: [C]
 *   - diff_x: [N, C]
 * - The value of the target tensor should be in range of [0, C-1] or equal to \p ignore_index.
 * - \p filter and \p w_desc must be NULL or not NULL at the same time.
 * - The \p total_filter and \p tw_desc can be NULL when \p algorithm is CNNL_REDUCTION_NONE or
 *   CNNL_REDUCTION_SUM, otherwise they should have a value and the dimension of \p tw_desc should
 *   be [1].
 *
 * @par API Dependency
 * - Before calling this function to implement nllloss backward, you need to prepare all the
 *   input parameters from the output of ::cnnlNlllossForward.
 *
 * @note
 * - This function only supports 1D input tensor currently.
 *
 * @par Requirements
 * - None.
 *
 * @par Formula
 * - See "Nllloss Backward" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Example
 * - The example of the nllloss backward operation is as follows:
     @verbatim
       input four arrays by 2, 2, 3 and 1
       --> diff_y: [1, 1]

       --> target: [0,1]

       --> filter: [1,2,3]

       --> total_filter: [5]

       param:
         ignore_index: 0, algorithm: CNNL_REDUCTION_NONE

       output array by 2 * 3
       --> diff_x: [[0,0,0], [0,-2,0]]
     @endverbatim
 *
 */
CNNL_DEPRECATED_FOR(cnnlNlllossBackward_v2)
cnnlStatus_t CNNL_WIN_API cnnlNlllossBackward(cnnlHandle_t handle,
                                              cnnlNlllossAlgorithm_t algorithm,
                                              const cnnlTensorDescriptor_t diff_y_desc,
                                              const void *diff_y,
                                              const cnnlTensorDescriptor_t t_desc,
                                              const void *target,
                                              const int ignore_index,
                                              const cnnlTensorDescriptor_t w_desc,
                                              const void *filter,
                                              const cnnlTensorDescriptor_t tw_desc,
                                              const void *total_filter,
                                              const cnnlTensorDescriptor_t diff_x_desc,
                                              void *diff_x);

// Group:NlllossBackward
/*!
 * @brief Computes an nllloss backward on input tensor \p diff_y with \p target, \p weight
 * and \p total_weight, and returns the results in the output tensor \p diff_x. Compared with
 * ::cnnlNlllossBackward, this function additionally supports int64 data type for \p target.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   nllloss backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] algorithm
 *   Input. The algorithm used to compute the nllloss backward. The algorithms are defined
 *   in the ::cnnlNlllossAlgorithm_t.
 * @param[in] diff_y_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] diff_y
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] t_desc
 *   Input. The descriptor of the target tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] target
 *   Input. Pointer to the MLU memory that stores the target tensor.
 * @param[in] ignore_index
 *   Input. An index that specifies a target value that is ignored and does not
 *   contribute to the input gradient.
 * @param[in] w_desc
 *   Input. The descriptor of the weight tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] weight
 *   Input. Pointer to the MLU memory that stores the weight tensor.
 * @param[in] tw_desc
 *   Input. The descriptor of the total_filter tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] total_weight
 *   Input. Pointer to the MLU memory that stores the total_filter tensor, which means
 *   the sum of the weight.
 * @param[in] diff_x_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 * @par Data Type
 * - Data types of input tensors \p diff_y, \p weight, \p total_weight and output tensor \p diff_x
 *   must be the same while data type of target can be int32 or int64.
 * - The supported data types are as follows:
 *   - diff_y: float, half, bfloat16.
 *   - target: int32, int64.
 *   - weight: float, half, bfloat16.
 *   - total_weight: float, half, bfloat16.
 *   - diff_x: float, half, bfloat16.
 *
 * @par Limitations
 * - The dimension of the input tensor, target tensor, weight tensor and output tensor
 *   must meet the following requirements:
 *   - diff_y: [N] in NONE mode or [1] in SUM and MEAN mode
 *   - target: [N]
 *   - weight: [C]
 *   - diff_x: [N, C]
 * - The value of the target tensor should be in range of [0, C-1] or equal to \p ignore_index.
 * - \p weight and \p w_desc must be NULL or not NULL at the same time.
 * - The \p total_weight and \p tw_desc can be NULL when \p algorithm is CNNL_REDUCTION_NONE or
 *   CNNL_REDUCTION_SUM, otherwise they should have a value and the dimension of \p tw_desc should
 *   be [1].
 *
 * @par API Dependency
 * - Before calling this function to implement nllloss backward, you need to prepare all the
 *   input parameters from the output of ::cnnlNlllossForward.
 *
 * @note
 * - This function only supports 1D input tensor currently.
 *
 * @par Requirements
 * - None.
 *
 * @par Formula
 * - See "Nllloss Backward" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Example
 * - The example of the nllloss backward operation is as follows:
     @verbatim
       input four arrays by 2, 2, 3 and 1
       --> diff_y: [1, 1]

       --> target: [0,1]

       --> weight: [1,2,3]

       --> total_weight: [5]

       param:
         ignore_index: 0, algorithm: CNNL_REDUCTION_NONE

       output array by 2 * 3
       --> diff_x: [[0,0,0], [0,-2,0]]
     @endverbatim
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlNlllossBackward_v2(cnnlHandle_t handle,
                                                 cnnlNlllossAlgorithm_t algorithm,
                                                 const cnnlTensorDescriptor_t diff_y_desc,
                                                 const void *diff_y,
                                                 const cnnlTensorDescriptor_t t_desc,
                                                 const void *target,
                                                 const int64_t ignore_index,
                                                 const cnnlTensorDescriptor_t w_desc,
                                                 const void *weight,
                                                 const cnnlTensorDescriptor_t tw_desc,
                                                 const void *total_weight,
                                                 const cnnlTensorDescriptor_t diff_x_desc,
                                                 void *diff_x);

// Group:L1LossBackward
/*!
 * @brief Computes the gradient of input \p grad_input in artificial intelligence.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in this operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] reduction_mode
 *   Input. The reduction mode used to compute \p grad_input. The reduction mode are defined in the
 *   ::cnnlLossReduction_t enum.
 * @param[in] input_desc
 *   Input. The descriptor of \p input tensor, which is the predict value.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the \p input tensor.
 * @param[in] target_desc
 *   Input. The descriptor of \p target tensor, which is the ground truth value.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] target
 *   Input. Pointer to the MLU memory that stores the \p target tensor.
 * @param[in] grad_output_desc
 *   Input. The descriptor of \p grad_output, which is the gradient with respect to \p output. In
 *   general, it is the output of artificial intelligence. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] grad_output
 *   Input. Pointer to the MLU memory that stores the \p grad_output.
 * @param[in] grad_input_desc
 *   Input. The descriptor of \p grad_input, which is the gradient with respect to \p input.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] grad_input
 *   Output. Pointer to the MLU memory that stores the \p grad_input.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 * @par Formula
 * - See "L1 Loss Backward Out Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of \p input tensor, \p target tensor, \p grad_output tensor, and \p grad_input tensor must be the same.
 * - The supported data types of \p input tensor, \p target tensor, \p grad_output tensor, and \p grad_input tensor are as follows:
 *   - input tensor: half, float.
 *   - target tensor: half, float.
 *   - grad_output tensor: half, float.
 *   - grad_input tensor: half, float.
 * - On MLU500 series or above, this function also supports bfloat16 data type.
 *
 * @par Scale Limitation
 * - The scale of \p input tensor, \p target tensor, \p grad_output tensor, and \p grad_input tensor
 *   must meet the following requirements:
 *   - If \p reduction_mode is set to NONE:
 *     - The shape of \p target equals the shape of \p input.
 *     - The shape of \p grad_output equals the shape of \p input.
 *     - The shape of \p grad_input equals the shape of \p input.
 *   - If \p reduction_mode is set to MEAN or SUM:
 *     - The shape of \p target equals the shape of \p input.
 *     - The shape of \p grad_output is [1], which is a scalar.
 *     - The shape of \p grad_input equals the shape of \p input.
 *
 * @note
 * - You can specify the stride of all dimensions for input_desc, target_desc, grad_output_desc
 *   and grad_input_desc with ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the l1 loss backward out operation is as follows:
     @verbatim
     input: a tensor of 2 --> [1,0,3,2]
     target: a tensor of 2 --> [0,2,4,1]
     grad_output: a tensor of 2 --> [0.1, 0.5, 0.2, 0.3]
     reduction: NONE

     Then we will get the grad_input:
     grad_input: a tensor of 2 --> [0.1, -0.5, -0.2, 0.3]
     @endverbatim
 *
 * @par Reference
 * - http://www.pytorch.org/docs/master/_modules/torch/nn/modules/loss.html
 *
 */

cnnlStatus_t CNNL_WIN_API cnnlL1LossBackward(cnnlHandle_t handle,
                                             cnnlLossReduction_t reduction_mode,
                                             const cnnlTensorDescriptor_t input_desc,
                                             const void *input,
                                             const cnnlTensorDescriptor_t target_desc,
                                             const void *target,
                                             const cnnlTensorDescriptor_t grad_output_desc,
                                             const void *grad_output,
                                             const cnnlTensorDescriptor_t grad_input_desc,
                                             void *grad_input);
// Group:SmoothL1LossForward
/*!
 * @brief Computes the smoothl1 loss of \p x and \p target in AI networks.
 *
 * @deprecated
 *  This function is deprecated and will be removed in future release.
 *   Use ::cnnlSmoothL1LossForward_v2 instead, which supports
 *   param \p beta used to specify the threshold between L1 loss and L2 loss and another
 *   two parameters \p workspace and \p workspace_size, which provide the extra
 *   space needed in computation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in this operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of \p x tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the \p x tensor, which is the predicted value.
 * @param[in] t_desc
 *   Input. The descriptor of \p target tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] target
 *   Input. Pointer to the MLU memory that stores the \p target tensor, which is the ground truth value.
 * @param[in] algorithm
 *   Input. The reduction mode used to compute \p y. The reduction mode are defined in
 *   ::cnnlSmoothL1LossAlgorithm_t enum.
 * @param[in] y_desc
 *   Input. The descriptor of \p y.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the \p y, which is the smoothl1 loss value of \p x and \p target.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 * @par Formula
 * - See "SmoothL1 Loss Operation" section in "Cambricon CNNL User Guide" for details.
 * @par Data Type
 * - Data types of input tensors \p x, \p target and output tensors \p y must be the same.
 * - The supported data types are as follows:
 *   - x: float, half.
 *   - target: float, half.
 *   - y: float, half.
 * @par Scale Limitation
 * - The scale of \p x tensor, \p target tensor, \p y tensor must meet the following requirements:
 *   - If \p algorithm is set to NONE:
 *     - The shape of \p target equals the shape of \p x.
 *     - The shape of \p y equals the shape of \p x.
 *   - If \p algorithm is set to MEAN or SUM:
 *     - The shape of \p target equals the shape of \p x.
 *     - The shape of \p y is [1], which is a scalar.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the smoothl1 loss operation is as follows:
     @verbatim
     x: a tensor of 2 * 3 --> [[1,2,3],[4,5,6]]
     target: a tensor of 2 * 3 --> [[6,5,4],[3,2,1]]
     beta: 2.0
     algorithm: NONE
     Then we will get the y:
     y: a tensor of  2 * 3 --> [[4.0,2.0,0.25],[0.25,2.0,4.0]]
     @endverbatim
 *
 * @par Reference
 *  - http://www.pytorch.org/docs/master/_modules/torch/nn/modules/loss.html
 *
 */
CNNL_DEPRECATED_FOR(cnnlSmoothL1LossForward_v2)
cnnlStatus_t CNNL_WIN_API
cnnlSmoothL1LossForward(cnnlHandle_t handle,
                        const cnnlTensorDescriptor_t x_desc,
                        const void *x,
                        const cnnlTensorDescriptor_t t_desc,
                        const void *target,
                        const cnnlTensorDescriptor_t y_desc,
                        void *y,
                        cnnlSmoothL1LossAlgorithm_t algorithm);

// Group:SmoothL1LossForward
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace
 * to optimize the computation of smoothl1 loss.
 *
 * The size of the extra workspace is based on the given information of the computation of smoothl1 loss,
 * including the input tensor descriptor \p x_desc and reduction mode \p algorithm.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the computation of smoothl1 loss. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] algorithm
 *   Input. The reduction mode used to compute \p y. The reduction mode are defined in
 *   ::cnnlSmoothL1LossAlgorithm_t enum.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *   the computation of smoothl1 loss.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlCreateTensorDescriptor and
 *   ::cnnlSetTensorDescriptor functions to create and set the tensor descriptors \p x_desc.
 * - The allocated extra workspace should be passed to the ::cnnlSmoothL1LossForward_v2 function
 *   to perform the computation of smoothl1 loss.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetSmoothL1LossForwardWorkspaceSize(cnnlHandle_t handle,
                                        const cnnlTensorDescriptor_t x_desc,
                                        const cnnlSmoothL1LossAlgorithm_t algorithm,
                                        size_t *size);
// Group:SmoothL1LossForward
/*!
 * @brief Computes the smoothl1 loss of \p x and \p target in AI networks.
 * Compared with ::cnnlSmoothL1LossForward, this function adds a parameter \p beta used to specify the threshold
 * between L1 loss and L2 loss and another two parameters \p workspace and \p workspace_size,
 * which provide the extra space needed in computation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in this operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of \p x tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the \p x tensor, which is the predicted value.
 * @param[in] t_desc
 *   Input. The descriptor of \p target tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] target
 *   Input. Pointer to the MLU memory that stores the \p target tensor, which is the ground truth value.
 * @param[in] beta
 *   Input. A float value that specifies the threshold between L1 loss and L2 loss.
 * @param[in] algorithm
 *   Input. The reduction mode used to compute \p y. The reduction mode are defined in
 *   ::cnnlSmoothL1LossAlgorithm_t enum.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   computation of smoothl1 loss. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the computation of smoothl1 loss. You can get the size of the workspace with
 *   the ::cnnlGetSmoothL1LossForwardWorkspaceSize function.
 * @param[in] y_desc
 *   Input. The descriptor of \p y.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the \p y, which is the smoothl1 loss value of \p x and \p target.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 * @par Formula
 * - See "SmoothL1 Loss Operation" section in "Cambricon CNNL User Guide" for details.
 * @par Data Type
 * - Data types of input tensors \p x, \p target and output tensors \p y must be the same.
 * - The supported data types are as follows:
 *   - x: float, half, bfloat16.
 *   - target: float, half, bfloat16.
 *   - beta: float.
 *   - y: float, half, bfloat16.
 *
 *   The bfloat16 data type is supported only on MLU500 series.
 * @par Scale Limitation
 * - The scale of \p x tensor, \p target tensor, \p y tensor must meet the following requirements:
 *   - If \p algorithm is set to NONE:
 *     - The shape of \p target equals the shape of \p x.
 *     - The shape of \p y equals the shape of \p x.
 *   - If \p algorithm is set to MEAN or SUM:
 *     - The shape of \p target equals the shape of \p x.
 *     - The shape of \p y is [1], which is a scalar.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the smoothl1 loss operation is as follows:
     @verbatim
     x: a tensor of 2 * 3 --> [[1,2,3],[4,5,6]]
     target: a tensor of 2 * 3 --> [[6,5,4],[3,2,1]]
     algorithm: NONE
     Then we will get the y:
     y: a tensor of  2 * 3 --> [[4.5,2.5,0.5],[0.5,2.5,4.5]]
     @endverbatim
 *
 * @par Reference
 *  - http://www.pytorch.org/docs/master/_modules/torch/nn/modules/loss.html
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlSmoothL1LossForward_v2(cnnlHandle_t handle,
                           const cnnlTensorDescriptor_t x_desc,
                           const void *x,
                           const cnnlTensorDescriptor_t t_desc,
                           const void *target,
                           const float beta,
                           const cnnlSmoothL1LossAlgorithm_t algorithm,
                           void *workspace,
                           const size_t workspace_size,
                           const cnnlTensorDescriptor_t y_desc,
                           void *y);

// Group:SmoothL1LossBackward
/*!
 * @brief Computes the gradient with respect to input.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlSmoothL1LossBackward_v2 instead, which supports
 *   param \p beta used to specify the threshold between L1 loss and L2 loss and another
 *   two parameters \p workspace and \p workspace_size, which provide the extra
 *   space needed in computation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in this operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of \p x tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the \p x tensor, which is the predicted value.
 * @param[in] target_desc
 *   Input. The descriptor of \p target tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] target
 *   Input. Pointer to the MLU memory that stores the \p target tensor, which is the ground truth value.
 * @param[in] dy_desc
 *   Input. The descriptor of \p dy.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] dy
 *   Input. Pointer to the MLU memory that stores the \p dy, which is the gradient with respect to \p output.
 *   In general, it is the output of artificial intelligence.
 * @param[in] dx_desc
 *   Input. The descriptor of \p dx.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] dx
 *   Output. Pointer to the MLU memory that stores the \p dx, which is the gradient with respect to \p input.
 * @param[in] algo
 *   Input. The algorithm used to compute \p grad_input. The algorithms are defined in the
 *   ::cnnlSmoothL1LossAlgorithm_t enum.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 * @par Formula
 * - See "Smooth L1 Loss Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported data types are as follows:
 *   - x(input): float, half.
 *   - target(input): float, half.
 *   - dy(input): float, half.
 *   - beta(input): float.
 *   - dx(output): float, half.
 *
 * @note
 * - Data type of x should be the same as target.
 * - Each dimension of x should be the same as target.
 * - The scale of \p x tensor, \p target tensor, \p dy tensor, \p dx tensor
 *   must meet the following requirements:
 *   - If algorithm is set to NONE:
 *     - The shape of \p target equals the shape of \p x.
 *     - The shape of \p dy equals the shape of \p x.
 *     - The shape of \p dx equals the shape of \p x.
 *   - If algorithm is set to MEAN or SUM:
 *     - The shape of \p target equals the shape of \p x.
 *     - The shape of \p dy is 1 * 1, which is a scalar.
 *     - The shape of \p dx equals the shape of \p x.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the smoothl1loss backward operation is as follows:
     @verbatim
     x: a tensor of 2 --> [5,5]
     target: a tensor of 2 --> [1,4]
     dy: a tensor of 2 --> [1, 1]
     beta: 2
     reduction: NONE

     Then we will get the dx:
     dx: a tensor of 2 --> [1,0.5]
     @endverbatim
 *
 * @par Reference
 * - http://www.pytorch.org/docs/master/_modules/torch/nn/modules/loss.html
 *
 */
CNNL_DEPRECATED_FOR(cnnlSmoothL1LossBackward_v2)
cnnlStatus_t CNNL_WIN_API
cnnlSmoothL1LossBackward(cnnlHandle_t handle,
                         const cnnlTensorDescriptor_t x_desc,
                         const void *x,
                         const cnnlTensorDescriptor_t target_desc,
                         const void *target,
                         const cnnlTensorDescriptor_t dy_desc,
                         const void *dy,
                         const cnnlTensorDescriptor_t dx_desc,
                         void *dx,
                         cnnlSmoothL1LossAlgorithm_t algo);

// Group:SmoothL1LossBackward
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace
 * to optimize the computation of gradient for smoothl1 loss.
 *
 * The size of the extra workspace is based on the given information of the computation of gradient for
 * smoothl1 loss, including the input tensor descriptor \p x_desc and reduction mode \p algorithm.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the computation of gradient for smoothl1 loss. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] algorithm
 *   Input. The reduction mode used to compute \p dx. The reduction mode are defined in
 *   ::cnnlSmoothL1LossAlgorithm_t enum.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *   the computation of gradient for smoothl1 loss.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlCreateTensorDescriptor and
 *   ::cnnlSetTensorDescriptor functions to create and set the tensor descriptors \p x_desc.
 * - The allocated extra workspace should be passed to the ::cnnlSmoothL1LossBackward_v2 function
 *   to perform the computation of gradient for smoothl1 loss.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetSmoothL1LossBackwardWorkspaceSize(cnnlHandle_t handle,
                                         const cnnlTensorDescriptor_t x_desc,
                                         const cnnlSmoothL1LossAlgorithm_t algorithm,
                                         size_t *size);

// Group:SmoothL1LossBackward
/*!
 * @brief Computes the gradient with respect to input for smoothl1 loss.
 * Compared with ::cnnlSmoothL1LossBackward, this function adds a parameter \p beta used to specify the threshold
 * between L1 loss and L2 loss and another two parameters \p workspace and \p workspace_size,
 * which provide the extra space needed in computation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in this operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of \p x tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the \p x tensor, which is the predicted value.
 * @param[in] target_desc
 *   Input. The descriptor of \p target tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] target
 *   Input. Pointer to the MLU memory that stores the \p target tensor, which is the ground truth value.
 * @param[in] dy_desc
 *   Input. The descriptor of \p dy.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] dy
 *   Input. Pointer to the MLU memory that stores the \p dy, which is the gradient with respect to \p output.
 *   In general, it is the output of artificial intelligence.
 * @param[in] beta
 *   Input. A float value that specifies the threshold between L1 loss and L2 loss.
 * @param[in] algorithm
 *   Input. The reduction mode used to compute \p grad_input. The algorithms are defined in the
 *   ::cnnlSmoothL1LossAlgorithm_t enum.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   computation of gradient for smoothl1 loss. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the computation of gradient for smoothl1 loss. You can get the size of the workspace with
 *   the ::cnnlGetSmoothL1LossBackwardWorkspaceSize function.
 * @param[in] dx_desc
 *   Input. The descriptor of \p dx.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] dx
 *   Output. Pointer to the MLU memory that stores the \p dx, which is the gradient with respect to \p input.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 * @par Formula
 * - See "Smooth L1 Loss Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported data types are as follows:
 *   - x(input): float, half, bfloat16.
 *   - target(input): float, half, bfloat16.
 *   - dy(input): float, half, bfloat16.
 *   - beta(input): float.
 *   - dx(output): float, half, bfloat16.
 *
 *   The bfloat16 data type is supported only on MLU500 series.
 *
 * @note
 * - Data type of x should be the same as target.
 * - Each dimension of x should be the same as target.
 * - The scale of \p x tensor, \p target tensor, \p dy tensor, \p dx tensor
 *   must meet the following requirements:
 *   - If algorithm is set to NONE:
 *     - The shape of \p target equals the shape of \p x.
 *     - The shape of \p dy equals the shape of \p x.
 *     - The shape of \p dx equals the shape of \p x.
 *   - If algorithm is set to MEAN or SUM:
 *     - The shape of \p target equals the shape of \p x.
 *     - The shape of \p dy is 1 * 1, which is a scalar.
 *     - The shape of \p dx equals the shape of \p x.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the smoothl1loss backward operation is as follows:
     @verbatim
     x: a tensor of 2 --> [1,1]
     target: a tensor of 2 --> [0,1]
     dy: a tensor of 2 --> [1, 1]
     reduction: NONE

     Then we will get the dx:
     dx: a tensor of 2 --> [1,0]
     @endverbatim
 *
 * @par Reference
 * - http://www.pytorch.org/docs/master/_modules/torch/nn/modules/loss.html
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlSmoothL1LossBackward_v2(cnnlHandle_t handle,
                            const cnnlTensorDescriptor_t x_desc,
                            const void *x,
                            const cnnlTensorDescriptor_t target_desc,
                            const void *target,
                            const cnnlTensorDescriptor_t dy_desc,
                            const void *dy,
                            const float beta,
                            const cnnlSmoothL1LossAlgorithm_t algorithm,
                            void *workspace,
                            const size_t workspace_size,
                            const cnnlTensorDescriptor_t dx_desc,
                            void *dx);

/*!
 * @brief Enumeration variables describing the type of random generator that can distinguish
 * between different random generators.
 *
 * You need to call the ::cnnlRandCreateGenerator function to set the random generator type to
 * a random generator defined in ::cnnlRandGenerator_t.
 */
typedef enum {
  CNNL_RAND_RNG_FAST,
  /*!< Generates random numbers with MLU hardware random generator. The random numbers
       cannot reappear on CPU. And it performs better on MLU200 series than
       MLU300 series.*/
  CNNL_RAND_RNG_MTGP32,
  /*!< Generates random numbers with MTGP32 generator. The random numbers can reappear
       on CPU with the same MTGP32 algorithm and seed. And it performs better on MLU300
       series than MLU200 series.*/
  CNNL_RAND_RNG_PHILOX,
  /*!< Generates random numbers with Philox generator. The random numbers can reappear
       on CPU with the same Philox algorithm and seed.*/
  CNNL_RAND_RNG_ALIGN,
  /*!< Generates random numbers with Philox generator. The random numbers
       are aligned with PyTorch. Thread limit is 221184. */
  CNNL_RAND_RNG_ALIGN_270336,
  /*!< Generates random numbers with Philox generator. The random numbers
       are aligned with PyTorch. Thread limit is 270336. */
  CNNL_RAND_RNG_ALIGN_159744,
  /*!< Generates random numbers with Philox generator. The random numbers
       are aligned with PyTorch. Thread limit is 159744. */
  CNNL_RAND_RNG_ALIGN_141312,
  /*!< Generates random numbers with Philox generator. The random numbers
       are aligned with PyTorch. Thread limit is 141312. */
} cnnlRandRngType_t;

/*!
 * @brief Enumeration variables describing the sequence period of MTGP32.
 *
 * You need to call the ::cnnlRandSetMTGP32Period function to set the period to
 * a \p CNNL_RAND_RNG_MTGP32 random generator that is defined in ::cnnlRandGenerator_t.
 */
typedef enum {
  CNNL_RAND_MTGP32_P11213, /*!< The Mersene Twister sequence period of 11213 is used.*/
} cnnlRandMTGP32PeriodType_t;

/*!
 * The descriptor of the random operation that holds the random generator information including
 * seed, period, and random generator type.
 *
 * You need to call the ::cnnlRandCreateGenerator function to create a descriptor,
 * and call the ::cnnlRandSetPseudoRandomGeneratorSeed function to set a seed to the generator
 * or call the ::cnnlRandSetMTGP32Period function to set the period to the generator. Also,
 * you need to destroy the Cambricon CNNL context at the end with the ::cnnlRandDestroyGenerator function.
 */
typedef struct cnnlRandGeneratorStruct *cnnlRandGenerator_t;

/*!
 * The descriptor of the random operation that holds the state parameter information of MTGP32 including
 * period, parameter table and temper table.
 *
 * You need to call the ::cnnlRandGetMTGP32HostParam function to get the descriptor.
 */
typedef struct MTGP32FastParams *cnnlMTGP32FastParams_t;

// Group:Rand
/*!
 * @brief Creates a generator descriptor pointed by \p generator for a random
 *   operation, and allocates memory for holding the information about the random
 *   operation. The information is defined in ::cnnlRandGenerator_t. For more
 *   information about descriptor, see "Cambricon CNNL User Guide".
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[out] generator
 *   Output. A host pointer to the random \p generator descriptor that holds information about the
 *   random operation.
 * @param[in] rng_type
 *   Input. The type of random \p generator defined in ::cnnlRandRngType_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - After calling this function, you can call ::cnnlRandSetPseudoRandomGeneratorSeed
 *   function to set a seed to the \p generator or call ::cnnlRandSetMTGP32Period function
 *   to set the period to the \p generator. Also, you need to destroy the Cambricon CNNL context at the end
 *   with the ::cnnlRandDestroyGenerator function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlRandCreateGenerator(cnnlRandGenerator_t *generator,
                                                  cnnlRandRngType_t rng_type);

// Group:Rand
/*!
 * @brief Creates a generator descriptor pointed by \p generator for a random
 *   operation, and allocates memory for holding the information about the random
 *   operation. The information is defined in ::cnnlRandGenerator_t. For more
 *   information about descriptor, see "Cambricon CNNL User Guide".
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[out] generator
 *   Input. A host pointer to the random \p generator descriptor that holds information about the
 *   random operation.
 * @param[in] rng_type
 *   Input. The type of random \p generator defined in ::cnnlRandRngType_t.
 * @param[in] seed
 *   Input. The seed used to generate random numbers in MTGP mode.
 *   If \p rng_type is not CNNL_RAND_RNG_MTGP, \p seed can be set to any value.
 * @param[in] seed1
 *   Input. The parameter is used to initialize the key variable in the Philox algorithm.
 *   If \p rng_type is not CNNL_RAND_RNG_PHILOX, \p seed1 can be set to any value.
 * @param[in] seed2
 *   Input. The parameter is used to initialize the counter variable in the Philox algorithm.
 *   If \p rng_type is not CNNL_RAND_RNG_PHILOX, \p seed2 can be set to any value.
 * @param[in] offset
 *   Input. The parameter is used to select a sequence of random numbers.
 *   If \p rng_type is not CNNL_RAND_RNG_PHILOX, \p offset can be set to any value.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - After calling this function, you can call ::cnnlRandSetPseudoRandomGeneratorSeed
 *   function to set a seed to the \p generator or call ::cnnlRandSetMTGP32Period function
 *   to set the period to the \p generator. Also, you need to destroy the Cambricon CNNL context at the end
 *   with the ::cnnlRandDestroyGenerator function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlRandCreateGenerator_v2(cnnlRandGenerator_t *generator,
                                                     cnnlRandRngType_t rng_type,
                                                     int seed,
                                                     size_t seed1,
                                                     size_t seed2,
                                                     size_t offset);

// Group:Rand
/*!
 * @brief Sets the member \p seed1 of the \p generator, and uses this member to reinitialize the
 *   members \p key0 and \p key1. The information is defined in ::cnnlRandGenerator_t. For more
 *   information about descriptor, see "Cambricon CNNL User Guide".
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[out] generator
 *   Output. A host pointer to the random \p generator descriptor that holds information about the
 *   random operation.
 * @param[in] seed
 *   Input. The parameter is used to initialize the key variable in the Philox algorithm.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, you can call ::cnnlRandCreateGenerator function to
 *   create \p generator.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlRandSetPhiloxSeed(cnnlRandGenerator_t generator,
                                                 size_t seed);

// Group:Rand
/*!
 * @brief Sets the member \p offset of the \p generator.
 *   The information is defined in ::cnnlRandGenerator_t. For more
 *   information about descriptor, see "Cambricon CNNL User Guide".
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[out] generator
 *   Output. A host pointer to the random \p generator descriptor that holds information about the
 *   random operation.
 * @param[in] offset
 *   Input. The parameter is used to select a sequence of random numbers.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, you can call ::cnnlRandCreateGenerator function
 *   to create \p generator.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlRandSetPhiloxOffset(cnnlRandGenerator_t generator,
                                                  size_t offset);

// Group:Rand
/*!
 * @brief Sets the member \p seed of the \p generator.
 *   The information is defined in ::cnnlRandGenerator_t. For more
 *   information about descriptor, see "Cambricon CNNL User Guide".
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[out] generator
 *   Output. A host pointer to the random \p generator descriptor that holds information about the
 *   random operation.
 * @param[in] seed
 *   Input. The seed used to generate random numbers in MTGP mode.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, you can call ::cnnlRandCreateGenerator function
 *   to create \p generator.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlRandSetMtgpSeed(cnnlRandGenerator_t generator,
                                              int seed);

// Group:Rand
/*!
 * @brief Returns the members \p rng_type of the structure generator.
 *   For more information about descriptor, see "Cambricon CNNL User Guide".
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] generator
 *   Input. A host pointer to the random \p generator descriptor that holds information about the
 *   random operation.
 * @param[out] rng_type
 *   Output. A host pointer to a variable whose type is random \p generator.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, you can call ::cnnlRandCreateGenerator function to
 *   create \p generator.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlRandGetGeneratorRngType(const cnnlRandGenerator_t generator,
                                                      cnnlRandRngType_t *rng_type);

// Group:Rand
/*!
 * @brief Returns the members MTGP \p seed of the structure generator.
 *   For more information about descriptor, see "Cambricon CNNL User Guide".
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] generator
 *   Input. A host pointer to the random \p generator descriptor that holds information about the
 *   random operation.
 * @param[out] seed
 *   Output. A host pointer to a variable whose type is int.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, you can call ::cnnlRandCreateGenerator function to
 *   create \p generator.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlRandGetGeneratorMtgpSeed(const cnnlRandGenerator_t generator,
                                                       int *seed);

// Group:Rand
/*!
 * @brief Returns the members PHILOX \p seed of the structure generator.
 *   For more information about descriptor, see "Cambricon CNNL User Guide".
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] generator
 *   Input. A host pointer to the random \p generator descriptor that holds information about the
 *   random operation.
 * @param[out] seed
 *   Output. A host pointer to a variable whose type is size_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, you can call ::cnnlRandCreateGenerator function  to
 *   create \p generator.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlRandGetGeneratorPhiloxSeed(const cnnlRandGenerator_t generator,
                                                          size_t *seed);

// Group:Rand
/*!
 * @brief Returns the members \p offset of the structure generator.
 *   For more information about descriptor, see "Cambricon CNNL User Guide".
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] generator
 *   Input. A host pointer to the random \p generator descriptor that holds information about the
 *   random operation.
 * @param[out] offset
 *   Output. A host pointer to a variable whose type is size_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, you can call ::cnnlRandCreateGenerator function to create
 *   \p generator.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlRandGetGeneratorPhiloxOffset(const cnnlRandGenerator_t generator,
                                                           size_t *offset);

// Group:Rand
/*!
 * @brief Returns the numbers of threads simulated on the current arch using the Philox random
 *   number algorithm.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGetRandSimulateThreadNum_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[out] thread_num
 *   Output. A host pointer to a variable that contains the number of simulated threads.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - None.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlGetRandSimulateThreadNum_v2)
cnnlStatus_t CNNL_WIN_API cnnlRandGetSimulateThreadNum(cnnlHandle_t handle,
                                                       int *thread_num);

// Group:Rand
/*!
 * @brief Returns the numbers of threads simulated on the current arch using the Philox random
 *   number algorithm.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGetRandSimulateThreadNum_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[out] thread_num
 *   Output. A host pointer to a variable that contains the number of simulated threads.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - None.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlGetRandSimulateThreadNum_v2)
cnnlStatus_t CNNL_WIN_API cnnlGetRandSimulateThreadNum(cnnlHandle_t handle,
                                                       int *thread_num);

// Group:Rand
/*!
 * @brief Returns the numbers of threads simulated on the current arch using the Philox random
 *   number algorithm.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] rng_type
 *   Input. The rng type of random generator.
 *   Support \p CNNL_RAND_RNG_PHILOX and \p CNNL_RAND_RNG_ALIGN.
 * @param[out] thread_num
 *   Output. A host pointer to a variable that contains the number of simulated threads.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - None.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetRandSimulateThreadNum_v2(cnnlHandle_t handle,
                                                          cnnlRandRngType_t rng_type,
                                                          int *thread_num);

// Group:Rand
/*!
 * @brief Destroys a random \p generator descriptor.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] generator
 *   Input. The random \p generator descriptor to be destroyed.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlRandCreateGenerator function, or
 *   ::cnnlRandSetPseudoRandomGeneratorSeed function, or ::cnnlRandSetMTGP32Period function.
 *   Otherwise, ::CNNL_STATUS_BAD_PARAM is returned.
 * - This function should be called to destroy the random \p generator descriptor.
 *   Otherwise, the memory leak may occur.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlRandDestroyGenerator(cnnlRandGenerator_t generator);

// Group:Rand
/*!
 * @brief Sets a \p seed to a random \p generator descriptor.
 * A default \p seed will be used if this function is not called.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] generator
 *   Input. The descriptor of random \p generator. For detailed information,
 *   see ::cnnlRandGenerator_t.
 * @param[in] seed
 *   Input. The \p seed to be set.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlRandSetPseudoRandomGeneratorSeed(cnnlRandGenerator_t generator,
                                                               int seed);

// Group:Rand
/*!
 * @brief Sets MTGP32 \p period to a random \p generator.
 * Only \p CNNL_RAND_MTGP32_P11213 is supported. If this function is
 * not called, a default value of \p CNNL_RAND_MTGP32_P11213 will be used.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] generator
 *   Input. The descriptor of random \p generator. For detailed information,
 *   see ::cnnlRandGenerator_t.
 * @param[in] period
 *   Input. Period of Mersene Twister sequence. For detailed information,
 *   see ::cnnlRandMTGP32PeriodType_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlRandSetMTGP32Period(cnnlRandGenerator_t generator,
                                                  cnnlRandMTGP32PeriodType_t period);

// Group:Rand
/*!
 * @brief Gets MTGP32 state size in bytes when using \p CNNL_RAND_RNG_MTGP32 \p generator type.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] generator
 *   Input. The descriptor of random \p generator. For detailed information,
 *   see ::cnnlRandGenerator_t.
 * @param[out] size
 *   Input. A host pointer to the returned size of the state in bytes that is used in
 *   the random operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - After calling this function, you can call ::cnnlRandMakeMTGP32State function to
 *   initialize state workspace.
 * - The allocated extra workspace should be passed to perform the random operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlRandGetMTGP32StateSize(const cnnlRandGenerator_t generator,
                                                     size_t *size);

// Group:Rand
/*!
 * @brief Gets the size of MTGP32 kernel parameter data in bytes when using \p CNNL_RAND_RNG_MTGP32
 * \p generator type.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] generator
 *   Input. The descriptor of random \p generator. For detailed information,
 *   see ::cnnlRandGenerator_t.
 * @param[out] size
 *   Input. A host pointer to the returned size of the state in bytes that is used in
 *   the random operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - After calling this function, you can call ::cnnlRandMakeMTGP32Constants function to
 *   initialize kernel parameter data.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlRandGetMTGP32KernelParamSize(const cnnlRandGenerator_t generator,
                                                           size_t *size);

// Group:Rand
/*!
 * @brief Gets a pointer to the MTGP32 host state parameter that is defined in
 * ::cnnlMTGP32FastParams_t when using \p CNNL_RAND_RNG_MTGP32 \p generator type.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] generator
 *   Input. The descriptor of random \p generator. For detailed information,
 *   see ::cnnlRandGenerator_t.
 * @param[out] param
 *   Input. A host pointer to the returned parameter that is used in
 *   the random operation. For detailed information, see ::cnnlMTGP32FastParams_t.

 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - After calling this function, you can call ::cnnlRandMakeMTGP32Constants function to
 *   initialize kernel parameter data.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlRandGetMTGP32HostParam(const cnnlRandGenerator_t generator,
                                                     cnnlMTGP32FastParams_t *param);

// Group:Rand
/*!
 * @brief Initializes the MTGP32 kernel parameter data on device.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] params
 *   Input. Pointer to host memory that stores MTGP32 state parameter.
 *   For detailed information, see ::cnnlMTGP32FastParams_t.
 * @param[in] kernel_params
 *   Input. Pointer to MLU memory that stores MTGP32 state parameter.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlRandGetMTGP32HostParam function to
 *   get state host parameter and calling the ::cnnlRandGetMTGP32KernelParamSize function to
 *   initialize the MTGP32 kernel parameter data.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlRandMakeMTGP32Constants(cnnlHandle_t handle,
                                                      const cnnlMTGP32FastParams_t params,
                                                      void *kernel_params);

// Group:Rand
/*!
 * @brief Initializes the MTGP32 state on device.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in,out] state
 *   Input and output. Pointer to MLU memory that stores MTGP32 state.
 * @param[in] params
 *   Input. Pointer to host memory that stores MTGP32 state parameter.
 *   For detailed information, see ::cnnlMTGP32FastParams_t.
 * @param[in] kernel_params
 *   Input. Pointer to MLU memory that stores MTGP32 state parameter.
 * @param[in] seed
 *   Input. The \p seed to be used to initialize the MTGP32 state.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlRandGetMTGP32StateSize function to
 *   initialize MTGP32 state workspace and calling the ::cnnlRandMakeMTGP32Constants function to
 *   initialize the MTGP32 kernel parameter data.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlRandMakeMTGP32KernelState(cnnlHandle_t handle,
                                                        void *state,
                                                        const cnnlMTGP32FastParams_t params,
                                                        const void *kernel_params,
                                                        const uint32_t seed);

// Group:Rand
/*!
 * @brief Initializes the MTGP32 state.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] generator
 *   Input. The descriptor of random \p generator. For detailed information,
 *   see ::cnnlRandGenerator_t.
 * @param[in] state
 *   Input. Pointer to MLU memory that stores MTGP32 state.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlRandGetMTGP32StateSize function to
 *   initialize state workspace.
 * - The allocated extra workspace should be passed to perform the random operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlRandMakeMTGP32State(cnnlHandle_t handle,
                                                  cnnlRandGenerator_t generator,
                                                  void *state);

// Group:Rand
/*!
 * @brief Generates random numbers of uniform distribution in half or float data type
 * on MLU devices.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGenerateRandUniform instead, which supports
 *   \p philox mode only.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] generator
 *   Input. The descriptor of random \p generator. For detailed information,
 *   see ::cnnlRandGenerator_t.
 * @param[in] type
 *   Input. Data type of output. For detailed information,
 *   see ::cnnlDataType_t.
 * @param[in,out] state
 *   Input and output. Pointer to device state data, which is used to generate random sequence.
 * Set NULL if you use \p CNNL_RAND_RNG_FAST \p generator type.
 * @param[in] num
 *   Input. The total number of random numbers to be generated.
 * @param[in] min
 *   Input. The minimum value of random numbers to be generated.
 * @param[in] max
 *   Input. The maximum value of random numbers to be generated.
 * @param[out] out
 *   Output. Pointer to device output data, which is the random sequence.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "RandomUniform Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Scale Limitation
 * - The \p num is greater than 0.
 * - The \p max is greater than the \p min.
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlRandCreateGenerator function.
 *   Additionally, if using ::CNNL_RAND_RNG_MTGP32 random generator type, you also need to
 *   call the ::cnnlRandSetPseudoRandomGeneratorSeed function, the ::cnnlRandGetMTGP32StateSize
 *   function and the ::cnnlRandMakeMTGP32KernelState function before calling this function.


 * @par Data Type
 * - The supported data types of output are as follows:
 *   - output : half, float, bfloat16.

 *   The bfloat16 data type is supported only on MLU500 series.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      --> type: CNNL_DTYPE_FLOAT;
      --> num: 2048;
      --> min: 0;
      --> max: 1;
      Then we will get the output:
      --> out: an array [2048] of float type between 0 and 1;
     @endverbatim
 */
CNNL_DEPRECATED_FOR(cnnlGenerateRandUniform)
cnnlStatus_t CNNL_WIN_API cnnlRandGenerateUniform(cnnlHandle_t handle,
                                                  const cnnlRandGenerator_t generator,
                                                  cnnlDataType_t type,
                                                  void *state,
                                                  size_t num,
                                                  float min,
                                                  float max,
                                                  void *out);

// Group:Rand
/*!
 * @brief Generates random numbers of uniform distribution in half or float data type
 * on MLU devices.
 *
 * It only supports Philox mode. To use other modes, call
 * ::cnnlRandGenerateUniform.
 * The Philox algorithm has three parameters: seed, offset and subsequence. Seed specifies the key
 * in the algorithm, and offset and subsequence specify the counter in the algorithm. On MLU devices
 * , we use index [0, THREAD_NUM) to bind to subsequence, so only seed and offset arguments are needed.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] rng_type
 *   Input. The rng type of random generator.
 *   Support \p CNNL_RAND_RNG_PHILOX and \p CNNL_RAND_RNG_ALIGN.
 * @param[in] captured
 *   Input. If \p captured is true, use \p seed_ptr, \p offset_ptr and \p offset_intragraph to specify
 *   seed and offset. If \p captured is false, use \p seed and \p offset.
 * @param[in] seed
 *   Input. Specify the seed of the Philox algorithm when \p captured is false.
 * @param[in] offset
 *   Input. Specify the offset of the Philox algorithm when \p captured is false.
 * @param[in] seed_ptr
 *   Input. Pointer to device seed data. Specify the seed of the Philox algorithm when \p captured is true.
 * @param[in] offset_ptr
 *   Input. Pointer to device offset data. Specify the offset of the Philox algorithm when \p captured is true.
 * @param[in] offset_intragraph
 *   Input. The offset in a graph. Specify the offset of the Philox algorithm when \p captured is true.
 * @param[in] min
 *   Input. The minimum value of random numbers to be generated.
 * @param[in] max
 *   Input. The maximum value of random numbers to be generated.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to device output data, which is the random sequence.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "RandomUniform Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Scale Limitation
 * - \p num is greater than 0.
 * - \p max is greater than \p min.
 *
 * @par Data Type
 * - The supported data types of output are as follows:
 *   - output : half, float, bfloat16.
 *   The bfloat16 data type is supported only on MLU500 series.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      --> type: CNNL_DTYPE_FLOAT;
      --> num: 2048;
      --> min: 0;
      --> max: 1;
      Then we will get the output:
      --> out: an array [2048] of float type between 0 and 1;
     @endverbatim
 */
cnnlStatus_t CNNL_WIN_API cnnlGenerateRandUniform(cnnlHandle_t handle,
                                                  cnnlRandRngType_t rng_type,
                                                  const bool captured,
                                                  const uint64_t seed,
                                                  const uint64_t offset,
                                                  const int64_t *seed_ptr,
                                                  const int64_t *offset_ptr,
                                                  const uint32_t offset_intragraph,
                                                  const float min,
                                                  const float max,
                                                  const cnnlTensorDescriptor_t output_desc,
                                                  void *output);

// Group:Rand
/*!
 * @brief Generates random numbers of uniform distribution in integer
 * data type on MLU devices.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGenerateRandDiscreteUniform instead, which supports \p philox mode only.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] generator
 *   Input. The descriptor of random \p generator. For detailed information,
 *   see ::cnnlRandGenerator_t.
 * @param[in,out] state
 *   Input and output. Pointer to device state data, which is used to generate random sequence.
 * Set NULL if you use \p CNNL_RAND_RNG_FAST \p generator type.
 * @param[in] num
 *   Input. The total number of random numbers to be generated.
 * @param[in] min
 *   Input. The minimum value of random numbers to be generated.
 * @param[in] max
 *   Input. The maximum value of random numbers to be generated.
 * @param[out] out
 *   Output. Pointer to device output data, which is the random sequence.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "RandomUniformInt Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Scale Limitation
 * - The \p num is greater than 0.
 * - The \p max is greater than the \p min.
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlRandCreateGenerator function.
 *   Additionally, if using ::CNNL_RAND_RNG_MTGP32 random generator type, you also need to
 *   call the ::cnnlRandSetPseudoRandomGeneratorSeed function, the ::cnnlRandGetMTGP32StateSize
 *   function and the ::cnnlRandMakeMTGP32KernelState function before calling this function.

 * @par Data Type
 * - The supported data type of output is as follows:
 *  - output: int32.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
       --> num: 2048;
       --> min: 0;
       --> max: 1;
       Then we will get the output:
       --> out: an array [2048] of integer type between 0 and 1;
     @endverbatim
 */
CNNL_DEPRECATED_FOR(cnnlGenerateRandDiscreteUniform)
cnnlStatus_t CNNL_WIN_API cnnlRandGenerateUniformInt(cnnlHandle_t handle,
                                                     const cnnlRandGenerator_t generator,
                                                     void *state,
                                                     size_t num,
                                                     int min,
                                                     int max,
                                                     void *out);
// Group:Rand
/*!
 * @brief Generates random numbers from the discrete uniform distribution over [\p min, \p max - 1]
 * on MLU devices.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGenerateRandDiscreteUniform instead, which supports \p philox mode only.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] generator
 *   Input. The descriptor of random \p generator. For detailed information,
 *   see ::cnnlRandGenerator_t.
 * @param[in] type
 *   Input. Data type of output. For detailed information,
 *   see ::cnnlDataType_t.
 * @param[in,out] state
 *   Input and output. Pointer to device state data, which is used to generate random sequence.
 * Set NULL if you use \p CNNL_RAND_RNG_FAST \p generator type.
 * @param[in] num
 *   Input. The total number of random numbers to be generated.
 * @param[in] min
 *   Input. The minimum value of random numbers to be generated.
 * @param[in] max
 *   Input. The maximum value of random numbers to be generated.
 * @param[out] out
 *   Output. Pointer to device output data, which is the random sequence.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "RandomUniformInt Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Scale Limitation
 * - The \p num is greater than 0.
 * - The \p max is greater than the \p min.
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlRandCreateGenerator function.
 *   Additionally, if using ::CNNL_RAND_RNG_MTGP32 random generator type, you also need to
 *   call the ::cnnlRandSetPseudoRandomGeneratorSeed function, the ::cnnlRandGetMTGP32StateSize
 *   function and the ::cnnlRandMakeMTGP32KernelState function before calling this function.

 * @par Data Type
 * - The supported data types of output are as follows:
 *  - output: half, bfloat16, float, int32.
 *
 * @note
 * - When \p type is CNNL_DTYPE_FLOAT, \p min and \p max should be in range of
 *   [\f$-2^{31}\f$, \f$2^{31} - 1\f$].
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
       --> type: CNNL_DTYPE_FLOAT;
       --> num: 2048;
       --> min: 0;
       --> max: 5;
       Then you will get the output:
       --> out: an array [2048] of float data type in range of [0, 5);
     @endverbatim
 */

CNNL_DEPRECATED_FOR(cnnlGenerateRandDiscreteUniform)
cnnlStatus_t cnnlRandGenerateDescreteUniform(cnnlHandle_t handle,
                                             const cnnlRandGenerator_t generator,
                                             cnnlDataType_t type,
                                             void *state,
                                             size_t num,
                                             int min,
                                             int max,
                                             void *out);

// Group:Rand
/*!
 * @brief Generates random numbers from the discrete uniform distribution within the
 * limit of [\p base, \p base + \p range) on MLU devices. Compare with ::cnnlRandGenerateDescreteUniform,
 * this API provides the base and range parameters of type int64 to represent the limit,
 * and supports the random numbers of type int64.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGenerateRandDiscreteUniform instead, which supports
 *   \p philox mode only.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] generator
 *   Input. The descriptor of random \p generator. For detailed information,
 *   see ::cnnlRandGenerator_t.
 * @param[in] output_desc
 *   Input. The descriptor of \p output tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in,out] state
 *   Input and output. Pointer to device \p state data, which is used to generate random sequence.
 *   Set it to NULL if \p CNNL_RAND_RNG_FAST or \p CNNL_RAND_RNG_PHILOX \p generator type is used.
 * @param[in] num
 *   Input. The total number of random numbers to be generated.
 * @param[in] range
 *   Input. The range value of random numbers to be generated.
 * @param[in] base
 *   Input. The minimum value of random numbers to be generated.
 * @param[out] output
 *   Output. Pointer to device \p output data, which is the random sequence.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "RandomUniformInt Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Scale Limitation
 * - \p num is greater than 0.
 * - \p range is greater than 0.
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlRandCreateGenerator function.
 *   Additionally, if ::CNNL_RAND_RNG_MTGP32 random generator type is used,
 *   before calling this function, you also need to call the ::cnnlRandSetPseudoRandomGeneratorSeed,
 *   ::cnnlRandGetMTGP32StateSize and ::cnnlRandMakeMTGP32KernelState functions.
 *
 * @par Data Type
 * - The supported data types of output are as follows:
 *  - output: half, bfloat16, float, int32, int64.
 *
 * @note
 * - When \p dtype is CNNL_DTYPE_FLOAT, \p base and \p base + \p range should be in range of
 *   [\f$-2^{31}\f$, \f$2^{31} - 1\f$].
 * - When \p dtype is CNNL_DTYPE_INT64, \p base and \p base + \p range should be in range of
 *   [\f$-2^{47}\f$, \f$2^{47} - 1\f$).
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
       --> type: CNNL_DTYPE_FLOAT;
       --> num: 2048;
       --> base: 1;
       --> range: 5;
       Then you will get the output:
       --> out: an array [2048] of float data type in range of [1, 6);
     @endverbatim
 */
CNNL_DEPRECATED_FOR(cnnlGenerateRandDiscreteUniform)
cnnlStatus_t cnnlRandGenerateDiscreteUniform(cnnlHandle_t handle,
                                             const cnnlRandGenerator_t generator,
                                             const cnnlTensorDescriptor_t output_desc,
                                             void *state,
                                             size_t num,
                                             int64_t range,
                                             int64_t base,
                                             void *output);

// Group:Rand
/*!
 * @brief Generates random numbers from the discrete uniform distribution within the
 * limit of [\p base, \p base + \p range) on MLU devices.
 *
 * It only supports \p philox mode. To use other modes, call
 * ::cnnlRandGenerateDiscreteUniform.
 * The Philox algorithm has three parameters: seed, offset and subsequence. Seed specifies the key
 * in the algorithm, and offset and subsequence specify the counter in the algorithm. On MLU devices
 * , we use index [0, THREAD_NUM) to bind to subsequence, so only seed and offset arguments are needed.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] rng_type
 *   Input. The rng type of random generator.
 *   Only support \p CNNL_RAND_RNG_PHILOX or \p CNNL_RAND_RNG_ALIGN.
 * @param[in] captured
 *   Input. If \p captured is true, use \p seed_ptr, \p offset_ptr and \p offset_intragraph to specify
 *   seed and offset. If \p captured is false, use \p seed and \p offset.
 * @param[in] seed
 *   Input. Specify the seed of the Philox algorithm when \p captured is false.
 * @param[in] offset
 *   Input. Specify the offset of the Philox algorithm when \p captured is false.
 * @param[in] seed_ptr
 *   Input. Pointer to device seed data. Specify the seed of the Philox algorithm when \p captured is true.
 * @param[in] offset_ptr
 *   Input. Pointer to device offset data. Specify the offset of the Philox algorithm when \p captured is true.
 * @param[in] offset_intragraph
 *   Input. The offset in a graph. Specify the offset of the Philox algorithm when \p captured is true.
 * @param[in] range
 *   Input. The range value of random numbers to be generated.
 * @param[in] base
 *   Input. The minimum value of random numbers to be generated.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to device output data, which is the random sequence.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "RandomUniform Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Scale Limitation
 * - \p num is greater than 0.
 * - \p range is greater than 0.
 *
 * @par Data Type
 * - The supported data types of output are as follows:
 *   - output: half, bfloat16, float, int32, int64, bool.
 *   The bfloat16 data type is supported only on MLU500 series.
 *
 * @note
 * - When \p dtype is CNNL_DTYPE_FLOAT, \p base and \p base + \p range should be in range of
 *   [\f$-2^{31}\f$, \f$2^{31} - 1\f$].
 * - When \p dtype is CNNL_DTYPE_INT64, \p base and \p base + \p range should be in range of
 *   [\f$-2^{47}\f$, \f$2^{47} - 1\f$).
 * - When \p dtype is CNNL_DTYPE_BOOL, \p base and \p base + \p range - 1 should be in range of [0, 1],
 *   \p range should be greater than 0.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
       --> type: CNNL_DTYPE_FLOAT;
       --> num: 2048;
       --> base: 1;
       --> range: 5;
       Then you will get the output:
       --> out: an array [2048] of float data type in range of [1, 6);
     @endverbatim
 */
cnnlStatus_t CNNL_WIN_API
cnnlGenerateRandDiscreteUniform(cnnlHandle_t handle,
                                cnnlRandRngType_t rng_type,
                                const bool captured,
                                const uint64_t seed,
                                const uint64_t offset,
                                const int64_t *seed_ptr,
                                const int64_t *offset_ptr,
                                const uint32_t offset_intragraph,
                                int64_t range,
                                int64_t base,
                                const cnnlTensorDescriptor_t output_desc,
                                void *output);

// Group:Rand
/*!
 * @brief Generates random numbers of normal distribution in half or float
 * data type on MLU devices.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGenerateRandNormal instead, which supports
 *   \p philox mode only.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] generator
 *   Input. The descriptor of random \p generator. For detailed information,
 *   see ::cnnlRandGenerator_t.
 * @param[in] type
 *   Input. Data type of output. For detailed information,
 *   see ::cnnlDataType_t.
 * @param[in,out] state
 *   Input and output. Pointer to device state data, which is used to generate random sequence.
 * Set NULL if you use \p CNNL_RAND_RNG_FAST \p generator type.
 * @param[in] num
 *   Input. The total number of random numbers to be generated.
 * @param[in] mean
 *   Input. The mean value of random numbers to be generated.
 * @param[in] stddev
 *   Input. The standard deviation value of random numbers to be generated.
 * @param[out] out
 *   Output. Pointer to device output data, which is the random sequence.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "RandomNormal Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Scale Limitation
 * - The \p num is greater than 0.
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlRandCreateGenerator function.
 *   Additionally, if using ::CNNL_RAND_RNG_MTGP32 random generator type, you also need to
 *   call the ::cnnlRandSetPseudoRandomGeneratorSeed function, the ::cnnlRandGetMTGP32StateSize
 *   function and the ::cnnlRandMakeMTGP32KernelState function before calling this function.

 * @par Data Type
 * - The supported data types of output are as follows:
 *   - output : half, float, bfloat16.
 *
 *   The bfloat16 data type is supported only on MLU500 series.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      --> type: CNNL_DTYPE_FLOAT;
      --> num: 2048;
      --> mean: 0;
      --> stddev: 1;
      Then we will get the output:
      --> out: an array [2048] of float type with mean equals 0 and standard deviation value
          equals 1;
     @endverbatim
 */
CNNL_DEPRECATED_FOR(cnnlGenerateRandNormal)
cnnlStatus_t CNNL_WIN_API cnnlRandGenerateNormal(cnnlHandle_t handle,
                                                 const cnnlRandGenerator_t generator,
                                                 cnnlDataType_t type,
                                                 void *state,
                                                 size_t num,
                                                 float mean,
                                                 float stddev,
                                                 void *out);

// Group:Rand
/*!
 * @brief Generates random numbers of normal distribution in half or float
 * data type on MLU devices.
 *
 * It only supports \p philox mode. To use other modes, call
 * ::cnnlRandGenerateNormal.
 * The Philox algorithm has three parameters: seed, offset and subsequence. Seed specifies the key
 * in the algorithm, and offset and subsequence specify the counter in the algorithm. On MLU devices
 * , we use index [0, THREAD_NUM) to bind to subsequence, so only seed and offset arguments are needed.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] rng_type
 *   Input. The rng type of random generator. Only support \p CNNL_RAND_RNG_PHILOX or \p CNNL_RAND_RNG_ALIGN.
 * @param[in] captured
 *   Input. If \p captured is true, use \p seed_ptr, \p offset_ptr and \p offset_intragraph to specify
 *   seed and offset. If \p captured is false, use \p seed and \p offset.
 * @param[in] seed
 *   Input. Specify the seed of the Philox algorithm when \p captured is false.
 * @param[in] offset
 *   Input. Specify the offset of the Philox algorithm when \p captured is false.
 * @param[in] seed_ptr
 *   Input. Pointer to device seed data. Specify the seed of the Philox algorithm when \p captured is true.
 * @param[in] offset_ptr
 *   Input. Pointer to device offset data. Specify the offset of the Philox algorithm when \p captured is true.
 * @param[in] offset_intragraph
 *   Input. The offset in a graph. Specify the offset of the Philox algorithm when \p captured is true.
 * @param[in] mean
 *   Input. The mean value of random numbers to be generated.
 * @param[in] stddev
 *   Input. The standard deviation value of random numbers to be generated.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to device output data, which is the random sequence.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "RandomNormal Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Scale Limitation
 * - \p num is greater than 0.
 *
 * @par Data Type
 * - The supported data types of output are as follows:
 *   - output : half, float, bfloat16.
 *   The bfloat16 data type is supported only on MLU500 series.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      --> type: CNNL_DTYPE_FLOAT;
      --> num: 2048;
      --> mean: 0;
      --> stddev: 1;
      Then we will get the output:
      --> out: an array [2048] of float type with mean equals 0 and standard deviation value
          equals 1;
     @endverbatim
 */
cnnlStatus_t CNNL_WIN_API cnnlGenerateRandNormal(cnnlHandle_t handle,
                                                 cnnlRandRngType_t rng_type,
                                                 const bool captured,
                                                 const uint64_t seed,
                                                 const uint64_t offset,
                                                 const int64_t *seed_ptr,
                                                 const int64_t *offset_ptr,
                                                 const uint32_t offset_intragraph,
                                                 const float mean,
                                                 const float stddev,
                                                 const cnnlTensorDescriptor_t output_desc,
                                                 void *output);

// Group:Rand
/*!
 * @brief Generates random numbers of truncated normal distribution
 * in half or float data type on MLU devices.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGenerateRandTruncatedNormal instead, which supports
 *   \p philox mode only.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] generator
 *   Input. The descriptor of random \p generator. For detailed information,
 *   see ::cnnlRandGenerator_t.
 * @param[in] type
 *   Input. Data type of output. For detailed information,
 *   see ::cnnlDataType_t.
 * @param[in,out] state
 *   Input and output. Pointer to device state data, which is used to generate random sequence.
 * Set NULL if you use \p CNNL_RAND_RNG_FAST \p generator type.
 * @param[in] num
 *   Input. The total number of random numbers to be generated.
 * @param[in] mean
 *   Input. The mean value of random numbers to be generated.
 * @param[in] stddev
 *   Input. The standard deviation value of random numbers to be generated.
 * @param[out] out
 *   Output. Pointer to device output data, which is the random sequence between
 * (\p mean - 2 * \p stddev) and (\p mean + 2 * \p stddev).
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "RandomTruncatedNormal Operator" section in "Cambricon CNNL User Guide" for details.
 * @par Scale Limitation
 * - The \p num is greater than 0.
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlRandCreateGenerator function.
 *   Additionally, if using ::CNNL_RAND_RNG_MTGP32 random generator type, you also need to
 *   call the ::cnnlRandSetPseudoRandomGeneratorSeed function, the ::cnnlRandGetMTGP32StateSize
 *   function and the ::cnnlRandMakeMTGP32KernelState function before calling this function.

 * @par Data Type
 * - The supported data types of output are as follows:
 *   - output : half, float on MLU300 series. half, float and bfloat16 on MLU500 series.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      --> type: CNNL_DTYPE_FLOAT;
      --> num: 2048;
      --> mean: 0;
      --> stddev: 1;
      Then we will get the output:
      --> out: an array [2048] of float type between -2 and 2;
     @endverbatim
 */
CNNL_DEPRECATED_FOR(cnnlGenerateRandTruncatedNormal)
cnnlStatus_t CNNL_WIN_API cnnlRandGenerateTruncatedNormal(cnnlHandle_t handle,
                                                          const cnnlRandGenerator_t generator,
                                                          cnnlDataType_t type,
                                                          void *state,
                                                          size_t num,
                                                          float mean,
                                                          float stddev,
                                                          void *out);

// Group:Rand
/*!
 * @brief Generates random numbers of truncated normal distribution in half or float
 * data type on MLU devices.
 *
 * It only supports \p philox mode. To use other modes, call
 * ::cnnlRandGenerateTruncatedNormal.
 * The Philox algorithm has three parameters: seed, offset and subsequence. Seed specifies the key
 * in the algorithm, and offset and subsequence specify the counter in the algorithm. On MLU devices
 * , we use index [0, THREAD_NUM) to bind to subsequence, so only seed and offset arguments are needed.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] rng_type
 *   Input. The rng type of random generator. Only support \p CNNL_RAND_RNG_PHILOX.
 * @param[in] captured
 *   Input. If \p captured is true, use \p seed_ptr, \p offset_ptr and \p offset_intragraph to specify
 *   seed and offset. If \p captured is false, use \p seed and \p offset.
 * @param[in] seed
 *   Input. Specify the seed of the Philox algorithm when \p captured is false.
 * @param[in] offset
 *   Input. Specify the offset of the Philox algorithm when \p captured is false.
 * @param[in] seed_ptr
 *   Input. Pointer to device seed data. Specify the seed of the Philox algorithm when \p captured is true.
 * @param[in] offset_ptr
 *   Input. Pointer to device offset data. Specify the offset of the Philox algorithm when \p captured is true.
 * @param[in] offset_intragraph
 *   Input. The offset in a graph. Specify the offset of the Philox algorithm when \p captured is true.
 * @param[in] mean
 *   Input. The mean value of random numbers to be generated.
 * @param[in] stddev
 *   Input. The standard deviation value of random numbers to be generated.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to device output data, which is the random sequence between
 * (\p mean - 2 * \p stddev) and (\p mean + 2 * \p stddev).
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "RandomNormal Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Scale Limitation
 * - \p num is greater than 0.
 *
 * @par Data Type
 * - The supported data types of output are as follows:
 *   - output : half, float, bfloat16.
 *   The bfloat16 data type is supported only on MLU500 series.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      --> type: CNNL_DTYPE_FLOAT;
      --> num: 2048;
      --> mean: 0;
      --> stddev: 1;
      Then we will get the output:
      --> out: an array [2048] of float type between -2 and 2;
     @endverbatim
 */
cnnlStatus_t CNNL_WIN_API
cnnlGenerateRandTruncatedNormal(cnnlHandle_t handle,
                                cnnlRandRngType_t rng_type,
                                const bool captured,
                                const uint64_t seed,
                                const uint64_t offset,
                                const int64_t *seed_ptr,
                                const int64_t *offset_ptr,
                                const uint32_t offset_intragraph,
                                const float mean,
                                const float stddev,
                                const cnnlTensorDescriptor_t output_desc,
                                void *output);

// Group:RandGenerateMultinomial
/*!
 * @brief Returns in \p size of the MLU memory that is used as an extra workspace to optimize
 * the random multinomial operation.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   random multinomial operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out]  workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used
 *   in the random multinomial operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetRandGenerateMultinomialWorkspaceSize(cnnlHandle_t handle,
                                            const cnnlTensorDescriptor_t x_desc,
                                            size_t *workspace_size);
// Group:RandGenerateMultinomial
/*!
 * @brief Generates random numbers where each row contains indices sampled from the multinomial
 * probability distribution located in the corresponding row of input tensor.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGenerateRandMultinomial instead, which supports \p philox mode only.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] generator
 *   Input. The descriptor of random \p generator. For detailed information,
 *   see ::cnnlRandGenerator_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input data.
 * @param[in] is_replacement
 *   Input. A Boolean scalar. When \p is_replacement is true, the random multinomial operation is
 *   sampling with replacement. When \p is_replacement is false, the random multinomial operation is
 *   sampling without replacement.
 * @param[in,out] state
 *   Input and output. Pointer to device state data, which is used to generate random sequence.
 *   Set NULL if you use \p CNNL_RAND_RNG_FAST \p generator type.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   random multinomial operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the random multinomial operation. You can get the size of the workspace with
 *   the ::cnnlGetRandGenerateMultinomialWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output data.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "RandomMultinomial Operator" section in "Cambricon CNNL User Guide" for details.
 * @par Scale Limitation
 *   - The input and output can be vectors or matrix. In other words, the input
 *     tensor and output tensor support no more than two dimensions.
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlRandCreateGenerator function.
 *   Additionally, if using ::CNNL_RAND_RNG_MTGP32 random generator type, you also need to
 *   call the ::cnnlRandSetPseudoRandomGeneratorSeed function, the ::cnnlRandGetMTGP32StateSize
 *   function and the ::cnnlRandMakeMTGP32KernelState function before calling this function.
 *
 * @par Data Layouts
 * - The supported data layouts of the input tensor and output tensor are as follows:
 *   - input tensor: \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @par Data Type
 * - The supported data types of output are as follows:
 *   - input: half, float.
 *   - output: int32, int64.
 *
 * @note
 * - Users do not need to perform normalization in this operation.
 * - The sum of each line of \p input must be positive and cannot be infinity or NaN.
 * - When the operation perform no-replacement sampling, the non-zero element of each \p input line
 *   must be greater than the number of samples.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      --> input: data type of CNNL_DTYPE_FLOAT, shape of (2, 50048);
      --> output: data type of CNNL_DTYPE_INT32, shape of (2, 1);
      --> is_replacement: true;
      Then you will get the output:
      --> output: an matrix with 2 rows, each row of which is a sampled index
          from an input row with length of 50048;
     @endverbatim
 */
CNNL_DEPRECATED_FOR(cnnlGenerateRandMultinomial)
cnnlStatus_t CNNL_WIN_API
cnnlRandGenerateMultinomial(cnnlHandle_t handle,
                            const cnnlRandGenerator_t generator,
                            const cnnlTensorDescriptor_t input_desc,
                            const void *input,
                            const bool is_replacement,
                            void *state,
                            void *workspace,
                            size_t workspace_size,
                            const cnnlTensorDescriptor_t output_desc,
                            void *output);

// Group:RandGenerateMultinomial
/*!
 * @brief Generates random numbers where each row contains indices sampled from the multinomial
 * probability distribution located in the corresponding row of input tensor.
 *
 * Compared with ::cnnlRandGenerateMultinomial, this function allows \p input to be negative.
 * In this case, each row of \p input represents the unnormalized log-probabilities for all classes.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGenerateRandMultinomial instead, which supports
 *   \p philox mode only.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] generator
 *   Input. The descriptor of random \p generator. For detailed information,
 *   see ::cnnlRandGenerator_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input data.
 * @param[in] is_replacement
 *   Input. A Boolean scalar. When \p is_replacement is true, the random multinomial operation is
 *   sampling with replacement. When \p is_replacement is false, the random multinomial operation is
 *   sampling without replacement.
 * @param[in] is_logits
 *   Input. A Boolean scalar. When \p is_logits is true, each row of \p input represents
 *   the unnormalized log-probabilities for all classes. When \p is_logits is false, the rows of
 *   \p input represents filter, which do not need to sum to one, but must be non-negative,
 *   finite and have a non-zero sum.
 * @param[in,out] state
 *   Input and output. Pointer to device state data, which is used to generate random sequence.
 *   Set NULL if you use \p CNNL_RAND_RNG_FAST \p generator type.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   random multinomial operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the random multinomial operation. You can get the size of the workspace with
 *   the ::cnnlGetRandGenerateMultinomialWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output data.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "RandomMultinomial Operator" section in "Cambricon CNNL User Guide" for details.
 * @par Scale Limitation
 *   - The input and output can be vectors or matrix. In other words, the input
 *     tensor and output tensor support no more than two dimensions.
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlRandCreateGenerator function.
 *   Additionally, if using ::CNNL_RAND_RNG_MTGP32 random generator type, you also need to
 *   call the ::cnnlRandSetPseudoRandomGeneratorSeed function, the ::cnnlRandGetMTGP32StateSize
 *   function and the ::cnnlRandMakeMTGP32KernelState function before calling this function.
 *
 * @par Data Layouts
 * - The supported data layouts of the input tensor and output tensor are as follows:
 *   - input tensor: \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @par Data Type
 * - The supported data types of output are as follows:
 *   - input: half, float, bfloat16.
 *   - output: int32, int64.
 *
 *   The bfloat16 data type is supported only on MLU500 series and when \p is_replacement is false.
 *
 * @note
 * - Users do not need to perform normalization in this operation.
 * - The sum of each line of \p input must be positive and cannot be infinity or NaN.
 * - When the operation perform no-replacement sampling, the non-zero element of each \p input line
 *   must be greater than the number of samples.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      --> input: data type of CNNL_DTYPE_FLOAT, shape of (2, 50048);
      --> output: data type of CNNL_DTYPE_INT32, shape of (2, 1);
      --> is_replacement: true;
      Then you will get the output:
      --> output: an matrix with 2 rows, each row of which is a sampled index
          from an input row with length of 50048;
     @endverbatim
 */
CNNL_DEPRECATED_FOR(cnnlGenerateRandMultinomial)
cnnlStatus_t CNNL_WIN_API
cnnlRandGenerateMultinomial_v2(cnnlHandle_t handle,
                               const cnnlRandGenerator_t generator,
                               const cnnlTensorDescriptor_t input_desc,
                               const void *input,
                               const bool is_replacement,
                               const bool is_logits,
                               void *state,
                               void *workspace,
                               size_t workspace_size,
                               const cnnlTensorDescriptor_t output_desc,
                               void *output);

// Group:RandGenerateMultinomial
/*!
 * @brief Generates random numbers where each row contains indices sampled from the multinomial
 * probability distribution located in the corresponding row of input tensor.
 *
 * It only supports \p philox mode. To use other modes, call
 * ::cnnlRandGenerateMultinomial_v2.
 * The Philox algorithm has three parameters: seed, offset and subsequence. Seed specifies the key
 * in the algorithm, and offset and subsequence specify the counter in the algorithm. On MLU devices
 * , we use index [0, THREAD_NUM) to bind to subsequence, so only seed and offset arguments are needed.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] rng_type
 *   Input. The rng type of random generator. Only support \p CNNL_RAND_RNG_PHILOX.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input data.
 * @param[in] captured
 *   Input. If \p captured is true, use \p seed_ptr, \p offset_ptr and \p offset_intragraph to specify
 *   seed and offset. If \p captured is false, use \p seed and \p offset.
 * @param[in] seed
 *   Input. Specify the seed of the Philox algorithm when \p captured is false.
 * @param[in] offset
 *   Input. Specify the offset of the Philox algorithm when \p captured is false.
 * @param[in] seed_ptr
 *   Input. Pointer to device seed data. Specify the seed of the Philox algorithm when \p captured is true.
 * @param[in] offset_ptr
 *   Input. Pointer to device offset data. Specify the offset of the Philox algorithm when \p captured is true.
 * @param[in] offset_intragraph
 *   Input. The offset in a graph. Specify the offset of the Philox algorithm when \p captured is true.
 * @param[in] is_replacement
 *   Input. A Boolean scalar. When \p is_replacement is true, the random multinomial operation is
 *   sampling with replacement. When \p is_replacement is false, the random multinomial operation is
 *   sampling without replacement.
 * @param[in] is_logits
 *   Input. A Boolean scalar. When \p is_logits is true, each row of \p input represents
 *   the unnormalized log-probabilities for all classes. When \p is_logits is false, the rows of
 *   \p input represent filter, which do not need to sum to one, but must be non-negative,
 *   finite and have a non-zero sum.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   random multinomial operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the random multinomial operation. You can get the size of the workspace with
 *   the ::cnnlGetRandGenerateMultinomialWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output data.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "RandomMultinomial Operator" section in "Cambricon CNNL User Guide" for details.
 * @par Scale Limitation
 *   - The input and output can be vectors or matrix. In other words, the input
 *     tensor and output tensor support no more than two dimensions.
 *
 * @par Data Layouts
 * - The supported data layouts of the input tensor and output tensor are as follows:
 *   - input tensor: \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @par Data Type
 * - The supported data types of output are as follows:
 *   - input: half, float, bfloat16.
 *   - output: int32, int64.
 *
 *   The bfloat16 data type is supported only on MLU500 series and when \p is_replacement is false.
 *
 *
 * @note
 * - Users do not need to perform normalization in this operation.
 * - The sum of each line of \p input must be positive and cannot be infinity or NaN.
 * - When the operation performs no-replacement sampling, the non-zero element of each \p input line
 *   must be greater than the number of samples.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      --> input: data type of CNNL_DTYPE_FLOAT, shape of (2, 50048);
      --> output: data type of CNNL_DTYPE_INT32, shape of (2, 1);
      --> is_replacement: true;
      Then you will get the output:
      --> output: an matrix with 2 rows, each row of which is a sampled index
          from an input row with length of 50048;
     @endverbatim
 */
cnnlStatus_t CNNL_WIN_API cnnlGenerateRandMultinomial(cnnlHandle_t handle,
                                                      cnnlRandRngType_t rng_type,
                                                      const cnnlTensorDescriptor_t input_desc,
                                                      const void *input,
                                                      const bool captured,
                                                      const uint64_t seed,
                                                      const uint64_t offset,
                                                      const int64_t *seed_ptr,
                                                      const int64_t *offset_ptr,
                                                      const uint32_t offset_intragraph,
                                                      const bool is_replacement,
                                                      const bool is_logits,
                                                      void *workspace,
                                                      size_t workspace_size,
                                                      const cnnlTensorDescriptor_t output_desc,
                                                      void *output);

// Group:RNN
/*!
 * @brief Computes the backward process of RNN network in the training scenario. The specific
 *        network structure is determined by the descriptor \p rnn_desc set by the user.
 *        Using the input data \p y, \p dy, \p x, \p hx, \p dhy, \p cx, \p dcy and \p weightspace,
 *        and according to the specific network structure, this function writes the calculation
 *        result into the output memory \p dx, \p dhx, \p dcx and \p dweightspace.
 *
 * This function requires three additional MLU memory \p reservespace_forward, \p reservespace_backward
 * and \p workspace to improve RNN network performance. You can get the size of the \p workspace
 * and \p reservespace_forward and \p reservespace_backward with ::cnnlGetRNNTempSizes. You can get
 * the size of the \p weightspace with ::cnnlGetRNNWeightSpaceSize.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the RNN operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] rnn_desc
 *   Input. The descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[in] dev_seq_lengths
 *   Input. A copy of \p seqLengthArray set in \p x_desc or \p y_desc RNN data descriptor.
 *   \p dev_seq_lengths array must be stored in MLU memory.
 * @param[in] y_desc
 *   Input. The descriptor of input sequence data.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores input sequence data.
 * @param[in] dy
 *   Input. Pointer to the MLU memory that stores the gradient of the loss function
 *   with respect to \p y.
 * @param[in] x_desc
 *   Input. A previously initialized RNN data descriptor corresponding to the
 *   gradient of the loss function with respect to the RNN primary model input.
 * @param[out] x
 *   Output. Pointer to the MLU memory that stores the RNN primary input x.
 * @param[out] dx
 *   Output. Pointer to the MLU memory that stores the gradient of the loss function
 *   with respect to the RNN primary input \p x.
 * @param[in] hx_desc
 *   Input. The descriptor of the hidden state tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] hx
 *   Input. Pointer to the MLU memory that stores the hidden state tensor.
 * @param[in] dhy
 *   Input. Pointer to the MLU memory that stores gradient deltas \p dhy.
 * @param[out] dhx
 *   Output. Pointer to the MLU memory that stores gradient deltas \p dhx.
 * @param[in] cx_desc
 *   Input. The descriptor of the cell state tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] cx
 *   Input. Pointer to the MLU memory that stores input cell state tensor.
 * @param[in] dcy
 *   Input. Pointer to the MLU memory that stores gradient deltas \p dcy.
 * @param[out] dcx
 *   Output. Pointer to the MLU memory that stores gradient deltas \p dcx.
 * @param[in] weightspace
 *   Input. Pointer to the MLU memory that stores filter and bias.
 * @param[in] dweightspace
 *   Input. Pointer to the MLU memory that stores the gradient of the loss function
 *   with respect to \p weightspace.
 * @param[in] weightspace_size_bytes
 *   Input. Specifies The size of the buffer in bytes that stores filters.
 *   You can call ::cnnlGetRNNWeightSpaceSize to get the size of the buffer to be used.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   RNN operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size_bytes
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the RNN operation.
 *   You can get the size of the workspace with ::cnnlGetRNNTempSizes.
 * @param[in] reservespace_forward
 *   Input. Pointer to the MLU memory that is used as an extra memory space for saving
 *   intermediate results of RNN forward operation.
 * @param[in] reservespace_backward
 *   Input. Pointer to the MLU memory that is used as an extra memory space for saving
 *   intermediate results of RNN backward operation.
 * @param[in] reservespace_size_bytes
 *   Input. The size of the extra reservespace in bytes that needs to be used in
 *   the RNN operation.
 *   You can get the size of the reservespace with ::cnnlGetRNNTempSizes.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "RNN Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layouts set in descriptors are as follows:
 *   - \p x_desc must be set to \p ::CNNL_SEQDATA_TNC, ::CNNL_SEQDATA_NTC or ::CNNL_SEQDATA_TNC_PACKED.
 *   - \p y_desc must be set to be the same layout as \p x_desc.
 *   - \p hx_desc must be set to \p ::CNNL_LAYOUT_ARRAY, and \p dimNb in \p hx_desc must be 3.
 *   - \p cx_desc must be set to \p ::CNNL_LAYOUT_ARRAY, and \p dimNb in \p cx_desc must be 3.
 *
 * @par Scale Limitation
 * - The size of the dim C of \p x_desc should be less than 3100.
 * - The size of the dim C of \p hx_desc should be less than 3100.
 * - The size of the dim C of \p cx_desc should be less than 3100.
 * - When \p cnnlSeqDataLayout_t
 *   is CNNL_SEQDATA_TNC_PACKED, \p dev_seq_lengths must be batch's sequence and in the descending order, and the length of
 *   \p dev_seq_lengths must be equal to x_desc->dims[0].
 *
 * @par API Dependency
 * - Before calling this function, you need to call ::cnnlCreateRNNDescriptor and ::cnnlSetRNNDescriptor_v2
 *   to create and set the RNN operation descriptor, and call ::cnnlCreateSeqDataDescriptor and
 *   ::cnnlSetSeqDataDescriptor to create and set the sequence data descriptors \p x_desc and \p y_desc.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance,
 *   set the layout of the input sequence data \p x_desc and output sequence data \p y_desc
 *   to ::CNNL_SEQDATA_TNC.
 *
 * @note
 * - \p rnn_mode of \p rnn_desc only supports ::CNNL_LSTM with projection layer.
 * - \p input_mode of \p rnn_desc only supports ::CNNL_RNN_LINEAR_INPUT.
 * - \p padding_mode of \p rnn_desc only supports ::CNNL_RNN_PADDED_IO_DISABLED.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      --> x: data type of CNNL_DTYPE_FLOAT, shape of (1, 1, 8);
      --> hx: data type of CNNL_DTYPE_FLOAT, shape of (2, 1, 10);
      --> cx: data type of CNNL_DTYPE_FLOAT, shape of (2, 1, 10);
      --> dy: data type of CNNL_DTYPE_FLOAT, shape of (1, 1, 20);
      --> dhy: data type of CNNL_DTYPE_FLOAT, shape of (2, 1, 10);
      --> dcy: data type of CNNL_DTYPE_FLOAT, shape of (2, 1, 10);
      --> weightspace: data type of CNNL_DTYPE_FLOAT, shape of (1440);
      --> reservespace_forward: data type of CNNL_DTYPE_FLOAT, shape of (120);
      Then you will get the output:
      --> dx: data type of CNNL_DTYPE_FLOAT, shape of (1, 1, 8);
      --> dhx: data type of CNNL_DTYPE_FLOAT, shape of (2, 1, 10);
      --> dcx: data type of CNNL_DTYPE_FLOAT, shape of (2, 1, 10);
      --> dweightspace: data type of CNNL_DTYPE_FLOAT, shape of (1440);
      --> reservespace_backward: data type of CNNL_DTYPE_FLOAT, shape of (120);
     @endverbatim
 */
cnnlStatus_t CNNL_WIN_API cnnlRNNBackward(cnnlHandle_t handle,
                                          const cnnlRNNDescriptor_t rnn_desc,
                                          const int32_t *dev_seq_lengths,
                                          const cnnlSeqDataDescriptor_t y_desc,
                                          const void *y,
                                          const void *dy,
                                          const cnnlSeqDataDescriptor_t x_desc,
                                          const void *x,
                                          void *dx,
                                          const cnnlTensorDescriptor_t hx_desc,
                                          const void *hx,
                                          const void *dhy,
                                          void *dhx,
                                          const cnnlTensorDescriptor_t cx_desc,
                                          const void *cx,
                                          const void *dcy,
                                          void *dcx,
                                          const void *weightspace,
                                          void *dweightspace,
                                          size_t weightspace_size_bytes,
                                          void *workspace,
                                          size_t workspace_size_bytes,
                                          void *reservespace_forward,
                                          void *reservespace_backward,
                                          size_t reservespace_size_bytes);
// Group:RNN
/*!
 * @brief Computes the backward process of RNN network in the training scenario. The specific
 *        network structure is determined by the descriptor \p rnn_desc set by the user.
 *        Using the input data \p y, \p dy, \p hx, \p dhy, \p cx, \p dcy and \p weightspace,
 *        and according to the specific network structure, this function writes the calculation
 *        result into the output memory \p dx, \p dhx and \p dcx.
 *
 * This function requires three additional MLU memory \p reservespace_forward, \p reservespace_backward
 * and \p workspace to improve the RNN network performance. You can get the size of the \p workspace
 * and \p reservespace_forward, and \p reservespace_backward with ::cnnlGetRNNTempSizes. You can get
 * the size of the \p weightspace with ::cnnlGetRNNWeightSpaceSize.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the RNN operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] rnn_desc
 *   Input. The descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[in] dev_seq_lengths
 *   Input. A copy of \p seqLengthArray set in \p x_desc or \p y_desc RNN data descriptor.
 *   \p dev_seq_lengths array must be stored in MLU memory.
 * @param[in] y_desc
 *   Input. The descriptor of input sequence data.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores input sequence data.
 * @param[in] dy
 *   Input. Pointer to the MLU memory that stores the gradient of the loss function
 *   with respect to \p y.
 * @param[in] x_desc
 *   Input. A previously initialized RNN data descriptor corresponding to the
 *   gradient of the loss function with respect to the RNN primary model input.
 * @param[out] dx
 *   Output. Pointer to the MLU memory that stores the gradient of the loss function
 *   with respect to the RNN primary input x.
 * @param[in] hx_desc
 *   Input. The descriptor of the hidden state tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] hx
 *   Input. Pointer to the MLU memory that stores the hidden state tensor.
 * @param[in] dhy
 *   Input. Pointer to the MLU memory that stores gradient deltas \p dhy.
 * @param[out] dhx
 *   Output. Pointer to the MLU memory that stores gradient deltas \p dhx.
 * @param[in] cx_desc
 *   Input. The descriptor of the cell state tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] cx
 *   Input. Pointer to the MLU memory that stores input cell state tensor.
 * @param[in] dcy
 *   Input. Pointer to the MLU memory that stores gradient deltas \p dcy.
 * @param[out] dcx
 *   Output. Pointer to the MLU memory that stores gradient deltas \p dcx.
 * @param[in] weightspace
 *   Input. Pointer to the MLU memory that stores filter and bias.
 * @param[in] weightspace_size
 *   Input. The size of the buffer in bytes that stores filters.
 *   You can call ::cnnlGetRNNWeightSpaceSize to get the size of the buffer to be used.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   RNN operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the RNN operation.
 *   You can get the size of the workspace with ::cnnlGetRNNTempSizes.
 * @param[in] reservespace
 *   Input. Pointer to the MLU memory that is used as an extra memory space for saving
 *   intermediate results of RNN operation.
 * @param[in] reservespace_size
 *   Input. The size of the extra reservespace in bytes that needs to be used in
 *   the RNN operation.
 *   You can get the size of the reservespace with ::cnnlGetRNNTempSizes.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "RNN Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layouts set in descriptors are as follows:
 *   - \p x_desc must be set to ::CNNL_SEQDATA_TNC, ::CNNL_SEQDATA_NTC or ::CNNL_SEQDATA_TNC_PACKED.
 *   - \p y_desc must be set to be the same layout as \p x_desc.
 *   - \p hx_desc must be set to ::CNNL_LAYOUT_ARRAY, and \p dimNb in \p hx_desc must be 3.
 *   - \p cx_desc must be set to ::CNNL_LAYOUT_ARRAY, and \p dimNb in \p cx_desc must be 3.
 *
 * @par Scale Limitation
 * - The size of the dim C of \p x_desc should be less than 3100.
 * - The size of the dim C of \p hx_desc should be less than 3100.
 * - The size of the dim C of \p cx_desc should be less than 3100.
 * - When \p cnnlSeqDataLayout_t is CNNL_SEQDATA_TNC_PACKED,
 *   \p dev_seq_lengths must be batch's sequence and in the descending order, and the length of
 *   \p dev_seq_lengths must be equal to x_desc->dims[0].
 *
 * @par API Dependency
 * - Before calling this function, you need to call ::cnnlCreateRNNDescriptor and ::cnnlSetRNNDescriptor_v2
 *   to create and set the RNN operation descriptor, and call ::cnnlCreateSeqDataDescriptor and
 *   ::cnnlSetSeqDataDescriptor to create and set the sequence data descriptors \p x_desc and \p y_desc.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance,
 *   set the layout of the input sequence data \p x_desc and output sequence data \p y_desc
 *   to ::CNNL_SEQDATA_TNC.
 *
 * @note
 * - \p rnn_mode of \p rnn_desc only supports ::CNNL_LSTM with projection layer.
 * - \p input_mode of \p rnn_desc only supports ::CNNL_RNN_LINEAR_INPUT.
 * - \p padding_mode of \p rnn_desc only supports ::CNNL_RNN_PADDED_IO_DISABLED.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlRNNBackwardData(cnnlHandle_t handle,
                                              const cnnlRNNDescriptor_t rnn_desc,
                                              const int32_t dev_seq_lengths[],
                                              const cnnlSeqDataDescriptor_t y_desc,
                                              const void *y,
                                              const void *dy,
                                              const cnnlSeqDataDescriptor_t x_desc,
                                              void *dx,
                                              const cnnlTensorDescriptor_t hx_desc,
                                              const void *hx,
                                              const void *dhy,
                                              void *dhx,
                                              const cnnlTensorDescriptor_t cx_desc,
                                              const void *cx,
                                              const void *dcy,
                                              void *dcx,
                                              const void *weightspace,
                                              size_t weightspace_size,
                                              void *workspace,
                                              size_t workspace_size,
                                              void *reservespace,
                                              size_t reservespace_size);

// Group:LSTMGatesForward
/*!
 * @brief Returns the size of the MLU memory that is used as an extra workspace to optimize
 * the ::cnnlLSTMGatesForward operation, and pass the intermediate results to
 * ::cnnlLSTMGatesBackward operation.
 *
 * For more information about the extra \p workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the LSTM operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] gates_desc
 *   Input. The descriptor of gates tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] reservespace_size
 *   Output. The size of the reservespace in bytes that needs to be used in
 *   the LSTM operation. The reservespace is used to pass intermediate results.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The allocated extra workspace memory should be passed to the ::cnnlLSTMGatesForward function,
 *   and ::cnnlLSTMGatesBackward function to perform the LSTM forward or backward operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */

cnnlStatus_t CNNL_WIN_API cnnlGetLSTMGatesTempSize(cnnlHandle_t handle,
                                                   const cnnlTensorDescriptor_t gates_desc,
                                                   size_t *reservespace_size);

// Group:LSTMGatesBackward
/*!
 * @brief Computes the backward process of LSTM cell in the training scenario. The specific
 *        network structure is determined to get the gradient of hidden gates in LSTM cell.
 *        Based on input data \p cx, \p cy, \p grad_hy, \p grad_cy, and \p reservespace,
 *        it writes the calculation result into the output memory \p grad_gates and \p grad_cx.
 *
 * This function requires additional MLU memory as the \p reservespace to
 * improve the performance. You can get the size of the \p reservespace
 * \p reservespace_size with the ::cnnlGetLSTMGatesTempSize function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the LSTM operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] grad_hy_desc
 *   Input. The descriptor of hidden state tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] grad_hy
 *   Input. Pointer to the MLU memory that stores gradient of output hidden state tensor.
 * @param[in] cx_desc
 *   Input. The descriptor of cell state tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] cx
 *   Input. Pointer to the MLU memory that stores input cell state tensor.
 * @param[in] cy
 *   Input. Pointer to the MLU memory that stores output cell state tensor.
 * @param[in] grad_cy
 *   Input. Pointer to the MLU memory that stores gradient of output cell state tensor.
 * @param[out] grad_cx
 *   Output. Pointer to the MLU memory that stores gradient of input cell state tensor.
 * @param[in] grad_gates_desc
 *   Input. The descriptor of gates tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] grad_gates
 *   Output. Pointer to the MLU memory that stores gradient of gates tensor which consists of
 *   input gates, forget gates, hidden gates and output gates, and the order is \p CNNL_LSTM_IFGO.
 * @param[in] reservespace
 *   Input. Pointer to the MLU memory that is used as an extra memory space for saving
 *   intermediate results of LSTM operation.
 * @param[in] reservespace_size
 *   Input. The size of the extra reservespace in bytes that needs to be used in
 *   the LSTM operation.
 *   You can get the size of the reservespace with the ::cnnlGetLSTMGatesTempSize function.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "LSTMGatesBackward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layouts set in descriptors are as follows:
 *   - \p grad_hy_desc must be set to ::CNNL_LAYOUT_NC and \p dimNb in \p grad_hy_desc must be 2.
 *   - \p cx_desc must be set to ::CNNL_LAYOUT_NC and \p dimNb in \p cx_desc must be 2.
 *   - \p grad_gates_desc must be set to ::CNNL_LAYOUT_GNC or ::CNNL_LAYOUT_NGC and \p dimNb
 *     in \p grad_gates_desc must be 3.
 *
 * @par Scale Limitation
 * - The \p grad_hy will be initialized to zero when \p grad_hy is set to NULL, and \p grad_cy will
 *   also be initialized to zero when \p grad_cy is set to NULL.
 *
 * @par API Dependency
 * - Before calling this function to implement LSTM operation, you need to prepare
 *   all the parameters passed to this function. See each parameter description
 *   for details. And you need to call ::cnnlGetLSTMGatesTempSize function before calling this
 *   function.
 *
 */

cnnlStatus_t CNNL_WIN_API cnnlLSTMGatesBackward(cnnlHandle_t handle,
                                                const cnnlTensorDescriptor_t grad_hy_desc,
                                                const void *grad_hy,
                                                const cnnlTensorDescriptor_t cx_desc,
                                                const void *cx,
                                                const void *cy,
                                                const void *grad_cy,
                                                void *grad_cx,
                                                const cnnlTensorDescriptor_t grad_gates_desc,
                                                void *grad_gates,
                                                void *reservespace,
                                                size_t reservespace_size);

/*!
 * @brief Enumeration variables describing the mode that is used in the quantization strategy
 *        function.
 *
 */
typedef enum {
  CNNL_ADAPTIVE_BITWIDTH_AND_INTERVAL = 0, /*!< The adaptive bitwidth and interval is applied.*/
  CNNL_ADAPTIVE_INTERVAL = 1, /*!< The adaptive interval and constant bitwidth is applied.*/
  CNNL_ADAPTIVE_BITWIDTH = 2, /*!< The adaptive bitwidth and constant interval is applied.*/
} cnnlQuantizeStrategyMode_t;

/*!
 * @brief Enumeration variables describing the maximum output bitwidth that is used in the
 *        quantization strategy function.
 *
 */
typedef enum {
  CNNL_MAX_BITWIDTH_INT16 = 0, /*!< The output bitwidth cannot exceed 16.*/
  CNNL_MAX_BITWIDTH_INT31 = 1, /*!< The output bitwidth cannot exceed 31.*/
} cnnlQuantizeStrategyMaxBitwidth_t;

/*!
 * @brief A struct describing the hyperparameters
 *        that are used in the quantization strategy function.
 *
 */
typedef struct cnnlQuantizeStrategyParam {
  float alpha; /*!< Related to moving position with the scope of (0, 0.4).*/
  float beta;  /*!< Related to interval with the scope of (0, 1).*/
  float gamma; /*!< Related to interval with the scope of [0, 100].*/
  float delta; /*!< Related to bitwidth with the scope of (0, 1000).*/
  float th;    /*!< Related to bitwidth with the scope of (0, 0.5).*/
} cnnlQuantizeStrategyParam_t;

// Group:QuantizeStrategy
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra
 *        workspace to optimize the quantization strategy operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is
 *   used in the quantization strategy operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetQuantizeStrategyWorkspaceSize(cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t input_desc,
                                     size_t *workspace_size);
// Group:QuantizeStrategy
/*!
 * @brief Creates an operation of quantization strategy. The operation is used to
 *        update \p bitwidth, \p position, moving position, \p interval,
 *        \p is_exceed_max_bitwidth.
 *        For more information, see "Cambricon CNNL User Guide".
 *
  * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues. For detailed information, see ::cnnlHandle_t.
 * @param[in] quant_strategy_mode
 *   Input. An enum with information of adaptive quantization modes used in this operation.
 *   For detailed information, see ::cnnlQuantizeStrategyMode_t.
 * @param[in] quant_strategy_param
 *   Input. A struct with information of hyper parameters used in this operation. For detailed
 *   information, see ::cnnlQuantizeStrategyParam_t.
 * @param[in] max_bitwidth
 *   Input. An enum with information of maximum bitwidth used in this operation. For detailed
 *   information, see ::cnnlQuantizeStrategyMaxBitwidth_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input data.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   quantization strategy operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the quantization strategy operation. You can get the size of the workspace with
 *   the ::cnnlGetQuantizeStrategyWorkspaceSize function.
 * @param[in] bitwidth_input
 *   Input. Pointer to the scalar of input bitwidth.
 * @param[in] position_input
 *   Input. Pointer to the scalar of input position.
 * @param[in] moving_position_input
 *   Input. Pointer to the scalar of input moving position.
 * @param[out] bitwidth_output
 *   Output. Pointer to the scalar of output bitwidth.
 * @param[out] position_output
 *   Output. Pointer to the scalar of output position.
 * @param[out] moving_position_output
 *   Output. Pointer to the scalar of output moving position.
 * @param[out] interval
 *   Output. Pointer to the scalar of output interval.
 * @param[out] is_exceed_max_bitwidth
 *   Output. Pointer to the scalar of output. When the maximum bitwidth exceeds the
 *   input \p max_bitwidth, it will output 1. Otherwise, it will output 0.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - input: float, half.
 * - bitwidth: int32.
 * - position: int32.
 * - moving position: float.
 * - interval: int32.
 * - is_exceed_max_bitwdith: int32.
 *
 * @par API Dependency
 * - ::cnnlGetQuantizeStrategyWorkspaceSize should be called to get the workspace
 *     size before ::cnnlQuantizeStrategy.
 *
 * @note
 * - The \p bitwidth_input pointer can be the same as the \p bitwidth_output pointer.
 * - The \p position_input pointer can be the same as the \p position_output pointer.
 * - The \p moving_position_input pointer can be the same as the \p moving_position_output
 *     pointer.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlQuantizeStrategy(cnnlHandle_t handle,
                     const cnnlQuantizeStrategyMode_t quant_strategy_mode,
                     const cnnlQuantizeStrategyParam_t quant_strategy_param,
                     const cnnlQuantizeStrategyMaxBitwidth_t max_bitwidth,
                     const cnnlTensorDescriptor_t input_desc,
                     const void *input,
                     void *workspace,
                     size_t workspace_size,
                     const void *bitwidth_input,
                     const void *position_input,
                     const void *moving_position_input,
                     void *bitwidth_output,
                     void *position_output,
                     void *moving_position_output,
                     void *interval,
                     void *is_exceed_max_bitwidth);


// Group:QuantizeParam
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra
 *        workspace to optimize the computing quantization parameters operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is
 *   used in the computing quantization parameters operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlQuantizeParam function
 *   to perform the computing quantization parameters operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetQuantizeParamWorkspaceSize(cnnlHandle_t handle,
                                   const cnnlTensorDescriptor_t input_desc,
                                   size_t *workspace_size);

// Group:QuantizeParam
/*!
 * @brief Creates an operation of computing quantization parameters used in quantization. For more
 * information about quantization, see "Cambricon CNNL User Guide".
 *
 * @deprecated
 * Quantization from half to int16 is deprecated and will be removed in future release.
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlQuantizeParam_v2 instead, which supports quantization parameters in vector
 * form, and more quantified data types such as int4*2 and fp8.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues. For detailed information, see ::cnnlHandle_t.
 * @param[in] mode
 *   Input. An enum with information of quantization mode. For more information, ::cnnlQuantizeMode_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input data.
 * @param[in] bitwidth
 *   Input. A scalar of quantization width, it supports 8, 16, 31.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   computing quantization parameter operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the computing quantization parameter operation. You can get the size of the workspace with
 *   the ::cnnlGetQuantizeParamWorkspaceSize function.
 * @param[out] position_output
 *   Output. Pointer to the scalar of position.
 * @param[out] scale_output
 *   Output. Pointer to the scalar of scale.
 * @param[out] offset_output
 *   Output. Pointer to the scalar of offset.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - input: float, half, bfloat16.
 * - position: int32.
 * - scale: float.
 * - offset: int32.
 *
 * The bfloat16 data type is supported only on MLU500 series.
 *
 * @par API Dependency
 * - ::cnnlGetQuantizeParamWorkspaceSize should be called to get the workspace
 *   size before ::cnnlQuantizeParam.
 *
 * @note
 * - When the mode is ::CNNL_QUANTIZE_POSITION, \p scale_output and \p offset_output can either be
 *   NULL or not NULL.
 * - When the mode is ::CNNL_QUANTIZE_POSITION_SCALE, \p offset_output can either be NULL or
 *   not NULL.
 * - When the mode is ::CNNL_QUANTIZE_POSITION_SCALE_OFFSET, \p position_output, \p scale_output and
 *   \p offset_output cannot be NULL.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlQuantizeParam_v2)
cnnlStatus_t CNNL_WIN_API cnnlQuantizeParam(cnnlHandle_t handle,
                                            cnnlQuantizeMode_t mode,
                                            const cnnlTensorDescriptor_t input_desc,
                                            const void *input,
                                            int bitwidth,
                                            void *workspace,
                                            size_t workspace_size,
                                            void *position_output,
                                            void *scale_output,
                                            void *offset_output);
// Group:QuantizeParam
/*!
 * @brief Creates an operation of computing quantization parameters used in quantization. For more
 * information about quantization, see "Cambricon CNNL User Guide".
 *
 * @deprecated
 * - Quantization from half to int16 is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues. For detailed information, see ::cnnlHandle_t.
 * @param[in] quant_scheme
 *   Input. An enum with information of quantization mode, see ::cnnlQuantizeScheme_t.
 * @param[in] quant_mode
 *    Input. An enum with information of quantization parameters. For more information, see ::cnnlQuantizeMode_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input data.
 * @param[in] data_type
 *   Input. Quantized data type, which supports int8, int16 and int31.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   computing quantization parameter operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the computing quantization parameter operation. You can get the size of the workspace with
 *   the ::cnnlGetQuantizeParamWorkspaceSize function.
 * @param[out] position_desc
 *   Output. The descriptor of the position tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] position_output
 *   Output. Pointer to the tensor of position.
 * @param[out] scale_desc
 *   Output. The descriptor of the scale tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] scale_output
 *   Output. Pointer to the tensor of scale.
 * @param[out] offset_desc
 *   Output. The descriptor of the offset tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] offset_output
 *   Output. Pointer to the tensor of offset.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - input: float, half, bfloat16.
 * - position: int32.
 * - scale: float.
 * - offset: int32.
 *
 * The bfloat16 data type is supported only on MLU500 series. When input data type is bfloat16,
 * parameter \p data_type should be CNNL_DTYPE_INT8.
 *
 * @par API Dependency
 * - Before calling ::cnnlQuantizeParam_v2, call ::cnnlGetQuantizeParamWorkspaceSize to
 *   get the workspace size.
 *
 * @note
 * - If you do not want to compute a quantization parameter, set its corresponding output pointer to nullptr.
 * - The quantization parameters are only supported as scalars currently. Quantization parameters in the
 *   form of tensor will be supported in future release.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlQuantizeParam_v2(cnnlHandle_t handle,
                                               cnnlQuantizeScheme_t quant_scheme,
                                               cnnlQuantizeMode_t quant_mode,
                                               const cnnlTensorDescriptor_t input_desc,
                                               const void *input,
                                               cnnlDataType_t data_type,
                                               void *workspace,
                                               size_t workspace_size,
                                               cnnlTensorDescriptor_t position_desc,
                                               void *position_output,
                                               cnnlTensorDescriptor_t scale_desc,
                                               void *scale_output,
                                               cnnlTensorDescriptor_t offset_desc,
                                               void *offset_output);

// Group:Quantize
/*!
 * @brief Creates an operation of quantization for quantizing floating-point data to fixed-point
 *  data with the given input descriptor \p input_desc of the input data\p input, and the
 *  quantization mode \p mode. ::cnnlQuantizeV1 supports the quantization parameter \p inputs from
 *  host. For more information about quantization, see "Cambricon CNNL User Guide".
 *
 * @deprecated
 * Quantization from half to int16 is deprecated and will be removed in future release.
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlQuantize instead, which supports quantization parameter \p inputs
 * from host or MLU device.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the quantization operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] mode
 *   Input. An enum with information of quantization mode. For more information, see ::cnnlQuantizeMode_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input data.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Input. Pointer to the MLU memory that stores the quantized data.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Data Type
 * - input: float, half, bfloat16.
 * - output: int8, int16, int31.
 *
 * The bfloat16 data type is supported only on MLU500 series.
 *
 * @par API Dependency
 * - The ::cnnlSetTensorDescriptorPosition, ::cnnlSetTensorDescriptorPositionAndScale or
 *   ::cnnlSetTensorDescriptorPositionScaleAndOffset function should be called to set the
 *   quantization parameters to the output descriptor \p output_desc before this function.
 *   ::cnnlSetTensorDescriptorPosition is used in ::CNNL_QUANTIZE_POSITION mode.
 *   ::cnnlSetTensorDescriptorPositionAndScale is used in ::CNNL_QUANTIZE_POSITION_SCALE mode.
 *   ::cnnlSetTensorDescriptorPositionScaleAndOffset is used in
 *   ::CNNL_QUANTIZE_POSITION_SCALE_OFFSET mode.
 *
 * @note
 * - When the data type of \p output_desc descriptor is CNNL_DTYPE_INT31,
 *   ::CNNL_QUANTIZE_POSITION_SCALE_OFFSET is not supported.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlQuantize)
cnnlStatus_t CNNL_WIN_API cnnlQuantizeV1(cnnlHandle_t handle,
                                         cnnlQuantizeMode_t mode,
                                         const cnnlTensorDescriptor_t input_desc,
                                         const void *input,
                                         const cnnlTensorDescriptor_t output_desc,
                                         void *output);

// Group:Quantize
/*!
 * @brief Creates an operation of quantization for quantizing floating-point data to fixed-point
 *  data with the given input descriptor \p input_desc of the input data\p input, and the
 *  quantization mode \p mode. ::cnnlQuantizeV2 supports the quantization parameter \p inputs from
 *  MLU device. For more information about quantization, see "Cambricon CNNL User Guide".
 *
 * @deprecated
 * Quantization from half to int16 is deprecated and will be removed in future release.
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlQuantize instead, which supports quantization parameter \p inputs
 * from host or MLU device.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the quantization operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] mode
 *   Input. An enum with information of quantization mode. For more information, see ::cnnlQuantizeMode_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input data.
 * @param[in] position
 *   Input. A pointer to the scalar of fixed position.
 * @param[in] scale
 *   Input. A pointer to the scalar of scale factor.
 * @param[in] offset
 *   Input. A pointer to the scalar of offset.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Input. Pointer to the MLU memory that stores the quantized data.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Data Type
 * - input: float, half, bfloat16.
 * - position: int32.
 * - scale: float.
 * - offset: int32.
 * - output: int8, int16, int31.
 *
 * The bfloat16 data type is supported only on MLU500 series.
 *
 * @note
 * - When the data type of \p output_desc descriptor is CNNL_DTYPE_INT31,
 *   ::CNNL_QUANTIZE_POSITION_SCALE_OFFSET is not supported.
 * - When the mode is ::CNNL_QUANTIZE_POSITION, scale and offset can either be NULL or
 *   not NULL.
 * - When the mode is ::CNNL_QUANTIZE_POSITION_SCALE, offset can either be NULL or not NULL.
 * - When the mode is ::CNNL_QUANTIZE_POSITION_SCALE_OFFSET, position, scale and offset
 *   cannot be NULL.
 * - The difference between ::cnnlQuantizeV1 and ::cnnlQuantizeV2 is that:
 *   ::cnnlQuantizeV1 supports the quantization parameter \p inputs from host.
 *   ::cnnlQuantizeV2 supports the quantization parameter \p inputs from MLU device.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlQuantize)
cnnlStatus_t CNNL_WIN_API cnnlQuantizeV2(cnnlHandle_t handle,
                                         cnnlQuantizeMode_t mode,
                                         const cnnlTensorDescriptor_t input_desc,
                                         const void *input,
                                         const void *position,
                                         const void *scale,
                                         const void *offset,
                                         const cnnlTensorDescriptor_t output_desc,
                                         void *output);

// Group:Quantize
/*!
 * @brief Creates an operation of quantization for quantizing floating-point data to fixed-point
 *  data with the given input descriptor \p input_desc of the input data\p input, and the
 *  quantization mode \p mode. ::cnnlQuantize supports the quantization parameter \p inputs from host
 *  or MLU device. For more information about quantization, see "Cambricon CNNL User Guide".
 *
 * @deprecated
 * Quantization from half to int16 is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the quantization operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] quant_desc
 *   Input. A quantization descriptor that holds quantization information. For detailed information,
 *   see ::cnnlQuantizeExDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input data.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Input. Pointer to the MLU memory that stores the quantized data.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Data Type
 * - input: float, half, bfloat16.
 * - output: int8, int16, int31.
 *
 * The bfloat16 data type is supported only on MLU500 series.
 *
 * @note
 * - When the data type of \p output_desc descriptor is CNNL_DTYPE_INT31,
 *   ::CNNL_QUANTIZE_POSITION_SCALE_OFFSET is not supported.
 * - When the mode is ::CNNL_QUANTIZE_POSITION, scale and offset can either be NULL or
 *   not NULL.
 * - When the mode is ::CNNL_QUANTIZE_POSITION_SCALE, offset can either be NULL or not NULL.
 * - When the mode is ::CNNL_QUANTIZE_POSITION_SCALE_OFFSET, position, scale and offset
 *   cannot be NULL.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlQuantize(cnnlHandle_t handle,
                                       const cnnlQuantizeExDescriptor_t quant_desc,
                                       const cnnlTensorDescriptor_t input_desc,
                                       const void *input,
                                       const cnnlTensorDescriptor_t output_desc,
                                       void *output);

/******************************************************************************
 * Cambricon CNNL OP: CTC_Loss
 ******************************************************************************/
/*!
 * @brief Enumeration variables controlling the input normalization modes for CTC_Loss operation.
 * This type is used with ::cnnlSetCTCLossDescriptor.
 *
 */
typedef enum {
  CNNL_NONE_NORMALIZATION = 0,
  /*!< The input data of ::cnnlCTCLoss() function is expected to be the unnormalized activation,
       and the output gradient is the gradient of loss with respect to the unnormalized
       activation. Internally the probability is computed by softmax normalization.*/
  CNNL_SOFTMAX_NORMALIZATION = 1,
  /*!< The input data of ::cnnlCTCLoss() function is expected to be the normalized probability
       by softmax, and the output gradient is the gradient of loss with respect to the
       unnormalized activation.*/
  CNNL_LOG_SOFTMAX_NORMALIZATION = 2,
  /*!< The input data of ::cnnlCTCLoss() function is expected to be the normalized probability
       by log_softmax, and the output gradient is the gradient of loss with respect to the
       unnormalized activation.*/
} cnnlCTCLossNormalizationMode_t;

/*!
 * @brief Enumeration variables describing the reduction mode of loss for CTC_Loss operation.
 * This type is used with ::cnnlSetCTCLossDescriptor.
 *
 */
typedef enum {
  CNNL_REDUCE_MODE_NONE = 0,
  /*!< The loss of ::cnnlCTCLoss will not be reduced, and the dimension of loss should be equal to
       the batch size of input.*/
  CNNL_REDUCE_MODE_SUM = 1,
  /*!< The loss of ::cnnlCTCLoss will be summed.*/
  CNNL_REDUCE_MODE_MEAN_BY_BATCH = 2,
  /*!< The loss of ::cnnlCTCLoss will be summed and divided by the batch size of input.*/
  CNNL_REDUCE_MODE_MEAN_BY_INPUT_LENGTHS = 3,
  /*!< The loss of ::cnnlCTCLoss will be summed and divided by the input lengths of ::cnnlCTCLoss.*/
  CNNL_REDUCE_MODE_MEAN_BY_LABEL_LENGTH_AND_BATCH = 4,
  /*!< The loss of ::cnnlCTCLoss will be summed and divided by the label lengths of ::cnnlCTCLoss
       and the batch size of input.*/
} cnnlCTCLossReduceMode_t;


/*!
 * @brief Enumeration variables describing whether to zero infinite losses and the associated
 * gradients. This type is used with ::cnnlSetCTCLossDescriptor.
 *
 */
typedef enum {
  CNNL_ZERO_INFINITY = 0,
  /*!< Zeros infinite losses and the associated gradients.*/
  CNNL_NONE_ZERO_INFINITY = 1,
  /*!< Implementation withouting setting infinite losses and the associated gradients to zero.*/
  CNNL_NONE_ZERO_INFINITY_PROBS_GRADS = 2,
  /*!< Implementation withouting setting infinite losses to zero, but setting the associated gradients
       with the probabilities normalized by softmax.*/
} cnnlCTCLossZeroInfinityMode_t;

// Group:CTCLoss
/*!
 * @brief Creates a tensor descriptor that holds cnnlCTCLossNormalizationMode_t,
 * cnnlCTCLossReduceMode_t, cnnlCTCLossZeroInfinityMode_t, blank, maximum input length,
 * and maximum label length.
 *
 * @param[out] ctc_loss_desc
 *   Output. Pointer to the struct that holds information about the CTC_Loss descriptor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * par API Dependency
 *  The ::cnnlDestroyCTCLossDescriptor function needs to be called to destroy the
 * CTC_Loss descriptor later.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateCTCLossDescriptor(cnnlCTCLossDescriptor_t *ctc_loss_desc);

// Group:CTCLoss
/*!
 * @brief Initializes the CTC_Loss descriptor pointed by \p ctc_loss_desc that was previously
 * created with the ::cnnlCreateCTCLossDescriptor function.
 *
 * @param[in,out] ctc_loss_desc
 *   Output. The descriptor of the CTC_Loss operation. For detailed information,
 *   see ::cnnlCTCLossDescriptor_t.
 * @param[in] norm_mode
 *   Input. The normalization mode of the input data. For detailed information,
 *   see ::cnnlCTCLossNormalizationMode_t.
 * @param[in] reduce_mode
 *   Input. Pointer to the host memory that holds information about the reduce mode.
 *   For detailed information, see ::cnnlCTCLossReduceMode_t.
 * @param[in] zero_infinity
 *   Input. The zero infinity mode whether to zero infinite losses and the associated gradients.
 *   For detailed information, see ::cnnlCTCLossZeroInfinityMode_t.
 * @param[in] blank
 *   Input. Blank label.
 * @param[in] max_input_length
 *   Input. A scalar of maximum input length.
 * @param[in] max_label_length
 *   Input. A scalar of maximum label length.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @note
 *  - The supported combinations of \p norm_mode, \p reduce_mode, \p zero_infinity are shown below
 *    with the following order:
 *    \p norm_mode - \p reduce_mode - \p zero_infinity.
 *    - ::CNNL_NONE_NORMALIZATION - ::CNNL_REDUCE_MODE_NONE - ::CNNL_ZERO_INFINITY
 *    - ::CNNL_NONE_NORMALIZATION - ::CNNL_REDUCE_MODE_SUM - ::CNNL_ZERO_INFINITY
 *    - ::CNNL_NONE_NORMALIZATION - ::CNNL_REDUCE_MODE_NONE - ::CNNL_NONE_ZERO_INFINITY_PROBS_GRADS
 *    - ::CNNL_LOG_SOFTMAX_NORMALIZATION - ::CNNL_REDUCE_MODE_NONE - ::CNNL_ZERO_INFINITY
 *    - ::CNNL_LOG_SOFTMAX_NORMALIZATION - ::CNNL_REDUCE_MODE_NONE - ::CNNL_NONE_ZERO_INFINITY
 *    - ::CNNL_LOG_SOFTMAX_NORMALIZATION - ::CNNL_REDUCE_MODE_SUM - ::CNNL_ZERO_INFINITY
 *    - ::CNNL_LOG_SOFTMAX_NORMALIZATION - ::CNNL_REDUCE_MODE_SUM - ::CNNL_NONE_ZERO_INFINITY
 *    - ::CNNL_LOG_SOFTMAX_NORMALIZATION - ::CNNL_REDUCE_MODE_MEAN_BY_LABEL_LENGTH_AND_BATCH - ::CNNL_ZERO_INFINITY
 *    - ::CNNL_LOG_SOFTMAX_NORMALIZATION - ::CNNL_REDUCE_MODE_MEAN_BY_LABEL_LENGTH_AND_BATCH - ::CNNL_NONE_ZERO_INFINITY
 * - When \p zero_infinity is not ::CNNL_NONE_ZERO_INFINITY_PROBS_GRADS, the \p blank must be within [0, channel),
 *   otherwise must be channel - 1.
 * - \p max_input_length must be less than or equal to input's first dimension.
 * - \p max_label_length must be equal to or greater than 0.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetCTCLossDescriptor(cnnlCTCLossDescriptor_t ctc_loss_desc,
                         const cnnlCTCLossNormalizationMode_t norm_mode,
                         const cnnlCTCLossReduceMode_t reduce_mode,
                         const cnnlCTCLossZeroInfinityMode_t zero_infinity,
                         int blank,
                         int max_input_length,
                         int max_label_length);

// Group:CTCLoss
/*!
 * @brief Retrieves a CTC_Loss descriptor \p ctc_loss_desc that was previously created with the
 * ::cnnlCreateCTCLossDescriptor function, and sets the information about
 * ::cnnlCTCLossNormalizationMode_t, ::cnnlCTCLossReduceMode_t, ::cnnlCTCLossZeroInfinityMode_t,
 * blank, maximum input length, maximum label length.
 *
 * @param[in] ctc_loss_desc
 *   Input. The descriptor of the CTC_Loss. For detailed information,
 *   see ::cnnlCTCLossDescriptor_t.
 * @param[out] norm_mode
 *   Output. Pointer to the host memory that holds information about the normalization mode.
 *   For detailed information, see ::cnnlCTCLossNormalizationMode_t.
 * @param[out] reduce_mode
 *   Output. Pointer to the host memory that holds information about the reduce mode.
 *   For detailed information, see ::cnnlCTCLossReduceMode_t.
 * @param[out] zero_infinity
 *   Output. Pointer to the host memory that holds information about the zero infinity mode.
 *   For detailed information, see ::cnnlCTCLossZeroInfinityMode_t.
 * @param[out] blank
 *   Output. Pointer to the host memory that holds information about the blank label.
 * @param[out] max_input_length
 *   Output. Pointer to the host memory that holds information about the maximum input length.
 * @param[out] max_label_length
 *   Output. Pointer to the host memory that holds information about the maximum label length.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetCTCLossDescriptor(cnnlCTCLossDescriptor_t ctc_loss_desc,
                                                   cnnlCTCLossNormalizationMode_t *norm_mode,
                                                   cnnlCTCLossReduceMode_t *reduce_mode,
                                                   cnnlCTCLossZeroInfinityMode_t *zero_infinity,
                                                   int *blank,
                                                   int *max_input_length,
                                                   int *max_label_length);

// Group:CTCLoss
/*!
 * @brief Destroys a CTC_Loss descriptor that was created by
 *        ::cnnlCreateCTCLossDescriptor.
 *
 * @param[in] ctc_loss_desc
 *   Input. A CTC_Loss descriptor created by ::cnnlCreateCTCLossDescriptor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyCTCLossDescriptor(cnnlCTCLossDescriptor_t ctc_loss_desc);

// Group:CTCLoss
/*!
 * @brief Returns \p workspace_size which is the size of the MLU memory that is used as an extra
 *        workspace to optimize the CTC_Loss operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues. For detailed information, see ::cnnlHandle_t.
 * @param[in] ctc_loss_desc
 *   Input. The descriptor of CTC_Loss. For detailed information,
 *   see ::cnnlCTCLossDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of input. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] is_backward
 *   Input. A Boolean scalar. When \p is_backward is false, the ctc_loss forward operation is performed.
 *   When \p is_backward is true, the ctc_loss backward operation is performed.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is
 *   used in the CTC_Loss operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlCTCLoss function
 *   to perform the CTC_Loss operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t  CNNL_WIN_API cnnlGetCTCLossWorkspaceSize(cnnlHandle_t handle,
                                                       const cnnlCTCLossDescriptor_t ctc_loss_desc,
                                                       const cnnlTensorDescriptor_t input_desc,
                                                       bool is_backward,
                                                       size_t *workspace_size);

// Group:CTCLoss
/*!
 * @brief Calculates loss between a continuous time series and a target sequence. CTC is
 * the acronym of Connectionist Temporal Classification. This function returns the CTC costs
 * and gradients, given the unnormalized activation and labels.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues. For detailed information, see ::cnnlHandle_t.
 * @param[in] ctc_loss_desc
 *   Input. The descriptor of CTC_Loss. For detailed information,
 *   see ::cnnlCTCLossDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input data.
 * @param[in] labels_desc
 *   Input. The descriptor of the labels tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] labels
 *   Input. Pointer to the MLU memory that stores the labels, which is an initialized list
 *   of labels.
 * @param[in] input_lengths_desc
 *   Input. The descriptor of the input lengths tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input_lengths
 *   Input. Pointer to the MLU memory that stores the input lengths, which is an initialized
 *   list of the lengths of the timing steps in each batch.
 * @param[in] label_lengths_desc
 *   Input. The descriptor of the label lengths tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] label_lengths
 *   Input. Pointer to the MLU memory that stores the labels data, which is an initialized
 *   list of the lengths of the label in each batch.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   CTC_Loss operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the CTC_Loss operation. You can get the size of the workspace with
 *   the ::cnnlGetCTCLossWorkspaceSize function.
 * @param[in] loss_desc
 *   Input. The descriptor of the loss tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] loss
 *   Output. Pointer to the MLU memory that stores the loss of CTC.
 * @param[in] grads_desc
 *   Input. The descriptor of the \p grads tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] grads
 *   Output. Pointer to the MLU memory that stores the gradients of CTC. These computed
 *   gradient outputs are with respect to the unnormalized activation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 *  @par Formula
 *  - See "CTC_Loss Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - input: float.
 * - labels: int, int64.
 * - input_lengths: int.
 * - label_lengths: int.
 * - loss: float.
 * - grads: float.
 *
 * @par API Dependency
 * - ::cnnlGetCTCLossWorkspaceSize should be called to get the workspace
 *   size before ::cnnlCTCLoss.
 *
 * @note
 *  - The supported combinations of \p norm_mode, \p reduce_mode, \p zero_infinity are shown below
 *    with the following order:
 *    \p norm_mode - \p reduce_mode - \p zero_infinity.
 *    - ::CNNL_NONE_NORMALIZATION - ::CNNL_REDUCE_MODE_NONE - ::CNNL_ZERO_INFINITY
 *    - ::CNNL_NONE_NORMALIZATION - ::CNNL_REDUCE_MODE_SUM - ::CNNL_ZERO_INFINITY
 *    - ::CNNL_NONE_NORMALIZATION - ::CNNL_REDUCE_MODE_NONE - ::CNNL_NONE_ZERO_INFINITY_PROBS_GRADS
 *    - ::CNNL_LOG_SOFTMAX_NORMALIZATION - ::CNNL_REDUCE_MODE_NONE - ::CNNL_ZERO_INFINITY
 *    - ::CNNL_LOG_SOFTMAX_NORMALIZATION - ::CNNL_REDUCE_MODE_NONE - ::CNNL_NONE_ZERO_INFINITY
 *    - ::CNNL_LOG_SOFTMAX_NORMALIZATION - ::CNNL_REDUCE_MODE_SUM - ::CNNL_ZERO_INFINITY
 *    - ::CNNL_LOG_SOFTMAX_NORMALIZATION - ::CNNL_REDUCE_MODE_SUM - ::CNNL_NONE_ZERO_INFINITY
 *    - ::CNNL_LOG_SOFTMAX_NORMALIZATION - ::CNNL_REDUCE_MODE_MEAN_BY_LABEL_LENGTH_AND_BATCH - ::CNNL_ZERO_INFINITY
 *    - ::CNNL_LOG_SOFTMAX_NORMALIZATION - ::CNNL_REDUCE_MODE_MEAN_BY_LABEL_LENGTH_AND_BATCH - ::CNNL_NONE_ZERO_INFINITY
 *  - When \p norm_mode is ::CNNL_NONE_NORMALIZATION, the data type of \p labels only supports int32.
 *  - When \p zero_infinity is not ::CNNL_NONE_ZERO_INFINITY_PROBS_GRADS, the \p blank of \p ctc_loss_desc
 *    must be within [0, channel), otherwise must be channel - 1.
 *  - The maximum input length of \p ctc_loss_desc must be less than or equal to input's
 *    highest dimension.
 *  - The layout of \p input and \p grads only support ::CNNL_LAYOUT_TNC.
 *  - The layout of \p labels only supports ::CNNL_LAYOUT_ARRAY.
 *  - The layout of \p input_lengths only supports ::CNNL_LAYOUT_ARRAY.
 *  - The layout of \p label_lengths only supports ::CNNL_LAYOUT_ARRAY.
 *  - When ::cnnlCTCLoss is the operation of CTC_Loss forward, \p grads_desc and \p grads
 *    must be NULL.
 *  - When ::cnnlCTCLoss is the operation of CTC_Loss backward, \p grads_desc and \p grads
 *    cannot be NULL.
 *  - The size of each inputs and outputs should be less than the biggest number of int32_t.
 *  - Suppose the shape of \p input is [T, N, C], then T is recommended to be in the range
 *    of [1, 1000], N is recommended to be in the range [1, 128] and C is recommended to be in
 *    the range of [1, 1000] for higher precision when \p zero_infinity is ::CNNL_ZERO_INFINITY.
 *  - T is recommended to be in range of [1, 150], N is recommended to be in the range [1, 128]
 *    and C is recommended to be in range of [1, 500] for higher precision when \p zero_infinity
 *    is ::CNNL_NONE_ZERO_INFINITY_PROBS_GRADS.
 *  - When the channel of \p input is 1 or \p input contains NaN/infinity, it may cause
 *    undefined behavior.
 *  - Any element in \p input_lengths must be within [1, max_input_length], otherwise it may
 *    cause undefined behavior.
 *  - Any element in \p label_lengths must be within [0, max_label_length] and all elements in
 *    \p label_lengths cannot be 0 at the same time, otherwise it may cause undefined behavior.
 *  - Any element in \p label cannot be the same as \p blank of \p ctc_loss_desc
 *    and must be within [0, channel), otherwise it may cause undefined behavior.
 *  - The shape of label is 1D(sum of label_lengths) or 2D([batch_size,
 *    max_label_length])tensor.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCTCLoss(cnnlHandle_t handle,
                                       const cnnlCTCLossDescriptor_t ctc_loss_desc,
                                       const cnnlTensorDescriptor_t input_desc,
                                       const void *input,
                                       const cnnlTensorDescriptor_t labels_desc,
                                       const void *labels,
                                       const cnnlTensorDescriptor_t input_lengths_desc,
                                       const void *input_lengths,
                                       const cnnlTensorDescriptor_t label_lengths_desc,
                                       const void *label_lengths,
                                       void *workspace,
                                       size_t workspace_size,
                                       const cnnlTensorDescriptor_t loss_desc,
                                       void *loss,
                                       const cnnlTensorDescriptor_t grads_desc,
                                       void *grads);

/******************************************************************************
 * Cambricon CNNL OP: RNNT_Loss
 ******************************************************************************/
// Group:RNNTLoss
/*!
 * @brief Creates a descriptor pointed by \p desc for a RNNT_Loss operation,
 * and allocated memory for holding the information about the RNNT_Loss operation.
 *
 * The information is defined in ::cnnlRNNTLossDescriptor_t. For more information
 * about descriptor, see "Cambricon CNNL user Guide".
 *
 * @param[out] rnnt_loss_desc
 *   Output. Pointer to the struct that holds information about the RNNT_Loss descriptor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetRNNTLossDescriptor function to
 * initialize and set information to the RNNT_Loss descriptor.
 * - You need to call the ::cnnlDestroyRNNTLossDescriptor function to destroy the descriptor
 * at the end of the context.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateRNNTLossDescriptor(cnnlRNNTLossDescriptor_t *rnnt_loss_desc);

// Group:RNNTLoss
/*!
 * @brief Destroys the RNNT_Loss descriptor that was created by ::cnnlCreateRNNTLossDescriptor.
 *
 * @param[in] rnnt_loss_desc
 *   Input. The RNNT_Loss descriptor created by ::cnnlCreateCTCLossDescriptor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyRNNTLossDescriptor(cnnlRNNTLossDescriptor_t rnnt_loss_desc);

// Group:RNNTLoss
/*!
 * @brief Initializes the RNNT_Loss descriptor pointed by \p rnnt_loss_desc that was previously
 * created with the ::cnnlCreateRNNTLossDescriptor function.
 *
 * @param[in,out] rnnt_loss_desc
 *   Input/output. The descriptor of the RNNT_Loss operation. For detailed information,
 *   see ::cnnlRNNTLossDescriptor_t.
 * @param[in] reduce_mode
 *   Input. Pointer to the host memory that holds information about the reduce mode.
 *   For detailed information, see ::cnnlLossReduction_t.
 * @param[in] blank
 *   Input. The blank label.
 * @param[in] clamp
 *   Input. The clamp for gradients.
 * @param[in] fused_log_softmax
 *   Input. A Boolean value indicating whether to call log softmax within loss.
 * @param[in] max_logit_length
 *   Input. The maximum length of each sequence from encoder.
 * @param[in] max_target_length
 *   Input. The maximum length of targets for each sequence.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @note
 * - \p reduce_mode must be CNNL_LOSS_REDUCTION_NONE.
 * - \p blank must be within [0, num_classes).
 * - \p fused_log_softmax must be true.
 * - \p max_logit_length must be equal to the second dimension of logits.
 * - \p max_target_length plus one must be equal to the third dimension of logits.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetRNNTLossDescriptor(cnnlRNNTLossDescriptor_t rnnt_loss_desc,
                                                    const cnnlLossReduction_t reduce_mode,
                                                    const int64_t blank,
                                                    const double clamp,
                                                    const bool fused_log_softmax,
                                                    const int64_t max_logit_length,
                                                    const int64_t max_target_length);

// Group:RNNTLoss
/*!
 * @brief Retrieves a RNNTLoss descriptor \p rnnt_loss_desc that was previously created with the
 * ::cnnlCreateRNNTLossDescriptor function, and gets the information about
 * ::cnnlLossReduction_t, blank, clamp, fused_log_softmax, maximum logit length,
 * and maximum target length.
 *
 * @param[in] rnnt_loss_desc
 *   Input. The descriptor of the RNNT_Loss operation. For detailed information,
 *   see ::cnnlRNNTLossDescriptor_t.
 * @param[out] reduce_mode
 *   Output. Pointer to the host memory that holds information about the reduce mode.
 *   For detailed information, see ::cnnlLossReduction_t.
 * @param[out] blank
 *   Output. Pointer to the host memory that holds information about the blank label.
 * @param[out] fused_log_softmax
 *   Output. Pointer to the host memory that holds information about the fused log softmax.
 * @param[out] clamp
 *   Output. Pointer to the host memory that holds information about the clamp for gradients.
 * @param[out] max_logit_length
 *   Output. Pointer to the host memory that holds information about the maximum length of each
 *   sequence from encoder.
 * @param[out] max_target_length
 *   Output. Pointer to the host memory that holds information about the maximum length of
 *   targets for each sequence.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetRNNTLossDescriptor(cnnlRNNTLossDescriptor_t rnnt_loss_desc,
                                                    cnnlLossReduction_t *reduce_mode,
                                                    int64_t *blank,
                                                    double *clamp,
                                                    bool *fused_log_softmax,
                                                    int64_t *max_logit_length,
                                                    int64_t *max_target_length);

// Group:RNNTLoss
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the RNNT_Loss operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues. For detailed information, see ::cnnlHandle_t.
 * @param[in] rnnt_loss_desc
 *   Input. The descriptor of RNNT_Loss. For detailed information,
 *   see ::cnnlRNNTLossDescriptor_t.
 * @param[in] logits_desc
 *   Input. The descriptor of the \p logits tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] grads_desc
 *   Input. The descriptor of the \p gradients tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is
 *   used in the RNNT_Loss operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlRNNTLoss function
 *   to perform the RNNT_Loss operation.
 *
 * @note
 * - \p handle, \p rnnt_loss_desc and \p logits_desc cannot be NULL.
 * - When \p grads_desc is NULL, this operation computes workspace size for rnnt_loss forward.
 * - When \p grads_desc is not NULL, this operation computes workspace size for rnnt_loss backward.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetRNNTLossWorkspaceSize(cnnlHandle_t handle,
                             const cnnlRNNTLossDescriptor_t rnnt_loss_desc,
                             const cnnlTensorDescriptor_t logits_desc,
                             const cnnlTensorDescriptor_t grads_desc,
                             size_t *workspace_size);

// Group:RNNTLoss
/*!
 * @brief Calculates loss between a continuous time series and a target sequence. This function
 * returns the RNNT loss and gradients, given the unnormalized activation and targets.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues. For detailed information, see ::cnnlHandle_t.
 * @param[in] rnnt_loss_desc
 *   Input. The descriptor of RNNT_Loss. For detailed information,
 *   see ::cnnlRNNTLossDescriptor_t.
 * @param[in] logits_desc
 *   Input. The descriptor of the \p logits tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] logits
 *   Input. Pointer to the MLU memory that stores the logit data, which is the output form joiner.
 * @param[in] targets_desc
 *   Input. The descriptor of the \p targets tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] targets
 *   Input. Pointer to the MLU memory that stores the target data, which is an initialized list
 *   of targets.
 * @param[in] logit_lengths_desc
 *   Input. The descriptor of the \p logit_lengths tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] logit_lengths
 *   Input. Pointer to the MLU memory that stores the logit length data, which is an initialized
 *   list of the lengths of each sequence from encoder.
 * @param[in] target_lengths_desc
 *   Input. The descriptor of the \p target_lengths tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] target_lengths
 *   Input. Pointer to the MLU memory that stores the target length data, which is an initialized
 *   list of the lengths of the target for each sequence.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   RNNT_Loss operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the RNNT_Loss operation. You can get the size of the workspace with
 *   the ::cnnlGetRNNTLossWorkspaceSize function.
 * @param[in] loss_desc
 *   Input. The descriptor of the \p loss tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] loss
 *   Output. Pointer to the MLU memory that stores the loss of RNNT.
 * @param[in] grads_desc
 *   Input. The descriptor of the \p grads tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] grads
 *   Output. Pointer to the MLU memory that stores the gradients of RNNT. These computed
 *   gradient outputs are with respect to the unnormalized activation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Scale Limitation
 * - The size of the first dimension of \p logits multiplied by the second and third dimensions
 *   of \p logits should be no larger than 536870912.
 * - The size of the fourth dimension of \p logits should be no larger than 54611 in MLU300 series.
 * - The size of the fourth dimension of \p logits should be no larger than 32766 in MLU500 series.
 * - The size of the third dimension of \p logits multiplied by two then plus the
 *   fourth dimension of \p logits should be no larger than 163440 in MLU300 series.
 * - The size of the third dimension of \p logits multiplied by two then plus the
 *   fourth dimension of \p logits should be no larger than 97904 in MLU500 series.
 * - This operation does not support large tensor.
 *
 *  @par Formula
 *  - See "RNNT_Loss Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The data types of \p logits, \p loss and \p grads tensors must be the same.
 * - The supported data types of the input tensors and output tensors are as follows:
 *   - logits: float, half.
 *   - labels: int32.
 *   - input_lengths: int32.
 *   - label_lengths: int32.
 *   - loss: float, half.
 *   - grads: float, half.
 *
 * @par API Dependency
 * - ::cnnlGetRNNTLossWorkspaceSize should be called to get the workspace
 *   size before ::cnnlRNNTLoss.
 * - ::cnnlCreateRNNTLossDescriptor and ::cnnlSetRNNTLossDescriptor should be called to create and
 *   set the descriptor of RNNT_Loss \p rnnt_loss_desc before ::cnnlRNNTLoss.
 *
 * @note
 * - The shape of \p logits is 4D([batch_size, max_logit_length, max_target_length + 1,
 *   num_classes]). When \p grads_desc is not NULL, the shape of \p logits and
 *   \p grads must be the same.
 * - The shape of \p targets is 2D([batch_size, max_target_length]).
 * - The shape of \p input_lengths, \p label_lengths and \p loss must be 1D([batch_size]).
 * - The first dimension of \p logits, \p targets, \p logit_lengths, \p target_lengths and \p loss
 *   must be the same.
 * - The blank label of \p rnnt_loss_desc must be within [0, num_classes).
 * - The maximum logit length in \p rnnt_loss_desc must be equal to the second dimension of
 *   \p logits.
 * - The maximum target length in \p rnnt_loss_desc must be one less than the third
 *   dimension of \p logits.
 * - The maximum target length in \p rnnt_loss_desc must be equal to the second dimension of
 *   \p targets.
 * - When ::cnnlRNNTLoss is the operation of RNNT_Loss forward, \p grads_desc and \p grads
 *   must be NULL.
 * - When ::cnnlRNNTLoss is the operation of RNNT_Loss backward, \p grads_desc and \p grads
 *   cannot be NULL.
 * - When num_classes is 1 or \p logits contains NaN/infinity, it may cause
 *   undefined behavior.
 * - When \p logit_lengths or \p target_lengths contains zero or negative number, it may cause
 *   undefined behavior.
 * - Any elements in \p targets cannot be the same as \p blank of \p rnnt_loss_desc, otherwise
 *   it may cause undefined behavior.
 * - This operation supports MLU300 series and MLU500 series.
 * - This operation is not supported on the CE3226 or 1V platforms currently.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - http://github.com/pytorch/audio/blob/v0.10.0/torchaudio/csrc/rnnt/gpu/compute.cu
 */
cnnlStatus_t CNNL_WIN_API cnnlRNNTLoss(cnnlHandle_t handle,
                                       const cnnlRNNTLossDescriptor_t rnnt_loss_desc,
                                       const cnnlTensorDescriptor_t logits_desc,
                                       const void *logits,
                                       const cnnlTensorDescriptor_t targets_desc,
                                       const void *targets,
                                       const cnnlTensorDescriptor_t logit_lengths_desc,
                                       const void *logit_lengths,
                                       const cnnlTensorDescriptor_t target_lengths_desc,
                                       const void *target_lengths,
                                       void *workspace,
                                       size_t workspace_size,
                                       const cnnlTensorDescriptor_t loss_desc,
                                       void *loss,
                                       const cnnlTensorDescriptor_t grads_desc,
                                       void *grads);

/******************************************************************************
 * Cambricon CNNL OP: Deformable Convolution
 ******************************************************************************/

// Group:DCNForward
/*!
 * @brief Creates a descriptor pointed by \p dcn_desc for a deformable convolution forward
 *        or backward operation, and allocates memory for holding the information about the
 *        deformable convolution operation. The information is defined in
 *        ::cnnlDCNDescriptor_t. For more information about descriptor,
 *        see "Cambricon CNNL User Guide".
 *
 * @param[out] dcn_desc
 *  Output. A host pointer to the deformable convolution descriptor that holds information about the
 *  deformable convolution operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_NOT_INITIALIZED
 *
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetDCNDescriptor function to initialize
 *   and set the information to the deformable convolution descriptor.
 * - You need to call the ::cnnlDestroyDCNDescriptor function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateDCNDescriptor(cnnlDCNDescriptor_t *dcn_desc);

// Group:DCNForward
/*!
 * @brief Initializes the deformable convolution descriptor \p dcn_desc that was previously
 *        created with the ::cnnlCreateDCNDescriptor function, and sets the information about the
 *        deformable convolution forward and backward operation to the deformable convolution descriptor
 *        \p dcn_desc. The information includes the number of deformable convolution dimensions \p dimNb,
 *        the padding size for each dimension \p pad, the stride of the sliding window for each dimension
 *        \p stride, the dilation factor for each dimension \p dilation, the number of groups of input
 *        offset to be split along the input channel \p deformable_group, the number of convolution group
 *        \p conv_group, and the maximum image number per deformable convolution computing \p im2col_step.
 *
 * @param[in,out] dcn_desc
 *   Input/output. The descriptor of the deformable convolution operation. For detailed information,
 *   see ::cnnlDCNDescriptor_t.
 * @param[in] dimNb
 *   Input. The number of dimensions in the input tensor of the deformable convolution operation.
 *   Currently, the value of this parameter can only be set to 4 and should be the
 *   same as the one you set in the input tensor descriptor.
 * @param[in] pad
 *   Input. An array that stores the zero-padding size for each dimension of the input tensor
 *   used in the deformable convolution operation.
 *   For each dimension, the padding size represents the number of zeros to be concatenated at the
 *   start and end of that dimension. For 2D deformable convolution, the padding is
 *   on top, bottom, left, and right.
 * @param[in] stride
 *   Input. An array that stores the filter stride for each dimension of the input tensor
 *   used in the deformable convolution operation. For each dimension, the filter stride represents
 *   the number of elements to slide over the input tensor. For 2D deformable
 *   convolution, the stride should be set in height and width order.
 * @param[in] dilation
 *   Input. An array that stores the dilation factor for each dimension of the filter tensor
 *   used in the deformable convolution operation. For each dimension, the dilation factor
 *   represents the spacing between the kernel points. For 2D deformable convolution,
 *   the dilation should be set in height and width order.
 * @param[in] deformable_group
 *   Input. The number of deformable offset groups that split the input offset along the channel
 *   of input tensor. Each deformable group is deformed separately for detecting different input parts.
 * @param[in] conv_group
 *   Input. The number of groups that the input data is split by the number of channels
 *   in the input tensor. Each convolution group is convolved separately. The filter used for
 *   each group is the filter tensor divides \p conv_group. The result of
 *   the deformable convolution operation is the concatenation of all the group convolution results
 *   along the output channel dimension.
 * @param[in] im2col_step
 *   Input. The maximum number of images per deformable convolution computing. This parameter
 *   affects both the workspace size and the computing efficiency.
 *   A larger \p im2col_step will consume a larger workspace size and have a higher performance,
 *   while a smaller one will consume a smaller workspace size and have a lower performance.
 * @param[in] compute_type
 *   Input. The data type of temporary result in convolution operation. Only supports
 *   \p CNNL_DTYPE_FLOAT type.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, ::cnnlCreateDCNDescriptor should be called.
 *
 * @note
 * - Currently, only supports 4D input tensor for deformable convolution forward
 *   and backward operation.
 * - The values of \p pad should be greater than or equal to 0.
 * - The values of \p stride should be greater than or equal to 1.
 * - The values of \p dilation should be greater than or equal to 1.
 * - The value of \p deformable_group should be greater than or equal to 1 and
 *   less than or equal to the number of channels in the input tensor, and input channel must be
 *   divisible by \p deformable_group.
 *   - If \p deformable_group is set to 1, the same input offset is applied to all channels
 *   of one pixel.
 *   - If the value of \p deformable_group is between 1 and the number of channels of input tensor,
 *     the input channel will be split into \p deformable_group parts. Each part is responsible for
 *     detecting different input parts, which results in a more flexible geometric transformation.
 * - The value of \p conv_group should be greater than or equal to 1 and less than or equal to the
 *   number of channels in the input tensor, and input channels and output channels must both be
 *   divisible by \p conv_group.
 * - The value of \p im2col_step should be greater than or equal to 1 and less than or equal to
 *   the number of batch size in the input tensor, and input batch should be divisible by
 *   \p im2col_step.
 *
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetDCNDescriptor(cnnlDCNDescriptor_t dcn_desc,
                                               int dimNb,
                                               const int pad[],
                                               const int stride[],
                                               const int dilation[],
                                               int deformable_group,
                                               int conv_group,
                                               int im2col_step,
                                               const cnnlDataType_t compute_type);

// Group:DCNForward
/*!
 * @brief Destroys a deformable convolution descriptor \p dcn_desc that was previously created with
 *        the ::cnnlCreateDCNDescriptor function.
 *
 * @param[in] dcn_desc
 *   Input. The deformable convolution descriptor to be destroyed.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - You need to call this function after calling the ::cnnlDCNForward, ::cnnlDCNBackwardData,
 *   or ::cnnlDCNBackwardWeight function. Otherwise, ::CNNL_STATUS_BAD_PARAM is returned.
 * - This function should be called to destroy the deformable convolution descriptor.
 *   Otherwise, the memory leak may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyDCNDescriptor(cnnlDCNDescriptor_t dcn_desc);

// Group:DCNForward
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 *        workspace to optimize the deformable convolution forward operation.
 *
 * The size of the extra workspace is based on the given information of the deformable convolution
 * forward operation, including the deformable convolution descriptor \p dcn_desc,
 * input tensor descriptor \p input_desc, offset tensor
 * descriptor \p offset_desc, mask tensor descriptor \p mask_desc, filter tensor descriptor
 * \p filter_desc, bias tensor descriptor \p bias_desc, and output tensor descriptor \p output_desc.
 * For more information about the workspace, see "Cambricon CNNL User Guide."
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   deformable convolution operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] dcn_desc
 *   Input. The descriptor of the deformable convolution operation. For detailed information, see
 *   ::cnnlDCNDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] offset_desc
 *   Input. The descriptor of the offset tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] mask_desc
 *   Input. The descriptor of the mask tensor. Set this parameter to NULL if mask is not needed. For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @param[in] filter_desc
 *   Input. The descriptor of the filter tensor used as a filter in the deformable convolution
 *   operation. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] bias_desc
 *   Input. The descriptor of the bias tensor. Set this parameter to NULL if bias is not needed.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   deformable convolution forward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions
 *   to create and set the tensor descriptors \p input_desc, \p offset_desc, \p mask_desc (optional),
 *   \p filter_desc, \p bias_desc (optional) before calling this function.
 * - The allocated extra workspace should be passed to the ::cnnlDCNForward function to perform
 *   the deformable convolution forward operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetDCNForwardWorkspaceSize(cnnlHandle_t handle,
                               const cnnlDCNDescriptor_t dcn_desc,
                               const cnnlTensorDescriptor_t input_desc,
                               const cnnlTensorDescriptor_t offset_desc,
                               const cnnlTensorDescriptor_t mask_desc,
                               const cnnlTensorDescriptor_t filter_desc,
                               const cnnlTensorDescriptor_t bias_desc,
                               const cnnlTensorDescriptor_t output_desc,
                               size_t *workspace_size);

// Group:DCNForward
/*!
 * @brief Performs a 2D deformable convolution forward operation. Compared with the standard
 *        convolution, the deformable convolution introduces 2D offsets and masks to make
 *        the convolution adapt to the geometric variation of objects.
 *        Offsets act on the regular grid sampling locations, which enables a free form
 *        deformation of the sampling grid. Mask is a modulation mechanism to improve the ability
 *        to focus on pertinent image regions. Both offsets and masks are
 *        learnable parameters obtained from additional convolutional layers.
 *
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues. For
 *   detailed information, see ::cnnlHandle_t.
 * @param[in] dcn_desc
 *   Input. The descriptor of deformable convolution. For detailed information, see
 *   ::cnnlDCNDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] offset_desc
 *   Input. The descriptor of the offset tensor to be applied for each position in the convolution kernel.
 *   The shape of offset should be (batch, out_height, out_width, 2 * deformable_group *
 *   filter_height * filter_width). For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] offset
 *   Input. Pointer to the MLU memory that stores the offset tensor.
 * @param[in] mask_desc
 *   Input. The descriptor of the scaling factor to be applied for each position in the convolution
 *   kernel. The shape of mask should be (batch, out_height, out_width,
 *   deformable_group * filter_height * filter_width). Set this parameter to NULL when
 *   mask is not requested. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] mask
 *   Input. Pointer to the MLU memory that stores the mask tensor. Set this parameter to NULL
 *   when mask is not requested.
 * @param[in] filter_desc
 *   Input. The descriptor of the filter tensor used as a filter in the deformable convolution
 *   operation. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the filter tensor.
 * @param[in] bias_desc
 *   Input. The descriptor of the bias tensor. Set this parameter to NULL when bias is not
 *   requested. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the bias tensor. Set this parameter to NULL when bias is not
 *   requested.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   deformable convolution operation. For more information about workspace, see
 *   "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the deformable
 *   convolution operation. You can get the size of the workspace with the
 *   ::cnnlGetDCNForwardWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. The shape of output is the same as the
 *   shape of output in the convolution.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_NOT_SUPPORTED, ::CNNL_STATUS_NUMERICAL_OVERFLOW
 * @par Formula
 * - See "Deformable Convolution Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Offchip data type of \p input, \p offset, \p mask, \p filter, \p bias, and \p output must be the same.
 * - The supported offchip data types of the input tensor and output tensor are as follows:
 *   - input, offset, mask, filter, bias, output: half, float.
 * - This function supports any combinations of the following onchip data types for input tensor
 *   \p input and \p filter on MLU200 series and CE3226.
 *   - \p input onchip data type: int16, int31.
 *   - \p filter onchip data type: int16, int31.
 * - On MLU300 series or above, the onchip data type of \p input and \p filter
 *   should be \p CNNL_DTYPE_INVALID or the same as the corresponding offchip data type.
 *
 * @par Data Layout
 * - The supported data layouts of the input tensor, filter, bias tensor, and output tensor are
 *   as follows:
 *   - input, offset, mask, filter, output: \p CNNL_LAYOUT_NHWC.
 *   - bias: \p CNNL_LAYOUT_ARRAY
 *
 * @par Scale Limitation
 * - The input, offset, mask, filter, bias, output and the deformable convolution descriptor
 *   (including pad, stride, dilation, deformable_group, conv_group, im2col_step) must meet the
 *   following requirements:
 *   - input tensor: \p batch > 0, \p height > 0, \p width > 0, \p channel > 0
 *   - offset tensor: \p batch should be equal to the batch size of input tensor, \p height and \p width
 *     should be equal to the height and width of output tensor accordingly. \p channel should be equal to
 *     deformable_group * filter_height * filter_width * 2.
 *   - mask tensor: When mask is needed, \p batch should be equal to the batch size of input tensor,
 *     \p height and \p width should be equal to the height and width of output tensor accordingly.
 *     \p channel should be equal to deformable_group * filter_height * filter_width.
 *   - The value of (im2col_step * out_height * out_filter * filter_h * filter_w * input_channel)
 *     should be less than or equal to the INT_MAX defined in limits.h.
 * @par API Dependency
 * - Before calling this function to implement deformable convolution, you need to prepare
 *   all the parameters passed to this function. See each parameter description
 *   for details.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, set the im2col_step equal to the batch
 *   size of the input tensor.
 *
 * @note
 * - The alignment of \p input, \p offset, \p mask, \p filter, \p bias, \p output,
 *   should be contiguous in the MLU memory.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the deformable convolution forward operation is as follows:
     @verbatim

     input tensor by 1 * 3 * 3 * 2 --> input:
     [[[[0.7944, 0.4922], [0.2008, 0.2081], [0.9998, 0.3053]],
       [[0.1815, 0.9210], [0.8463, 0.1819], [0.9159, 0.4917]],
       [[0.6668, 0.2843], [0.8364, 0.2765], [0.7150, 0.6780]]]]
     offset tensor by 1 * 3 * 3 * 2 --> offset:
     [[[[-0.6317, -1.4928], [-0.0696,  1.1910], [ 0.8778,  0.5145]],
       [[-0.9248, -0.9889], [ 0.6157,  0.2157], [-1.1540, -0.1283]],
       [[-0.5704,  1.0237], [ 0.7956,  1.1203], [-0.0129, -0.2686]]]]
     mask tensor by 1 * 3 * 3 * 1 --> mask:
     [[[[ 0.4581], [-1.1605], [ 0.5951]],
       [[ 0.4313], [ 0.1070], [ 0.0225]],
       [[ 0.7484], [ 0.6262], [ 1.1908]]]]
     filter tensor by 2 * 1 * 1 * 2 --> filter:
     [[[[0.8928, 0.9682]]], [[[0.9301, 0.6817]]]]
     bias tensor by 2 --> bias:
     [0.4356, 0.0840]

     param:
       pad: (0, 0, 0, 0), stride: (1, 1), dilation: (1, 1)

     output tensor by 1 * 3 * 3 * 2 --> output:
     [[[[ 0.4356,  0.0840], [-0.6024, -0.9101], [ 0.8056,  0.4252]],
       [[ 0.4412,  0.0890], [ 0.5478,  0.1898], [ 0.4562,  0.1037]],
       [[ 1.1652,  0.7876], [ 0.5814,  0.2109], [ 1.8874,  1.3752]]]]
     @endverbatim
 *
 * @par Reference
 * - https://github.com/msracver/Deformable-ConvNets
 * - Deformable Convolutional Networks, Jifeng Dai, et al., 2017.
 * - Deformable ConvNets v2: More Deformable, Better Results, Xizhou Zhu, et al., 2018.
 */
cnnlStatus_t CNNL_WIN_API cnnlDCNForward(cnnlHandle_t handle,
                                         const cnnlDCNDescriptor_t dcn_desc,
                                         const cnnlTensorDescriptor_t input_desc,
                                         const void *input,
                                         const cnnlTensorDescriptor_t offset_desc,
                                         const void *offset,
                                         const cnnlTensorDescriptor_t mask_desc,
                                         const void *mask,
                                         const cnnlTensorDescriptor_t filter_desc,
                                         const void *filter,
                                         const cnnlTensorDescriptor_t bias_desc,
                                         const void *bias,
                                         void *workspace,
                                         size_t workspace_size,
                                         const cnnlTensorDescriptor_t output_desc,
                                         void *output);

// Group:DCNBackwardData
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 *        workspace to optimize the deformable convolution backward data operation.
 *
 * The size of the extra workspace is based on the given information of the deformable convolution
 * backward data operation, including the deformable convolution descriptor \p dcn_desc,
 * input tensor descriptor \p input_desc, offset tensor
 * descriptor \p offset_desc, mask tensor descriptor \p mask_desc, filter tensor descriptor
 * \p filter_desc, gradient with respect to the output tensor \p grad_output_desc, the gradient
 * with respect to the input tensor \p grad_input_desc, the gradient with respect to the offset
 * tensor \p grad_offset, and the gradient with respect to the mask tensor \p grad_mask_desc.
 * For more information about the workspace, see "Cambricon CNNL User Guide."
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   deformable convolution operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] dcn_desc
 *   Input. The descriptor of the deformable convolution operation. For detailed information, see
 *   ::cnnlDCNDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] offset_desc
 *   Input. The descriptor of the offset tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] mask_desc
 *   Input. The descriptor of the mask tensor. Set this parameter to NULL if mask is not needed. For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @param[in] filter_desc
 *   Input. The descriptor of the filter tensor as a filter in the deformable convolution
 *   operation. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] grad_output_desc
 *   Input. The descriptor of the gradient with respect to the output tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] grad_input_desc
 *   Input. The descriptor of the gradient with respect to the input tensor.
 *   This parameter is requested to be the same as \p input_desc. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] grad_offset_desc
 *   Input. The descriptor of the gradient with respect to the offset tensor.
 *   This parameter is requested to be the same as \p offset_desc.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] grad_mask_desc
 *   Input. The descriptor of the gradient with respect to the mask tensor.
 *   This parameter is requested to be the same as \p mask_desc. Set this parameter to NULL when mask and
 *   grad_mask are not needed. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   deformable convolution backward data operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions
 *   to create and set the tensor descriptors \p input, \p offset, \p mask (optional), \p filter,
 *   \p grad_output, \p grad_input, \p grad_offset, \p grad_mask (optional) before calling this
 *   function.
 * - The allocated extra workspace should be passed to the ::cnnlDCNBackwardData function to perform
 *   the deformable convolution backward data operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetDCNBakcwardDataWorkspaceSize(cnnlHandle_t handle,
                                    const cnnlDCNDescriptor_t dcn_desc,
                                    const cnnlTensorDescriptor_t input_desc,
                                    const cnnlTensorDescriptor_t offset_desc,
                                    const cnnlTensorDescriptor_t mask_desc,
                                    const cnnlTensorDescriptor_t filter_desc,
                                    const cnnlTensorDescriptor_t grad_output_desc,
                                    const cnnlTensorDescriptor_t grad_input_desc,
                                    const cnnlTensorDescriptor_t grad_offset_desc,
                                    const cnnlTensorDescriptor_t grad_mask_desc,
                                    size_t *workspace_size);

// Group:DCNBackwardData
/*!
 * @brief Performs the back-propagation of a deformable convolution operation to compute
 *        the gradient with respect to input \p grad_input, offset \p grad_offset, and mask
 *        \p grad_mask based on the gradient of response \p grad_output.
 *
 * This function needs extra MLU memory as the workspace to improve the performance.
 * You can get the workspace size with the ::cnnlGetDCNBakcwardDataWorkspaceSize
 * function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the deformable convolution backward data operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] dcn_desc
 *   Input. The descriptor of the deformable convolution operation. For detailed information,
 *   see ::cnnlDCNDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] offset_desc
 *   Input. The descriptor of the offset to be applied for each position in the convolution kernel.
 *   The shape of offset should be (batch, out_height, out_width, 2 * deformable_group *
 *   filter_height * filter_width). For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] offset
 *   Input. Pointer to the MLU memory that stores the offset tensor.
 * @param[in] mask_desc
 *   Input. The descriptor of the scaling factor to be applied for each position in the convolution
 *   kernel. The shape of mask should be (batch, out_height, out_width,
 *   deformable_group * filter_height * filter_width). Set this parameter to NULL when
 *   mask is not requested. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] mask
 *   Input. Pointer to the MLU memory that stores the mask tensor. Set this parameter to NULL when mask is not
 *   requested.
 * @param[in] filter_desc
 *   Input. The descriptor of the filter tensor used as a filter in the deformable convolution operation.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the filter tensor.
 * @param[in] grad_output_desc
 *   Input. The descriptor of the gradient with respect to the output tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] grad_output
 *   Input. Pointer to the MLU memory that stores the gradient with respect to the output.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   deformable convolution backward data operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the deformable convolution backward data operation. You can get the size of the workspace
 *   with the ::cnnlGetDCNBakcwardDataWorkspaceSize function.
 * @param[in] grad_input_desc
 *   Input. The descriptor of the gradient with respect to the input tensor.
 *   This parameter is requested to be the same as \p input_desc. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] grad_input
 *   Output. Pointer to the MLU memory that stores the gradient with respect to \p input.
 * @param[in] grad_offset_desc
 *   Input. The descriptor of the gradient with respect to the offset tensor.
 *   This parameter is requested to be the same as \p offset_desc.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] grad_offset
 *   Output. Pointer to the MLU memory that stores the gradient with respect to \p offset.
 * @param[in] grad_mask_desc
 *   Input. The descriptor of the gradient with respect to the mask tensor.
 *   This parameter is requested to be the same as \p mask_desc. Set this parameter to NULL when mask and
 *   grad_mask are not needed. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] grad_mask
 *   Output. Pointer to the MLU memory that stores the gradient with respect to \p mask.
 *   Set this parameter to NULL when mask and grad_mask are not needed.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_NOT_SUPPORTED, ::CNNL_STATUS_NUMERICAL_OVERFLOW
 *
 * @par Formula
 * - See "Deformable Convolution Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Offchip data type of \p input, \p offset, \p mask, \p filter, \p grad_output,
 *   \p grad_input, \p grad_offset, and \p grad_mask must be the same.
 * - The supported offchip data types of the input tensor and output tensor are as follows:
 *   - input, offset, mask, filter, grad_output, grad_input, grad_offset, grad_mask: half, float.
 * - This function supports any combinations of the following onchip data types for input tensor
 *   \p grad_output and \p filter on MLU200 series and CE3226.
 *   - \p grad_output onchip data type: int16, int31.
 *   - \p filter onchip data type: int16, int31.
 * - On MLU300 series or above, the onchip data type of \p input and \p filter
 *   should be \p CNNL_DTYPE_INVALID or the same as the corresponding offchip data type.
 *
 * @par Data Layout
 * - The data layout of the input, offset, mask, filter, grad_output, grad_input, grad_offset,
 *   and grad_mask should be \p CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - The input, offset, mask, filter, grad_output, grad_input, grad_offset, grad_mask and
 *   the deformable convolution descriptor
 *   (including pad, stride, dilation, deformable_group, conv_group, im2col_step) must meet the
 *   following requirements:
 *   - input tensor: \p batch > 0, \p height > 0, \p width > 0, \p channel > 0
 *   - offset tensor: \p batch should be equal to the batch size of input tensor, \p height and \p width
 *     should be equal to the height and width of output tensor accordingly. \p channel should be equal to
 *     deformable_group * filter_height * filter_width * 2.
 *   - grad offset tensor: the data type, layout, and shape of grad offset should be equal to the
 *     offset tensor.
 *   - mask tensor: When mask is needed, \p batch should be equal to the batch size of input tensor,
 *     \p height and \p width should be equal to the height and width of output tensor accordingly.
 *     \p channel should be equal to deformable_group * filter_height * filter_width.
 *   - grad mask tensor: the data type, layout and shape of the grad mask should be equal to
 *     the mask tensor. When mask is passed NULL, grad mask must be NULL.
 *   - The data bytes of (im2col_step * out_height * out_filter * filter_h * filter_w * input_channel)
 *     should be less than or equal to the INT_MAX defined in limits.h.
 *   - When mask is not needed, \p mask, \p mask_desc, \p grad_mask and \p grad_mask_desc should be
 *     set to NULL. When it is needed, any of \p mask, \p mask_desc, \p grad_mask and
 *     \p grad_mask_desc cannot be NULL.
 *
 * @par API Dependency
 * - Before calling this function to implement deformable convolution backward data,
 *   you need to prepare all the parameters passed to this function. See each parameter
 *   description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the im2col_step of to the batch size.
 *
 * @note
 * - \p input, \p mask, \p filter, and \p grad_output should be smaller enough to prevent the result
 *   from data overflow especially when the data type is \p CNNL_DTYPE_HALF.
 * - \p offset with NaN is not supported on MLU300 series and lower platforms.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://github.com/msracver/Deformable-ConvNets
 * - Deformable Convolutional Networks, Jifeng Dai, et al., 2017.
 * - Deformable ConvNets v2: More Deformable, Better Results, Xizhou Zhu, et al., 2018.
 */
cnnlStatus_t CNNL_WIN_API cnnlDCNBackwardData(cnnlHandle_t handle,
                                              const cnnlDCNDescriptor_t dcn_desc,
                                              const cnnlTensorDescriptor_t input_desc,
                                              const void *input,
                                              const cnnlTensorDescriptor_t offset_desc,
                                              const void *offset,
                                              const cnnlTensorDescriptor_t mask_desc,
                                              const void *mask,
                                              const cnnlTensorDescriptor_t filter_desc,
                                              const void *filter,
                                              const cnnlTensorDescriptor_t grad_output_desc,
                                              const void *grad_output,
                                              void *workspace,
                                              const size_t workspace_size,
                                              const cnnlTensorDescriptor_t grad_input_desc,
                                              void* grad_input,
                                              const cnnlTensorDescriptor_t grad_offset_desc,
                                              void* grad_offset,
                                              const cnnlTensorDescriptor_t grad_mask_desc,
                                              void* grad_mask);

// Group:DCNBackwardWeight
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 *        workspace to optimize the deformable convolution backward filter operation.
 *
 * The size of the extra workspace is based on the given information of the deformable convolution
 * backward filter operation, including the deformable convolution descriptor \p dcn_desc,
 * input tensor descriptor \p input_desc, offset tensor
 * descriptor \p offset_desc, mask tensor descriptor \p mask_desc, gradient with respect to
 * the output tensor \p grad_output_desc, the gradient with respect to the filter tensor
 * \p grad_filter_desc, and the gradient with respect to the bias tensor \p grad_bias_desc.
 * For more information about the workspace, see "Cambricon CNNL User Guide."
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   deformable convolution operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] dcn_desc
 *   Input. The descriptor of the deformable convolution operation. For detailed information, see
 *   ::cnnlDCNDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] offset_desc
 *   Input. The descriptor of the offset tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] mask_desc
 *   Input. The descriptor of the mask tensor. Set this parameter to NULL if mask is not needed. For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @param[in] grad_output_desc
 *   Input. The descriptor of the gradient with respect to the output tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] grad_filter_desc
 *   Input. The descriptor of the gradient with respect to the filter tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] grad_bias_desc
 *   Input. The descriptor of the gradient with respect to the bias tensor.
 *   Set this parameter to NULL if the gradient with respect to bias is not needed.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   deformable convolution backward filter operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions
 *   to create and set the tensor descriptors \p input, \p offset, \p mask (optional),
 *   \p grad_output, \p grad_filter, \p grad_bias (optional), before calling this
 *   function.
 * - The allocated extra workspace should be passed to the ::cnnlDCNBackwardWeight function to
 *   perform the deformable convolution backward filter operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetDCNBackwardWeightWorkspaceSize(cnnlHandle_t handle,
                                      const cnnlDCNDescriptor_t dcn_desc,
                                      const cnnlTensorDescriptor_t input_desc,
                                      const cnnlTensorDescriptor_t offset_desc,
                                      const cnnlTensorDescriptor_t mask_desc,
                                      const cnnlTensorDescriptor_t grad_output_desc,
                                      const cnnlTensorDescriptor_t grad_filter_desc,
                                      const cnnlTensorDescriptor_t grad_bias_desc,
                                      size_t *workspace_size);

// Group:DCNBackwardWeight
/*!
 * @brief Performs the back-propagation of a deformable convolution operation to compute
 *        the gradient with respect to filter \p grad_filter and bias \p grad_bias
 *        based on the gradient of response \p grad_output.
 *
 * This function needs extra MLU memory as the workspace to improve the performance.
 * You can get the workspace size with the
 * ::cnnlGetDCNBackwardWeightWorkspaceSize function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the deformable convolution backward filter operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] dcn_desc
 *   Input. The descriptor of the deformable convolution operation. For detailed information,
 *   see ::cnnlDCNDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] offset_desc
 *   Input. The descriptor of the offset to be applied for each position in the convolution kernel.
 *   The shape of offset should be (batch, out_height, out_width, 2 * deformable_group *
 *   weight_height * filter_width). For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] offset
 *   Input. Pointer to the MLU memory that stores the offset tensor.
 * @param[in] mask_desc
 *   Input. The descriptor of the scaling factor to be applied for each position in the convolution
 *   kernel. The shape of mask should be (batch, out_height, out_width,
 *   deformable_group * filter_height * filter_width). Set this parameter to NULL when
 *   mask is not requested. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] mask
 *   Input. Pointer to the MLU memory that stores the mask tensor. Set this parameter to NULL when mask is not
 *   requested.
 * @param[in] grad_output_desc
 *   Input. The descriptor of the gradient with respect to the output tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] grad_output
 *   Input. Pointer to the MLU memory that stores the gradient with respect to the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   deformable convolution backward filter operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the deformable convolution backward filter operation. You can get the size of the workspace
 *   with the ::cnnlGetDCNBackwardWeightWorkspaceSize function.
 * @param[in] grad_filter_desc
 *   Input. The descriptor of the gradient with respect to the filter tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] grad_filter
 *   Output. Pointer to the MLU memory that stores the gradient with respect to the filter tensor.
 * @param[in] grad_bias_desc
 *   Input. The descriptor of the gradient with respect to the bias tensor. Set this parameter to NULL if the
 *   gradient of the bias tensor is not needed. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] grad_bias
 *   Output. Pointer to the MLU memory that stores the gradient with respect to the bias tensor.
 *   Set this parameter to NULL if the gradient of the bias tensor is not needed.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_NOT_SUPPORTED, ::CNNL_STATUS_NUMERICAL_OVERFLOW
 *
 * @par Formula
 * - See "Deformable Convolution Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Offchip data type of \p input, \p offset, \p mask, \p grad_output, \p grad_filter,
 *   and \p grad_bias must be the same.
 * - The supported offchip data types of the input tensor and output tensor are as follows:
 *   - input, offset, mask, grad_output, grad_filter, grad_bias, grad_mask: half, float.
 * - This function supports any combinations of the following onchip data types for input tensor
 *   \p grad_output and \p input on MLU200 series and CE3226.
 *   - \p grad_output onchip data type: int16, int31.
 *   - \p filter onchip data type: int16, int31.
 * - On MLU300 series or above, the onchip data type of \p input and \p grad_output should be
 *   \p CNNL_DTYPE_INVALID or the same as the corresponding offchip data type.
 *
 * @par Data Layout
 * - The data layout of the input, offset, mask, grad_output, and grad_filter
 *   should be \p CNNL_LAYOUT_NHWC.
 * - The data layout of grad_bias should be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The input, offset, mask, grad_output, grad_filter, grad_bias and
 *   the deformable convolution descriptor
 *   (including pad, stride, dilation, deformable_group, conv_group, im2col_step) must meet the
 *   following requirements:
 *   - input tensor: \p batch > 0, \p height > 0, \p width > 0, \p channel > 0
 *   - offset tensor: \p batch should be equal to the batch of input tensor, \p height and \p width
 *     should be equal to the height and width of output tensor. \p channel should be equal to
 *     deformable_group * filter_height * filter_width * 2.
 *   - mask tensor: When mask is needed, \p batch should be equal to the batch of input tensor,
 *     \p height and \p width should be equal to the height and width of output tensor.
 *     \p channel should be equal to deformable_group * filter_height * filter_width.
 *   - grad bias tensor: When the gradient of bias is needed, the \p grad_bias should be a
 *     one-dimensional array with the length of \p out_channel.
 *   - The value of (im2col_step * out_height * out_filter * filter_h * filter_w * input_channel)
 *     should be less than or equal to the INT_MAX defined in limits.h.

 * @par API Dependency
 * - Before calling this function to implement the backward filter of deformable convolution,
 *   you need to prepare all the parameters passed to this function. See each parameter
 *   description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the im2col_step to the batch size.
 *
 * @note
 * - The alignment of \p input, \p offset, \p mask, \p grad_output, \p grad_filter, \p grad_bias
 *   should be contiguous in the MLU memory.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://github.com/msracver/Deformable-ConvNets
 * - Deformable Convolutional Networks, Jifeng Dai, et al., 2017.
 * - Deformable ConvNets v2: More Deformable, Better Results, Xizhou Zhu, et al., 2018.
 */
cnnlStatus_t CNNL_WIN_API
cnnlDCNBackwardWeight(cnnlHandle_t handle,
                      const cnnlDCNDescriptor_t dcn_desc,
                      const cnnlTensorDescriptor_t input_desc,
                      const void *input,
                      const cnnlTensorDescriptor_t offset_desc,
                      const void *offset,
                      const cnnlTensorDescriptor_t mask_desc,
                      const void *mask,
                      const cnnlTensorDescriptor_t grad_output_desc,
                      const void *grad_output,
                      void *workspace,
                      size_t workspace_size,
                      const cnnlTensorDescriptor_t grad_filter_desc,
                      void *grad_filter,
                      const cnnlTensorDescriptor_t grad_bias_desc,
                      void *grad_bias);

/******************************************************************************
 * Cambricon CNNL OP: CARAFE
 ******************************************************************************/

// Group:CarafeForward
/*!
 * @brief Creates a descriptor pointed by \p carafe_desc for CARAFE upsampling forward
 *        and backward operations, and allocates memory holding the configuration parameters.
 *        The information is defined in ::cnnlCarafeDescriptor_t.
 *        For more information about descriptor, see "Cambricon CNNL User Guide".
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[out] carafe_desc
 *  Output. A host pointer to the CARAFE descriptor that holds information about the
 *  CARAFE operator.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_NOT_INITIALIZED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetCarafeDescriptor function to initialize
 *   and set the information to the CARAFE descriptor.
 * - You need to call the ::cnnlDestroyCarafeDescriptor function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlCreateCarafeDescriptor(cnnlCarafeDescriptor_t *carafe_desc);

// Group:CarafeForward
/*!
 * @brief Initializes the CARAFE descriptor \p carafe_desc that was previously created with the
 *        ::cnnlCreateCarafeDescriptor function, and sets the information about the
 *        CARAFE forward and backward operations to the descriptor \p carafe_desc.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in,out] carafe_desc
 *   Output. The descriptor of the CARAFE operator. For detailed information,
 *   see ::cnnlCarafeDescriptor_t.
 * @param[in] dimNb
 *   Input. The number of dimensions in the input tensor of the CARAFE operation.
 * @param[in] kernel_size
 *   Input. The width of the upsampling kernel window.
 * @param[in] group_size
 *   Input. The number of channel groups. The channels in the same group share the same upsampling filter.
 * @param[in] scale_factor
 *   Input. The upsampling ratio by which the resolution of the input feature map will be
 *   increased, i.e., the height and width of the output feature maps would be \p scale_factor times
 *   of the height and width of the input feature maps, respectively.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, ::cnnlCreateCarafeDescriptor should be called.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlSetCarafeDescriptor(cnnlCarafeDescriptor_t carafe_desc,
                                                  const int dimNb,
                                                  const int kernel_size,
                                                  const int group_size,
                                                  const int scale_factor);

// Group:CarafeForward
/*!
 * @brief Destroys a CARAFE descriptor \p carafe_desc that was previously created by
 *        the ::cnnlCreateCarafeDescriptor function.
 *
 * The CARAFE descriptor is defined in ::cnnlCarafeDescriptor_t
 * and holds the information about the CARAFE forward or backward operations.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] carafe_desc
 *   Input. The CARAFE descriptor to be destroyed. For detailed information,
 *   see ::cnnlCarafeDescriptor_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - You need to call this function after calling the ::cnnlCarafeForward,
 *   or ::cnnlCarafeBackward function. Otherwise, ::CNNL_STATUS_BAD_PARAM is returned.
 * - This function should be called to destroy the CARAFE descriptor.
 *   Otherwise, memory leak may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlDestroyCarafeDescriptor(cnnlCarafeDescriptor_t carafe_desc);

// Group:CarafeForward
/*!
 * @brief Performs the CARAFE (Content-Aware ReAssembly of FEatures) upsampling operation
 *        on the input feature maps \p input using weighted combination, where the
 *        filter are set with \p mask. The upsampled feature maps are returned in
 *        the output tensor \p output.
 *
 * CARAFE performs upsampling at each output location by weighted summation in a nearby mask
 * window around the corresponding input location. The mask filter are defined separately
 * for each output location, which offers the ability of content-aware handling.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the carafe forward operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] carafe_desc
 *   Input. The descriptor of the CARAFE operator. For detailed information,
 *   see ::cnnlCarafeDescriptor_t.
 * @param[in] input_desc
 *   Input. The tensor descriptor of the input feature maps.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] mask_desc
 *   Input. The tensor descriptor of the mask applied to the input feature maps.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] mask
 *   Input. Pointer to the MLU memory that stores the mask tensor.
 * @param[in] output_desc
 *   Input. The tensor descriptor of the output upsampled feature maps.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "CARAFE Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - \p input: half, float.
 * - \p mask: half, float.
 * - \p output: half, float.
 * - Data types of \p input, \p mask and \p output tensors must be the same.
 *
 * @par Data Layout
 * - Data layout of the \p input, \p mask, and \p output tensors should be \p CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - Parameters specified in \p carafe_desc should satisfy:
 *   - \p dimNb = 4.
 *   - \p kernel_size should be an odd number, i.e., 2*n+1 (n>=0), and \p kernel_size <= 45.
 *   - \p group_size and \p scale_factor are positive integers, and \p scale_factor <= 5.
 * - The dimensions specified by \p input_desc, \p mask_desc and \p output_desc should
 *   be \p input[N, H, W, C], \p mask[N, Ho, Wo, Cm] and \p output[N, Ho, Wo, C], respectively,
 *   in which:
 *   - The length of all dimensions should be non-negative integers.
 *   - The dimensions denoted by the same symbol should have the same value.
 *   - \p C should be divisible by \p group_size, i.e., \p C = N * \p group_size (N>=1).
 *   - \p Cm = \p group_size * \p kernel_size * \p kernel_size.
 *   - \p Ho = \p scale_factor * \p H.
 *   - \p Wo = \p scale_factor * \p W.
 *
 * @par API Dependency
 * - Before calling this function to implement CARAFE forward operation, you need to
 *   prepare all the parameters passed to this function. See each parameter description
 *   for details.
 *
 * @par Performance Optimization
 * - None.
 *
 * @note
 * - If any dimension in \p input_desc, \p mask_desc, or \p output_desc is zero,
 *   which represents an empty tensor, ::CNNL_STATUS_SUCCESS is returned without
 *   any changes to the \p output tensor.
 * - This operation is not supported on the 1V platforms.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - Example of CARAFE forward operation is as follows:
     @verbatim
      input tensor by 1 * 2 * 2 * 1 --> input:
        [[[[ 0.34064351], [-0.8246629 ]],
          [[-0.71797801], [-0.51707748]]]]

      mask tensor by 1 * 4 * 4 * 1 --> mask:
        [[[[ 0.97630979], [-0.06261992], [ 0.91232837], [-0.1598553 ]],
          [[-0.72060206], [ 0.48904262], [-0.65568251], [ 0.12801235]],
          [[-0.85134485], [-1.27589059], [ 3.00143314], [ 0.61258706]],
          [[-0.50308504], [-0.93015218], [-1.1125597 ], [ 0.67302385]]]]

      param:
        kernel_size: 3, group_size: 1, scale_factor: 2

      output tensor by 1 * 4 * 4 * 1 --> output:
        [[[[ 0.33257359], [-0.02133107], [-0.75236336], [ 0.13182674]],
          [[-0.24546842], [ 0.1665892 ], [ 0.54071704], [-0.10556703]],
          [[ 0.61124688], [ 0.91606138], [-1.55197348], [-0.31675497]],
          [[ 0.36120399], [ 0.66782881], [ 0.57527956], [-0.34800548]]]]
     @endverbatim
 *
 * @par Reference
 * - https://github.com/open-mmlab/mmcv/tree/master/mmcv/ops/carafe.py
 * - CARAFE: Content-Aware ReAssembly of FEatures, Jiaqi Wang et al., 2019.
 */
CNNL_DEPRECATED_FOR()
  cnnlStatus_t CNNL_WIN_API cnnlCarafeForward(cnnlHandle_t handle,
                                              const cnnlCarafeDescriptor_t carafe_desc,
                                              const cnnlTensorDescriptor_t input_desc,
                                              const void *input,
                                              const cnnlTensorDescriptor_t mask_desc,
                                              const void *mask,
                                              const cnnlTensorDescriptor_t output_desc,
                                              void *output);

// Group:CarafeBackward
/*!
 * @brief Performs the back-propagation of CARAFE (Content-Aware ReAssembly of FEatures)
 *        operator to compute the gradient with respect to input \p grad_input and
 *        mask \p grad_mask based on the gradient of response \p grad_output.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the CARAFE backward operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] carafe_desc
 *   Input. The descriptor of the CARAFE operator. For detailed information,
 *   see ::cnnlCarafeDescriptor_t.
 * @param[in] input_desc
 *   Input. The tensor descriptor of the input feature maps. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] mask_desc
 *   Input. The tensor descriptor of the mask applied to the input feature maps.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] mask
 *   Input. Pointer to the MLU memory that stores the mask tensor.
 * @param[in] grad_output_desc
 *   Input. The tensor descriptor of the gradient with respect to the output feature maps.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] grad_output
 *   Input. Pointer to the MLU memory that stores the gradient with respect to the
 *   upsampled feature maps.
 * @param[in] grad_input_desc
 *   Input. The tensor descriptor of the gradient with respect to the input feature maps.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] grad_input
 *   Output. Pointer to the MLU memory that stores the gradient with respect to the
 *   input feature maps.
 * @param[in] grad_mask_desc
 *   Input. The descriptor of the gradient tensor with respect to the \p mask tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] grad_mask
 *   Output. Pointer to the MLU memory that stores the gradient with respect to \p mask.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "CARAFE Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - \p input: half or float.
 * - \p mask: half or float.
 * - \p output: half or float.
 * - Data types of \p input, \p mask, \p grad_output, \p grad_input and \p grad_mask
 *   tensors must be the same.
 *   For MLU200 series, it is not recommended to use half data type for tensors due to the
 *   low precision.
 *
 * @par Data Layout
 * - Data layout of the \p input, \p mask, \p grad_output, \p grad_input and \p grad_mask
 *   tensors should be \p CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - Parameters specified in \p carafe_desc should satisfy:
 *   - \p dimNb = 4.
 *   - \p kernel_size should be an odd number, i.e., 2*n+1 (n>=0), and \p kernel_size <= 137.
 *   - \p group_size and \p scale_factor should be positive integers.
 * - The dimensions specified by \p input_desc, \p mask_desc, \p grad_output_desc,
 *   \p grad_input_desc and \p grad_mask_desc should be \p input[N, H, W, C],
 *   \p mask[N, Ho, Wo, Cm], \p grad_output[N, Ho, Wo, C], \p grad_input[N, H, W, C]
 *   and \p grad_mask[N, Ho, Wo, Cm], respectively, in which:
 *   - The length of all dimensions should be non-negative integers.
 *   - The dimensions denoted by the same symbol should have the same value.
 *   - \p C should be divisible by \p group_size, i.e., \p C = n * \p group_size (n>=1).
 *   - \p Cm = \p group_size * \p kernel_size * \p kernel_size.
 *   - \p Ho = \p scale_factor * \p H.
 *   - \p Wo = \p scale_factor * \p W.
 *
 * @par API Dependency
 * - Before calling this function to implement CARAFE backward operation, you need to
 *   prepare all the parameters passed to this function. See each parameter description
 *   for details.
 *
 * @par Performance Optimization
 * - None.
 *
 * @note
 * - If any dimension in \p input_desc, \p mask_desc, \p grad_output_desc, \p grad_input_desc
 *   or \p grad_mask_desc is zero, which represents an empty tensor, ::CNNL_STATUS_SUCCESS is
 *   returned without any changes to the \p grad_output and \p grad_input tensors.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://github.com/open-mmlab/mmcv/tree/master/mmcv/ops/carafe.py
 * - CARAFE: Content-Aware ReAssembly of FEatures, Jiaqi Wang, et al., 2019.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlCarafeBackward(cnnlHandle_t handle,
                                             const cnnlCarafeDescriptor_t carafe_desc,
                                             const cnnlTensorDescriptor_t input_desc,
                                             const void *input,
                                             const cnnlTensorDescriptor_t mask_desc,
                                             const void *mask,
                                             const cnnlTensorDescriptor_t grad_output_desc,
                                             const void *grad_output,
                                             const cnnlTensorDescriptor_t grad_input_desc,
                                             void *grad_input,
                                             const cnnlTensorDescriptor_t grad_mask_desc,
                                             void *grad_mask);

/******************************************************************************
 * Cambricon CNNL OP: GatherV2
 ******************************************************************************/
// Group:GatherV2
/*!
 * @brief Gathers slices from \p params at axis \p axis according to \p indices.
 *        To gather slices in different batches, call ::cnnlBatchGatherV2.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlBatchGatherV2_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the gather_v2
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] axis
 *   Input. An integer value that determines the axis to gather value from.
 * @param[in] params_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] params
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] params_shape
 *   Input. Pointer to the MLU memory that stores the shape tensor of \p params.
 * @param[in] indices_desc
 *   Input. The descriptor of the index tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] indices
 *   Input. Pointer to the MLU memory that stores the tensor used to store index of each element of
 *   \p output in the corresponding dimension of input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "GatherV2 Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for input tensor
 *   \p params, index tensor \p indices, and output tensor \p output.
 *   - input tensor: 1-byte, 2-byte, 4-byte, 8-byte.
 *   - index tensor: int32.
 *   - output tensor: 1-byte, 2-byte, 4-byte, 8-byte.
 *
 *   The byte width of a data type can be got with the ::cnnlGetSizeOfDataType function.
 *
 *   Note that the data type of input tensor \p input and output tensor \p output must be the same.
 *
 * @par Scale Limitation
 * - The input tensor, output tensor and axis must meet the following requirements:
 *   - input tensor: The shape should be the same as the shape of output except the \p axis dimension.
 *   - axis: It should be greater than -1 and less than the dimension of \p params.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the gather_v2 operation is as follows:
     @verbatim
     input two arrays by 2 * 2 and 1 --> params: [[1., 2.], [3., 4.]]

     --> indices: [0]

     param:
       axis: 0, params_shape: [2, 2]

     output array by 2 --> output: [1., 2.]
     @endverbatim
 *
 * @par Reference
 * - https://tensorflow.org/api_docs/python/tf/raw_ops/GatherV2
 */
CNNL_DEPRECATED_FOR(cnnlBatchGatherV2_v2)
cnnlStatus_t CNNL_WIN_API cnnlGatherV2(cnnlHandle_t handle,
                                       const int axis,
                                       const cnnlTensorDescriptor_t params_desc,
                                       const void *params,
                                       const int *params_shape,
                                       const cnnlTensorDescriptor_t indices_desc,
                                       const int *indices,
                                       const cnnlTensorDescriptor_t output_desc,
                                       void *output);
// Group:BatchGatherV2
/*!
 * @brief Gathers slices in different batches from \p params at dimension \p axis
 *        according to \p indices.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlBatchGatherV2_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the gather_v2 operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] axis
 *   Input. An integer value that determines the dimension to gather value from.
 * @param[in] batch_dims
 *   Input. An integer value that determines the batch dimensions of inputs and output.
 *   The batch dimensions are from dimension 0 to dimension (batch_dims -1).
 * @param[in] params_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] params
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] indices_desc
 *   Input. The descriptor of the index tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] indices
 *   Input. Pointer to the MLU memory that stores the tensor used to store index
 *   of each element of \p output in the corresponding dimension of \p params tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "BatchGatherV2 Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for input tensor
 *   \p params, index tensor \p indices, and output tensor \p output.
 *   The data type of input tensor and output tensor must be the same.
 *   - input tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *   - index tensor: int32, int64.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *
 * @par Scale Limitation
 * - The shape of input tensor should be the same as the shape of output tensor in
 *   every dimension except the dimension \p axis.
 *
 * @note
 * - The \p axis must be in range of [-rank, rank), where rank is the dimension size of the \p params.
 *   Negative \p axis refers to 'axis + rank'.
 * - The \p batch_dims must be in range of [-rank, rank), where rank is the dimension size of the \p indices.
 *   Negative \p batch_dims refers to 'batch_dims + rank'.
 * - The data in \p indices is expected in range of [-rank, rank), where rank is the size of \p axis dimension in
 *   \p params. If the data in \p indices is not in the range, the corresponding positions of the output will be
 *   filled with 0.
 *
 * @par Requirements
 * - \p batch_dims and \p axis should meet the following requirements:
 *   - The \p axis must be greater than or equal to \p batch_dims.
 *
 * @par Example
 * - The example of the gather_v2 operation is as follows:
     @verbatim
     input two arrays by 2 * 2 * 2 and 2 * 3:
       --> params: [[[1., 2.], [3., 4.]],
                    [[5., 6.], [7., 8.]]]

       --> indices: [[0, 1, -1], [1, 0, 2]]

     param:
       axis: 1, batch_dims: 1

     output array by 2 * 3 * 2:
       --> output: [[[1., 2.], [3., 4.], [3., 4.]],
                    [[7., 8.], [5., 6.], [0., 0.]]]
     @endverbatim
 *
 * @par Reference
 * - https://tensorflow.org/api_docs/python/tf/raw_ops/GatherV2
 */
CNNL_DEPRECATED_FOR(cnnlBatchGatherV2_v2)
cnnlStatus_t CNNL_WIN_API cnnlBatchGatherV2(cnnlHandle_t handle,
                                            const int axis,
                                            const int batch_dims,
                                            const cnnlTensorDescriptor_t params_desc,
                                            const void *params,
                                            const cnnlTensorDescriptor_t indices_desc,
                                            const void *indices,
                                            const cnnlTensorDescriptor_t output_desc,
                                            void *output);
// Group:BatchGatherV2
/*!
 * @brief Gathers slices in different batches from \p params at dimension \p axis
 *        according to \p indices. Compared with cnnlBatchGatherV2, this function add a
 *        \p mode parameter to specify whether the \p indices contain negative numbers.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the gather_v2 operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] axis
 *   Input. An integer value that determines the dimension to gather value from.
 * @param[in] batch_dims
 *   Input. An integer value that determines the batch dimensions of inputs and output.
 *   The batch dimensions are from dimension 0 to dimension (batch_dims -1).
 * @param[in] mode
 *   Input. An integer value that determines the mode to use. 0 means that \p indices
 *   contain negative numbers. 1 means that \p indices not contain negative numbers.
 * @param[in] params_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] params
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] indices_desc
 *   Input. The descriptor of the index tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] indices
 *   Input. Pointer to the MLU memory that stores the tensor used to store index
 *   of each element of \p output in the corresponding dimension of \p params tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "BatchGatherV2 Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for input tensor
 *   \p params, index tensor \p indices, and output tensor \p output.
 *   - input tensor: 1-byte, 2-byte, 4-byte, 8-byte.
 *   - index tensor: int32, int64, uint32, uint64.
 *   - output tensor: 1-byte, 2-byte, 4-byte, 8-byte.
 *
 *   The byte width of a data type can be got with the ::cnnlGetSizeOfDataType function.
 *
 *   Note that the data type of input tensor \p input and output tensor \p output must be the same.
 *
 * @par Scale Limitation
 * - The element number of \p params should be no more than INT_MAX on MLU300 series and CE3226.
 *
 * @note
 * - The tensor shapes should satisfy the following relationship:
 *   output.shape = params.shape[:axis] + indices.shape[batch_dims:] + params.shape[axis + 1:]
 * - The \p axis must be in range of [-rank, rank), where rank is the dimension size of the \p params.
 *   Negative \p axis refers to 'axis + rank'.
 * - The \p batch_dims must be in range of [-rank, rank), where rank is the dimension size of the \p indices.
 *   Negative \p batch_dims refers to 'batch_dims + rank'.
 * - The data in \p indices is expected in range of [-rank, rank), where rank is the size of \p axis dimension in
 *   \p params. If the data in \p indices is not in the range, the corresponding positions of the output will be
 *   filled with 0.
 *
 * @par Requirements
 * - \p batch_dims and \p axis should meet the following requirements:
 *   - \p axis must be greater than or equal to \p batch_dims.
 *
 * @par Example
 * - The example of the gather_v2 operation is as follows:
     @verbatim
     input two arrays by 2 * 2 * 2 and 2 * 3:
       --> params: [[[1., 2.], [3., 4.]],
                    [[5., 6.], [7., 8.]]]

       --> indices: [[0, 1, -1], [1, 0, 2]]

     param:
       axis: 1, batch_dims: 1

     output array by 2 * 3 * 2:
       --> output: [[[1., 2.], [3., 4.], [3., 4.]],
                    [[7., 8.], [5., 6.], [0., 0.]]]
     @endverbatim
 *
 * @par Reference
 * - https://tensorflow.org/api_docs/python/tf/raw_ops/GatherV2
 */
cnnlStatus_t CNNL_WIN_API cnnlBatchGatherV2_v2(cnnlHandle_t handle,
                                               const int axis,
                                               const int batch_dims,
                                               const int mode,
                                               const cnnlTensorDescriptor_t params_desc,
                                               const void *params,
                                               const cnnlTensorDescriptor_t indices_desc,
                                               const void *indices,
                                               const cnnlTensorDescriptor_t output_desc,
                                               void *output);
// Group:Fill
/*!
 * @brief Fills the output tensor \p output with a scale \p value.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlFill_v4 instead, which supports the parameter \p value_desc
 * to define \p value information.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the fill
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] value
 *   value. A scale value to fill the output tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - This function supports the following data types for output tensor \p output.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *
 * @note
 * - You can specify the stride of all dimensions for \p output_desc with
 *   ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the fill operation is as follows:
     @verbatim
      param:value: 5

      output array by 2 * 3 * 2 --> output: [[[5,5],[5,5],[5,5]],
                                             [[5,5],[5,5],[5,5]]]
     @endverbatim
 *
 * @par Reference
 * - http://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/Fill.cpp
 *
 */
CNNL_DEPRECATED_FOR(cnnlFill_v4)
cnnlStatus_t CNNL_WIN_API cnnlFill(cnnlHandle_t handle,
                                   float value,
                                   const cnnlTensorDescriptor_t output_desc,
                                   void *output);

// Group:Fill
/*!
 * @brief Fills the output tensor \p output with the value in \p value tensor.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlFill_v4 instead, which supports the parameter \p value_desc that sets \p value
 *   to host pointer or device pointer.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the fill
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] value_desc
 *   Input. The descriptor of the \p value tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] value
 *   Input. Pointer to the MLU memory that stores the \p value tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - This function supports the following data types for value tensor \p value and output tensor \p output.
 *   - value tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *
 * @note
 * - Data type of value tensor \p value and output tensor \p output should be the same.
 * - The number of elements of value tensor \p value only supports one.
 * - You can specify the stride of all dimensions for \p output_desc with
 *   ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the fill operation is as follows:
     @verbatim
      input array by 1 --> value: [5]

      output array by 2 * 3 * 2 --> output: [[[5,5],[5,5],[5,5]],
                                             [[5,5],[5,5],[5,5]]]
     @endverbatim
 *
 * @par Reference
 * - http://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/Fill.cpp
 *
 */
CNNL_DEPRECATED_FOR(cnnlFill_v4)
cnnlStatus_t CNNL_WIN_API cnnlFill_v2(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t value_desc,
                                      const void *value,
                                      const cnnlTensorDescriptor_t output_desc,
                                      void *output);
// Group:Fill
/*!
 * @brief Fills the output tensor \p output with \p value.
 *
 * @deprecated
 *   This function is deprecated and will be removed in future release.
 *   Use ::cnnlFill_v4 instead, which supports the parameter \p value_desc
 *   to define \p value information instead of \p pointer_mode.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the fill
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] pointer_mode
 *   Input.  An enum value that indicates that the scalar value \p value is
 *   passed by reference on the host or device. The information is defined in ::cnnlPointerMode_t.
 * @param[in] value
 *   Input.  A pointer to scaling factor of tensor input.
 *   If the \p pointer_mode is \p CNNL_POINTER_MODE_DEVICE, the \p value should be a device pointer.
 *   If the \p pointer_mode is \p CNNL_POINTER_MODE_HOST, the \p value should be a host pointer.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Data Type
 * - This function supports the following data types for \p value and output tensor \p output.
 *   - value: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, bfloat16, float, complex<float>.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, bfloat16, float, complex<float>.
 *
 * @note
 * - Data type of \p value and output tensor \p output should be the same.
 * - The number of elements of \p value only supports one.
 * - You can specify the stride of all dimensions for \p output_desc with
 *   ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the fill operation is as follows:
     @verbatim
      param:value: 5

      output array by 2 * 3 * 2 --> output: [[[5,5],[5,5],[5,5]],
                                             [[5,5],[5,5],[5,5]]]
     @endverbatim
 *
 * @par Reference
 * - http://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/Fill.cpp
 *
 */
CNNL_DEPRECATED_FOR(cnnlFill_v4)
cnnlStatus_t CNNL_WIN_API cnnlFill_v3(cnnlHandle_t handle,
                                      const cnnlPointerMode_t pointer_mode,
                                      const void *value,
                                      const cnnlTensorDescriptor_t output_desc,
                                      void *output);
// Group:Fill
/*!
 * @brief Fills the output tensor \p output with \p value.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the fill
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] value_desc
 *   Input. The descriptor of the value tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] value
 *   Input. Pointer to the MLU memory or host memory that stores the value to fill with.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out] output
 *   Input/Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Data Type
 * - This function supports the following data types for \p value and output tensor \p output.
 *   - value: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, bfloat16, float, complex<float>.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, bfloat16, float, complex<float>.
 *   - Data type of \p value and output tensor \p output should be the same.
 *
 * @note
 * - The number of elements of \p value only supports one.
 * - You can specify the stride of all dimensions for \p output_desc with
 *   ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the fill operation is as follows:
     @verbatim
      param:value: 5

      output array by 2 * 3 * 2 --> output: [[[5,5],[5,5],[5,5]],
                                             [[5,5],[5,5],[5,5]]]
     @endverbatim
 *
 * @par Reference
 * - http://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/Fill.cpp
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlFill_v4(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t value_desc,
                                      const void *value,
                                      const cnnlTensorDescriptor_t output_desc,
                                      void *output);

// Group:Sin
/*!
 * @brief Computes sine of input tensor \p x, and returns the results in the output tensor \p y.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the sin
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \p x.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor \p y.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Sin Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensor and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, bfloat16, float.
 *   - output tensor: half, bfloat16, float.
 *
 *   The bfloat16 data type is supported only on MLU500 series.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - To achieve high-accuracy output data, the data value of each element in the input tensor \p x
 *   must belong to [-1e3,1e3] for all data types.
 *
 * @note
 * - The input tensor and output tensor must have the same shape.
 * - You can specify the stride of all dimensions for \p x_desc and \p y_desc
 *   with ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/torch.html?highlight=sin#torch.sin
 */
cnnlStatus_t CNNL_WIN_API cnnlSin(cnnlHandle_t handle,
                                  const cnnlTensorDescriptor_t x_desc,
                                  const void *x,
                                  const cnnlTensorDescriptor_t y_desc,
                                  void *y);
// Group:Sin
/*!
 * @brief Computes sine of input tensor \p x, and returns the results in the output tensor \p y.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlSin instead, which does not support the \p prefer parameter.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the sin
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. The \p prefer modes defined in ::cnnlComputationPreference_t enum.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \p x.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor \p y.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Sin Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensor and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, bfloat16, float.
 *   - output tensor: half, bfloat16, float.
 *
 *   The bfloat16 data type is supported only on MLU500 series.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - To achieve high-accuracy output data, the data value of each element in the input tensor \p x
 *   must belong to [-1e3,1e3] for all data types.
 *
 * @note
 * - The input tensor and output tensor must have the same shape.
 * - You can specify the stride of all dimensions for \p x_desc and \p y_desc
 *   with ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/torch.html?highlight=sin#torch.sin
 */
CNNL_DEPRECATED_FOR(cnnlSin)
cnnlStatus_t CNNL_WIN_API cnnlSin_v2(cnnlHandle_t handle,
                                     const cnnlComputationPreference_t prefer,
                                     const cnnlTensorDescriptor_t x_desc,
                                     const void *x,
                                     const cnnlTensorDescriptor_t y_desc,
                                     void *y);

// Group:Cos
/*!
 * @brief Computes cosine of input tensor \p x, and returns the results in the output tensor \p y.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the cos
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \p x.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor \p y.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Cos Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensor and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, bfloat16, float.
 *   - output tensor: half, bfloat16, float.
 *
 *   The bfloat16 data type is supported only on MLU500 series.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - To achieve high-accuracy output data, the data value of each element in the input tensor \p x
 *   must belong to [-1e3,1e3] for all data types.
 *
 * @note
 * - The input tensor and output tensor must have the same shape.
 * - You can specify the stride of all dimensions for \p x_desc and \p y_desc
 *   with ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/torch.html?highlight=cos#torch.cos
 */
cnnlStatus_t CNNL_WIN_API cnnlCos(cnnlHandle_t handle,
                                  const cnnlTensorDescriptor_t x_desc,
                                  const void *x,
                                  const cnnlTensorDescriptor_t y_desc,
                                  void *y);
// Group:Cos
/*!
 * @brief Computes cosine of input tensor \p x, and returns the results in the output tensor \p y.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlCos instead, which does not support the \p prefer parameter.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the cos
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. The \p prefer modes defined in ::cnnlComputationPreference_t enum.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \p x.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor \p y.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Cos Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensor and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, bfloat16, float.
 *   - output tensor: half, bfloat16, float.
 *
 *   The bfloat16 data type is supported only on MLU500 series.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - To achieve high-accuracy output data, the data value of each element in the input tensor \p x
 *   must belong to [-1e3,1e3] for all data types.
 *
 * @note
 * - The input tensor and output tensor must have the same shape.
 * - You can specify the stride of all dimensions for \p x_desc and \p y_desc
 *   with ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/torch.html?highlight=cos#torch.cos
 */
CNNL_DEPRECATED_FOR(cnnlCos)
cnnlStatus_t CNNL_WIN_API cnnlCos_v2(cnnlHandle_t handle,
                                     const cnnlComputationPreference_t prefer,
                                     const cnnlTensorDescriptor_t x_desc,
                                     const void *x,
                                     const cnnlTensorDescriptor_t y_desc,
                                     void *y);

/*! The descriptor of the transpose operation that holds transpose information
 *  including \p dimensions and \p permute.
 *
 *  You need to call the ::cnnlCreateTransposeDescriptor function to create a descriptor,
 *  and call the ::cnnlSetTransposeDescriptor function to set the information of
 *  transpose operation to the descriptor. Also, you need to destroy the Cambricon CNNL context
 *  at the end with the ::cnnlDestroyTransposeDescriptor function.
 */
typedef struct cnnlTransposeStruct *cnnlTransposeDescriptor_t;

// Group:Transpose
/*!
 * @brief Creates a descriptor pointed by \p desc for a transpose operation,
 *        and allocated memory for holding the information about the transpose operation.
 *
 * The information is defined in ::cnnlTransposeDescriptor_t. For more information
 * about descriptor, see "Cambricon CNNL user Guide".
 *
 * @param[out] desc
 *   Output. A host pointer to the transpose descriptor that holds information about
 *   the transpose operation.
 * @par Return
 *   ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetTransposeDescriptor
 *   function to initialize and set information to the transpose descriptor.
 * - You need to call the ::cnnlDestroyTransposeDescriptor function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateTransposeDescriptor(cnnlTransposeDescriptor_t *desc);

// Group:Transpose
/*!
 * @brief Initializes the transpose descriptor \p desc that was previously created
 * with the ::cnnlCreateTransposeDescriptor function, and set the information
 * about the transpose operation to the transpose descriptor \p desc.
 * The information includes the permute dimensions \p dims and permute rules \p permute.
 *
 * @param[in,out] desc
 *   Input/output. The descriptor of the transpose operation. For detailed information,
 *   see ::cnnlTransposeDescriptor_t.
 * @param[in] dims
 *   Input. The number of dimensions in the permute tensor of the transpose operation.
 *   Currently, the value of this parameter should be less than or equal to 8.
 * @param[in] permute
 *   Input. The order of transpose. Currently, for each dimension, the value of permute
 *   should be in range of [0,...,dims -1], and should not be the same in each dimension.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetTransposeDescriptor(cnnlTransposeDescriptor_t desc,
                                                     const int dims,
                                                     const int permute[]);

// Group:Transpose
/*!
 * @brief Destroys a transpose descriptor \p desc that was previously created with the
 *        ::cnnlCreateTensorDescriptor function.
 *
 * The transpose descriptor is defined in ::cnnlTransposeDescriptor_t and holds the information
 * about the transpose operation.
 *
 * @param[in] desc
 *   Input. The transpose descriptor to be destroyed. For detailed information,
 *   see ::cnnlTransposeDescriptor_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyTransposeDescriptor(cnnlTransposeDescriptor_t desc);

// Group:Transpose
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace
 * to optimize the transpose operation.
 *
 * The size of the extra workspace is based on the given information of the transpose operation,
 * including the input tensor descriptor \p x_desc and transpose descriptor \p desc.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the transpose operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] desc
 *   Input. The descriptor of the transpose operation. For detailed information,
 *   see ::cnnlTransposeDescriptor_t.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *   the transpose operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlCreateTensorDescriptor and
 *   ::cnnlSetTensorDescriptor functions to create and set the tensor descriptors \p x_desc.
 * - The allocated extra workspace should be passed to the ::cnnlTranspose_v2 function
 *   to perform the transpose operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetTransposeWorkspaceSize(cnnlHandle_t handle,
                                                        const cnnlTensorDescriptor_t x_desc,
                                                        const cnnlTransposeDescriptor_t desc,
                                                        size_t *size);
// Group:Transpose
/*!
 * @brief Reorders the dimension according to the value of \p permute. To have better performance
 *        for over 4D transpose with large-scale cases, call the
 *        ::cnnlTranspose_v2 function.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlTranspose_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the transpose operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] desc
 *   Input. The descriptor of the transpose operation. For detailed information,
 *          see ::cnnlTransposeDescriptor_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *          see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *          see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \p x and
 *   output tensor \p y.
 *   Note that the data type of input tensor and output tensor should be the same.
 *   - input tensor: uint8, int8, uint16, int16, uint32, int31, int32, uint64, int64, bool, half,
 *     float, complex_half, complex_float, double.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int31, int32, uint64, int64, bool, half,
 *     float, complex_half, complex_float, double.
 *
 * @par Data Layout
 * - The dimension of input tensor should be less than or equal to 8-dimension.
 *
 * @par Scale Limitation
 * - The \p x, \p y and \p permute have the same shape.
 * - The dimension size of \p x, \p y and \p permute should be less than or equal to
 *   CNNL_DIM_MAX.
 * - The \p permute i-th dimension is in the range [0,...n-1], where n is the rank of the \p x.
 * - The \p y i-th dimension will correspond to the \p x permute[i]-th dimension.
 * - The process of computing, the copy times of memcpy should be less than 65536.
 *
 * @par API Dependency
 * - Before calling this function to implement transpose, you need to prepare all the parameters
 *   passed to this function. See each parameter description for details.
 *
 * @note
 * - None.
 *
 * @par Example
 * - The example of the transpose operation is as follows:
     @verbatim
      input array by 3 * 2 -->
          input: [[1, 4],
                  [2, 5],
                  [3, 6]]
      param:
        dims: 2, permute: (1, 0),

      output array by 2 * 3 --> output: [[1, 2, 3],
                                         [4, 5, 6]]
     @endverbatim
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/transpose
 */
CNNL_DEPRECATED_FOR(cnnlTranspose_v2)
cnnlStatus_t CNNL_WIN_API cnnlTranspose(cnnlHandle_t handle,
                                        const cnnlTransposeDescriptor_t desc,
                                        const cnnlTensorDescriptor_t x_desc,
                                        const void *x,
                                        const cnnlTensorDescriptor_t y_desc,
                                        void *y);
// Group:Transpose
/*!
 * @brief Reorders the dimension according to the value of \p permute. Compared with
 *        ::cnnlTranspose, ::cnnlTranspose_v2 provides better performance for above 4D
 *        transpose with extra input space.
 *
 * This function needs extra MLU memory as the workspace to work.
 * You can get the workspace size with the
 * ::cnnlGetTransposeWorkspaceSize function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the transpose operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] desc
 *   Input. The descriptor of the transpose operation. For detailed information,
 *   see ::cnnlTransposeDescriptor_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[out] y_desc
 *   Output. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   transpose operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the transpose operation. You can get the size of the workspace with
 *   the ::cnnlGetTransposeWorkspaceSize function.

 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Scale Limitation
 * - The \p x, \p y and \p permute have the same shape.
 * - The dimension size of \p x, \p y and \p permute should be less than or equal to CNNL_DIM_MAX.
 * - The \p permute i-th dimension is in the range [0,...n-1], where n is the rank of the \p x.
 * - The \p y i-th dimension will correspond to \p x permute[i]-th dimension.
 * - The process of computing, the copy times of memcpy should be less than 65536.
 *
 * @par Formula
 * - See "Transpose Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \p x and
 *   output tensor \p y.
 *   Note that the data type of input tensor and output tensor should be the same.
 *   - input tensor: uint8, int8, uint16, int16, uint32, int31, int32, uint64, int64, bool, half,
 *     float, complex_half, complex_float, bfloat16, double.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int31, int32, uint64, int64, bool, half,
 *     float, complex_half, complex_float, bfloat16, double.
 *
 * @par Data Layout
 * - The dimension of input tensor should be less than or equal to 8-dimension.

 * @par API Dependency
 * - Before calling this function to implement transpose, you need to prepare
 *   all the parameters passed to this function. See each parameter description
 *   for details.
 *
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the transpose operation is as follows:
 *   @verbatim
 *    input array by 3 * 2 -->
 *         input: [[1, 4],
 *                 [2, 5],
 *                 [3, 6]]
 *     param:
 *       dims: 2, permute: (1, 0),
 *
 *     output array by 2 * 3 --> output: [[1, 2, 3],
 *                                        [4, 5, 6]]
 *    @endverbatim
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/transpose
 */
cnnlStatus_t CNNL_WIN_API cnnlTranspose_v2(cnnlHandle_t handle,
                                           const cnnlTransposeDescriptor_t desc,
                                           const cnnlTensorDescriptor_t x_desc,
                                           const void *x,
                                           const cnnlTensorDescriptor_t y_desc,
                                           void *y,
                                           void *workspace,
                                           size_t workspace_size);

// Group:Reorg
/*!
 * @brief Creates a descriptor pointed by \p desc for a reorg operation,
 *        and allocates memory for holding the information about the reorg
 *        operation. The information is defined in ::cnnlReorgDescriptor_t. For more
 *        information about descriptor, see "Cambricon CNNL User Guide".
 *
 * @param[out] desc
 *  Output. A host pointer to the reorg descriptor that holds information about the reorg operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetReorgDescriptor function to initialize
 *   and set the information to the reorg descriptor.
 * - You need to call the ::cnnlDestroyReorgDescriptor function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateReorgDescriptor(cnnlReorgDescriptor_t *desc);

// Group:Reorg
/*!
 * @brief Destroys a reorg descriptor \p desc that was previously created with the
 *        ::cnnlCreateReorgDescriptor function.
 *
 * The reorg descriptor is defined in ::cnnlReorgDescriptor_t
 * and holds the information about the reorg operation.
 *
 *
 * @param[in] desc
 *   Input. The reorg descriptor to be destroyed.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - You need to call this function after calling the ::cnnlReorg function.
 * - This function should be called to destroy the reorg descriptor.
 *   Otherwise, the memory leak may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyReorgDescriptor(cnnlReorgDescriptor_t desc);

// Group:Reorg
/*!
 * @brief Initializes the reorg descriptor \p desc that was previously created
 * with the ::cnnlCreateReorgDescriptor function, and sets the information
 * about the reorg operation to the reorg descriptor \p desc.
 * The information includes the coefficient in the height dimension \p reorg_h and
 * in the width dimension \p reorg_w, and reorg is forward or not \p forward.
 *
 * @param[in,out] desc
 *   Input/output. The descriptor of the reorg operation. For detailed information,
 *   see ::cnnlReorgDescriptor_t.
 *  @param[in] reorg_h
 *    Input. The remodeling coefficient in the height dimension of input tensor.
 *  @param[in] reorg_w
 *    Input. The remodeling coefficient in the width dimension of input tensor.
 *  @param[in] forward
 *    Input. bool value, false is forward and true is reverse (splitting or merging).
 *    In the following formula output_channel is channel dimension of output shape,
 *    input_channel is channel dimension of input shape.
 *    If \p forward is true,
 *    output_channel = (input_channel * \p reorg_h * \p reorg_w);
 *    If \p forward is false,
 *    output_channel = (input_channel / \p reorg_h / \p reorg_w);
 *    Only supports forward = false currently.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - The formula of calculating the height dimension of output is as follows, where
 *   output_height is height dimension of output tensor, input_height is height
 *   dimension of input tensor.
 *   If \p forward is false, the height dimension of the output tensor is as follows:
 *   output_height = (input_height / \p reorg_h);
 *   If \p forward is true, the height dimension of the output tensor is as follows:
 *   output_height = (input_height * \p reorg_h);
 * - The formula of calculating the width dimension of output is as follows, where
 *   output_width is width dimension of output tensor, input_width is width
 *   dimension of input tensor.
 *   If \p forward is false, the width dimension of the output tensor is as follows:
 *   output_width = (input_width / \p reorg_w);
 *   If \p forward is true, the width dimension of the output tensor is as follows:
 *   output_width = (input_width * \p reorg_w);
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetReorgDescriptor(cnnlReorgDescriptor_t desc,
                                                 int reorg_h,
                                                 int reorg_w,
                                                 bool forward);

// Group:Reorg
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace
 * to optimize the reorg operation.
 *
 * The size of the extra workspace is based on the given information of the reorg operation,
 * including the input tensor descriptor \p x_desc, output tensor descriptor \p y_desc and
 * the reorg descriptor \p desc.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the reorg operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] desc
 *   Input.The descriptor of the reorg operation. For detailed information,
 *   see ::cnnlReorgDescriptor_t.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra workspace in bytes
 *   that is used in the reorg operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called after
 *   the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions
 *   to create and set the tensor descriptors \p x_desc, \p y_desc before
 *   calling this function.
 * - The allocated extra workspace should be passed to the ::cnnlReorg function
 *   to perform the reorg operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetReorgWorkspaceSize(cnnlHandle_t handle,
                                                    const cnnlTensorDescriptor_t x_desc,
                                                    const cnnlTensorDescriptor_t y_desc,
                                                    const cnnlReorgDescriptor_t desc,
                                                    size_t *size);
// Group:Reorg
/*!
 * @brief Rearranges input tensor \p x_ptr according to reorg descriptor \p desc,
 *        and returns the results in the output tensor \p y_ptr.
 *
 * This function needs extra MLU memory as the workspace to work.
 * You can get the workspace size with the
 * ::cnnlGetReorgWorkspaceSize function.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlReorg_v2 instead, which needs user to allocate extra space.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the reorg operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] desc
 *   Input. The descriptor of the descriptor operation. For detailed information,
 *   see ::cnnlReorgDescriptor_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   reorg operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the reorg operation. You can get the size of the workspace with
 *   the ::cnnlGetReorgWorkspaceSize function.
 * @param[out] y_desc
 *   Output. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Reorg Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 *   This function supports the combinations of input tensor and output tensor must be
 *   half-half or float-float.
 *
 * @par Data Layout
 * - Data layouts of input tensor \p input and output tensor \p output must be the same.
 * - The supported data layout of the input tensor is as follows:
 *   - input tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NTC, \p CNNL_LAYOUT_NC.
 *   - output tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NTC, \p CNNL_LAYOUT_NC.
 *
 * @par Scale Limitation
 * - If tensor layout is CNNL_LAYOUT_NTC, then reorg_h, reorg_w and
 *   dimension size of timing steps(T in NTC) must be 1.
 * - If tensor layout is CNNL_LAYOUT_NC, then reorg_h and reorg_w must be 1.
 *
 * @par API Dependency
 * - Before calling this function to implement reorg, you need to prepare
 *   all the parameters passed to this function. See each parameter description
 *   for details.
 *
 * @note
 * - When the input data or parameter contains NaN or infinity:
 *   - On MLU200 series, MLU300 series and CE3226:
 *     - If \p x is NaN, then \p output is random value.
 *     - If \p x is infinity, then \p output is random value.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the reorg operation is as follows:
     @verbatim
      input array by 1 * 26 * 26 * 512

      param:
        input reorg_h = 2, reorg_w = 2, forward = false.

      output array by 1 * 13 * 13 *2048
     @endverbatim
 *
 */
CNNL_DEPRECATED_FOR(cnnlReorg_v2)
cnnlStatus_t CNNL_WIN_API cnnlReorg(cnnlHandle_t handle,
                                    const cnnlReorgDescriptor_t desc,
                                    const cnnlTensorDescriptor_t x_desc,
                                    const void *x,
                                    const cnnlTensorDescriptor_t y_desc,
                                    void *y,
                                    void *workspace,
                                    size_t workspace_size);
// Group:Reorg
/*!
 * @brief Returns in \p extra_size the size of the MLU memory and host memory that is used as an extra
 * input data to optimize the reorg operation. You need to allocate memory both on host and MLU based on
 * the size returned in \p extra_size.
 *
 * The size of extra input data is based on the given information of the reorg operation,
 * including the input tensor descriptor \p x_desc, output tensor descriptor \p y_desc and
 * the reorg descriptor \p desc.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the reorg operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] desc
 *   Input.The descriptor of the reorg operation. For detailed information,
 *   see ::cnnlReorgDescriptor_t.
 * @param[in]  x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] extra_size
 *   Output. A host pointer to the returned size of the extra input data in bytes
 *   that is used in the reorg operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlCreateTensorDescriptor and
 *   ::cnnlSetTensorDescriptor functions to create and set the tensor descriptors
 *   \p x_desc, \p y_desc.
 * - After calling this function, you need to call ::cnnlInitReorgExtraInput to initialize the memory on host.
 * - The allocated extra input should be passed to the ::cnnlReorg_v2 function
 *   to perform the reorg operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetReorgExtraInputSize(cnnlHandle_t handle,
                                                     const cnnlReorgDescriptor_t desc,
                                                     const cnnlTensorDescriptor_t x_desc,
                                                     const cnnlTensorDescriptor_t y_desc,
                                                     size_t *extra_size);
// Group:Reorg
/*!
 * @brief Initializes the extra input data space \p extra_host_input on host.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the reorg operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] desc
 *   Input.The descriptor of the reorg operation. For detailed information,
 *   see ::cnnlReorgDescriptor_t.
 * @param[in]  x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] extra_host_input
 *   Output. Pointer to the host memory that is used as an extra input space
 *   for the reorg operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to get the size of the extra input data with ::cnnlGetReorgExtraInputSize.
 *   The memory of the extra input data should be allocated before calling this function.
 * - This function must be called after the ::cnnlCreateTensorDescriptor and
 *   ::cnnlSetTensorDescriptor functions to create and set the tensor descriptors
 *   \p x_desc, \p y_desc.
 * - The allocated extra input should be passed to the ::cnnlReorg_v2 function
 *   to perform the reorg operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlInitReorgExtraInput(cnnlHandle_t handle,
                                                  const cnnlReorgDescriptor_t desc,
                                                  const cnnlTensorDescriptor_t x_desc,
                                                  const cnnlTensorDescriptor_t y_desc,
                                                  void *extra_host_input);
// Group:Reorg
/*!
 * @brief Rearranges input tensor \p x_ptr according to reorg descriptor \p desc,
 *        and returns the results in the output tensor \p y_ptr. Compared with
 *        ::cnnlReorg, ::cnnlReorg_v2 provides better performance with extra input space.
 *
 * This function needs extra MLU memory as the workspace to work.
 * You can get the workspace size with the
 * ::cnnlGetReorgWorkspaceSize function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the reorg operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] desc
 *   Input. The descriptor of the reorg operation. For detailed information,
 *   see ::cnnlReorgDescriptor_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] extra_device_input
 *   Input. Pointer to the MLU memory that stores the extra input data.
 *   You need to copy the extra input data to MLU from the host that is
 *   initialized with ::cnnlInitReorgExtraInput. For more information
 *   about extra input data, see Cambricon CNNL User Guide.
 * @param[in] extra_input_size
 *   Input. The size of the extra input data in bytes that needs to be used in
 *   the reorg operation. You can get the size of the extra input data with
 *   the ::cnnlGetReorgExtraInputSize function.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   reorg operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the reorg operation. You can get the size of the workspace with
 *   the ::cnnlGetReorgWorkspaceSize function.
 * @param[out] y_desc
 *   Output. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Reorg Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 *   This function supports the combinations of input tensor and output tensor must be
 *   half-half or float-float.
 *
 * @par Data Layout
 * - Data layouts of input tensor \p input and output tensor \p output must be the same.
 * - The supported data layout of the input tensor is as follows:
 *   - input tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NTC, \p CNNL_LAYOUT_NC.
 *   - output tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NTC, \p CNNL_LAYOUT_NC.
 *
 * @par Scale Limitation
 * - If tensor layout is CNNL_LAYOUT_NTC, then reorg_h, reorg_w and
 *   dimension size of timing steps(T in NTC) must be 1.
 * - If tensor layout is CNNL_LAYOUT_NC, then reorg_h and reorg_w must be 1.
 *
 * @par API Dependency
 * - Before calling this function to implement reorg, you need to prepare
 *   all the parameters passed to this function. See each parameter description
 *   for details.
 *
 * @note
 * - When the input data or parameter contains NaN or infinity:
 *   - On MLU200 series, MLU300 series and CE3226:
 *     - If \p x is NaN, then \p output is random value.
 *     - If \p x is infinity, then \p output is random value.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the reorg operation is as follows:
     @verbatim
      input array by 1 * 26 * 26 * 512

      param:
        input reorg_h = 2, reorg_w = 2, forward = false.

      output array by 1 * 13 * 13 *2048
     @endverbatim
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlReorg_v2(cnnlHandle_t handle,
                                       const cnnlReorgDescriptor_t desc,
                                       const cnnlTensorDescriptor_t x_desc,
                                       const void * x,
                                       const void * extra_device_input,
                                       size_t extra_input_size,
                                       const cnnlTensorDescriptor_t y_desc,
                                       void * y,
                                       void * workspace,
                                       size_t workspace_size);

// Group:Shufflechannel
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace
 * for the ::cnnlShuffleChannel function to optimize the shuffle channel operation.
 *
 * The size of the extra workspace is based on the given information of the shuffle channel
 * operation, including input tensor descriptor \p input_desc. For more information about
 * the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in
 *   the ::cnnlShuffleChannel function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetShufflechannelWorkspaceSize(const cnnlTensorDescriptor_t input_desc, size_t *size);

// Group:Shufflechannel
/*!
 * @brief Shuffles channels of input tensor \p input according to the given
 *        parameter \p group, and returns the output tensor \p output.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the shuffle channel operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. The pointer to the MLU memory that stores the input tensor.
 * @param[in] group
 *   Input. The number of groups which the channels of input tensor are divided into.
 * @param[in] workspace
 *   Input. The pointer to the MLU memory that is used as an extra workspace for
 *   shuffle channel operation. You can get the workspace with the
 *   ::cnnlGetShufflechannelWorkspaceSize function.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   shuffle channel operation. You can get the size of workspace with the
 *   ::cnnlGetShufflechannelWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. The pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - The (I/O)function supports the following data widths for \p input and \p output tensors.
 *   - input tensor: 1-byte, 2-byte, 4-byte(int31 not supported)
 *   - output tensor: 1-byte, 2-byte, 4-byte(int31 not supported)
 *
 *   The byte width of a data type can be got with the ::cnnlGetSizeOfDataType function.
 *
 *   Note that the data type of input tensor \p input and output tensor \p output must be the same.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor must have the same shape.
 * - The size of C (the shuffle dimension) should be divided by \p group.
 * - The parameter \p group must meet the requirements:
 *   - group > 0 and group <= c.
 *   - group must be an integer.
 * - C * sizeof(datatype of \p input) should not be larger than 2500 KB on MLU300 series.
 * - C * sizeof(datatype of \p input) should not be larger than 1500 KB on MLU500 series.
 * - C * sizeof(datatype of \p input) should not be larger than  600 KB on CE3226.
 * - C * sizeof(datatype of \p input) should not be larger than 1100 KB on 1V.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the layout of the input
 *   tensor and output tensor to NHWC.
 *
 * @par API Dependency
 * - Before calling this function, you need to call the
 *   ::cnnlGetShufflechannelWorkspaceSize function to get the \p workspace_size.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the shuffle channel operation is as follows:
     @verbatim
      input tensor with the layout of NHWC, shape is 1 * 1 * 2 * 6
      --> input: [[[1,2,3,4,5,6],
                   [7,8,9,10,11,12]]]

      input parameter group
      --> group: 3

      output tensor with the layout of NHWC, shape is 1 * 1 * 2 * 6
      --> output: [[[1,3,5,2,4,6],
                    [7,9,11,8,10,12]]]
      @endverbatim
 *
 * @par Reference
 * - https://arxiv.org/abs/1707.01083
 * - https://github.com/MG2033/ShuffleNet/blob/master/layers.py
 * - https://www.paddlepaddle.org.cn/documentation/docs/en/api/layers/shuffle_channel.html
 */
cnnlStatus_t CNNL_WIN_API cnnlShuffleChannel(cnnlHandle_t handle,
                                             const cnnlTensorDescriptor_t input_desc,
                                             const void *input,
                                             const int group,
                                             void *workspace,
                                             size_t workspace_size,
                                             const cnnlTensorDescriptor_t output_desc,
                                             void *output);

// Group:Maximum
/*!
 * @brief Returns in \p size the size of the MLU memory in bytes that is used as
 *        an extra workspace to optimize the maximum operation.
 *
 * The size of the extra workspace is based on the given information of the input
 * and output tensor descriptors, \p a_desc, \p b_desc, and \p c_desc. For more
 * information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the maximum operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is
 *   used in the maximum operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlMaximum function
 *   to perform the maximum operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetMaximumWorkspaceSize(cnnlHandle_t handle,
                                                      const cnnlTensorDescriptor_t c_desc,
                                                      size_t *size);

// Group:Maximum
/*!
 * @brief Computes the element-wise maximum value between two input tensors \p a
 *        and \p b, and returns the results in the output tensor \p c.
 *
 * This function is used to perform an operation of maximum, supporting 1 to 8
 * dimensions of maximum operation with tensor broadcast. It is used in resnet50
 * on TensorFlow framework.
 *
 * This function supports in-place operation, which means one of the input
 * tensors \p a or \p b can share the same memory address with the output tensor \p c.
 * This function also supports tensor broadcasting as long as \p a, \p b, and \p c
 * satisfy the broadcast conditions. For more details about tensor broadcasting, see
 * Limitations section.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the maximum operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. Pointer to the MLU memory that stores the first input tensor.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. Pointer to the MLU memory that stores the second input tensor.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] c
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   maximum operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the maximum operation. You can get the size of the workspace with
 *   the ::cnnlGetMaximumWorkspaceSize function.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "Convolution Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input and output tensors.
 *   - On MLU300 series and CE3226:
 *     - input tensor: int32, half, float
 *     - output tensor: int32, half, float
 *   - On MLU500 series and 1V:
 *     - input tensor: int32, half, float, bfloat16
 *     - output tensor: int32, half, float, bfloat16
 *   Note that all data types of two input tensors and the output tensor must be
 *   the same.
 *
 * @par Limitations
 * - For each dimension of the two input tensors, the length of the dimension
 *   should be the same or one of them should equal to 1.
 * - Each dimension of the output tensor should equal to the larger one of the
 *   corresponding dimension of two input tensors.
 * - On 1V, the data value of int32 tensors should be within [-16777216, 16777215].
 *
 * @par API Dependency
 * - Before calling this function to perform maximum operation, you need to get the
 *   size of workspace by ::cnnlGetMaximumWorkspaceSize.
 *
 * @par Performance Optimization
 * - None.
 *
 * @note
 * - When the input data or parameter contains NaN or infinity:
 *   - If \p a or \p b is NaN, then \p c is NaN.
 *   - Between \p a and \p b, if one is positive infinity and the other is finite value, then
 *     \p c is positive infinity.
 *   - Between \p a and \p b, if one is negative infinity and the other is finite value, then
 *     \p c is negative infinity.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlMaximum(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t a_desc,
                                      const void *a,
                                      const cnnlTensorDescriptor_t b_desc,
                                      const void *b,
                                      const cnnlTensorDescriptor_t c_desc,
                                      void *c,
                                      void *workspace,
                                      size_t workspace_size);
// Group:Minimum
/*!
 * @brief Returns in \p size the size of the MLU memory in bytes that is used as
 *        an extra workspace to optimize the minimum operation.
 *
 * The size of the extra workspace is based on the given information of the input
 * and output tensor descriptors, \p a_desc, \p b_desc, and \p c_desc. For more
 * information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the minimum operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is
 *   used in the minimum operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlMinimum function
 *   to perform the minimum operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetMinimumWorkspaceSize(cnnlHandle_t handle,
                                                      const cnnlTensorDescriptor_t c_desc,
                                                      size_t *size);
// Group:Minimum
/*!
 * @brief Computes the element-wise minimum value between two input tensors \p a
 *        and \p b, and returns the results in the output tensor \p c.
 *
 * This function is used to perform an operation of minimum, supporting 1 to 8
 * dimensions of minimum operation with tensor broadcast. It is used in resnet50
 * on TensorFlow framework.
 *
 * This function supports in-place operation, which means one of the input
 * tensors \p a or \p b can share the same memory address with the output tensor \p c.
 * This function also supports tensor broadcasting as long as \p a, \p b, and \p c
 * satisfy the broadcast conditions. For more details about tensor broadcasting, see
 * Limitations section.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the minimum operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. Pointer to the MLU memory that stores the first input tensor.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. Pointer to the MLU memory that stores the second input tensor.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] c
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   minimum operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the minimum operation. You can get the size of the workspace with
 *   the ::cnnlGetMinimumWorkspaceSize function.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "Convolution Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input and output tensors.
 *   - On MLU300 series and CE3226:
 *     - input tensor: int32, half, float
 *     - output tensor: int32, half, float
 *   - On MLU500 series and 1V:
 *     - input tensor: int32, half, float, bfloat16
 *     - output tensor: int32, half, float, bfloat16
 *   Note that all data types of two input tensors and the output tensor must be
 *   the same.
 *
 * @par Limitations
 * - For each dimension of the two input tensors, the length of the dimension
 *   should be the same or one of them should equal to 1.
 * - Each dimension of the output tensor should equal to the larger one of the
 *   corresponding dimension of two input tensors.
 * - On 1V, the data value of int32 tensors should be within [-16777216, 16777215].
 *
 * @par API Dependency
 * - Before calling this function to perform minimum operation, you need to get the
 *   size of workspace by the ::cnnlGetMinimumWorkspaceSize function.
 *
 * @par Performance Optimization
 * - None.
 *
 * @note
 * - When the input data or parameter contains NaN or infinity:
 *   - If \p a or \p b is NaN, then \p c is NaN.
 *   - Between \p a and \p b, if one is positive infinity and the other is finite value, then
 *     \p c is positive infinity.
 *   - Between \p a and \p b, if one is negative infinity and the other is finite value, then
 *     \p c is negative infinity.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlMinimum(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t a_desc,
                                      const void *a,
                                      const cnnlTensorDescriptor_t b_desc,
                                      const void *b,
                                      const cnnlTensorDescriptor_t c_desc,
                                      void *c,
                                      void *workspace,
                                      size_t workspace_size);

/******************************************************************************
 * Cambricon CNNL OP: ConvolutionBackwardData
 ******************************************************************************/

// Group:ConvolutionBackwardData
/*!
 * @brief Returns the most suited algorithm that can be used in the convolution
 *        backward data operation.
 *
 * The returned algorithm is chosen from all the supported convolution
 * backward data algorithms defined in ::cnnlConvolutionBwdDataAlgo_t and is
 * based on the given filter descriptor \p weight_desc, input descriptor
 * \p diff_y_desc, convolution descriptor \p conv_desc, output descriptor
 * \p diff_x_desc, and the computing performance preferences \p preference.
 *
 * The computing performance options \p preference is defined in the
 * ::cnnlConvolutionBwdDataPreference_t enum, and only supports the high speed mode.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGetConvolutionBackwardDataAlgo instead.
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the convolution backward data operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] filter_desc
 *   Input. The descriptor of the filter tensor used as a filter in the
 *   convolution backward data operation. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] diff_y_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] conv_desc
 *   Input. The descriptor of the convolution backward data operation. For
 *   detailed information, see ::cnnlConvolutionDescriptor_t.
 * @param[in] diff_x_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] preference
 *   Input. The options for implementing the convolution backward data operation
 *   to get better performance. This parameter only supports
 *   \p CNNL_CONVOLUTION_BWD_DATA_FASTEST.
 * @param[out] algo
 *   Output. The returned algorithm that is best suited for performing the
 *   convolution backward data operation. The algorithms are defined in the
 *   ::cnnlConvolutionBwdDataAlgo_t enum.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions
 *   should be called in sequence to create and set the tensor descriptors
 *   \p filter_desc, \p diff_y_desc, and \p diff_x_desc before this function.
 * - The ::cnnlCreateConvolutionDescriptor and ::cnnlSetConvolutionDescriptor
 *   functions should be called in sequence to create and set the convolution
 *   descriptor \p conv_desc before this function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlGetConvolutionBackwardDataAlgo)
cnnlStatus_t CNNL_WIN_API
cnnlGetConvolutionBackwardDataAlgorithm(cnnlHandle_t handle,
                                        const cnnlTensorDescriptor_t filter_desc,
                                        const cnnlTensorDescriptor_t diff_y_desc,
                                        const cnnlConvolutionDescriptor_t conv_desc,
                                        const cnnlTensorDescriptor_t diff_x_desc,
                                        const cnnlConvolutionBwdDataPreference_t preference,
                                        cnnlConvolutionBwdDataAlgo_t *algo);

// Group:ConvolutionBackwardData
/*!
 * @brief Returns the most suited algorithm that can be used in the convolution
 *        backward data operation.
 *
 * The returned algorithm is chosen from all the supported convolution
 * backward data algorithms defined in ::cnnlConvolutionBwdDataAlgo_t and is
 * based on the given filter descriptor \p weight_desc, input descriptor
 * \p diff_y_desc, convolution descriptor \p conv_desc, output descriptor
 * \p diff_x_desc, and the computing performance preferences \p preference.
 *
 * The computing performance options \p preference is defined in the
 * ::cnnlConvolutionBwdDataPreference_t enum, and only supports the high speed mode.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the convolution backward data operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] filter_desc
 *   Input. The descriptor of the filter tensor used as a filter in the
 *   convolution backward data operation. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] diff_y_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] conv_desc
 *   Input. The descriptor of the convolution backward data operation. For
 *   detailed information, see ::cnnlConvolutionDescriptor_t.
 * @param[in] diff_x_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] preference
 *   Input. The options for implementing the convolution backward data operation
 *   to get better performance. This parameter only supports
 *   \p CNNL_CONVOLUTION_BWD_DATA_FASTEST.
 * @param[out] algo
 *   Output. The returned algorithm that is best suited for performing the
 *   convolution backward data operation. The algorithms are defined in the
 *   ::cnnlConvolutionBwdDataAlgo_t enum.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions
 *   should be called in sequence to create and set the tensor descriptors
 *   \p filter_desc, \p diff_y_desc, and \p diff_x_desc before this function.
 * - The ::cnnlCreateConvolutionDescriptor and ::cnnlSetConvolutionDescriptor
 *   functions should be called in sequence to create and set the convolution
 *   descriptor \p conv_desc before this function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetConvolutionBackwardDataAlgo(cnnlHandle_t handle,
                                   const cnnlTensorDescriptor_t filter_desc,
                                   const cnnlTensorDescriptor_t diff_y_desc,
                                   const cnnlConvolutionDescriptor_t conv_desc,
                                   const cnnlTensorDescriptor_t diff_x_desc,
                                   const cnnlConvolutionBwdDataPreference_t preference,
                                   cnnlConvolutionBwdDataAlgo_t *algo);

// Group:ConvolutionBackwardData
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra
 *        workspace to optimize the convolution backward data operation.
 *
 * The size of the extra workspace is based on the given information of
 * the convolution backward data operation, including the filter descriptor
 * \p filter_desc, input descriptor \p diff_y_desc, convolution descriptor
 * \p conv_desc, output descriptor \p diff_x_desc, and the convolution backward
 * data algorithm \p algo. For more information about the workspace,
 * see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the convolution backward data operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] filter_desc
 *   Input. The descriptor of the filter tensor used as a filter in the
 *   convolution backward data operation. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] diff_y_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] conv_desc
 *   Input. The descriptor of the convolution backward data operation. For
 *   detailed information, see ::cnnlConvolutionDescriptor_t.
 * @param[in] diff_x_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] algo
 *   Input. The algorithm used to perform the convolution backward data operation.
 *   The algorithms are defined in the ::cnnlConvolutionBwdDataAlgo_t enum. You can
 *   get the best suited algorithm with the ::cnnlGetConvolutionBackwardDataAlgo
 *   function.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is
 *   used in the convolution backward data operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The ::cnnlGetConvolutionBackwardDataAlgo function must be called to get
 *   the best suited algorithm \p algo before this function.
 * - The ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions
 *   should be called in sequence to create and set the tensor descriptors
 *   \p filter_desc, \p diff_y_desc, and \p diff_x_desc before this function.
 * - The ::cnnlCreateConvolutionDescriptor and ::cnnlSetConvolutionDescriptor
 *   functions should be called in sequence to create and set the convolution
 *   descriptor \p conv_desc before this function.
 * - The allocated extra workspace should be passed to the
 *   ::cnnlConvolutionBackwardData function to perform the convolution backward
 *   data operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetConvolutionBackwardDataWorkspaceSize(cnnlHandle_t handle,
                                            const cnnlTensorDescriptor_t filter_desc,
                                            const cnnlTensorDescriptor_t diff_y_desc,
                                            const cnnlConvolutionDescriptor_t conv_desc,
                                            const cnnlTensorDescriptor_t diff_x_desc,
                                            const cnnlConvolutionBwdDataAlgo_t algo,
                                            size_t *workspace_size);

// Group:ConvolutionBackwardData
/*!
 * @brief Performs the backpropagation of ::cnnlConvolutionForward to compute the gradient of
 * input \p diff_x based on the gradient of response \p diff_y and the filter tensor \p filter.
 * For more information about quantization, see "Cambricon CNNL User Guide".
 *
 * The algorithm of ::cnnlConvolutionBackwardData and ::cnnlDeconvolution are the same,
 * except the supporting of bias. If bias is needed, call ::cnnlDeconvolution.
 *
 * This function needs extra MLU memory as the workspace to improve the performance.
 * You can get the workspace size with the
 * ::cnnlGetConvolutionBackwardDataWorkspaceSize function. The convolution backward
 * data operation is computed based on the algorithm set in \p algo. You can call
 * the ::cnnlGetConvolutionBackwardDataAlgo to get the most suited algorithm.
 *
 * Depthwise convolution backward data operation is performed when the \p group_count
 * parameter of the convolution descriptor \p conv_desc is set to the number of
 * channels of the output tensor \p diff_x.
 *
 * The scaling factors \p alpha and \p beta are not supported currently.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the convolution backward data operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] alpha, beta
 *   Input. Reserved scaling parameters for future use. Set the value of
 *   these parameters to NULL.
 * @param[in] filter_desc
 *   Input. The descriptor of the filter tensor used as a filter in the
 *   convolution backward data operation. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the filter tensor.
 * @param[in] diff_y_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] diff_y
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] conv_desc
 *   Input. The descriptor of the convolution operation. For detailed information,
 *   see ::cnnlConvolutionDescriptor_t.
 * @param[in] algo
 *   Input. The algorithm used to compute the convolution backward data operation.
 *   The algorithms are defined in the ::cnnlConvolutionBwdDataAlgo_t enum.
 *   You can get the best suited algorithm with the
 *   ::cnnlGetConvolutionBackwardDataAlgo function.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   convolution backward data operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the convolution backward data operation. You can get the size of the workspace
 *   with the ::cnnlGetConvolutionBackwardDataWorkspaceSize function.
 * @param[in] diff_x_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Convolution Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The onchip data type is set as \p onchip_dtype parameter in tensor descriptor.
 *   For more detailed information, see ::cnnlSetTensorDescriptorOnchipDataType.
 *
 * - The combinations of \p dtype should follow the rule: tensors \p diff_y and \p filter
 *   should be both floating-point (half, float or bfloat16).
 *
 * - The \p dtype combination with the order of \p diff_y - \p filter - \p diff_x should be
 *   half-half-half, float-float-float or bfloat16-bfloat16-bfloat16.
 *
 * \rst
 * .. table:: cnnlConvolutionBackwardData dtype, onchip_dtype and compute_type combination (CNNL_DTYPE_XXX)
 *    :widths: grid
 *
 *    +--------------------------------+--------------------+--------------------+----------------------+-------------+
 *    |diff_y, filter and diff_x dtype |diff_y onchip_dtype |filter onchip_dtype |diff_x onchip_dtype   |compute_type |
 *    +================================+====================+====================+======================+=============+
 *    |HALF                            |INVALID / HALF      |INVALID / HALF      |INVALID / HALF        |HALF / FLOAT |
 *    |                                |                    |                    +----------------------+-------------+
 *    |                                |                    |                    |FLOAT                 |FLOAT        |
 *    +--------------------------------+--------------------+--------------------+----------------------+-------------+
 *    |FLOAT                           |INVALID / FLOAT     |INVALID / FLOAT     |INVALID / FLOAT       |FLOAT        |
 *    +--------------------------------+--------------------+--------------------+----------------------+-------------+
 *    |BFLOAT16                        |INVALID / BFLOAT16  |INVALID / BFLOAT16  |INVALID / FLOAT       |FLOAT        |
 *    +--------------------------------+--------------------+--------------------+----------------------+-------------+
 * \endrst
 *
 * @par Data Layout
 * - If \p dimNb is set to 4, the supported data layouts of the input tensor, filter tensor, and output tensor
 *   are as follows:
 *   - input tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, and \p CNNL_LAYOUT_NCHW.
 *   - filter tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, and \p CNNL_LAYOUT_NCHW.
 *   - output tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, and \p CNNL_LAYOUT_NCHW.
 *
 * - If \p dimNb is set to 5, the supported data layouts of the input tensor,
 *   filter tensor and output tensor are as follows:
 *   - input tensor: \p CNNL_LAYOUT_NDHWC.
 *   - filter tensor: \p CNNL_LAYOUT_NDHWC.
 *   - output tensor: \p CNNL_LAYOUT_NDHWC.
 *
 * @par Scale Limitation
 * - The input tensor, filter tensor, output tensor and the convolution descriptor
 *   should meet the following requirements:
 *   - \p pad >= 0
 *   - \p stride >= 1
 *   - \p dilation >= 1
 *   - \p group_count >= 1. If the convolution operation is not depthwise, additional conditions
 *     should be satisfied for parameter \p group_count as below:
 *     - The number of channels of both \p diff_y and \p filter should be the multiple of \p group_count.
 *     - The number of output channels of \p filter multiplying \p group_count should be
 *       the number of channels of \p diff_x.
 *   - input tensor: \p batch > 0, \p height > 0, \p width > 0, \p channel > 0
 *   - filter tensor: \p height > 0, \p width > 0
 *   - output tensor: \p height > 0, \p width > 0, \p channel > 0
 *
 * - Specially, when the operation is depthwise, the parameters should meet the following
 *   additional requirements:
 *   - \p dimNb == 4
 *   - When \p dilation is greater than 1, the \p stride should be equal to 1, and the shape of
 *   input tensor \p diff_y should be equal to the shape of the output tensor \p diff_x. Additionally,
 *   the data type of \p diff_y, \p filter and \p diff_x should be float-float-float or half-half-half.
 *   - The number of channels of input tensor \p diff_y must be divisible by the number of channels
 *   of output tensor \p diff_x.
 *
 *   multiplier = \p diff_y_c / \p diff_x_c,
 *   sub_kh = (\p filter_h + \p stride_h - 1) / \p stride_h,
 *   sub_kw = (\p filter_w + \p stride_w - 1) / \p stride_w
 *
 *   Where \p diff_y_c and \p diff_x_c are the number of channels of \p diff_y
 *   and \p diff_x respectively, \p filter_h and \p filter_w are the height and width of
 *   filter tensor \p filter, \p stride_h and \p stride_w are stride parameters in height and
 *   width set in the convolution descriptor \p conv_desc.
 *   - If multiplier < 2 * sub_kh * sub_kw:
 *     sub_kh * sub_kw <= 2016
 *   - If multiplier >= 2 * sub_kh * sub_kw:
 *     sub_kh * sub_kw <= 126 / half_flag
 *
 *   Where, \p half_flag is 2 when the data type of output tensor \p diff_x are half; Otherwise,
 *   \p half_flag is 1.
 *
 * @par API Dependency
 * - Before calling this function to implement convolution backward data, you
 *   need to prepare all the parameters passed to this function. See each parameter
 *   description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the layout of the input
 *   tensor and output tensor to NHWC, and filter tensor to HWCN.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - Gradient-Based Learning Applied to Document Recognition, Yann Lecun, 1998.
 * - Multi-scale Context Aggregation by Dilated Convolution, Fisher Yu, 2016.
 */
cnnlStatus_t CNNL_WIN_API cnnlConvolutionBackwardData(cnnlHandle_t handle,
                                                      const void *alpha,
                                                      const cnnlTensorDescriptor_t filter_desc,
                                                      const void *filter,
                                                      const cnnlTensorDescriptor_t diff_y_desc,
                                                      const void *diff_y,
                                                      const cnnlConvolutionDescriptor_t conv_desc,
                                                      const cnnlConvolutionBwdDataAlgo_t algo,
                                                      void *workspace,
                                                      size_t workspace_size,
                                                      const void *beta,
                                                      const cnnlTensorDescriptor_t diff_x_desc,
                                                      void *diff_x);

// Group:ConvolutionBackwardData
/*!
 * @brief Converts the filter tensor \p filter and the gradient of response \p diff_y_desc from
 * floating-point to fixed-point numbers through quantization parameters,
 * and then computes the gradient of input data \p diff_x by the convolution backward data operation.
 * For more information about quantization, see "Cambricon CNNL User Guide".
 *
 * This function needs extra MLU memory as the workspace to improve the performance.
 * You can get the workspace size with the
 * ::cnnlGetConvolutionBackwardDataWorkspaceSize function. The convolution backward
 * data operation is computed based on the algorithm set in \p algo. You can call
 * the ::cnnlGetConvolutionBackwardDataAlgo to get the most suited algorithm.
 *
 * Depthwise convolution backward data operation is performed when the \p group_count
 * parameter of the convolution descriptor \p conv_desc is set to the number of
 * channels of the output tensor \p diff_x.
 *
 * The scaling factors \p alpha and \p beta are not supported currently.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the convolution backward data operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] alpha, beta
 *   Input. Reserved scaling parameters for future use. Set the value of
 *   these parameters to NULL.
 * @param[in] filter_desc
 *   Input. The descriptor of the filter tensor used as a filter in the
 *   convolution backward data operation. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the filter tensor.
 * @param[in] filter_position
 *   Input. Pointer to the MLU memory that stores the position factor for
 *   quantization of the filter tensor.
 * @param[in] filter_scale
 *   Input. Pointer to the MLU memory that stores the scale factor for
 *   quantization of the filter tensor.
 * @param[in] filter_offset
 *   Input. Pointer to the MLU memory that stores the offset factor for
 *   quantization of the filter tensor.
 * @param[in] diff_y_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] diff_y
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] diff_y_position
 *   Input. Pointer to the MLU memory that stores the position factor for
 *   quantization of the input tensor.
 * @param[in] diff_y_scale
 *   Input. Pointer to the MLU memory that stores the scale factor for
 *   quantization of the input tensor.
 * @param[in] diff_y_offset
 *   Input. Pointer to the MLU memory that stores the offset factor for
 *   quantization of the input tensor.
 * @param[in] conv_desc
 *   Input. The descriptor of the convolution operation. For detailed information,
 *   see ::cnnlConvolutionDescriptor_t.
 * @param[in] algo
 *   Input. The algorithm used to compute the convolution backward data operation.
 *   The algorithms are defined in the ::cnnlConvolutionBwdDataAlgo_t enum.
 *   You can get the best suited algorithm with the
 *   ::cnnlGetConvolutionBackwardDataAlgo function.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   convolution backward data operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the convolution backward data operation. You can get the size of the workspace
 *   with the ::cnnlGetConvolutionBackwardDataWorkspaceSize function.
 * @param[in] diff_x_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Convolution Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The onchip data type is set as \p onchip_dtype parameter in tensor descriptor.
 *   For more detailed information, see ::cnnlSetTensorDescriptorOnchipDataType.
 *
 * - The combinations of \p dtype should follow the rule: tensors \p diff_y and \p filter
 *   should be both fix-point (int8 or int16) or floating-point (half or float).
 *
 * - If the offchip data types \p dtype of \p diff_y and \p filter are fix-point data
 *   types, this function supports combinations of the following data types for the input tensor
 *   \p diff_y, filter tensor \p filter, and output tensor \p diff_x.
 *   - \p diff_y offchip data type \p dtype:
 *     - int8, int16.
 *   - \p filter offchip data type \p dtype:
 *     - int8, int16.
 *   - \p diff_x offchip data type \p dtype:
 *     - half, float.
 *   - The \p onchip_dtype of \p diff_y should be the same as the \p dtype of \p diff_y
 *     or \p CNNL_DTYPE_INVALID.
 *   - The \p onchip_dtype of \p filter should be the same as the \p dtype of \p filter
 *     or \p CNNL_DTYPE_INVALID.
 *
 * - If the offchip data types \p dtype of \p diff_y and \p filter are floating-point data
 *   types, this function supports combinations of the following data types for the input tensor
 *   \p diff_y, filter tensor \p filter, and output tensor \p diff_x.
 *   - \p diff_y offchip data type \p dtype:
 *     - half, float
 *   - \p diff_y onchip data type \p onchip_dtype:
 *     - int8, int16.
 *   - \p filter offchip data type \p dtype:
 *     - half, float
 *   - \p filter onchip data type \p onchip_dtype:
 *     - int8, int16.
 *   - \p diff_x offchip data type \p dtype:
 *     - half, float.
 *   - The \p dtype combination with the order of \p diff_y - \p filter - \p diff_x should be
 *     half-half-half or float-float-float.
 *
 * - The \p onchip_dtype of \p diff_x should be the same as the \p dtype of \p diff_x
 *   or \p CNNL_DTYPE_INVALID.
 *
 * @par Data Layout
 * - If \p dimNb is set to 4, the supported data layouts of the input tensor,
 *   filter tensor, and output tensor are as follows:
 *   - input tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, and \p CNNL_LAYOUT_NCHW.
 *   - filter tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, and \p CNNL_LAYOUT_NCHW.
 *     If the convolution backward data operation is depthwise, the layout of the filter
 *     tensor must be \p CNNL_LAYOUT_HWCN.
 *   - output tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, and \p CNNL_LAYOUT_NCHW.
 *
 * - If \p dimNb is set to 5, the supported data layouts of the input tensor,
 *   filter tensor and output tensor are as follows:
 *   - input tensor: \p CNNL_LAYOUT_NDHWC.
 *   - filter tensor: \p CNNL_LAYOUT_NDHWC.
 *   - output tensor: \p CNNL_LAYOUT_NDHWC.
 *
 * @par Scale Limitation
 * - The input tensor, filter tensor, output tensor and the convolution descriptor
 *   should meet the following requirements:
 *   - \p pad >= 0
 *   - \p stride >= 1
 *   - \p dilation >= 1
 *   - \p group_count >= 1. If the convolution operation is not depthwise, additional conditions
 *     should be satisfied for parameter \p group_count as below:
 *     - The number of channels of both \p diff_y and \p filter should be the multiple of \p group_count.
 *     - The number of output channels of \p filter multiplying \p group_count should be
 *       the number of channels of \p diff_x.
 *   - input tensor: \p batch > 0, \p height > 0, \p width > 0, \p channel > 0
 *   - filter tensor: \p height > 0, \p width > 0
 *   - output tensor: \p height > 0, \p width > 0, \p channel > 0
 *
 * - When the operation is depthwise, the parameters should meet the following
 *   additional requirements:
 *   - \p dimNb == 4
 *   - When \p dilation is greater than 1, the \p stride should be equal to 1, and the shape of
 *   input tensor \p diff_y should be equal to the shape of the output tensor \p diff_x. Additionally,
 *   the data type of \p diff_y, \p filter and \p diff_x should be float-float-float or half-half-half.
 *   - The number of channels of input tensor \p diff_y must be divisible by the number of channels
 *   of output tensor \p diff_x.
 *
 *   multiplier = \p diff_y_c / \p diff_x_c,
 *   sub_kh = (\p filter_h + \p stride_h - 1) / \p stride_h,
 *   sub_kw = (\p filter_w + \p stride_w - 1) / \p stride_w
 *
 *   Where \p diff_y_c and \p diff_x_c are the number of channels of \p diff_y
 *   and \p diff_x respectively, \p filter_h and \p filter_w are the height and width of
 *   filter tensor \p filter, \p stride_h and \p stride_w are stride parameters in height and
 *   width set in the convolution descriptor \p conv_desc.
 *   - If multiplier < 2 * sub_kh * sub_kw:
 *     sub_kh * sub_kw <= 2016
 *   - If multiplier >= 2 * sub_kh * sub_kw:
 *     sub_kh * sub_kw <= 126 / half_flag
 *
 *   Where, \p half_flag is 2 when the data type of output tensor \p diff_x are half; Otherwise,
 *   \p half_flag is 1.
 *
 * - The \p compute_type should meet the following requirements:
 *   - If the \p onchip_dtype of \p diff_x is \p CNNL_DTYPE_HALF, the \p compute_type should be
 *     \p CNNL_DTYPE_HALF or \p CNNL_DTYPE_FLOAT.
 *   - If the \p onchip_dtype of \p diff_x is \p CNNL_DTYPE_INVALID and the \p dtype of \p diff_x
 *     is \p CNNL_DTYPE_HALF, the \p compute_type should be \p CNNL_DTYPE_HALF or
 *     \p CNNL_DTYPE_FLOAT.
 *   - Otherwise, the \p compute_type should be \p CNNL_DTYPE_FLOAT.
 *
 * @par API Dependency
 * - Before calling this function to implement convolution backward data, you
 *   need to prepare all the parameters passed to this function. See each parameter
 *   description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the layout of the input
 *   tensor and output tensor to NHWC, and filter tensor to HWCN.
 *
 * @note
 * - This function does not support offline asymmetric quantization currently.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - Gradient-Based Learning Applied to Document Recognition, Yann Lecun, 1998.
 * - Multi-scale Context Aggregation by Dilated Convolution, Fisher Yu, 2016.
 */
CNNL_DEPRECATED_FOR(cnnlConvolutionBackwardData)
cnnlStatus_t CNNL_WIN_API
cnnlQuantizeConvolutionBackwardData(cnnlHandle_t handle,
                                    const void *alpha,
                                    const cnnlTensorDescriptor_t filter_desc,
                                    const void *filter,
                                    const void *filter_position,
                                    const void *filter_scale,
                                    const void *filter_offset,
                                    const cnnlTensorDescriptor_t diff_y_desc,
                                    const void *diff_y,
                                    const void *diff_y_position,
                                    const void *diff_y_scale,
                                    const void *diff_y_offset,
                                    const cnnlConvolutionDescriptor_t conv_desc,
                                    const cnnlConvolutionBwdDataAlgo_t algo,
                                    void *workspace,
                                    size_t workspace_size,
                                    const void *beta,
                                    const cnnlTensorDescriptor_t diff_x_desc,
                                    void *diff_x);

// Group:Deconvolution
/*!
 * @brief Creates a descriptor pointed by \p desc for a deconvolution operation, and
 * allocates memory for holding the information about the deconvolution operation. The
 * information is defined in ::cnnlDeconvolutionDescriptor_t. For more information about
 * descriptor, see "Cambricon CNNL User Guide".
 *
 * @param[out] desc
 *   Output. A host pointer to the deconvolution descriptor that holds information about the
 *   deconvolution operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetDeconvolutionDescriptor function to
 *   initialize and set the information to the deconvolution descriptor.
 * - You need to call the ::cnnlDestroyDeconvolutionDescriptor function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateDeconvolutionDescriptor(cnnlDeconvolutionDescriptor_t *desc);

// Group:Deconvolution
/*!
 * @brief Initializes the deconvolution descriptor \p desc that was previously created
 * with the ::cnnlCreateDeconvolutionDescriptor function, and sets the information
 * about the deconvolution operation to the deconvolution descriptor \p desc.
 * The information includes the number of the deconvolution dimensions \p dimNb,
 * the padding size for each dimension \p pad, the stride of the sliding window for
 * each dimension \p stride, the dilation factor for each dimension \p dilation,
 * the number of groups to be split into by channel \p group_count,
 * and the data type \p compute_type that is used in the accumulation.
 *
 * @param[in,out] desc
 *   Input/output. The descriptor of the deconvolution operation. For detailed information,
 *   see ::cnnlDeconvolutionDescriptor_t.
 * @param[in] dimNb
 *   Input. The number of dimensions in the input tensor of the deconvolution operation.
 *   Currently, the value of this parameter can only be set to 4 or 5.
 *   The value of this parameter should be the same as the one you set in the input tensor descriptor.
 * @param[in] pad
 *   Input. An array that stores the zero-padding size for each dimension of the input tensor
 *   used in the deconvolution operation.
 *   For each dimension, the padding size controls the number of zeros to be concatenated at the
 *   start and end of that dimension. The number of zeros is the result of
 *   'dilation * (kernel_size - 1) - pad'.
 *   If \p dimNb is set to 4, the padding is on top, bottom, left, and right.
 *   If \p dimNb is set to 5, the padding is on front, back, top, bottom, left, and right.
 *   The value of this parameter should be greater than or equal to 0.
 * @param[in] stride
 *   Input. An array that stores the filter stride for each dimension of the input tensor
 *   used in the deconvolution operation. For each dimension, the filter stride represents
 *   the spacing between the input points in the convolution operator. For details, see the
 *   first link in the Reference section of ::cnnlDeconvolution.
 *   If \p dimNb is set to 4, the stride is in height and width.
 *   If \p dimNb is set to 5, the stride is in depth, height and width.
 *   The value of this parameter should be greater than or equal to 1.
 * @param[in] dilation
 *   Input. An array that stores the dilation factor for each dimension of the filter tensor
 *   used in the deconvolution operation. For each dimension, the dilation factor represents
 *   the spacing between the kernel points. If \p dimNb is set to 4, the dilation should be set in
 *   height and width dimension. If \p dimNb is set to 5, the dilation should be set in depth,
 *   height and width dimension. The value of this parameter should be greater than or equal to 1.
 * @param[in] group_count
 *   Input. The number of groups that the input data is split by the number of channels
 *   in the input tensor. Each group is convolved separately. The filter used for each group is
 *   the filter tensor divides \p group_count. The result of the deconvolution operation
 *   is the concatenation of all the group deconvolution results by the number of channels
 *   in the input tensor. Make sure that the number of channels in the input tensor
 *   and the output tensor are divisible by \p group_count. The value of this parameter should
 *   be greater than or equal to 1. Currently, this parameter only supports 1 or the number
 *   of channels in the output tensor.
 *   - If \p group_count is set to 1, the input tensor is convolved without splitting into groups.
 *   - If \p group_count is set to the number of channels in the output tensor, the depthwise
 *     deconvolution is performed.
 * @param[in] compute_type
 *   Input. The data type of temporary result in deconvolution operation, only supports floating-point
 *   type. Currently, this parameter can only be set to the data type of the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - The value of \p dimNb can only be set to 4 or 5.
 * - The value of \p dimNb can only be set to 4, and the value of \p dilation can only be
 *   set to [1, 1], if depthwise deconvolution is performed.
 * - The value of \p compute_type can only be set to the data type of the output tensor.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetDeconvolutionDescriptor(cnnlDeconvolutionDescriptor_t desc,
                                                         int dimNb,
                                                         const int pad[],
                                                         const int stride[],
                                                         const int dilation[],
                                                         const int group_count,
                                                         const cnnlDataType_t compute_type);

// Group:Deconvolution
/*!
 * @brief Initializes the deconvolution descriptor \p desc that was previously created
 * with the ::cnnlCreateDeconvolutionDescriptor function, and sets the information
 * about the deconvolution forward and backward operations to the deconvolution descriptor
 * \p desc. This function also includes the \p allow_tf32 parameter that is used to
 * control whether to enable TensorFloat-32.
 *
 * @param[in] desc
 *   Input. The descriptor of the deconvolution operation. For detailed information,
 *   see ::cnnlDeconvolutionDescriptor_t.
 * @param[in] allow_tf32
 *   Input. An integer value that determines whether to enable TensorFloat-32.
 *   TensorFloat-32 is enabled by default.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - Currently, only supports 4D and 5D input tensor for deconvolution
 *   forward or backward operation.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetDeconvolutionDescriptorAllowTF32(cnnlDeconvolutionDescriptor_t desc,
                                        const int allow_tf32);

// Group:Deconvolution
/*!
 * @brief Sets the reorder type of the filter and bias tensors used in the deconvolution operation.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] deconv_desc
 *   Input. The descriptor of the deconvolution operation. For detailed information,
 *   see ::cnnlDeconvolutionDescriptor_t.
 * @param[in] filter_reorder_type
 *   Input. The reorder type of the deconvolution filter. For detailed information,
 *   see ::cnnlReorderType_t
 * @param[in] bias_reorder_type
 *   Input. The reorder type of the deconvolution bias. For detailed information,
 *   see ::cnnlReorderType_t
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - After calling this function, you need to call ::cnnlGetReorderDeconvolutionDataSize and
 *   ::cnnlHostReorderDeconvolutionData functions to reorder data for deconvolution operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlSetDeconvolutionDescriptorReorderType(cnnlDeconvolutionDescriptor_t deconv_desc,
                                          const cnnlReorderType_t filter_reorder_type,
                                          const cnnlReorderType_t bias_reorder_type);

// Group:Deconvolution
/*!
 * @brief Returns the reorder type of the filter and bias tensors used in the deconvolution operation.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] deconv_desc
 *   Input. The descriptor of the deconvolution operation. For detailed information,
 *   see ::cnnlDeconvolutionDescriptor_t.
 * @param[out] filter_reorder_type
 *   Output. The reorder type of the deconvolution filter. For detailed information,
 *   see ::cnnlReorderType_t
 * @param[out] bias_reorder_type
 *   Output. The reorder type of the deconvolution bias. For detailed information,
 *   see ::cnnlReorderType_t
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, you need to call ::cnnlSetDeconvolutionDescriptorReorderType
 *   function to set the reorder type for deconvolution operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlGetDeconvolutionDescriptorReorderType(const cnnlDeconvolutionDescriptor_t deconv_desc,
                                          cnnlReorderType_t *filter_reorder_type,
                                          cnnlReorderType_t *bias_reorder_type);

// Group:Deconvolution
/*!
 * @brief Sets the quantization information to the deconvolution descriptor \p desc that was
 *  previously created with ::cnnlCreateDeconvolutionDescriptor.
 *
 * @param[in] desc
 *   Input. The descriptor of the deconvolution operation. For detailed information,
 *   see ::cnnlDeconvolutionDescriptor_t.
 * @param[in] input0_quant_desc
 *   Input. The quantization descriptor that holds quantization information of the input.
 * @param[in] input1_quant_desc
 *   Input. The quantization descriptor that holds quantization information of the filter.
 * @param[in] output_quant_desc
 *   Input. The quantization descriptor that holds quantization information of the output.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - None
 * @note
 * - None
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetDeconvolutionDescriptorQuant(cnnlDeconvolutionDescriptor_t desc,
                                    const cnnlQuantizeExDescriptor_t input0_quant_desc,
                                    const cnnlQuantizeExDescriptor_t input1_quant_desc,
                                    const cnnlQuantizeExDescriptor_t output_quant_desc);


// Group:Deconvolution
/*!
 * @brief Destroys a deconvolution descriptor \p desc that was previously created with the
 *        ::cnnlCreateDeconvolutionDescriptor function.
 *
 * The deconvolution descriptor is defined in ::cnnlDeconvolutionDescriptor_t
 * and holds the information about the deconvolution operation.
 *
 * @param[in] desc
 *   Input. The deconvolution descriptor to be destroyed.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - You need to call this function after calling the ::cnnlDeconvolution function.
 *   Otherwise, ::CNNL_STATUS_BAD_PARAM is returned.
 * - This function should be called to destroy the deconvolution descriptor.
 *   Otherwise, the memory leak may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyDeconvolutionDescriptor(cnnlDeconvolutionDescriptor_t desc);

// Group:Deconvolution
/*!
 * @brief Returns the best suited deconvolution algorithm that can be used in the deconvolution
 *        operation.
 *
 * The returned algorithm is chosen from the supported deconvolution
 * algorithms defined in ::cnnlDeconvolutionAlgo_t and is
 * based on the given filter descriptor \p filter_desc, input descriptor
 * \p input_desc, deconvolution descriptor \p deconv_desc, output descriptor
 * \p output_desc, and the computing performance preferences \p preference.
 *
 * The computing performance options \p preference is defined in the
 * ::cnnlDeconvolutionPreference_t enum,  and only supports ::CNNL_CONVOLUTION_BWD_DATA_FASTEST.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlGetDeconvolutionAlgo instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the deconvolution operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] filter_desc
 *   Input. The descriptor of the filter tensor used as a filter in the
 *   deconvolution operation. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] deconv_desc
 *   Input. The descriptor of the deconvolution operation. For
 *   detailed information, see ::cnnlDeconvolutionDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] preference
 *   Input. The options for implementing the deconvolution operation
 *   to get better performance. This parameter only supports ::CNNL_CONVOLUTION_BWD_DATA_FASTEST.
 *  @param[out] algo
 *   Output. The returned algorithm that is best suited for performing the deconvolution operation.
 *   The algorithms are defined in the ::cnnlDeconvolutionAlgo_t enum.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions
 *   should be called in sequence to create and set the tensor descriptors
 *   \p input_desc, \p filter_desc and \p output_desc before this function.
 * - The ::cnnlCreateDeconvolutionDescriptor and ::cnnlSetDeconvolutionDescriptor
 *   functions should be called in sequence to create and set the deconvolution
 *   descriptor \p deconv_desc before this function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlGetDeconvolutionAlgo)
cnnlStatus_t CNNL_WIN_API
cnnlGetDeconvolutionAlgorithm(cnnlHandle_t handle,
                              const cnnlTensorDescriptor_t input_desc,
                              const cnnlTensorDescriptor_t filter_desc,
                              const cnnlDeconvolutionDescriptor_t deconv_desc,
                              const cnnlTensorDescriptor_t output_desc,
                              const cnnlDeconvolutionPreference_t preference,
                              cnnlDeconvolutionAlgo_t *algo);

// Group:Deconvolution
/*!
 * @brief Returns the best suited algorithm that can be used in the deconvolution
 *        operation.
 *
 * The returned algorithm is chosen from all the supported deconvolution
 * algorithms defined in ::cnnlDeconvolutionAlgo_t and is based on the given filter
 * descriptor \p filter_desc, input descriptor \p input_desc, bias descriptor
 * \p bias_desc, deconvolution descriptor \p deconv_desc, output descriptor \p output_desc,
 * and the computing performance preferences \p preference.
 *
 * The computing performance options \p preference is defined in the
 * ::cnnlDeconvolutionPreference_t enum, and only supports the high speed mode.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGetDeconvolutionAlgo instead, which allows to pass
 *   \p bias_desc as parameter.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the deconvolution operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor of deconvolution operation. For
 *   detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] filter_desc
 *   Input. The descriptor of the filter tensor used as a filter in the
 *   deconvolution operation. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] bias_desc
 *   Input. The descriptor of the bias tensor of deconvolution operation. For
 *   detailed information, see ::cnnlTensorDescriptor_t. Bias is optional. If there
 *   is no bias, this descriptor should be set nullptr.
 * @param[in] deconv_desc
 *   Input. The descriptor of the deconvolution operation. For detailed information,
 *   see ::cnnlDeconvolutionDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] preference
 *   Input. The options for implementing the deconvolution operation
 *   to get better performance. This parameter only supports
 *   ::CNNL_CONVOLUTION_BWD_DATA_FASTEST.
 * @param[out]  algo
 *   Output. The returned algorithm that is best suited for performing the
 *   deconvolution operation. The algorithms are defined in the
 *   ::cnnlDeconvolutionAlgo_t enum.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions
 *   should be called in sequence to create and set the tensor descriptors
 *   \p input_desc, \p filter_desc, \p bias_desc (optional) and \p output_desc
 *   before this function.
 * - The ::cnnlCreateDeconvolutionDescriptor and ::cnnlSetDeconvolutionDescriptor
 *   functions should be called in sequence to create and set the deconvolution
 *   descriptor \p deconv_desc before this function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
CNNL_DEPRECATED_FOR(cnnlGetDeconvolutionAlgo)
cnnlStatus_t CNNL_WIN_API
cnnlGetDeconvolutionAlgorithm_v2(cnnlHandle_t handle,
                                 const cnnlTensorDescriptor_t input_desc,
                                 const cnnlTensorDescriptor_t filter_desc,
                                 const cnnlTensorDescriptor_t bias_desc,
                                 const cnnlDeconvolutionDescriptor_t deconv_desc,
                                 const cnnlTensorDescriptor_t output_desc,
                                 const cnnlDeconvolutionPreference_t preference,
                                 cnnlDeconvolutionAlgo_t *algo);

// Group:Deconvolution
/*!
 * @brief Returns the best suited algorithm that can be used in the deconvolution
 *        operation.
 *
 * The returned algorithm is chosen from all the supported deconvolution
 * algorithms defined in ::cnnlDeconvolutionAlgo_t, and is based on the given filter
 * descriptor \p filter_desc, input descriptor \p input_desc, bias descriptor
 * \p bias_desc, deconvolution descriptor \p deconv_desc, output descriptor \p output_desc,
 * and the computing performance preferences \p preference.
 *
 * The computing performance options \p preference is defined in the
 * ::cnnlDeconvolutionPreference_t enum, and only supports the high speed mode.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the deconvolution operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor of deconvolution operation. For
 *   detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] filter_desc
 *   Input. The descriptor of the filter tensor used as a filter in the
 *   deconvolution operation. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] bias_desc
 *   Input. The descriptor of the bias tensor of deconvolution operation. For
 *   detailed information, see ::cnnlTensorDescriptor_t. Bias is optional. If there
 *   is no bias, this descriptor should be set nullptr.
 * @param[in] deconv_desc
 *   Input. The descriptor of the deconvolution operation. For detailed information,
 *   see ::cnnlDeconvolutionDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] preference
 *   Input. The options for implementing the deconvolution operation
 *   to get better performance. This parameter only supports
 *   ::CNNL_CONVOLUTION_BWD_DATA_FASTEST.
 * @param[out]  algo
 *   Output. The returned algorithm that is best suited for performing the
 *   deconvolution operation. The algorithms are defined in the
 *   ::cnnlDeconvolutionAlgo_t enum.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions
 *   should be called in sequence to create and set the tensor descriptors
 *   \p input_desc, \p filter_desc, \p bias_desc (optional) and \p output_desc
 *   before this function.
 * - The ::cnnlCreateDeconvolutionDescriptor and ::cnnlSetDeconvolutionDescriptor
 *   functions should be called in sequence to create and set the deconvolution
 *   descriptor \p deconv_desc before this function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetDeconvolutionAlgo(cnnlHandle_t handle,
                         const cnnlTensorDescriptor_t input_desc,
                         const cnnlTensorDescriptor_t filter_desc,
                         const cnnlTensorDescriptor_t bias_desc,
                         const cnnlDeconvolutionDescriptor_t deconv_desc,
                         const cnnlTensorDescriptor_t output_desc,
                         const cnnlDeconvolutionPreference_t preference,
                         cnnlDeconvolutionAlgo_t *algo);

// Group:Deconvolution
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra
 *        workspace to optimize the deconvolution operation.
 *
 * The size of the extra workspace is based on the given information of the deconvolution operation,
 * including the input descriptor \p input_desc, filter descriptor \p filter_desc,
 * deconvolution descriptor \p deconv_desc, output descriptor \p output_desc, and the
 * deconvolution algorithm \p algo. For more information about the workspace,
 * see "Cambricon CNNL User Guide".
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGetDeconvolutionWorkspaceSize_v2 instead, which allows to pass
 *   \p bias_desc as parameter.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the deconvolution operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] filter_desc
 *   Input. The descriptor of the filter tensor used as a filter in the deconvolution operation.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] deconv_desc
 *   Input. The descriptor of the deconvolution operation. For detailed information,
 *   see ::cnnlDeconvolutionDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] algo
 *   Input. The algorithm used to perform the deconvolution operation. The algorithms are defined
 *   in the ::cnnlDeconvolutionAlgo_t enum. You can get the best suited algorithm with the
 *   ::cnnlGetDeconvolutionAlgo function.
 * @param[out] size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   deconvolution operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_INTERNAL_ERROR
 *
 * @par API Dependency
 * - The ::cnnlGetDeconvolutionAlgo function must be called to get the best suited
 *   algorithm \p algo before this function.
 * - The ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions
 *   should be called in sequence to create and set the tensor descriptors
 *   \p input_desc, \p filter_desc, and \p output_desc before this function.
 * - The ::cnnlCreateDeconvolutionDescriptor and ::cnnlSetDeconvolutionDescriptor
 *   functions should be called in sequence to create and set the deconvolution
 *   descriptor \p deconv_desc before this function.
 * - The allocated extra workspace should be passed to the ::cnnlDeconvolution function to perform
 *   the deconvolution operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlGetDeconvolutionWorkspaceSize_v2)
cnnlStatus_t CNNL_WIN_API
cnnlGetDeconvolutionWorkspaceSize(cnnlHandle_t handle,
                                  const cnnlTensorDescriptor_t input_desc,
                                  const cnnlTensorDescriptor_t filter_desc,
                                  const cnnlDeconvolutionDescriptor_t deconv_desc,
                                  const cnnlTensorDescriptor_t output_desc,
                                  const cnnlDeconvolutionAlgo_t algo,
                                  size_t *size);

// Group:Deconvolution
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra
 *        workspace to optimize the deconvolution operation.
 *
 * The size of the extra workspace is based on the given information of
 * the deconvolution operation, including the filter descriptor
 * \p filter_desc, input descriptor \p input_desc, bias descriptor \p bias_desc,
 * deconvolution descriptor \p deconv_desc, output descriptor \p output_desc,
 * and the deconvolution data algorithm \p algo. For more information about the
 * workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the deconvolution operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] filter_desc
 *   Input. The descriptor of the filter tensor used as a filter in the
 *   deconvolution operation. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] bias_desc
 *   Input. The descriptor of the bias tensor of deconvolution operation. For
 *   detailed information, see ::cnnlTensorDescriptor_t. Bias is optional. If there
 *   is no bias, this descriptor should be set nullptr.
 * @param[in] deconv_desc
 *   Input. The descriptor of the deconvolution operation. For
 *   detailed information, see ::cnnlDeconvolutionDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] algo
 *   Input. The algorithm used to perform the deconvolution operation.
 *   The algorithms are defined in the ::cnnlDeconvolutionAlgo_t enum. You can
 *   get the best suited algorithm with the ::cnnlGetDeconvolutionAlgo
 *   function.
 * @param[out] size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is
 *   used in the deconvolution operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions
 *   should be called in sequence to create and set the tensor descriptors
 *   \p filter_desc, \p input_desc, \p bias_desc (optional), and \p output_desc
 *   before this function.
 * - The ::cnnlGetDeconvolutionAlgo function must be called to get
 *   the best suited algorithm \p algo before this function.
 * - The ::cnnlCreateDeconvolutionDescriptor and ::cnnlSetDeconvolutionDescriptor
 *   functions should be called in sequence to create and set the deconvolution
 *   descriptor \p deconv_desc before this function.
 * - The allocated extra workspace should be passed to the
 *   ::cnnlDeconvolution function to perform the deconvolution operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetDeconvolutionWorkspaceSize_v2(cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const cnnlTensorDescriptor_t filter_desc,
                                     const cnnlTensorDescriptor_t bias_desc,
                                     const cnnlDeconvolutionDescriptor_t deconv_desc,
                                     const cnnlTensorDescriptor_t output_desc,
                                     const cnnlDeconvolutionAlgo_t algo,
                                     size_t *size);

// Group:Deconvolution
/*!
 * @brief Computes a 2D or 3D transposed convolution operator on input tensor \p input with
 *        the filter \p filter, and returns the results in the output tensor \p output.
 *        This function can be seen as the transpose or gradient of ::cnnlConvolutionForward.
 *        The algorithm of ::cnnlDeconvolution and ::cnnlConvolutionBackwardData are the same,
 *        except the supporting of bias and quantization of output. If \p bias is not needed
 *        and \p output is floating-point, Use ::cnnlConvolutionBackwardData.
 *
 * This function also supports quantization of input tensor \p input, filter tensor \p filter,
 * and output tensor \p output, which means converting those tensors from floating-point to
 * fixed-point numbers through quantization parameters. The parameters should be set in the
 * corresponding descriptors by ::cnnlSetTensorDescriptorPositionAndScale in advance.
 * For more information about quantization, see "Cambricon CNNL User Guide".
 *
 * This function needs extra MLU memory as the workspace to improve the performance.
 * You can get the workspace size with the
 * ::cnnlGetDeconvolutionWorkspaceSize_v2 function.
 * The deconvolution operation is computed based on the algorithm set in \p algo.
 * You can get the algorithm with the ::cnnlGetDeconvolutionAlgo function.
 *
 * Depthwise deconvolution operation is performed when the \p group_count parameter of the
 * deconvolution descriptor \p deconv_desc is set to the number of channels of the output tensor
 * \p output.
 *
 * The scaling factors \p alpha and \p beta are not supported currently.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the deconvolution operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] alpha, beta
 *   Input. Reserved scaling parameters for future use. Set the value of
 *   these parameters to NULL.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] filter_desc
 *   Input. The descriptor of the filter tensor used as a filter in the
 *   deconvolution operation. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the filter tensor.
 * @param[in] bias_desc
 *   Input. The descriptor of the bias tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the bias tensor.
 * @param[in] deconv_desc
 *   Input. The descriptor of the deconvolution operation. For detailed information,
 *   see ::cnnlDeconvolutionDescriptor_t.
 * @param[in] algo
 *   Input. The algorithm used to compute the deconvolution operation.
 *   The algorithms are defined in the ::cnnlDeconvolutionAlgo_t enum.
 *   You can get the best suited algorithm with the ::cnnlGetDeconvolutionAlgo
 *   function.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for this operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the deconvolution
 *   operation. You can get the size of the workspace with the ::cnnlGetDeconvolutionWorkspaceSize_v2
 *   function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_INTERNAL_ERROR,
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Deconvolution Operator" section in "Cambricon CNNL User Guide"
 *   for details.
 *
 * @par Data Type
 * - The onchip data type is set as \p onchip_dtype parameter in tensor descriptor.
 *   For more detailed information, see ::cnnlSetTensorDescriptorOnchipDataType.
 *
 * - The combinations of \p dtype should follow the rule: tensors \p input and \p filter
 *   should be both fix-point (int8 or int16) or floating-point (half or float).
 *
 * - If the offchip data types \p dtype of \p input and \p filter are fix-point data
 *   types, this function supports combinations of the following data types for the input tensor
 *   \p input, filter tensor \p filter, and output tensor \p output.
 *   - \p input offchip data type \p dtype:
 *     - int8, int16.
 *   - \p filter offchip data type \p dtype:
 *     - int8, int16.
 *   - \p bias offchip data type \p dtype:
 *     - half, float.
 *   - \p output offchip data type \p dtype:
 *     - half, float.
 *
 * - If the offchip data types \p dtype of \p input and \p filter are floating-point data
 *   types, this function supports combinations of the following data types for the input tensor
 *   \p input, filter tensor \p filter, and output tensor \p output.
 *   - \p input offchip data type \p dtype:
 *     - half, float
 *   - \p filter offchip data type \p dtype:
 *     - half, float
 *   - \p bias offchip data type \p dtype:
 *     - half, float.
 *   - \p output offchip data type \p dtype:
 *     - half, float.
 *   - The \p dtype combination with the order of \p input - \p filter - \p bias - \p output
 *     should be half-half-half-half or float-float-float-float.
 *   - If the \p dtype combination with the order of \p input - \p filter - \p output is
 *     half-half-half, the \p onchip_dtype of \p input and \p filter should be half or
 *     \p CNNL_DTYPE_INVALID, and the \p onchip_dtype of \p output should be half, float or
 *     \p CNNL_DTYPE_INVALID.
 *
 * - The \p dtype of \p bias should be the same as \p output.
 * - The \p onchip_dtype of \p input should be the same as the \p dtype of \p input
 *   or \p CNNL_DTYPE_INVALID.
 * - The \p onchip_dtype of \p filter should be the same as the \p dtype of \p filter
 *   or \p CNNL_DTYPE_INVALID.
 * - The \p onchip_dtype of \p bias should be the same as the \p dtype of \p bias
 *   or \p CNNL_DTYPE_INVALID.
 * - The \p onchip_dtype of \p output should be the same as the \p dtype of \p output
 *   or \p CNNL_DTYPE_INVALID.
 *
 * @par Data Layout
 * - If \p dimNb is set to 4, the supported data layouts of the input tensor,
 *   filter tensor, bias tensor and output tensor are as follows:
 *   - input tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, and \p CNNL_LAYOUT_NCHW.
 *   - filter tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, and \p CNNL_LAYOUT_NCHW.
 *   - bias tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, and \p CNNL_LAYOUT_NCHW.
 *   - output tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, and \p CNNL_LAYOUT_NCHW.
 *
 * - If \p dimNb is set to 5, the supported data layouts of the input tensor,
 *   filter tensor, bias tensor and output tensor are as follows:
 *   - input tensor: \p CNNL_LAYOUT_NDHWC.
 *   - filter tensor: \p CNNL_LAYOUT_NDHWC.
 *   - bias tensor: \p CNNL_LAYOUT_NDHWC.
 *   - output tensor: \p CNNL_LAYOUT_NDHWC.
 *
 * @par Scale Limitation
 * - The input tensor, filter tensor, bias tensor, output tensor and the deconvolution descriptor
 *   should meet the following requirements:
 *   - \p pad >= 0
 *   - \p stride >= 1
 *   - \p dilation >= 1
 *   - \p group_count >= 1. If the deconvolution operation is not depthwise, additional conditions
 *     should be satisfied for parameter \p group_count as below:
 *     - The number of channels of both \p input and \p filter should be the multiple of \p group_count.
 *     - The number of output channels of \p filter multiplying \p group_count should be
 *       the number of channels of \p output and \p bias if \p bias exists.
 *   - input tensor: \p batch > 0, \p height > 0, \p width > 0, \p channel > 0
 *   - filter tensor: \p height > 0, \p width > 0
 *   - bias tensor: \p batch = 1, \p height = 1, \p width = 1
 *   - output tensor: \p height > 0, \p width > 0, \p channel > 0
 *
 * - When the operation is depthwise, the parameters should meet the following additional requirements:
 *   - \p dimNb == 4
 *   - \p dilation == 1
 *   - The number of channels of input tensor \p input must be divisible by the number of channels
 *     of output tensor \p output.
 *
 *     multiplier = \p input_c / \p output_c,
 *     sub_kh = (\p filter_h + \p stride_h - 1) / \p stride_h,
 *     sub_kw = (\p filter_w + \p stride_w - 1) / \p stride_w
 *
 *     Where \p input_c and \p output_c are the number of channels of \p input
 *     and \p output respectively, \p filter_h and \p filter_w are the height and width of
 *     filter tensor \p filter, \p stride_h and \p stride_w are stride parameters in height and
 *     width set in the deconvolution descriptor \p deconv_desc.
 *     - If multiplier < 2 * sub_kh * sub_kw:
 *       sub_kh * sub_kw <= 2016
 *     - If multiplier >= 2 * sub_kh * sub_kw:
 *       sub_kh * sub_kw <= 126 / half_flag
 *
 *     Where, \p half_flag is 2 when the data type of output tensor \p output are half; Otherwise,
 *     \p half_flag is 1.
 *
 * - The \p compute_type should meet the following requirements:
 *   - If the \p onchip_dtype of \p output is \p CNNL_DTYPE_HALF, the \p compute_type should be
 *     \p CNNL_DTYPE_HALF or \p CNNL_DTYPE_FLOAT.
 *   - If the \p onchip_dtype of \p output is \p CNNL_DTYPE_INVALID and the \p dtype of \p output
 *     is \p CNNL_DTYPE_HALF, the \p compute_type should be \p CNNL_DTYPE_HALF or
 *     \p CNNL_DTYPE_FLOAT.
 *   - Otherwise, the \p compute_type should be \p CNNL_DTYPE_FLOAT.
 *
 * @par API Dependency
 * - Before calling this function to implement deconvolution, you need to prepare all the
 *   parameters passed to this function. See each parameter description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the layout of the input
 *   tensor and output tensor to NHWC, and filter tensor to HWCN.
 *
 * @note
 * - There is no need to quantize both the input and output tensors in this function.
 *   This function also supports quantizing one of them only.
 * - There is no need to quantize both the input and filter tensors in this function.
 *   This function also supports quantizing the input tensor only. But it does not
 *   support quantizing the filter tensor only.
 * - Bias tensor does not support quantization in this function, which means that the data type of
 *   \p bias should be the same as the onchip data type of \p output.
 * - After CNNL 2.0, the combination of half input data type and int16 input onchip data type
 *   will be deprecated.
 * - After CNNL 2.0, the combination of half filter data type and int16 filter onchip data type
 *   will be deprecated.
 * - After CNNL 2.0, the combination of int16 output data type and half output onchip data type
 *   will be deprecated.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - http://github.com/vdumoulin/conv_arithmetic/blob/mater/README.md
 * - http://www.tensorflow.org/api_docs/python/tf/nn/conv2d_transpose
 * - http://www.pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html
 * - http://www.tensorflow.org/api_docs/python/tf/nn/conv3d_transpose
 * - http://www.pytorch.org/docs/stable/generated/torch.nn.ConvTranspose3d.html
 */
cnnlStatus_t CNNL_WIN_API cnnlDeconvolution(cnnlHandle_t handle,
                                            const void *alpha,
                                            const cnnlTensorDescriptor_t input_desc,
                                            const void *input,
                                            const cnnlTensorDescriptor_t filter_desc,
                                            const void *filter,
                                            const cnnlTensorDescriptor_t bias_desc,
                                            const void *bias,
                                            const cnnlDeconvolutionDescriptor_t deconv_desc,
                                            const cnnlDeconvolutionAlgo_t algo,
                                            void *workspace,
                                            size_t workspace_size,
                                            const void *beta,
                                            const cnnlTensorDescriptor_t output_desc,
                                            void *output);

// Group:Deconvolution
/*!
 * @brief Computes a 2D or 3D transposed convolution operator on input tensor \p input with
 * the filter \p filter, and returns the results in the output tensor \p output.
 * This function is only used for inference and can be seen as the transpose or gradient of
 * ::cnnlConvolutionForwardInference. For more information about quantization, see "Cambricon CNNL User Guide".
 *
 * This function needs extra MLU memory as the workspace to improve the performance.
 * You can get the workspace size with the
 * ::cnnlGetDeconvolutionWorkspaceSize_v2 function. The deconvolution operation is
 * computed based on the algorithm set in \p algo. You can call
 * the ::cnnlGetDeconvolutionAlgo to get the most suited algorithm.
 *
 * Depthwise deconvolution operation is performed when the \p group_count parameter of the
 * deconvolution descriptor \p deconv_desc is set to the number of channels of the output tensor
 * \p output.
 *
 * The scaling factors \p alpha and \p beta are not supported currently.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlDeconvolution instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the deconvolution operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] alpha, beta
 *   Input. Reserved scaling parameters for future use. Set the value of
 *   these parameters to NULL.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] filter_desc
 *   Input. The descriptor of the filter tensor used as a filter in the
 *   deconvolution operation. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the filter tensor.
 * @param[in] bias_desc
 *   Input. The descriptor of the bias tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the bias tensor.
 * @param[in] deconv_desc
 *   Input. The descriptor of the deconvolution operation. For detailed information,
 *   see ::cnnlDeconvolutionDescriptor_t.
 * @param[in] cast_mode
 *   Input. The quantization mode used for the deconvolution operation.
 *   For detailed information, see ::cnnlDeconvolutionCastMode_t.
 * @param[in] algo
 *   Input. The algorithm used to compute the deconvolution operation.
 *   The algorithms are defined in the ::cnnlDeconvolutionAlgo_t enum.
 *   You can get the best suited algorithm with the
 *   ::cnnlGetDeconvolutionAlgo function.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   deconvolution operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the deconvolution operation. You can get the size of the workspace
 *   with the ::cnnlGetDeconvolutionWorkspaceSize_v2 function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_INTERNAL_ERROR,
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * * @par Formula
 * - See "Deconvolution Operator" section in "Cambricon CNNL User Guide"
 *   for details.
 *
 * @par Data Type
 * - The onchip data type is set as \p onchip_dtype parameter in tensor descriptor.
 *   For more detailed information, see ::cnnlSetTensorDescriptorOnchipDataType.
 *
 * - The combinations of \p dtype should follow the rule: tensors \p input and \p filter
 *   should be both fix-point (int8 or int16) or floating-point (half or float).
 *
 * - If the offchip data types \p dtype of \p input and \p filter are fix-point data
 *   types, this function supports combinations of the following data types for the input tensor
 *   \p input, filter tensor \p filter, and output tensor \p output.
 *   - \p input offchip data type \p dtype:
 *     - int8, int16.
 *   - \p filter offchip data type \p dtype:
 *     - int8, int16.
 *   - \p bias offchip data type \p dtype:
 *     - half, float.
 *   - \p output offchip data type \p dtype:
 *     - half, float.
 *   - The \p onchip_dtype of \p input should be the same as the \p dtype of \p input
 *     or \p CNNL_DTYPE_INVALID.
 *   - The \p onchip_dtype of \p filter should be the same as the \p dtype of \p filter
 *     or \p CNNL_DTYPE_INVALID.
 *
 * - If the offchip data types \p dtype of \p input and \p filter are floating-point data
 *   types, this function supports combinations of the following data types for the input tensor
 *   \p input, filter tensor \p filter, and output tensor \p output.
 *   - \p input offchip data type \p dtype:
 *     - half, float
 *   - \p input onchip data type \p onchip_dtype:
 *     - int8, int16, half, float (on MLU300 series and MLU500 series).
 *     - int8, int16 (on 1V).
 *   - \p filter offchip data type \p dtype:
 *     - half, float
 *   - \p filter onchip data type \p onchip_dtype:
 *     - int8, int16, half, float (on MLU300 series and MLU500 series).
 *     - int8, int16 (on 1V).
 *   - \p bias offchip data type \p dtype:
 *     - half, float.
 *   - \p output offchip data type \p dtype:
 *     - half, float.
 *   - The \p dtype combination with the order of \p input - \p filter - \p bias - \p output
 *     should be half-half-half-half or float-float-float-float.
 *   - To perform the floating-point computation, the \p onchip_dtype of \p input should be the
 *     same as the \p dtype of \p input or \p CNNL_DTYPE_INVALID, and the \p onchip_dtype of
 *     \p filter should be the same as the \p dtype of \p filter or \p CNNL_DTYPE_INVALID.
 *   - To perform the fused-quantization computation, the \p onchip_dtype of \p input and
 *     \p filter should be fix-point.
 *
 * - The \p dtype of \p bias should be the same as \p output.
 * - The \p onchip_dtype of \p bias should be the same as the \p dtype of \p bias
 *   or \p CNNL_DTYPE_INVALID.
 * - The \p onchip_dtype of \p output should be the same as the \p dtype of \p output
 *   or \p CNNL_DTYPE_INVALID.
 *
 * @par Data Layout
 * - If \p dimNb is set to 4, the supported data layouts of the input tensor,
 *   filter tensor, bias_tensor and output tensor are as follows:
 *   - input tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, and \p CNNL_LAYOUT_NCHW.
 *   - filter tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, and \p CNNL_LAYOUT_NCHW.
 *   - bias tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, and \p CNNL_LAYOUT_NCHW.
 *   - output tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, and \p CNNL_LAYOUT_NCHW.
 *
 * - If \p dimNb is set to 5, the supported data layouts of the input tensor,
 *   filter tensor, bias tensor and output tensor are as follows:
 *   - input tensor: \p CNNL_LAYOUT_NDHWC.
 *   - filter tensor: \p CNNL_LAYOUT_NDHWC.
 *   - bias tensor: \p CNNL_LAYOUT_NDHWC.
 *   - output tensor: \p CNNL_LAYOUT_NDHWC.
 *
 * @par Scale Limitation
 * - The input tensor, filter tensor, bias_tensor, output tensor and the deconvolution descriptor
 *   should meet the following requirements:
 *   - \p pad >= 0
 *   - \p stride >= 1
 *   - \p dilation >= 1
 *   - \p group_count >= 1. If the deconvolution operation is not depthwise, additional conditions
 *     should be satisfied for parameter \p group_count as below:
 *     - The number of channels of both \p input and \p filter should be the multiple of \p group_count.
 *     - The number of output channels of \p filter multiplying \p group_count should be
 *       the number of channels of \p output and \p bias if \p bias exists.
 *   - input tensor: \p batch > 0, \p height > 0, \p width > 0, \p channel > 0
 *   - filter tensor: \p height > 0, \p width > 0
 *   - bias tensor: \p batch = 1, \p height = 1, \p width = 1
 *   - output tensor: \p height > 0, \p width > 0, \p channel > 0
 *
 * - When the operation is depthwise, the parameters should meet the following additional requirements:
 *   - \p dimNb == 4
 *   - \p dilation == 1
 *   - The number of channels of input tensor \p input must be divisible by the number of channels
 *     of output tensor \p output.
 *
 *     multiplier = \p input_c / \p output_c,
 *     sub_kh = (\p filter_h + \p stride_h - 1) / \p stride_h,
 *     sub_kw = (\p filter_w + \p stride_w - 1) / \p stride_w
 *
 *     Where \p input_c and \p output_c are the number of channels of \p input
 *     and \p output respectively, \p filter_h and \p filter_w are the height and width of
 *     filter tensor \p filter, \p stride_h and \p stride_w are stride parameters in height and
 *     width set in the deconvolution descriptor \p deconv_desc.
 *     - If multiplier < 2 * sub_kh * sub_kw:
 *       sub_kh * sub_kw <= 2016
 *     - If multiplier >= 2 * sub_kh * sub_kw:
 *       sub_kh * sub_kw <= 126 / half_flag
 *
 *     Where, \p half_flag is 2 when the data type of output tensor \p output are half; Otherwise,
 *     \p half_flag is 1.
 *
 * - The \p compute_type should meet the following requirements:
 *   - If the \p onchip_dtype of \p output is \p CNNL_DTYPE_HALF, the \p compute_type should be
 *     \p CNNL_DTYPE_HALF or \p CNNL_DTYPE_FLOAT.
 *   - If the \p onchip_dtype of \p output is \p CNNL_DTYPE_INVALID and the \p dtype of \p output
 *     is \p CNNL_DTYPE_HALF, the \p compute_type should be \p CNNL_DTYPE_HALF or
 *     \p CNNL_DTYPE_FLOAT.
 *   - Otherwise, the \p compute_type should be \p CNNL_DTYPE_FLOAT.
 *
 * @par API Dependency
 * - Before calling this function to implement convolution backward data, you
 *   need to prepare all the parameters passed to this function. See each parameter
 *   description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the layout of the input
 *   tensor and output tensor to NHWC, and filter tensor to HWCN.
 *
 * @note
 * - Depthwise deconvolution does not support onchip quantization currently.
 * - 3D deconvolution only supports ::CNNL_NO_QUANTIZE and ::CNNL_OFFLINE_SYMMETRIC_QUANTIZE as \p cast_mode currently.
 * - There is no need to quantize both the input and output tensors in this function.
 *   This function also supports quantizing one of them only.
 * - There is no need to quantize both the input and filter tensors in this function.
 *   This function also supports quantizing the input tensor only. But it does not
 *   support quantizing the filter tensor only.
 * - Bias tensor does not support quantization in this function, which means that the data type of
 *   \p bias should be the same as the onchip data type of \p output.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - http://github.com/vdumoulin/conv_arithmetic/blob/mater/README.md
 * - http://www.tensorflow.org/api_docs/python/tf/nn/conv2d_transpose
 * - http://www.pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html
 * - http://www.tensorflow.org/api_docs/python/tf/nn/conv3d_transpose
 * - http://www.pytorch.org/docs/stable/generated/torch.nn.ConvTranspose3d.html
 */
CNNL_DEPRECATED_FOR(cnnlDeconvolution)
cnnlStatus_t CNNL_WIN_API
cnnlDeconvolutionInference(cnnlHandle_t handle,
                           const void *alpha,
                           const cnnlTensorDescriptor_t input_desc,
                           const void *input,
                           const cnnlTensorDescriptor_t filter_desc,
                           const void *filter,
                           const cnnlTensorDescriptor_t bias_desc,
                           const void *bias,
                           const cnnlDeconvolutionDescriptor_t deconv_desc,
                           const cnnlDeconvolutionCastMode_t cast_mode,
                           const cnnlDeconvolutionAlgo_t algo,
                           void *workspace,
                           size_t workspace_size,
                           const void *beta,
                           const cnnlTensorDescriptor_t output_desc,
                           void *output);

// Group:Deconvolution
/*!
 * @brief Converts the input tensor \p input and the filter tensor \p filter from floating-point
 * to fixed-point numbers through quantization parameters, and then computes the output tensor
 * \p output by the deconvolution operation. For more information about quantization,
 * see "Cambricon CNNL User Guide".
 *
 * This function needs extra MLU memory as the workspace to improve the performance.
 * You can get the workspace size with the
 * ::cnnlGetDeconvolutionWorkspaceSize_v2 function. The deconvolution operation is
 * computed based on the algorithm set in \p algo. You can call
 * the ::cnnlGetDeconvolutionAlgo to get the most suited algorithm.
 *
 * Depthwise deconvolution operation is performed when the \p group_count parameter of the
 * deconvolution descriptor \p deconv_desc is set to the number of channels of the output tensor
 * \p output.
 * The scaling factors \p alpha and \p beta are not supported currently.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlDeconvolution instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the deconvolution operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] alpha, beta
 *   Input. Reserved scaling parameters for future use. Set the value of
 *   these parameters to NULL.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] input_position
 *   Input. Pointer to the MLU memory that stores the position factor for
 *   quantization of the input tensor.
 * @param[in] input_scale
 *   Input. Pointer to the MLU memory that stores the scale factor for
 *   quantization of the input tensor.
 * @param[in] input_offset
 *   Input. Pointer to the MLU memory that stores the offset factor for
 *   quantization of the input tensor.
 * @param[in] filter_desc
 *   Input. The descriptor of the filter tensor used as a filter in the
 *   deconvolution operation. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the filter tensor.
 * @param[in] filter_position
 *   Input. Pointer to the MLU memory that stores the position factor for
 *   quantization of the filter tensor.
 * @param[in] filter_scale
 *   Input. Pointer to the MLU memory that stores the scale factor for
 *   quantization of the filter tensor.
 * @param[in] filter_offset
 *   Input. Pointer to the MLU memory that stores the offset factor for
 *   quantization of the filter tensor.
 * @param[in] bias_desc
 *   Input. The descriptor of the bias tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the bias tensor.
 * @param[in] deconv_desc
 *   Input. The descriptor of the deconvolution operation. For detailed information,
 *   see ::cnnlDeconvolutionDescriptor_t.
 * @param[in] cast_mode
 *   Input. The quantization mode used for the deconvolution operation.
 *   For detailed information, see ::cnnlDeconvolutionCastMode_t.
 * @param[in] algo
 *   Input. The algorithm used to compute the deconvolution operation.
 *   The algorithms are defined in the ::cnnlDeconvolutionAlgo_t enum.
 *   You can get the best suited algorithm with the
 *   ::cnnlGetDeconvolutionAlgo function.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   deconvolution operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the deconvolution operation. You can get the size of the workspace
 *   with the ::cnnlGetDeconvolutionWorkspaceSize_v2 function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_INTERNAL_ERROR,
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * * @par Formula
 * - See "Deconvolution Operator" section in "Cambricon CNNL User Guide"
 *   for details.
 *
 * @par Data Type
 * - The onchip data type is set as \p onchip_dtype parameter in tensor descriptor.
 *   For more detailed information, see ::cnnlSetTensorDescriptorOnchipDataType.
 *
 * - The combinations of \p dtype should follow the rule: tensors \p input and \p filter
 *   should be both fix-point (int8 or int16) or floating-point (half or float).
 *
 * - If the offchip data types \p dtype of \p input and \p filter are fix-point data
 *   types, this function supports combinations of the following data types for the input tensor
 *   \p input, filter tensor \p filter, and output tensor \p output.
 *   - \p input offchip data type \p dtype:
 *     - int8, int16.
 *   - \p filter offchip data type \p dtype:
 *     - int8, int16.
 *   - \p bias offchip data type \p dtype:
 *     - half, float.
 *   - \p output offchip data type \p dtype:
 *     - half, float.
 *   - The \p onchip_dtype of \p input should be the same as the \p dtype of \p input
 *     or \p CNNL_DTYPE_INVALID.
 *   - The \p onchip_dtype of \p filter should be the same as the \p dtype of \p filter
 *     or \p CNNL_DTYPE_INVALID.
 *
 * - If the offchip data types \p dtype of \p input and \p filter are floating-point data
 *   types, this function supports combinations of the following data types for the input tensor
 *   \p input, filter tensor \p filter, and output tensor \p output.
 *   - \p input offchip data type \p dtype:
 *     - half, float
 *   - \p input onchip data type \p onchip_dtype:
 *     - int8, int16.
 *   - \p filter offchip data type \p dtype:
 *     - half, float
 *   - \p filter onchip data type \p onchip_dtype:
 *     - int8, int16.
 *   - \p bias offchip data type \p dtype:
 *     - half, float.
 *   - \p output offchip data type \p dtype:
 *     - half, float.
 *   - The \p dtype combination with the order of \p input - \p filter - \p bias - \p output
 *     should be half-half-half-half or float-float-float-float.
 *
 * - The \p dtype of \p bias should be the same as \p output.
 * - The \p onchip_dtype of \p bias should be the same as the \p dtype of \p bias
 *   or \p CNNL_DTYPE_INVALID.
 * - The \p onchip_dtype of \p output should be the same as the \p dtype of \p output
 *   or \p CNNL_DTYPE_INVALID.
 *
 * @par Data Layout
 * - If \p dimNb is set to 4, the supported data layouts of the input tensor, filter tensor, bias_tensor and output tensor
 *   are as follows:
 *   - input tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, and \p CNNL_LAYOUT_NCHW.
 *   - filter tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, and \p CNNL_LAYOUT_NCHW.
 *   - bias tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, and \p CNNL_LAYOUT_NCHW.
 *   - output tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, and \p CNNL_LAYOUT_NCHW.
 *
 * - If \p dimNb is set to 5, the supported data layouts of the input tensor,
 *   filter tensor, bias tensor and output tensor are as follows:
 *   - input tensor: \p CNNL_LAYOUT_NDHWC.
 *   - filter tensor: \p CNNL_LAYOUT_NDHWC.
 *   - bias tensor: \p CNNL_LAYOUT_NDHWC.
 *   - output tensor: \p CNNL_LAYOUT_NDHWC.
 *
 * @par Scale Limitation
 * - The input tensor, filter tensor, bias_tensor, output tensor and the deconvolution descriptor
 *   should meet the following requirements:
 *   - \p pad >= 0
 *   - \p stride >= 1
 *   - \p dilation >= 1
 *   - \p group_count >= 1. If the deconvolution operation is not depthwise, additional conditions
 *     should be satisfied for parameter \p group_count as below:
 *     - The number of channels of both \p input and \p filter should be the multiple of \p group_count.
 *     - The number of output channels of \p filter multiplying \p group_count should be
 *       the number of channels of \p output and \p bias if \p bias exists.
 *   - input tensor: \p batch > 0, \p height > 0, \p width > 0, \p channel > 0
 *   - filter tensor: \p height > 0, \p width > 0
 *   - bias tensor: \p batch = 1, \p height = 1, \p width = 1
 *   - output tensor: \p height > 0, \p width > 0, \p channel > 0
 *
 * - When the operation is depthwise, the parameters should meet the following additional requirements:
 *   - \p dimNb == 4
 *   - \p dilation == 1
 *   - The number of channels of input tensor \p input must be divisible by the number of channels
 *     of output tensor \p output.
 *
 *     multiplier = \p input_c / \p output_c,
 *     sub_kh = (\p filter_h + \p stride_h - 1) / \p stride_h,
 *     sub_kw = (\p filter_w + \p stride_w - 1) / \p stride_w
 *
 *     Where \p input_c and \p output_c are the number of channels of \p input
 *     and \p output respectively, \p filter_h and \p filter_w are the height and width of
 *     filter tensor \p filter, \p stride_h and \p stride_w are stride parameters in height and
 *     width set in the deconvolution descriptor \p deconv_desc.
 *     - If multiplier < 2 * sub_kh * sub_kw:
 *       sub_kh * sub_kw <= 2016
 *     - If multiplier >= 2 * sub_kh * sub_kw:
 *       sub_kh * sub_kw <= 126 / half_flag
 *
 *     Where, \p half_flag is 2 when the data type of output tensor \p output are half; Otherwise,
 *     \p half_flag is 1.
 *
 * @par API Dependency
 * - Before calling this function to implement convolution backward data, you
 *   need to prepare all the parameters passed to this function. See each parameter
 *   description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the layout of the input
 *   tensor and output tensor to NHWC, and filter tensor to HWCN.
 *
 * @note
 * - Depthwise deconvolution does not support onchip quantization currently.
 * - 3D deconvolution only supports ::CNNL_NO_QUANTIZE and ::CNNL_OFFLINE_SYMMETRIC_QUANTIZE as \p cast_mode currently.
 * - Bias tensor does not support quantization in this function, which means that the data type of
 *   \p bias should be the same as the onchip data type of \p output.
 * - This function does not support offline asymmetric quantization currently.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - http://github.com/vdumoulin/conv_arithmetic/blob/mater/README.md
 * - http://www.tensorflow.org/api_docs/python/tf/nn/conv2d_transpose
 * - http://www.pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html
 * - http://www.tensorflow.org/api_docs/python/tf/nn/conv3d_transpose
 * - http://www.pytorch.org/docs/stable/generated/torch.nn.ConvTranspose3d.html
 */
CNNL_DEPRECATED_FOR(cnnlDeconvolution)
cnnlStatus_t CNNL_WIN_API
cnnlQuantizeDeconvolution(cnnlHandle_t handle,
                          const void *alpha,
                          const cnnlTensorDescriptor_t input_desc,
                          const void *input,
                          const void *input_position,
                          const void *input_scale,
                          const void *input_offset,
                          const cnnlTensorDescriptor_t filter_desc,
                          const void *filter,
                          const void *filter_position,
                          const void *filter_scale,
                          const void *filter_offset,
                          const cnnlTensorDescriptor_t bias_desc,
                          const void *bias,
                          const cnnlDeconvolutionDescriptor_t deconv_desc,
                          const cnnlDeconvolutionCastMode_t cast_mode,
                          const cnnlDeconvolutionAlgo_t algo,
                          void *workspace,
                          size_t workspace_size,
                          const void *beta,
                          const cnnlTensorDescriptor_t output_desc,
                          void *output);

// Group:ConvolutionBackwardFilter
/*!
 * @brief Computes the filter gradient of convolution operation ::cnnlConvolutionForward.
 * For more information about quantization, see "Cambricon CNNL User Guide".
 *
 * This function needs extra MLU memory as the workspace to improve the performance.
 * You can get the workspace size
 * with the ::cnnlGetConvolutionBackwardFilterWorkspaceSize function. The convolution backward
 * filter operation is computed based on the algorithm set in \p algo.
 * You can call the ::cnnlGetConvolutionBackwardFilterAlgo function to get the most
 * suited algorithm.
 *
 * The scaling factors \p alpha and \p beta are not supported currently.
 *
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   convolution backward filter operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] alpha, beta
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in]  x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in]  diff_y_desc
 *   Input. The descriptor of the input differential tensor that is the gradient with respect to the output
 *   of the convolution. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  diff_y
 *   Input. Pointer to the MLU memory that stores the input differential tensor.
 * @param[in]  conv_desc
 *   Input. The descriptor of the convolution operation. For detailed information, see
 *   ::cnnlConvolutionDescriptor_t.
 * @param[in]  algo
 *   Input. The algorithm used to compute the convolution backward filter. The algorithm is
 *   defined in the ::cnnlConvolutionBwdFilterAlgo_t enum. You can get the best suited algorithm
 *   with the ::cnnlGetConvolutionBackwardFilterAlgo function.
 * @param[in]  workspace
 *   Input. Pointer to MLU memory that is used as an extra workspace for the convolution backward
 *   filter operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in]  workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the convolution
 *   backward filter operation. You can get the size of the workspace with the
 *   ::cnnlGetConvolutionBackwardFilterWorkspaceSize function.
 * @param[in]  diff_w_desc
 *   Input. The descriptor of the output differential tensor that is the gradient with respect to the filter
 *   of the convolution. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out]  diff_w
 *   Output. Pointer to the MLU memory that stores the output differential tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_ARCH_MISMATCH.
 *
 * @par Formula
 * - See "ConvolutionBackwardFilter Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are as follows in the order of
 *   \p x data type - \p diff_y data type - \p diff_w offchip data type - \p diff_w onchip data type:
 *   - half - half - half - half
 *   - half - half - half - float
 *   - BFloat16 - BFloat16 - BFloat16 - float (only on MLU500 series or above)
 *   - float - float - float - float
 *
 * @par Data Layout
 * - The supported data layouts of the input tensor, input differential tensor and output
 *   differential tensor are as follows.
 *   - dimNb is equal to 4:
 *     - \p x: \p CNNL_LAYOUT_NHWC
 *     - \p diff_y: \p CNNL_LAYOUT_NHWC
 *     - \p diff_w: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN and \p CNNL_LAYOUT_NCHW
 *   - dimNb is equal to 5:
 *     - \p x: \p CNNL_LAYOUT_NDHWC
 *     - \p diff_y: \p CNNL_LAYOUT_NDHWC
 *     - \p diff_w: \p CNNL_LAYOUT_NDHWC and \p CNNL_LAYOUT_NCDHW
 *
 * @par Scale Limitation
 * - The input tensor, input differential tensor, and the convolution descriptor
 *   (including pad, stride, dilation, and group_count) must meet the following
 *   requirements:
 *   - \p pad >= 0
 *   - \p stride >= 1
 *   - \p dilation >= 1
 *   - \p group_count equals 1 or is divisible by both the channel of \p x_desc and \p diff_y_desc.
 *   - \p x: \p batch > 0, \p height > 0, \p width > 0, \p channel > 0
 *   - \p diff_y: \p height > 0, \p width > 0, \p channel > 0
 *   - \p onchip_dtype of \p diff_w_desc must be the same as \p compute_type of \p conv_desc.
 *   - only support \p dimNb equals 4 or 5.
 *
 * @par API Dependency
 * - Before calling this function to implement convolution backward filter, you need to prepare
 *   all the parameters passed to this function. See each parameter description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the layout of the output differential
 *   tensor to \p CNNL_LAYOUT_HWCN.
 *
 * @note
 * - It is not necessary to set the onchip data type for input tensor \p x and \p diff_y,
 *   but if you do, it must be set to be the same as the offchip data type.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the convolution backward filter operation is as follows:
     @verbatim
      input two arrays by 1 * 3 * 3 * 2 and 1 * 2 * 2 * 1
      --> x: [[[[1, 2], [3, 4], [5, 6]],
                [[7, 8], [9, 10], [11, 12]],
                [[13, 14], [15, 16], [17, 18]]]]

      --> diff_y: [[[[1],[2]],
                    [[3],[4]]]]

      param:
        pad: (1, 1, 1, 1), stride: (2, 2), dilation: (1, 1),

      output differential array by 1 * 3 * 3 * 2
      --> diff_w: [[[[36, 40], [65, 72], [27, 30]],
                    [[66, 72], [115, 128], [48, 52]],
                    [[18, 20], [29, 32], [9, 10]]]]
     @endverbatim
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/compat/v1/nn/conv2d_backprop_filter.
 */

cnnlStatus_t CNNL_WIN_API cnnlConvolutionBackwardFilter(cnnlHandle_t handle,
                                                        const void *alpha,
                                                        const cnnlTensorDescriptor_t x_desc,
                                                        const void *x,
                                                        const cnnlTensorDescriptor_t diff_y_desc,
                                                        const void *diff_y,
                                                        const cnnlConvolutionDescriptor_t conv_desc,
                                                        cnnlConvolutionBwdFilterAlgo_t algo,
                                                        void *workspace,
                                                        size_t workspace_size,
                                                        const void *beta,
                                                        const cnnlTensorDescriptor_t diff_w_desc,
                                                        void *diff_w);

// Group:ConvolutionBackwardFilter
/*!
 * @brief Converts the floating-point data of input tensor \p x and
 * input differential tensor \p diff_y into fixed-point numbers according to the quantization
 * parameters, then computes the filter gradient of the convolution
 * and returns the results in the output differential tensor \p diff_w. For more information
 * about quantization, see "Cambricon CNNL User Guide".
 *
 * This function needs extra MLU memory as the workspace to improve the performance.
 * You can get the workspace size
 * with the ::cnnlGetConvolutionBackwardFilterWorkspaceSize function. The convolution backward
 * filter operation is computed based on the algorithm set in \p algo.
 * You can call the ::cnnlGetConvolutionBackwardFilterAlgo function to get the most
 * suited algorithm.
 *
 * The scaling factors \p alpha and \p beta are not supported currently.
 *
 * @deprecated
 * This function is deprecated and will be removed in future
 *   release. Use ::cnnlConvolutionBackwardFilter instead.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   convolution backward filter operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] alpha, beta
 *   Input. Reserved for future use. Set the value of these parameter to NULL.
 * @param[in]  x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in]  x_position
 *   Input. Pointer to the MLU memory that stores the position factor for input tensor.
 * @param[in]  x_scale
 *   Input. Pointer to the MLU memory that stores the scale factor for input tensor.
 *   Reserved for future use. Set the value to NULL.
 * @param[in]  x_offset
 *   Input. Pointer to the MLU memory that stores the offset factor for input tensor.
 *   Reserved for future use. Set the value to NULL.
 * @param[in]  diff_y_desc
 *   Input. The descriptor of the input differential tensor, gradient with respect to the output
 *   of the convolution. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  diff_y
 *   Input. Pointer to the MLU memory that stores the input differential tensor.
 * @param[in]  diff_y_position
 *   Input. Pointer to the MLU memory that stores the position factor for input differential tensor.
 * @param[in]  diff_y_scale
 *   Input. Pointer to the MLU memory that stores the scale factor for input differential tensor.
 *   Reserved for future use. Set the value to NULL.
 * @param[in]  diff_y_offset
 *   Input. Pointer to the MLU memory that stores the offset factor for input differential tensor.
 *   Reserved for future use. Set the value to NULL.
 * @param[in]  conv_desc
 *   Input. The descriptor of the convolution operation. For detailed information, see
 *   ::cnnlConvolutionDescriptor_t.
 * @param[in]  algo
 *   Input. The algorithm used to compute the convolution backward filter. The algorithm is
 *   defined in the ::cnnlConvolutionBwdFilterAlgo_t enum. You can get the best suited algorithm
 *   with the ::cnnlGetConvolutionBackwardFilterAlgo function.
 * @param[in]  workspace
 *   Input. Pointer to MLU memory that is used as an extra workspace for the convolution backward
 *   filter operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in]  workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the convolution
 *   backward filter operation. You can get the size of the workspace with the
 *   ::cnnlGetConvolutionBackwardFilterWorkspaceSize function.
 * @param[in]  diff_w_desc
 *   Input. The descriptor of the output differential tensor, gradient with respect to the filter
 *   of the convolution. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out]  diff_w
 *   Output. Pointer to the MLU memory that stores the output differential tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_ARCH_MISMATCH.
 *
 * @par Formula
 * - See "ConvolutionBackwardFilter Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are as follows in the order of
 *   \p x offchip data type - \p x onchip data type - \p diff_y offchip data type -
 *   \p diff_y offchip data type - \p diff_w offchip data type - \p diff_w onchip data type:
 *   - int8 - int8 - int8 - int8 - half - half
 *   - int8 - int8 - int8 - int8 - half - float
 *   - int8 - int8 - int8 - int8 - float - float
 *   - int16 - int16 - int16 - int16 - half - half
 *   - int16 - int16 - int16 - int16 - half - float
 *   - int16 - int16 - int16 - int16 - float - float
 *   - half - int8 - half - int8 - half - half
 *   - half - int8 - half - int8 - half - float
 *   - half - int16 - half - int16 - half - half
 *   - half - int16 - half - int16 - half - float
 *   - float - int8 - float - int8 - float - float
 *   - float - int16 - float - int16 - float - float
 *
 * @par Data Layout
 * - The supported data layouts of the input tensor, input differential tensor and output
 *   differential tensor are as follows.
 *   - dimNb equals 4:
 *     - \p x: \p CNNL_LAYOUT_NHWC
 *     - \p diff_y: \p CNNL_LAYOUT_NHWC
 *     - \p diff_w: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN and \p CNNL_LAYOUT_NCHW
 *   - dimNb equals 5:
 *     - \p x: \p CNNL_LAYOUT_NDHWC
 *     - \p diff_y: \p CNNL_LAYOUT_NDHWC
 *     - \p diff_w: \p CNNL_LAYOUT_NDHWC and \p CNNL_LAYOUT_NCDHW
 *
 * @par Scale Limitation
 * - The input tensor, input differential tensor, and the convolution descriptor
 *   (including pad, stride, dilation, and group_count) must meet the following
 *   requirements:
 *   - \p pad >= 0
 *   - \p stride >= 1
 *   - \p dilation >= 1
 *   - \p group_count equals 1 or is divisible by both the channel of \p x_desc and \p diff_y_desc.
 *   - \p x: \p batch > 0, \p height > 0, \p width > 0, \p channel > 0
 *   - \p diff_y: \p height > 0, \p width > 0, \p channel > 0
 *   - \p onchip_dtype of \p diff_w_desc must be the same as \p compute_type of \p conv_desc.
 *   - \p dimNb can only be set to 4 or 5.
 *
 * @par API Dependency
 * - Before calling this function to implement convolution backward filter, you need to prepare
 *   all the parameters passed to this function. See each parameter description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the layout of the output differential
 *   tensor to \p CNNL_LAYOUT_HWCN.
 *
 * @note
 * - This function does not support offline asymmetric quantization currently.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the convolution backward filter operation is as follows:
     @verbatim
      input two arrays by 1 * 3 * 3 * 2 and 1 * 2 * 2 * 1
      --> x: [[[[1, 2], [3, 4], [5, 6]],
               [[7, 8], [9, 10], [11, 12]],
               [[13, 14], [15, 16], [17, 18]]]]

      --> diff_y: [[[[1],[2]],
                    [[3],[4]]]]

      param:
        pad: (1, 1, 1, 1), stride: (2, 2), dilation: (1, 1),

      output differential array by 1 * 3 * 3 * 2
      --> diff_w: [[[[36, 40], [65, 72], [27, 30]],
                    [[66, 72], [115, 128], [48, 52]],
                    [[18, 20], [29, 32], [9, 10]]]]
     @endverbatim
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/compat/v1/nn/conv2d_backprop_filter.
 */
CNNL_DEPRECATED_FOR(cnnlConvolutionBackwardFilter)
cnnlStatus_t CNNL_WIN_API cnnlQuantizeConvolutionBackwardFilter(cnnlHandle_t handle,
                                                        const void *alpha,
                                                        const cnnlTensorDescriptor_t x_desc,
                                                        const void *x,
                                                        const void *x_position,
                                                        const void *x_scale,
                                                        const void *x_offset,
                                                        const cnnlTensorDescriptor_t diff_y_desc,
                                                        const void *diff_y,
                                                        const void *diff_y_position,
                                                        const void *diff_y_scale,
                                                        const void *diff_y_offset,
                                                        const cnnlConvolutionDescriptor_t conv_desc,
                                                        cnnlConvolutionBwdFilterAlgo_t algo,
                                                        void *workspace,
                                                        size_t workspace_size,
                                                        const void *beta,
                                                        const cnnlTensorDescriptor_t diff_w_desc,
                                                        void *diff_w);

// Group:ConvolutionBackwardFilter
/*!
 * @brief Returns the most suited algorithm that can be used in the convolution backward filter
 * operation.
 *
 * The returned algorithm is chosen from all the supported convolution backward filter
 * algorithms defined in ::cnnlConvolutionBwdFilterAlgo_t. It is chosen based on the given convolution
 * descriptor \p conv_desc, input tensor \p x_desc, input differential tensor \p diff_y_desc,
 * output differential tensor \p diff_w_desc,
 * and the computing performance preferences \p preference.
 *
 * The computing performance option \p preference is defined in the
 * ::cnnlConvolutionBwdFilterPreference_t enum.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGetConvolutionBackwardFilterAlgo instead.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   convolution backward filter operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  conv_desc
 *   Input. The descriptor of the convolution operation. For detailed information, see
 *   ::cnnlConvolutionDescriptor_t.
 * @param[in]  x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  diff_y_desc
 *   Input. The descriptor of the input differential tensor, gradient with respect to the output
 *   of the convolution. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  diff_w_desc
 *   Output. The descriptor of the output differential tensor, gradient with respect to the filter
 *   of the convolution. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  preference
 *   Input. The options for implementing the convolution backward filter opertion.
 * @param[out]  algo
 *   Output. The returned algorithm that is best suited for computing
 *   the convolution backward filter. The algorithms are defined in
 *   ::cnnlConvolutionBwdFilterAlgo_t enum.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlGetConvolutionBackwardFilterAlgo)
cnnlStatus_t CNNL_WIN_API
cnnlGetConvolutionBackwardFilterAlgorithm(cnnlHandle_t handle,
                                          const cnnlConvolutionDescriptor_t conv_desc,
                                          const cnnlTensorDescriptor_t x_desc,
                                          const cnnlTensorDescriptor_t diff_y_desc,
                                          const cnnlTensorDescriptor_t diff_w_desc,
                                          const cnnlConvolutionBwdFilterPreference_t preference,
                                          cnnlConvolutionBwdFilterAlgo_t *algo);

// Group:ConvolutionBackwardFilter
/*!
 * @brief Returns the most suited algorithm that can be used in the convolution backward filter
 * operation.
 *
 * The returned algorithm is chosen from all the supported convolution backward filter
 * algorithms defined in ::cnnlConvolutionBwdFilterAlgo_t. It is chosen based on the given convolution
 * descriptor \p conv_desc, input tensor \p x_desc, input differential tensor \p diff_y_desc,
 * output differential tensor \p diff_w_desc,
 * and the computing performance preferences \p preference.
 *
 * The computing performance option \p preference is defined in the
 * ::cnnlConvolutionBwdFilterPreference_t enum.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   convolution backward filter operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  conv_desc
 *   Input. The descriptor of the convolution operation. For detailed information, see
 *   ::cnnlConvolutionDescriptor_t.
 * @param[in]  x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  diff_y_desc
 *   Input. The descriptor of the input differential tensor, gradient with respect to the output
 *   of the convolution. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  diff_w_desc
 *   Output. The descriptor of the output differential tensor, gradient with respect to the filter
 *   of the convolution. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  preference
 *   Input. The options for implementing the convolution backward filter opertion.
 * @param[out]  algo
 *   Output. The returned algorithm that is best suited for computing
 *   the convolution backward filter. The algorithms are defined in
 *   ::cnnlConvolutionBwdFilterAlgo_t enum.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetConvolutionBackwardFilterAlgo(cnnlHandle_t handle,
                                     const cnnlConvolutionDescriptor_t conv_desc,
                                     const cnnlTensorDescriptor_t x_desc,
                                     const cnnlTensorDescriptor_t diff_y_desc,
                                     const cnnlTensorDescriptor_t diff_w_desc,
                                     const cnnlConvolutionBwdFilterPreference_t preference,
                                     cnnlConvolutionBwdFilterAlgo_t *algo);

// Group:ConvolutionBackwardFilter
/*!
 *  @brief Returns in \p size of the MLU memory that is used as an extra workspace to optimize the
 *  convolution backward filter operation.
 *
 *  The size of the extra workspace is based on the given information of the convolution backward
 *  filter operation, including the input tensor descriptor \p x_desc, input differential tensor
 *  descriptor \p diff_y_desc, output differential tensor descriptor \p diff_w_desc, convolution
 *  descriptor \p conv_desc, and the convolution backward filter algorithm \p algo. For more
 *  information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   convolution backward filter operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  diff_y_desc
 *   Input. The descriptor of the input differential tensor, gradient with respect to the output
 *   of the convolution. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  diff_w_desc
 *   Output. The descriptor of the output differential tensor, gradient with respect to the filter
 *   of the convolution. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  conv_desc
 *   Input. The descriptor of the convolution operation. For detailed information, see
 *   ::cnnlConvolutionDescriptor_t.
 * @param[in]  algo
 *   Input. The returned algorithm that is best suited for computing
 *   the convolution backward filter. The algorithms are defined in
 *   ::cnnlConvolutionBwdFilterAlgo_t enum.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *   the convolution backward filter operation.
 * @par Return
 * - CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM.
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlGetConvolutionBackwardFilterAlgo fucntion.
 *   You also need to call
 *   the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor funcions to create and set
 *   the tensor descriptor \p x_desc, \p diff_y_desc and \p diff_w_desc before calling this
 *   function. The Allocated extra workspace should be passed to the
 *   ::cnnlConvolutionBackwardFilter function to perform the convolution backward filter operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */

cnnlStatus_t CNNL_WIN_API
cnnlGetConvolutionBackwardFilterWorkspaceSize(cnnlHandle_t handle,
                                              const cnnlTensorDescriptor_t x_desc,
                                              const cnnlTensorDescriptor_t diff_y_desc,
                                              const cnnlTensorDescriptor_t diff_w_desc,
                                              const cnnlConvolutionDescriptor_t conv_desc,
                                              cnnlConvolutionBwdFilterAlgo_t algo,
                                              size_t *size);

// Group:FloorDiv
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the floordiv operation.
 *
 * The size of the extra workspace is based on the given information of the floordiv operation,
 * including the input tensor descriptors \p input1_desc and \p input2_desc, and the output
 * tensor descriptor \p output_desc. For more information about the workspace, see "Cambricon
 * CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the floordiv
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input1_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input2_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the floordiv
 *   operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - According to the rule of tensor broadcast, the parameters should satisfy the following
 *   conditions.
 *
 *   Take the lowest dimension for example, c1_dim and c2_dim represent the dimensions of
 *   \p input1 and \p input2, c3_dim represents the dimension of \p output:
 *
 *   min(c1_dim, c2_dim) == 1 or c1_dim == c2_dim
 *
 *   max(c1_dim, c2_dim) == c3_dim
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetFloorDivWorkspaceSize(cnnlHandle_t handle,
                                                       const cnnlTensorDescriptor_t input1_desc,
                                                       const cnnlTensorDescriptor_t input2_desc,
                                                       const cnnlTensorDescriptor_t output_desc,
                                                       size_t *workspace_size);


// Group:FloorDiv
/*!
 * @brief Computes floordiv on input tensor \p input1 and \p input2, and returns the results
 *        in the output tensor \p output.
 *
 * To set the computing with faster algorithm or higher precision, call ::cnnlFloorDiv_v2.
 *
 * Compared with ::cnnlFloorDivV2, this function is equivalent to floor_div in tensorflow rather
 * than pytorch.
 *
 * This function may need extra MLU memory as the workspace to improve the floordiv performance.
 * You can get the workspace size with the ::cnnlGetFloorDivWorkspaceSize
 * function.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release. Use ::cnnlDiv_v3 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 * floordiv operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input1_desc
 *   Input. The descriptor of the first input tensor. For detailed information, see
 * ::cnnlTensorDescriptor_t.
 * @param[in] input1
 *   Input. Pointer to the MLU memory that stores the dividend tensor.
 * @param[in] input2_desc
 *   Input. The descriptor of the second input tensor. For detailed information, see
 * ::cnnlTensorDescriptor_t.
 * @param[in] input2
 *   Input. Pointer to the MLU memory that stores the divisor tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 * ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the floordiv operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the floordiv
 * operation. You can get the size of the workspace with the ::cnnlGetFloorDivWorkspaceSize
 * function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "FloorDiv Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensors and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float, int32, int64.
 *   - output tensor: half, float, int32, int64.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - According to the rule of tensor broadcast, the parameters should satisfy the following
 *   conditions.
 *
 *   Take the lowest dimension for example, c1_dim and c2_dim represent the dimensions
 *   of \p input1 and \p input2, c3_dim represents the dimension of \p output:
 *
 *   min(c1_dim, c2_dim) == 1 or c1_dim == c2_dim
 *
 *   max(c1_dim, c2_dim) == c3_dim
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlFloorDiv function to perform the
 *   floordiv operation.
 *
 * @note
 * - The inputs \p input1 and \p input2 are multi-dimensional arrays, supporting up to CNNL_DIM_MAX
 * dimensions.
 * - This API returns -1 if the input \p input2 is fixed-point zero.
 * - In the situation when this function is in the COMPUTATION_FAST mode, accuracy problem may
 * occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlDiv_v3)
cnnlStatus_t CNNL_WIN_API cnnlFloorDiv(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t input1_desc,
                                       const void *input1,
                                       const cnnlTensorDescriptor_t input2_desc,
                                       const void *input2,
                                       const cnnlTensorDescriptor_t output_desc,
                                       void *output,
                                       void *workspace,
                                       size_t workspace_size);

// Group:FloorDiv
/*!
 * @brief Divides \p input1 / \p input2, rounding toward the most negative integer, and returns
 *        the results in the output tensor \p output.
 *
 * Compared with ::cnnlFloorDiv, this function allows you to choose whether to perform floordiv
 * operation with faster algorithm or higher precision.
 * Compared with ::cnnlFloorDivV2, this function is equivalent to floor_div in TensorFlow rather
 * than PyTorch.
 *
 * This function may need extra MLU memory as the workspace to improve the floordiv performance.
 * You can get the workspace size with the ::cnnlGetFloorDivWorkspaceSize
 * function.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release. Use ::cnnlDiv_v3 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 * floordiv operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. The \p prefer modes defined in ::cnnlComputationPreference_t enum.
 * @param[in] input1_desc
 *   Input. The descriptor of the first input tensor. For detailed information, see
 * ::cnnlTensorDescriptor_t.
 * @param[in] input1
 *   Input. Pointer to the MLU memory that stores the dividend tensor.
 * @param[in] input2_desc
 *   Input. The descriptor of the second input tensor. For detailed information, see
 * ::cnnlTensorDescriptor_t.
 * @param[in] input2
 *   Input. Pointer to the MLU memory that stores the divisor tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 * ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the floordiv operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the floordiv
 * operation. You can get the size of the workspace with the ::cnnlGetFloorDivWorkspaceSize
 * function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "FloorDiv Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensors and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float, int32, int64.
 *   - output tensor: half, float, int32, int64.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - According to the rule of tensor broadcast, the parameters should satisfy the following
 *   conditions.
 *
 *   Take the lowest dimension for example, c1_dim and c2_dim represent the dimensions
 *   of \p input1 and \p input2, c3_dim represents the dimension of \p output:
 *
 *   min(c1_dim, c2_dim) == 1 or c1_dim == c2_dim
 *
 *   max(c1_dim, c2_dim) == c3_dim
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlFloorDiv_v2 function to perform the
 *   floordiv operation.
 *
 * @note
 * - The inputs \p input1 and \p input2 are multi-dimensional arrays, supporting up to CNNL_DIM_MAX
 * dimensions.
 * - This API returns -1 if the input \p input2 is fixed-point zero.
 * - In the situation when this function is in the COMPUTATION_FAST mode, accuracy problem may
 * occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlDiv_v3)
cnnlStatus_t CNNL_WIN_API cnnlFloorDiv_v2(cnnlHandle_t handle,
                                          cnnlComputationPreference_t prefer,
                                          const cnnlTensorDescriptor_t input1_desc,
                                          const void *input1,
                                          const cnnlTensorDescriptor_t input2_desc,
                                          const void *input2,
                                          const cnnlTensorDescriptor_t output_desc,
                                          void *output,
                                          void *workspace,
                                          size_t workspace_size);

// Group:FloorDivV2
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the floordiv operation.
 *
 * The size of the extra workspace is based on the given information of the floordivV2 operation,
 * including the input tensor descriptors \p input1_desc and \p input2_desc, and the output
 * tensor descriptor \p output_desc. For more information about the workspace, see "Cambricon
 * CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 * floordivV2 operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input1_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 * ::cnnlTensorDescriptor_t.
 * @param[in] input2_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 * ::cnnlTensorDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 * ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 * floordivV2 operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - According to the rule of tensor broadcast, the parameters should satisfy the following
 *   conditions.
 *
 *   Take the lowest dimension for example, c1_dim and c2_dim represent the dimensions
 *   of \p input1 and \p input2, c3_dim represents the dimension of \p output:
 *
 *   min(c1_dim, c2_dim) == 1 or c1_dim == c2_dim
 *
 *   max(c1_dim, c2_dim) == c3_dim
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetFloorDivV2WorkspaceSize(cnnlHandle_t handle,
                                                         const cnnlTensorDescriptor_t input1_desc,
                                                         const cnnlTensorDescriptor_t input2_desc,
                                                         const cnnlTensorDescriptor_t output_desc,
                                                         size_t *workspace_size);

// Group:FloorDivV2
/*!
 * @brief Divides \p input1 by \p input2, rounding toward the most negative integer, and returns
 *        the results in the output tensor \p output. Equivalent to floor division in Python(the //
 *        operator), NumPy's np.floor_divide and torch.div(rounding_mode='floor').
 *
 * Compared with ::cnnlFloorDiv, this function is equivalent to floor_div in pytorch rather than
 * tensorflow.
 *
 * This function may need extra MLU memory as the workspace to improve the ::cnnlFloorDivV2 performance.
 * You can get the workspace size with the ::cnnlGetFloorDivV2WorkspaceSize
 * function.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release. Use ::cnnlDiv_v3 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 * floordivV2 operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. The \p prefer modes defined in ::cnnlComputationPreference_t enum.
 * @param[in] input1_desc
 *   Input. The descriptor of the first input tensor. For detailed information, see
 * ::cnnlTensorDescriptor_t.
 * @param[in] input1
 *   Input. Pointer to the MLU memory that stores the dividend tensor.
 * @param[in] input2_desc
 *   Input. The descriptor of the second input tensor. For detailed information, see
 * ::cnnlTensorDescriptor_t.
 * @param[in] input2
 *   Input. Pointer to the MLU memory that stores the divisor tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 * ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the floordivV2
 * operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the floordivV2
 * operation. You can get the size of the workspace with the ::cnnlGetFloorDivV2WorkspaceSize
 * function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "FloorDivV2 Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported data types of input and output tensors are as follows:
 *   - input1 tensor: half, float, bfloat16, int32, float, int64, int64.
 *   - input2 tensor: half, float, bfloat16, int32, int64, float, int64.
 *   - output tensor: half, float, bfloat16, int32, float, float, int64.
 *
 *   bfloat16 is only supported on MLU500 series.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - According to the rule of tensor broadcast, the parameters should satisfy the following
 *   conditions.
 *
 *   Take the lowest dimension for example, c1_dim and c2_dim represent the dimensions
 * of \p input1 and \p input2, c3_dim represents the dimension of \p output:
 *
 *   min(c1_dim, c2_dim) == 1 or c1_dim == c2_dim
 *
 *   max(c1_dim, c2_dim) == c3_dim
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlFloorDivV2 function to perform the
 *   floordivV2 operation.
 *
 * @note
 * - The inputs \p input1 and \p input2 are multi-dimensional arrays, supporting up to \p CNNL_DIM_MAX
 * dimensions.
 * - This API returns -1 if the input \p input1 is equal to or greater than zero when the input \p input2 is zero and the data type is int32.
 * - This API returns -2 if the input \p input1 is less than zero when the input \p input2 is zero and the data type is int32.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlDiv_v3)
cnnlStatus_t CNNL_WIN_API cnnlFloorDivV2(cnnlHandle_t handle,
                                         cnnlComputationPreference_t prefer,
                                         const cnnlTensorDescriptor_t input1_desc,
                                         const void *input1,
                                         const cnnlTensorDescriptor_t input2_desc,
                                         const void *input2,
                                         const cnnlTensorDescriptor_t output_desc,
                                         void *output,
                                         void *workspace,
                                         size_t workspace_size);

// Group:FloorDivV2
/*!
 * @brief Divides \p input1 by \p input2, rounding toward the most negative integer, and returns
 *        the results in the output tensor \p output. This API is equivalent to floor division in Python(the //
 *        operator), NumPy's np.floor_divide and torch.div(rounding_mode='floor').
 *
 * Compared with ::cnnlFloorDiv, this function is equivalent to floor_div in PyTorch rather than
 * TensorFlow. Compared with ::cnnlFloorDivV2, this function removes the \p prefer parameter.
 *
 * This function might need extra MLU memory as the workspace to improve the ::cnnlFloorDiv_v3 performance.
 * You can get the workspace size with the ::cnnlGetFloorDivV2WorkspaceSize
 * function.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release. Use ::cnnlDiv_v3 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 * floordivV2 operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input1_desc
 *   Input. The descriptor of the first input tensor. For detailed information, see
 * ::cnnlTensorDescriptor_t.
 * @param[in] input1
 *   Input. Pointer to the MLU memory that stores the dividend tensor.
 * @param[in] input2_desc
 *   Input. The descriptor of the second input tensor. For detailed information, see
 * ::cnnlTensorDescriptor_t.
 * @param[in] input2
 *   Input. Pointer to the MLU memory that stores the dividend tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 * ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the floordivV2
 * operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the floordivV2
 * operation. You can get the size of the workspace with the ::cnnlGetFloorDivV2WorkspaceSize
 * function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "FloorDivV2 Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensors and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input1 tensor: half, float, bfloat16, int32, float, int64, int64.
 *   - input2 tensor: half, float, bfloat16, int32, int64, float, int64.
 *   - output tensor: half, float, bfloat16, int32, float, float, int64.
 *
 *   bfloat16 is only supported on MLU500 series.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - According to the rule of tensor broadcast, the parameters should satisfy the following
 *   conditions.
 *
 *   Take the lowest dimension for example, c1_dim and c2_dim represent the dimensions
 *of \p input1 and \p input2, c3_dim represents the dimension of \p output:
 *
 *   min(c1_dim, c2_dim) == 1 or c1_dim == c2_dim
 *
 *   max(c1_dim, c2_dim) == c3_dim
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlFloorDivV2 function to perform the
 *   floordivV2 operation.
 *
 * @note
 * - The inputs \p input1 and \p input2 are multi-dimensional arrays, supporting up to \p CNNL_DIM_MAX
 * dimensions.
 * - This API returns -1 if the input \p input1 is equal to or greater than zero when the input \p input2 is zero and the data type is
 * int32.
 * - This API returns -2 if the input \p input1 is less than zero when the input \p input2 is zero and the data type is int32.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlDiv_v3)
cnnlStatus_t CNNL_WIN_API cnnlFloorDiv_v3(cnnlHandle_t handle,
                                          const cnnlTensorDescriptor_t input1_desc,
                                          const void *input1,
                                          const cnnlTensorDescriptor_t input2_desc,
                                          const void *input2,
                                          const cnnlTensorDescriptor_t output_desc,
                                          void *output,
                                          void *workspace,
                                          size_t workspace_size);

// Group:FloorMod
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the floormod operation.
 *
 * The size of the extra workspace is based on the given information of the floormod operation,
 * including the input tensor descriptors \p input1_desc and \p input2_desc, and the output
 * tensor descriptor \p output_desc. For more information about the workspace, see "Cambricon
 * CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 * floormod operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input1_desc
 *   Input. The descriptor of the first input tensor. For detailed information, see
 * ::cnnlTensorDescriptor_t.
 * @param[in] input2_desc
 *   Input. The descriptor of the second input tensor. For detailed information, see
 * ::cnnlTensorDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 * ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 * floormod operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetFloorModWorkspaceSize(cnnlHandle_t handle,
                                                       const cnnlTensorDescriptor_t input1_desc,
                                                       const cnnlTensorDescriptor_t input2_desc,
                                                       const cnnlTensorDescriptor_t output_desc,
                                                       size_t *workspace_size);

// Group:FloorMod
/*!
 * @brief Computes floormod on input tensor \p input1 and \p input2, and returns the results
 *        in the output tensor \p output.
 *
 * This function may need extra MLU memory as the workspace to improve the floormod performance.
 * You can get the workspace size with the ::cnnlGetFloorModWorkspaceSize
 * function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the floormod
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input1_desc
 *   Input. The descriptor of the first input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input1
 *   Input. Pointer to the MLU memory that stores the first input tensor.
 * @param[in] input2_desc
 *   Input. The descriptor of the second input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input2
 *   Input. Pointer to the MLU memory that stores the second input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the floormod operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the floormod operation.
 *   You can get the size of the workspace with the ::cnnlGetFloorModWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "FloorMod Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensors and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float, int32.
 *   - output tensor: half, float, int32.
 * - On MLU500 series, this function additionally supports the following data type combinations:
 *   - input tensor: bfloat16.
 *   - output tensor: bfloat16.
 *
 *  @par Scale Limitation
 *  - According to the rule of tensor broadcasting, the parameters should satisfy the following
 *    conditions.
 *
 *   Take the lowest dimension for example, c1_dim and c2_dim represent the dimensions of \p input1
 *    and \p input2, c3_dim represents the dimension of \p output:
 *
 *    min(c1_dim, c2_dim) == 1 or c1_dim == c2_dim
 *
 *    max(c1_dim, c2_dim) == c3_dim
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlFloorMod function to perform the
 *   floormod operation.
 *
 * @note
 * - The inputs \p input1 and \p input2 are multi-dimensional arrays, supporting up to CNNL_DIM_MAX dimensions.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlFloorMod(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t input1_desc,
                                       const void *input1,
                                       const cnnlTensorDescriptor_t input2_desc,
                                       const void *input2,
                                       const cnnlTensorDescriptor_t output_desc,
                                       void *output,
                                       void *workspace,
                                       size_t workspace_size);

// Group:FloorModTrunc
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as
 * an extra workspace to optimize the floormodtrunc operation. The size of extra
 * workspace is based on the given information of the floormodtrunc operation,
 * including the input tensor descriptors \p input1_desc and \p input2_desc, and
 * the output tensor descriptor \p output_desc. For more information about the
 * workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 * in the floormodtrunc operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input1_desc
 *   Input. The descriptor of the first input tensor. For detailed information, see
 * ::cnnlTensorDescriptor_t.
 * @param[in] input2_desc
 *   Input. The descriptor of the second input tensor. For detailed information, see
 * ::cnnlTensorDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 * ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is
 * used in the floormodtrunc operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */

cnnlStatus_t CNNL_WIN_API
cnnlGetFloorModTruncWorkspaceSize(cnnlHandle_t handle,
                                  const cnnlTensorDescriptor_t input1_desc,
                                  const cnnlTensorDescriptor_t input2_desc,
                                  const cnnlTensorDescriptor_t output_desc,
                                  size_t *workspace_size);

// Group:FloorModTrunc
/*!
 * @brief Computes the element-wise remainder of \p input1 tensor divided by
 * \p input2 tensor, and  returns the results in the output tensor \p output.
 * This operator is the requirement of Fmod operator under PyTorch framework.
 * This function may need extra MLU memory as the workspace to improve the
 * floormodtrunc performance. You can get the size of the workspace \p
 * workspace_size with the ::cnnlGetFloorModWorkspaceSize function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 * floormodtrunc operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input1_desc
 *   Input. The descriptor of the first input tensor. For detailed information, see
 * ::cnnlTensorDescriptor_t.
 * @param[in] input1
 *   Input. Pointer to the MLU memory or host memory that stores the first input tensor.
 * @param[in] input2_desc
 *   Input. The descriptor of the second input tensor. For detailed information, see
 * ::cnnlTensorDescriptor_t.
 * @param[in] input2
 *   Input. Pointer to the MLU memory or host memory that stores the second input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 * ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 * floormodtrunc operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the floormodtrunc
 * operation. You can get the size of the workspace with ::cnnlGetFloorModTruncWorkspaceSize
 * function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "FloorModTrunc Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensors and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float, int32.
 *   - output tensor: half, float, int32.
 * - On MLU500 series, this function additionally supports the following data type combinations:
 *   - input tensor: bfloat16.
 *   - output tensor: bfloat16.
 *
 *  @par Data Layout
 *  - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 *  @par Scale Limitation
 *  - According to the rule of tensor broadcast, the parameters should satisfy the following
 *    conditions.
 *
 *   Take the lowest dimension for example, c1_dim and c2_dim represent the dimensions
 *    of \p input1 and \p input2, c3_dim represents the dimension of \p output:
 *
 *    min(c1_dim, c2_dim) == 1 or c1_dim == c2_dim
 *
 *    max(c1_dim, c2_dim) == c3_dim
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlFloorModTrunc function to perform
 * the floormodtrunc operation.
 *
 * @note
 * - The inputs \p input1 and \p input2 are multi-dimensional arrays, supporting up to CNNL_DIM_MAX
 * dimensions.
 * - This function does not support that both tensors \p x and \p y are on host memory.
 * - When the dimension of input tensor \p x or \p y is 0, the dimensions of other input tensor
 *   \p y or \p x and output tensor \p z must be the same.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.fmod.html
 */

cnnlStatus_t CNNL_WIN_API
cnnlFloorModTrunc(cnnlHandle_t handle,
                  const cnnlTensorDescriptor_t input1_desc,
                  const void *input1,
                  const cnnlTensorDescriptor_t input2_desc,
                  const void *input2,
                  const cnnlTensorDescriptor_t output_desc,
                  void *output,
                  void *workspace,
                  size_t workspace_size);

// Group:FloorModPro
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the FloorModPro operation.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release. Use
 *   ::cnnlGetFloorModWorkspaceSize.
 *
 *  @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the FloorModPro
 *   operation. For detailed information, see ::cnnlHandle_t.
 *  @param[in]  output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 *  @param[out]  size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the FloorModPro
 *   operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlGetFloorModWorkspaceSize)
cnnlStatus_t CNNL_WIN_API cnnlGetFloorModProWorkspaceSize(cnnlHandle_t handle,
                                                          const cnnlTensorDescriptor_t output_desc,
                                                          size_t *size);

// Group:FloorModPro
/*!
 * @brief Computes floormod on input tensor \p input1 and \p input2, and returns the results
 *        in the output tensor \p output.
 *
 * This function may need extra MLU memory as the workspace to improve the FloorModPro performance.
 * You can get the workspace size with the ::cnnlGetFloorModProWorkspaceSize
 * function.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release. Use
 *   ::cnnlFloorMod.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the FloorModPro
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  input1_desc
 *   Input. The descriptor of the first input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  input1
 *   Input. Pointer to the MLU memory that stores the shape of the first input tensor.
 * @param[in]  workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the FloorModPro operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the FloorModPro operation.
 *   You can get the size of the workspace with the ::cnnlGetFloorModProWorkspaceSize function.
 * @param[in]  input1_shape
 *   Input. An MLU tensor, it stores the shape of the input1.
 * @param[in]  input2_desc
 *   Input. The descriptor of the second input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  input2
 *   Input. Pointer to the MLU memory that stores the shape of the second input tensor.
 * @param[in]  input2_shape
 *   Input. An MLU tensor, it stores the shape of the input2.
 * @param[in]  output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  output
 *   Output. Pointer to the MLU memory that stores the shape of the output tensor.
 * @param[in]  output_shape
 *   Input. An MLU tensor, it stores the shape of the output.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
  * @par Formula
 * - See "FloorModPro Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensors and output tensor must be the same.
 * - The data types of input and output tensors are as follows:
 *   - input tensor: half, float, int32.
 *   - output tensor: half, float, int32.
 *
 * @par Data Layout
 *  - The data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 *  - Tensors \p input1 and \p input2 support broadcasting and they should satisfy the broadcasting rules.
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlFloorModPro function to perform the
 *   FloorModPro operation.
 *
 * @note
 * - The inputs \p input1 and \p input2 are multi-dimensional arrays, supporting up to CNNL_DIM_MAX dimensions.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlFloorMod)
cnnlStatus_t CNNL_WIN_API cnnlFloorModPro(cnnlHandle_t handle,
                                          const cnnlTensorDescriptor_t input1_desc,
                                          const void *input1,
                                          void *workspace,
                                          size_t workspace_size,
                                          void *input1_shape,
                                          const cnnlTensorDescriptor_t input2_desc,
                                          const void *input2,
                                          void *input2_shape,
                                          const cnnlTensorDescriptor_t output_desc,
                                          void *output,
                                          void *output_shape);
// Group:Round
/*!
 *  @brief
 *  Creates a Round operation. Rounds the values of a tensor to the nearest even integer
 *  element-wise. This operation supports any dimensions of round operation.
 *
 *  @param[in]  handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *    queues in the round operation. For detailed information,
 *    see ::cnnlHandle_t.
 *  @param[in]  x_desc
 *    Input. The descriptor of input data. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[in]  x
 *    Input. Pointer to the MLU memory that stores the input tensor.
 *  @param[in]  y_desc
 *    Input. The descriptor of output data. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[out]  y
 *    Output. Pointer to the MLU memory that stores the output tensor.
 *
 *  @par Return
 *   - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *     ::CNNL_STATUS_ARCH_MISMATCH
 *
 *  @par Data Type
 *   - The data type of the input tensor should be the same as the data type of the output tensor.
 *   - input tensor: float, half, bfloat16
 *   - output tensor: float, half, bfloat16
 *
 *   The bfloat16 data type is supported only on MLU500 series.
 *
 * @par Scale Limitation
 *   - The layout of the input tensor and output tensor should be the same.
 *   - The shape of the input tensor and output tensor should be the same.
 *   - The total number of dimensions of the input tensor and output tensor should be the same.
 *
 * @note
 *  - You can specify the stride of all dimensions for input_desc and output_desc
 *      with ::cnnlSetTensorDescriptorEx.
 *  - This operation is supported on MLU300 and MLU500 series.
 *  - This operation is not supported on 1V.
 *
 * @par Reference
 * - https://tensorflow.google.cn/api_docs/python/tf/math/round
 */
cnnlStatus_t CNNL_WIN_API cnnlRound(cnnlHandle_t handle,
                                    const cnnlTensorDescriptor_t x_desc,
                                    const void *x,
                                    const cnnlTensorDescriptor_t y_desc,
                                    void *y);
// Group:Softmax
/*!
 * @brief Computes a Softmax or Log(Softmax) on the input tensor \p x, and returns the
 * results in the output tensor \p y.
 *
 * This function is applied to a 3D input tensor only.
 * So you need to reshape the input tensor to be 3D with ::cnnlTranspose
 * before invoking this operation. Also, you need to reshape the output tensor
 * to original shape with ::cnnlTranspose after invoking this operation.
 *
 * Compared with ::cnnlSoftmaxForward_v2, removes the \p prefer parameter.
 *
 * The scaling factors \p alpha and \p beta are not supported currently.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the softmax forward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] algorithm
 *   Input. The algorithm used to compute softmax. The algorithms are defined
 *   in the ::cnnlSoftmaxAlgorithm_t enum.
 * @param[in] mode
 *   Input. The mode used to compute softmax. The modes are defined
 *   in the ::cnnlSoftmaxMode_t enum.
 * @param[in] alpha, beta
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 * @par Data Type
 * - By the order of input - output, the supported data type combinations are as follows:
 *   - float - float
 *   - half - half
 *   - half - float
 *
 * @par Limitations
 * - When \p algorithm is \p CNNL_SOFTMAX_FAST, the range of input tensor should be in [-0.5, 0.5]
 *   on MLU200 series and in [-50, 50] on other platforms.
 * - The input tensor and output tensor should be 3D and the dimension length of output tensor
 *   and input tensor must be the same.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set \p mode to \p CNNL_SOFTMAX_MODE_LOW_DIMENSION.
 *
 * @note
 * - The algorithm \p CNNL_SOFTMAX_FAST is not recommended.
 *
 * @par Requirements
 * - None.
 *
 * @par Formula
 * - See "Softmax Forward" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Example
 * - The example of the softmax operation is as follows:
     @verbatim
       input array by 1 * 1 * 4
       --> x: [[[1, 1, 1, 1]]]

       param:
         algorithm: CNNL_SOFTMAX_ACCURATE, mode: CNNL_SOFTMAX_MODE_LOW_DIMENSION

       output array by 1 * 1 * 4
       --> y:[[[0.25, 0.25, 0.25, 0.25]]]
     @endverbatim
 *
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlSoftmaxForward(cnnlHandle_t handle,
                                             cnnlSoftmaxAlgorithm_t algorithm,
                                             cnnlSoftmaxMode_t mode,
                                             const void *alpha,
                                             const cnnlTensorDescriptor_t x_desc,
                                             const void *x,
                                             const void *beta,
                                             const cnnlTensorDescriptor_t y_desc,
                                             void *y);
// Group:Softmax
/*!
 * @brief Computes a Softmax or Log(Softmax) on the input tensor \p x, and returns the
 * results in the output tensor \p y.
 *
 * This function is applied to a 3D input tensor only.
 * So you need to reshape the input tensor to be 3D with ::cnnlTranspose
 * before invoking this operation. Also, you need to reshape the output tensor
 * to original shape with ::cnnlTranspose after invoking this operation.
 *
 * The scaling factors \p alpha and \p beta are not supported currently.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlSoftmaxForward instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the softmax forward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] algorithm
 *   Input. The algorithm used to compute softmax. The algorithms are defined
 *   in the ::cnnlSoftmaxAlgorithm_t enum.
 * @param[in] mode
 *   Input. The mode used to compute softmax. The modes are defined
 *   in the ::cnnlSoftmaxMode_t enum.
 * @param[in] prefer
 *   Input. The preference used to compute softmax. The preferences are defined
 *   in the ::cnnlComputationPreference_t enum.
 * @param[in] alpha, beta
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH,
 *   ::CNNL_STATUS_NOT_SUPPORTED
 * @par Data Type
 * - By the order of input - output, the supported data type combinations are as follows:
 *   - float - float
 *   - half - half
 *   - half - float
 *   - bfloat16 - bfloat16
 *   - bfloat16 - float
 *
 *   The bfloat16 data type is supported only on MLU500 series.
 *
 * @par Limitations
 * - When \p algorithm is \p CNNL_SOFTMAX_FAST, the range of input tensor should be in [-0.5, 0.5]
 *   on MLU200 series and in [-50, 50] on other platforms.
 * - The input tensor and output tensor should be 3D and the dimension length of output tensor
 *   and input tensor must be the same.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set \p mode to \p CNNL_SOFTMAX_MODE_LOW_DIMENSION.
 *
 * @note
 * - The algorithm \p CNNL_SOFTMAX_FAST is not recommended.
 *
 * @par Requirements
 * - None.
 *
 * @par Formula
 * - See "Softmax Forward" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Example
 * - The example of the softmax operation is as follows:
     @verbatim
       input array by 1 * 1 * 4
       --> x: [[[1, 1, 1, 1]]]

       param:
         algorithm: CNNL_SOFTMAX_ACCURATE, mode: CNNL_SOFTMAX_MODE_LOW_DIMENSION

       output array by 1 * 1 * 4
       --> y:[[[0.25, 0.25, 0.25, 0.25]]]
     @endverbatim
 *
 *
 */
CNNL_DEPRECATED_FOR(cnnlSoftmaxForward)
cnnlStatus_t CNNL_WIN_API cnnlSoftmaxForward_v2(cnnlHandle_t handle,
                                                cnnlSoftmaxAlgorithm_t algorithm,
                                                cnnlSoftmaxMode_t mode,
                                                cnnlComputationPreference_t prefer,
                                                const void *alpha,
                                                const cnnlTensorDescriptor_t x_desc,
                                                const void *x,
                                                const void *beta,
                                                const cnnlTensorDescriptor_t y_desc,
                                                void *y);
// Group:Abs
/*!
 * @brief Computes the absolute value for every element of the input tensor \p x and returns the output in \p y.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the abs operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Abs Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor:  int32, half, bfloat16, float.
 *   - output tensor: int32, half, bfloat16, float.
 *
 * @note
 * - You can specify the stride of all dimensions for \p x_desc and \p y_desc with
 *   ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the abs operation is as follows:
     @verbatim
      input arrays by 1 * 3 * 3 * 2 -->
          input: [[[[5, -11], [8, 1], [6, 4]],
                  [[3, 8], [2,6], [0, 6]],
                  [[8, 5], [7,4], [-9, 6]]]]

      output array by 1 * 3 * 3 * 2 -->
          output: [[[[5, 11], [8, 1], [6, 4]],
                   [[3, 8], [2,6], [0, 6]],
                   [[8, 5], [7,4], [9, 6]]]]
     @endverbatim
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/math/abs
 */
cnnlStatus_t CNNL_WIN_API cnnlAbs(cnnlHandle_t handle,
                                  const cnnlTensorDescriptor_t x_desc,
                                  const void *x,
                                  const cnnlTensorDescriptor_t y_desc,
                                  void *y);

// Group:LayerNormBackward
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the LayerNormBackward operation.
 *
 * The size of the extra workspace is based on the given information of the LayerNormBackward operation,
 * including the input tensor descriptor \p x_desc and the normalized dimension \p axis
 * of the input tensor.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   LayerNormBackward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] axis
 *   Input. The dimension from which the input tensor to be normalized over.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   LayerNormBackward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - This API is used along with ::cnnlLayerNormBackward_v2 and ::cnnlLayerNormBackward_v3,
 *   but this API is not required for ::cnnlLayerNormBackward.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetLayerNormBackwardWorkspaceSize(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t x_desc,
                                      int axis,
                                      size_t *workspace_size);

// Group:LayerNormBackward
/*!
 * @brief Performs the backward layer normalization operator computation. Layer normalization normalizes over
 *        the dimension of the input tensor \p x from axis to the end.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlLayerNormBackward_v3 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the layer normalization
 *   backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] axis
 *   Input. The dimension from which the input tensor to be normalized over.
 * @param[in] diff_z_desc
 *   Input. The descriptor of the backpropagated differential \p diff_z tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] diff_z
 *   Input. Pointer to the MLU memory that stores the backpropagated differential \p diff_z tensor.
 * @param[in] filter_bias_desc
 *   Input. The descriptor of the input \p filter, \p diff_filter, and \p diff_bias tensors. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the \p filter tensor. The value of this pointer can be NULL.
 * @param[in] mean_rstd_desc
 *   Input. The descriptor of the input \p mean and \p rstd tensors. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] saved_mean
 *   Input. Pointer to the MLU memory that stores the mean of input tensor that were computed during the
 *   layer normalization forward operation.
 * @param[out] saved_rstd
 *   Input. Pointer to the MLU memory that stores the inverse of the variance of input tensor that were computed
 *   during the layer normalization forward operation.
 * @param[in] diff_x_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the input gradients in output tensor.
 * @param[out] diff_filter
 *   Output. Pointer to the MLU memory that stores the filter gradients in \p diff_filter tensor.
 *   It must have the same shape as \p filter.
 * @param[out] diff_bias
 *   Output. Pointer to the MLU memory that stores the bias gradients in \p diff_bias tensor.
 *   It must have the same shape as \p filter.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "LayerNormBackward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are as follows in the order of
 *   x_tensor - diff_z_tensor - scale_tensor - saved_mean_tensor -
 *   saved_rstd_tensor - diff_x_tensor - diff_scale_tensor - diff_bias_tensor:
 *   - half - half - half - half - half - half - half - half
 *   - half - half - half - float - float - half - half - half
 *   - half - half - float - half - half - half - float - float
 *   - half - half - float - float - float - half - float - float
 *   - half - half - float - half - half - float - float - float
 *   - half - half - float - float - float - float - float - float
 *   - float - float - float - float - float - float - float - float
 *   - float - float - half - float - float - float - half - half
 *   - float - float - half - float - float - half - half - half
 * - On MLU500 series, this function additionally supports the following data type combinations:
 *   - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16
 *   - bfloat16 - bfloat16 - bfloat16 - float - float - bfloat16 - bfloat16 - bfloat16
 *   - bfloat16 - bfloat16 - float - bfloat16 - bfloat16 - bfloat16 - float - float
 *   - bfloat16 - bfloat16 - float - float - float - bfloat16 - float - float
 *   - bfloat16 - bfloat16 - float - bfloat16 - bfloat16 - float - float - float
 *   - bfloat16 - bfloat16 - float - float - float - float - float - float
 *   - float - float - bfloat16 - float - float - float - bfloat16 - bfloat16
 *   - float - float - bfloat16 - float - float - bfloat16 - bfloat16 - bfloat16
 *
 * @par Data Layout
 * - The supported data layout of the input tensors and output tensors must be \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - On MLU500 series, you can use only ::cnnlLayerNormBackward_v2 to perform the backward
 *   propagation of the layer normalization computation.
 *
 * @par Example
 * - The example of the layer normalization backward operation is as follows:
     @verbatim
      x: an array [64, 78, 1024]
      axis: 2 (layer normalization for the last dimension)
      diff_z: an array [64, 78, 1024]
      filter: [1024]
      saved_mean: [64,78]
      saved_rstd: [64,78]
      Then we will get the output:
      diff_x: an array [64, 78, 1024]
      diff_filter and diff_bias: [1024]
     @endverbatim
 *
 * @par Reference
 * - https://github.com/pytorch/pytorch/tree/master/caffe2/operators/layer_norm_op.cc
 */
CNNL_DEPRECATED_FOR(cnnlLayerNormBackward_v3)
cnnlStatus_t CNNL_WIN_API cnnlLayerNormBackward(cnnlHandle_t handle,
                                                const cnnlTensorDescriptor_t x_desc,
                                                const void *x,
                                                int axis,
                                                const cnnlTensorDescriptor_t diff_z_desc,
                                                const void *diff_z,
                                                const cnnlTensorDescriptor_t filter_bias_desc,
                                                const void *filter,
                                                const cnnlTensorDescriptor_t mean_rstd_desc,
                                                const void *saved_mean,
                                                const void *saved_rstd,
                                                const cnnlTensorDescriptor_t diff_x_desc,
                                                void *diff_x,
                                                void *diff_filter,
                                                void *diff_bias);

// Group:LayerNormBackward
/*!
 * @brief Performs the backward layer normalization operator computation. Layer normalization normalizes over
 *        the dimension of the input tensor \p x from axis to the end.
 *
 * Compared with ::cnnlLayerNormBackward, this function requires you to allocate some extra workspace as an input
 * parameter.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlLayerNormBackward_v3 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the layer normalization
 *   backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] axis
 *   Input. The dimension from which the input tensor to be normalized over.
 * @param[in] diff_z_desc
 *   Input. The descriptor of the backpropagated differential \p diff_z tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] diff_z
 *   Input. Pointer to the MLU memory that stores the backpropagated differential \p diff_z tensor.
 * @param[in] filter_bias_desc
 *   Input. The descriptor of the input \p filter, \p diff_filter, and \p diff_bias tensors. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the \p filter tensor. The value of this pointer can be NULL.
 * @param[in] mean_rstd_desc
 *   Input. The descriptor of the input \p mean and \p rstd tensors. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] saved_mean
 *   Input. Pointer to the MLU memory that stores the mean of input tensor that were computed during the
 *   layer normalization forward operation.
 * @param[out] saved_rstd
 *   Input. Pointer to the MLU memory that stores the inverse of the variance of input tensor that were computed
 *   during the layer normalization forward operation.
 * @param[in]  workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for
 *   ::cnnlLayerNormBackward_v2.
 * @param[in]  workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   ::cnnlLayerNormBackward_v2. You can get the size of the workspace with
 *   the ::cnnlGetLayerNormBackwardWorkspaceSize function.
 * @param[in] diff_x_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the input gradients in output tensor.
 * @param[out] diff_filter
 *   Output. Pointer to the MLU memory that stores the filter gradients in \p diff_filter tensor.
 *   It must have the same shape as \p filter.
 * @param[out] diff_bias
 *   Output. Pointer to the MLU memory that stores the bias gradients in \p diff_bias tensor.
 *   It must have the same shape as \p filter.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "LayerNormBackward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are as follows in the order of
 *   x_tensor - diff_z_tensor - scale_tensor - saved_mean_tensor -
 *   saved_rstd_tensor - diff_x_tensor - diff_scale_tensor - diff_bias_tensor:
 *   - half - half - half - half - half - half - half - half
 *   - half - half - half - float - float - half - half - half
 *   - half - half - float - half - half - half - float - float
 *   - half - half - float - float - float - half - float - float
 *   - half - half - float - half - half - float - float - float
 *   - half - half - float - float - float - float - float - float
 *   - float - float - float - float - float - float - float - float
 *   - float - float - half - float - float - float - half - half
 *   - float - float - half - float - float - half - half - half
 * - On MLU500 series, this function additionally supports the following data type combinations:
 *   - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16
 *   - bfloat16 - bfloat16 - bfloat16 - float - float - bfloat16 - bfloat16 - bfloat16
 *   - bfloat16 - bfloat16 - float - bfloat16 - bfloat16 - bfloat16 - float - float
 *   - bfloat16 - bfloat16 - float - float - float - bfloat16 - float - float
 *   - bfloat16 - bfloat16 - float - bfloat16 - bfloat16 - float - float - float
 *   - bfloat16 - bfloat16 - float - float - float - float - float - float
 *   - float - float - bfloat16 - float - float - float - bfloat16 - bfloat16
 *   - float - float - bfloat16 - float - float - bfloat16 - bfloat16 - bfloat16
 *
 * @par Data Layout
 * - The supported data layout of the input tensors and output tensors must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Example
 * - The example of the layer normalization backward operation is as follows:
     @verbatim
      x: an array [64, 78, 1024]
      axis: 2 (layer normalization for the last dimension)
      diff_z: an array [64, 78, 1024]
      filter: [1024]
      saved_mean: [64,78]
      saved_rstd: [64,78]
      Then we will get the output:
      diff_x: an array [64, 78, 1024]
      diff_filter and diff_bias: [1024]
     @endverbatim
 *
 * @par Reference
 * - https://github.com/pytorch/pytorch/tree/master/caffe2/operators/layer_norm_op.cc
 */
CNNL_DEPRECATED_FOR(cnnlLayerNormBackward_v3)
cnnlStatus_t CNNL_WIN_API cnnlLayerNormBackward_v2(cnnlHandle_t handle,
                                                   const cnnlTensorDescriptor_t x_desc,
                                                   const void *x,
                                                   int axis,
                                                   const cnnlTensorDescriptor_t diff_z_desc,
                                                   const void *diff_z,
                                                   const cnnlTensorDescriptor_t filter_bias_desc,
                                                   const void *filter,
                                                   const cnnlTensorDescriptor_t mean_rstd_desc,
                                                   const void *saved_mean,
                                                   const void *saved_rstd,
                                                   void *workspace,
                                                   size_t workspace_size,
                                                   const cnnlTensorDescriptor_t diff_x_desc,
                                                   void *diff_x,
                                                   void *diff_filter,
                                                   void *diff_bias);

// Group:LayerNormBackward
/*!
 * @brief Performs the backward layer normalization operator computation. Layer normalization normalizes over
 *        the dimension of the input tensor \p x from axis to the end. This function requires you to allocate some extra workspace as an input
 *        parameter.Compared with ::cnnlLayerNormBackward_v2,
 *        this API provides \p layernormbackward_desc parameter to control whether to enable memory efficiency.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the layer normalization
 *   backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] layernormbackward_desc
 *   Input. The descriptor of the layernorm backward operation. For detailed
 *   information, see ::cnnlNormDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 *   When memory efficiency is turned off, the meaning of the \p input is forward input x.
 *   When memory efficiency is turned on, the meaning of the \p input is forward output.
 * @param[in] axis
 *   Input. The dimension from which the input tensor to be normalized over.
 * @param[in] diff_z_desc
 *   Input. The descriptor of the backpropagated differential \p diff_z tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] diff_z
 *   Input. Pointer to the MLU memory that stores the backpropagated differential \p diff_z tensor.
 * @param[in] scale_desc
 *   Input. The descriptor of the input \p scale, \p diff_scale, and \p diff_bias tensors. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] scale
 *   Input. Pointer to the MLU memory that stores the \p scale tensor. The value of this pointer can be NULL.
 * @param[in] mean_rstd_desc
 *   Input. The descriptor of the input \p mean and \p rstd tensors. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] saved_mean
 *   Input. Pointer to the MLU memory that stores the mean of input tensor that is computed during the
 *   layer normalization forward operation.
 * @param[out] saved_rstd
 *   Input. Pointer to the MLU memory that stores the inverse of the variance of input tensor that is computed
 *   during the layer normalization forward operation.
 * @param[in]  workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for
 *   ::cnnlLayerNormBackward_v3.
 * @param[in]  workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   ::cnnlLayerNormBackward_v3. You can get the size of the workspace with
 *   the ::cnnlGetLayerNormBackwardWorkspaceSize function.
 * @param[in] diff_x_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the input gradients in output tensor.
 * @param[out] diff_scale
 *   Output. Pointer to the MLU memory that stores the scale gradients in \p diff_scale tensor.
 *   It must have the same shape as \p scale.
 * @param[out] diff_bias
 *   Output. Pointer to the MLU memory that stores the bias gradients in \p diff_bias tensor.
 *   It must have the same shape as \p scale.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "LayerNormBackward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are as follows in the order of
 *   x_tensor - diff_z_tensor - scale_tensor - saved_mean_tensor -
 *   saved_rstd_tensor - diff_x_tensor - diff_scale_tensor - diff_bias_tensor:
 *   - half - half - half - half - half - half - half - half
 *   - half - half - half - float - float - half - half - half
 *   - half - half - float - half - half - half - float - float
 *   - half - half - float - float - float - half - float - float
 *   - half - half - float - half - half - float - float - float
 *   - half - half - float - float - float - float - float - float
 *   - float - float - float - float - float - float - float - float
 *   - float - float - half - float - float - float - half - half
 *   - float - float - half - float - float - half - half - half
 * - On MLU500 series, this function additionally supports the following data type combinations:
 *   - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16
 *   - bfloat16 - bfloat16 - bfloat16 - float - float - bfloat16 - bfloat16 - bfloat16
 *   - bfloat16 - bfloat16 - float - bfloat16 - bfloat16 - bfloat16 - float - float
 *   - bfloat16 - bfloat16 - float - float - float - bfloat16 - float - float
 *   - bfloat16 - bfloat16 - float - bfloat16 - bfloat16 - float - float - float
 *   - bfloat16 - bfloat16 - float - float - float - float - float - float
 *   - float - float - bfloat16 - float - float - float - bfloat16 - bfloat16
 *   - float - float - bfloat16 - float - float - bfloat16 - bfloat16 - bfloat16
 *
 * @par Data Layout
 * - The supported data layout of the input tensors and output tensors must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Example
 * - The example of the layer normalization backward operation is as follows:
     @verbatim
      x: an array [64, 78, 1024]
      axis: 2 (layer normalization for the last dimension)
      diff_z: an array [64, 78, 1024]
      scale: [1024]
      saved_mean: [64,78]
      saved_rstd: [64,78]
      Then we will get the output:
      diff_x: an array [64, 78, 1024]
      diff_scale and diff_bias: [1024]
     @endverbatim
 *
 * @par Reference
 * - https://github.com/pytorch/pytorch/tree/master/caffe2/operators/layer_norm_op.cc
 */
cnnlStatus_t CNNL_WIN_API
cnnlLayerNormBackward_v3(cnnlHandle_t handle,
                         const cnnlNormDescriptor_t layernormbackward_desc,
                         const cnnlTensorDescriptor_t input_desc,
                         const void *input,
                         int axis,
                         const cnnlTensorDescriptor_t diff_z_desc,
                         const void *diff_z,
                         const cnnlTensorDescriptor_t scale_desc,
                         const void *scale,
                         const cnnlTensorDescriptor_t mean_rstd_desc,
                         const void *saved_mean,
                         const void *saved_rstd,
                         void *workspace,
                         size_t workspace_size,
                         const cnnlTensorDescriptor_t diff_x_desc,
                         void *diff_x,
                         void *diff_scale,
                         void *diff_bias);

// Group:RmsNormBackward
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the RmsNormBackward operation.
 *
 * The size of the extra workspace is based on the given information of the RmsNormBackward operation,
 * including the input tensor descriptor \p x_desc and the normalized dimension \p axis
 * of the input tensor.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   RmsNormBackward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] axis
 *   Input. The dimension from which the input tensor to be normalized over.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   RmsNormBackward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */

cnnlStatus_t CNNL_WIN_API cnnlGetRmsNormBackwardWorkspaceSize(cnnlHandle_t handle,
                                                              const cnnlTensorDescriptor_t x_desc,
                                                              int axis,
                                                              size_t *workspace_size);

// Group:RmsNormBackward
/*!
 * @brief Performs the backward Rms normalization operator computation. Rms normalization normalizes over
 *        the dimension of the input tensor \p x from axis to the end.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlRmsNormBackward_v2 instead, which supports parameter of \p rmsnormbackward_desc.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the rms normalization
 *   backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] axis
 *   Input. The dimension from which the input tensor to be normalized over.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] diff_z_desc
 *   Input. The descriptor of the backpropagated differential \p diff_z tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] diff_z
 *   Input. Pointer to the MLU memory that stores the backpropagated differential \p diff_z tensor.
 * @param[in] scale_desc
 *   Input. The descriptor of the input \p scale tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] scale
 *   Input. Pointer to the MLU memory that stores the \p scale tensor.
 * @param[in] rms_desc
 *   Input. The descriptor of the input \p rms tensors. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] saved_rms
 *   Input. Pointer to the MLU memory that stores the rms of input tensor that were computed during the
 *   rms normalization forward operation.
 * @param[in]  workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlRmsNormBackward operation.
 * @param[in]  workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the ::cnnlRmsNormBackward operation. You can get the size of the workspace with
 *   the ::cnnlGetRmsNormBackwardWorkspaceSize function.
 * @param[in] diff_x_desc
 *   Input. The descriptor of the output \p diff_x tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the input gradients in output tensor.
 * @param[in] diff_scale_bias_desc
 *   Input. The descriptor of output tensors \p diff_scale and \p diff_bias.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] diff_scale
 *   Output. Pointer to the MLU memory that stores the filter gradients in \p diff_filter tensor.
 * @param[out] diff_bias
 *   Output. Pointer to the MLU memory that stores the bias gradients in \p diff_bias tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "RmsNormBackward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - By the order of x_tensor - diff_z_tensor - scale_tensor - saved_rms_tensor
 *   - diff_x_tensor - diff_scale_tensor - diff_bias_tensor,
 *   the supported combinations of data types of this function are as follows:
 *   - half - half - half - half - half - half - half
 *   - float - float - float - float - float - float - float
 *   - half - half - half - float - half - half - half
 *
 *   On MLU500 series, the additional supported combinations of data types are as follows:
 *   - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16
 *   - bfloat16 - bfloat16 - bfloat16 - float - bfloat16 - bfloat16 - bfloat16
 *
 * @par Data Layout
 * - The supported data layout of the input tensors and output tensors must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Example
 * - The example of the rms normalization backward operation is as follows:
     @verbatim
      axis: 2
      x: an array [64, 78, 1024]
      diff_z: an array [64, 78, 1024]
      scale: [1024]
      saved_rms: [64,78]
      Then we will get the output:
      diff_x: an array [64, 78, 1024]
      diff_scale and diff_bias: [1024]
     @endverbatim
 *
 * @par Reference
 * - None.
 */

CNNL_DEPRECATED_FOR(cnnlRmsNormBackward_v2)
cnnlStatus_t CNNL_WIN_API cnnlRmsNormBackward(cnnlHandle_t handle,
                                              int axis,
                                              const cnnlTensorDescriptor_t x_desc,
                                              const void *x,
                                              const cnnlTensorDescriptor_t diff_z_desc,
                                              const void *diff_z,
                                              const cnnlTensorDescriptor_t scale_desc,
                                              const void *scale,
                                              const cnnlTensorDescriptor_t rms_desc,
                                              const void *saved_rms,
                                              void *workspace,
                                              size_t workspace_size,
                                              const cnnlTensorDescriptor_t diff_x_desc,
                                              void *diff_x,
                                              const cnnlTensorDescriptor_t diff_scale_bias_desc,
                                              void *diff_scale,
                                              void *diff_bias);

// Group:RmsNormBackward
/*!
 * @brief Performs the backward Rms normalization operator computation. Rms normalization normalizes over
 *        the dimension of the input tensor \p x from axis to the end. Compared with ::cnnlRmsNormBackward,
 *        this API provides \p rmsnormbackward_desc parameter to control whether to enable memory efficiency.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the rms normalization
 *   backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] axis
 *   Input. The dimension from which the input tensor to be normalized over.
 * @param[in] rmsnormbackward_desc
 *   Input. The descriptor of the rmsnorm backward operation. For detailed
 *   information, see ::cnnlNormDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 *   When memory efficiency is turned off, the meaning of the \p input  is forward input x.
 *   When memory efficiency is turned on, the meaning of the \p input  is forward output.
 * @param[in] diff_z_desc
 *   Input. The descriptor of the backpropagated differential \p diff_z tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] diff_z
 *   Input. Pointer to the MLU memory that stores the backpropagated differential \p diff_z tensor.
 * @param[in] scale_desc
 *   Input. The descriptor of the input \p scale tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] scale
 *   Input. Pointer to the MLU memory that stores the \p scale tensor.
 * @param[in] rms_desc
 *   Input. The descriptor of the input \p rms tensors. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] saved_rms
 *   Input. Pointer to the MLU memory that stores the rms of input tensor that were computed during the
 *   rms normalization forward operation.
 * @param[in]  workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlRmsNormBackward operation.
 * @param[in]  workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the ::cnnlRmsNormBackward operation. You can get the size of the workspace with
 *   the ::cnnlGetRmsNormBackwardWorkspaceSize function.
 * @param[in] diff_x_desc
 *   Input. The descriptor of the output \p diff_x tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the input gradients in output tensor.
 * @param[in] diff_scale_bias_desc
 *   Input. The descriptor of the output \p diff_scale and \p diff_bias tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] diff_scale
 *   Output. Pointer to the MLU memory that stores the filter gradients in \p diff_filter tensor.
 * @param[out] diff_bias
 *   Output. Pointer to the MLU memory that stores the bias gradients in \p diff_bias tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "RmsNormBackward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported data types of input and output tensors are as follows:
 *   - scale tensor: bfloat16, half, float.
 *   - saved_rms tensor: bfloat16, half, float.
 *   - input tensor: bfloat16, half, float.
 *   - diff_z tensor: bfloat16, half, float.
 *   - diff_x tensor: bfloat16, half, float.
 *   - diff_scale tensor: bfloat16, half, float.
 *
 *   The bfloat16 data type is supported only on MLU500 series.
 *   The bfloat16 data type and the half data type should not be set at the same time.
 *
 * @par Data Layout
 * - The supported data layout of the input tensors and output tensors must be \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - \p bias of \p rmsnormbackward_desc and \p diff_bias must be NULL.
 * - \p scale and \p diff_scale must both be NULL or not NULL.
 *
 * @par Example
 * - The example of the rms normalization backward operation is as follows:
     @verbatim
      axis: 2
      x: an array [64, 78, 1024]
      diff_z: an array [64, 78, 1024]
      scale: [1024]
      saved_rms: [64,78]
      Then we will get the output:
      diff_x: an array [64, 78, 1024]
      diff_scale and diff_bias: [1024]
     @endverbatim
 *
 * @par Reference
 * - None.
 */

cnnlStatus_t CNNL_WIN_API cnnlRmsNormBackward_v2(cnnlHandle_t handle,
                                                 int axis,
                                                 const cnnlNormDescriptor_t rmsnormbackward_desc,
                                                 const cnnlTensorDescriptor_t input_desc,
                                                 const void *input,
                                                 const cnnlTensorDescriptor_t diff_z_desc,
                                                 const void *diff_z,
                                                 const cnnlTensorDescriptor_t scale_desc,
                                                 const void *scale,
                                                 const cnnlTensorDescriptor_t rms_desc,
                                                 const void *saved_rms,
                                                 void *workspace,
                                                 size_t workspace_size,
                                                 const cnnlTensorDescriptor_t diff_x_desc,
                                                 void *diff_x,
                                                 const cnnlTensorDescriptor_t diff_scale_bias_desc,
                                                 void *diff_scale,
                                                 void *diff_bias);

// Group:InstanceNormInference
/*!
 * @brief Computes instance normalization forward over height and filter dimensions on input tensor
 *        \p x, \p mean, \p var, \p scale and \p bias, and returns the results in the output tensor \p y.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the instance
 *   normalization operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] mean_var_desc
 *   Input. The descriptor of the input mean and variance tensors. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] mean
 *   Input. Pointer to the MLU memory that stores the input mean tensor.
 * @param[in] var
 *   Input. Pointer to the MLU memory that stores the input variance tensor.
 * @param[in] scale_bias_desc
 *   Input. The descriptor of the input scale and bias tensors. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] scale
 *   Input. Pointer to the MLU memory that stores the scale tensor.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the bias tensor.
 * @param[in] eps
 *    Input. A float value added to the denominator for numerical stability.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. The pointer of output data.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *   ::CNNL_STATUS_NOT_SUPPORTED, CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "InstanceNorm Inference Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensors and output tensors should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layout of \p x and \p y is \p CNNL_LAYOUT_NHWC.
 *
 * @note
 * - Not supported on 1V platforms.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the instance normalization forward operation is as follows:
     @verbatim
      x: NHWC [2, 3, 4, 9]
      mean and var: array [2, 1, 1, 9] or [2, 9]
      scale and bias: array [1, 1, 1, 9] or [9]
      Then we will get the output:
      y: NHWC [2, 3, 4, 9] same as input
     @endverbatim
 *
 * @par Reference
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlInstanceNormInference(cnnlHandle_t handle,
                                                    const cnnlTensorDescriptor_t x_desc,
                                                    const void *x,
                                                    const cnnlTensorDescriptor_t mean_var_desc,
                                                    const void *mean,
                                                    const void *var,
                                                    const cnnlTensorDescriptor_t scale_bias_desc,
                                                    const void *scale,
                                                    const void *bias,
                                                    float eps,
                                                    const cnnlTensorDescriptor_t y_desc,
                                                    void *y);

// Group:InstanceNormForward
/*!
 * @brief Computes Instance Normalization for each channel and batch of data in the training
 *        scenario.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the instance
 *   normalization operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] eps
 *   Input. A float value added to the denominator for numerical stability.
 * @param[in] momentum
 *   Input. A float value used to implement moving average of \p moving_mean and \p moving_var.
 * @param[in] scale_bias_desc
 *   Input. The descriptor of the \p scale and \p bias tensors. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] scale
 *   Input. Pointer to the MLU memory that stores the input tensor \p scale, which is learnable
 *   parameter of size C of input_x.
 * @param[in] input_x_desc
 *   Input. The descriptor of the input tensor \p input_x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input_x
 *   Input. Pointer to the MLU memory that stores the input tensor \p input_x.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the input tensor \p bias, which is learnable
 *   parameter of size C of input_x.
 * @param[in] mean_var_desc
 *   Input. The descriptor of the \p moving_mean, \p moving_var, \p saved_mean and \p saved_invstd tensors.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in,out] moving_mean
 *   Input and Output. Pointer to the MLU memory that stores the tensor \p moving_mean, which is
 *   the moving average of mean computed over the N*C dimensions of the input tensor
 *   \p input_x. The value of this pointer can be NULL.
 * @param[in,out] moving_var
 *   Input and Output. Pointer to the MLU memory that stores the tensor \p moving_var, which is the
 *   moving average of variance computed over the N*C dimensions of the input tensor \p
 *   input_x. The value of this pointer can be NULL.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the operation.
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the operation.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor of the instance normalization forward operation.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor \p output.
 * @param[out] saved_mean
 *   Output. Pointer to the MLU memory that stores the output tensor \p saved_mean, which is
 *   the mean of the current sample computed over the N*C dimensions. The value of this pointer
 *   can be NULL.
 * @param[out] saved_invstd
 *   Output. Pointer to the MLU memory that stores the output tensor \p saved_invstd, which is
 *   the reciprocal root square of the current sample computed over the N*C dimensions. The value of
 *   this pointer can be NULL.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 * @par API Dependency
 * - Before calling this function to implement instance normalization forward, you need to prepare
 *   all the parameters passed to this function. See each parameter description for details.
 * @par Formula
 * - See "Instance Normalization Forward Operation" section in "Cambricon CNNL User Guide" for details.
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *   scale_tensor - input_x_tensor - bias_tensor - moving_mean_tensor - moving_var_tensor -
 *   output_tensor - saved_mean_tensor - saved_invstd_tensor.
 *   - half - half - half - half - half - half - half - half.
 *   - float - float - float - float - float - float - float - float.
 *   - float - half - float - float - float - half - float - float.
 * - On MLU500 series, the additional supported combinations of data types are as follows:
 *   - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16.
 *   - float - bfloat16 - float - float - float - bfloat16 - float - float.
 *
 * @par Data Layout
 * - The supported data layouts of the scale tensor, input_x tensor, bias tensor, moving_mean tensor,
 *   moving_var tensor, output tensor, saved_mean tensor and saved_invstd tensor are as follows:
 *   - scale tensor: \p CNNL_LAYOUT_ARRAY.
 *   - input_x tensor: \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NLC.
 *   - bias tensor: \p CNNL_LAYOUT_ARRAY.
 *   - moving_mean tensor: \p CNNL_LAYOUT_NC.
 *   - moving_var tensor: \p CNNL_LAYOUT_NC.
 *   - output tensor: \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NLC.
 *     The layout of the output tensor should be the same as input_x tensor.
 *   - saved_mean tensor: \p CNNL_LAYOUT_NC.
 *   - saved_invstd tensor: \p CNNL_LAYOUT_NC.
 *
 * @par Requirements
 * - None.
 *
 * @note
 * - For half input type, make sure the accumulate value on the interval (-65504, 65504),
 *   otherwise, there will be an overflow risk.
 *
 * @par Example
 * - The example of the instance normalization forward operation is as follows:
     @verbatim
      input_x: NDHWC [2, 3, 4, 5, 9]
      mean and var: array [2, 9]
      scale and bias: array [9]
      Then we will get the output:
      output: NDHWC [2, 3, 4, 5, 9] same as input
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm3d.html#torch.nn.InstanceNorm3d
 * - https://arxiv.org/abs/1607.08022
 */

cnnlStatus_t CNNL_WIN_API cnnlInstanceNormForward(cnnlHandle_t handle,
                                                  float eps,
                                                  float momentum,
                                                  const cnnlTensorDescriptor_t scale_bias_desc,
                                                  const void *scale,
                                                  const cnnlTensorDescriptor_t input_x_desc,
                                                  const void *input_x,
                                                  const void *bias,
                                                  const cnnlTensorDescriptor_t mean_var_desc,
                                                  void *moving_mean,
                                                  void *moving_var,
                                                  void *workspace,
                                                  size_t workspace_size,
                                                  const cnnlTensorDescriptor_t output_desc,
                                                  void *output,
                                                  void *saved_mean,
                                                  void *saved_invstd);
// Group:InstanceNormBackward
/*!
 * @brief Returns in \p size the size of the MLU memory that is used to get
 *        extra space size in instance normalization backward operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the instancenorm backward operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] input_x_desc
 *   Input. A descriptor of \p input_x tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is
 *   used in the InstanceNormBackward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlInstanceNormBackward function to
 *   perform the InstanceNormBackward operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetInstanceNormBackwardWorkspaceSize(cnnlHandle_t handle,
                                         const cnnlTensorDescriptor_t input_x_desc,
                                         size_t *workspace_size);
// Group:InstanceNormBackward
/*!
 * @brief Computes gradients with respect to instance normalization in the training scenario.
 *
 * Instance Normalization is used in artificial intelligence, including but not limited to
 * ResNet (Deep Residual Network).
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the instancenorm
 *   backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] eps
 *   Input. A float value added to the denominator for numerical stability.
 * @param[in] scale_desc
 *   The descriptor of the \p scale tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] scale
 *   Input. Pointer to the MLU memory that stores the input tensor \p scale, which is learnable
 *   parameter of size C of input_x.
 * @param[in] input_x_desc
 *   The descriptor of the input tensor \p input_x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input_x
 *   Input. Pointer to the MLU memory that stores the input tensor \p input_x.
 * @param[in] input_diff_z_desc
 *   The descriptor of the backpropagated differential tensor \p input_diff_z. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input_diff_z
 *   Input. Pointer to the MLU memory that stores the input tensor \p input_diff_z.
 * @param[in] mean_var_desc
 *   The descriptor of the \p saved_invstd, \p saved_mean tensor. For detailed information,
     see ::cnnlTensorDescriptor_t.
 * @param[in] saved_mean
 *   Input. Pointer to the MLU memory that stores the input tensor \p saved_mean, computed during
 *   the forward phase from the ::cnnlInstanceNormForward call. The value of this pointer can
 *   be NULL.
 * @param[in] saved_invstd
 *   Input. Pointer to the MLU memory that stores the input tensor \p saved_invstd, computed during
 *   the forward phase from the ::cnnlInstanceNormForward call. The value of this pointer can
 *   be NULL.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the operation.
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the operation.
 * @param[in] output_diff_x_desc
 *   The descriptor of the result differential tensor \p output_diff_x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output_diff_x
 *   Output. Pointer to the MLU memory that stores the output tensor \p output_diff_x.
 * @param[in] diff_scale_bias_desc
 *   The descriptor of the \p output_diff_scale and \p output_diff_bias tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output_diff_scale
 *   Output. Pointer to the MLU memory that stores the output tensor \p output_diff_scale.
 * @param[out] output_diff_bias
 *   Output. Pointer to the MLU memory that stores the output tensor \p output_diff_bias.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function to implement instance normalization backward, you need to prepare
 *   all the parameters passed to this function. See each parameter description for details.
 *
 * @par Formula
 * - See "InstanceNormBackward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensors and output tensors should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layouts of \p input_x, \p input_diff_z, \p scale, \p saved_mean,
 *   \p saved_invstd, \p output_diff_x, \p output_diff_scale and \p output_diff_bias are
 *   as follows:
 *   - input_x tensor: \p CNNL_LAYOUT_NLC, \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC.
 *   - input_diff_z tensor: \p CNNL_LAYOUT_NLC, \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC.
 *     The layout of the diff_z tensor should be the same as x tensor.
 *   - scale tensor: \p CNNL_LAYOUT_ARRAY, and the dimension must be 1D.
 *   - saved_mean tensor: \p CNNL_LAYOUT_NC.
 *   - saved_invstd tensor: \p CNNL_LAYOUT_NC.
 *   - output_diff_x tensor: \p CNNL_LAYOUT_NLC, \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC.
 *     The layout of the output tensor should be the same as x tensor.
 *   - output_diff_scale tensor: \p CNNL_LAYOUT_ARRAY, and the dimension must be 1D.
 *   - output_diff_bias tensor: \p CNNL_LAYOUT_ARRAY, and the dimension must be 1D.
 *
 * @note
 * - For half input type, make sure the accumulate value on the interval (-65504, 65504),
 *   otherwise, there will be an overflow risk.
 *
 * @par Requirements
 * - None.
 *
 *
 * @par Example
 * - The example of the instancenorm backward operation is as follows:
     @verbatim
      input five arrays by 1 * 2 * 3 * 2, 1 * 2 * 3 * 2, 2, 2 and 2
      --> input_x: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> input_diff_z: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
                    [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> scale: [0.5, 0.5]

      --> saved_mean: [1.0,1.0]

      --> saved_invstd: [0.0,0.0]

      param:
        eps: 0.00001

      output three arrays by 1 * 2 * 3 * 2, 2 and 2
      --> output_diff_x: [[[[0.0, 0.0],[0.0, 0.0],[0.0, 0.0]],
                    [[0.0, 0.0],[0.0, 0.0],[0.0, 0.0]]]]

      --> output_diff_scale: [0.0, 0.0]

      --> output_diff_bias: [12.0, 12.0]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm2d.html#torch.nn.InstanceNorm2d
 * - http://github.com/pytorch/pytorch/tree/master/torch/nn/modules/instancenorm.py
 * - http://github.com/pytorch/pytorch/tree/master/torch/nn/functional.py
 * - http://github.com/pytorch/pytorch/tree/master/caffe2/operators/instance_norm_op.cu
 *
 */

cnnlStatus_t CNNL_WIN_API
cnnlInstanceNormBackward(cnnlHandle_t handle,
                         float eps,
                         const cnnlTensorDescriptor_t scale_desc,
                         const void *scale,
                         const cnnlTensorDescriptor_t input_x_desc,
                         const void *input_x,
                         const cnnlTensorDescriptor_t input_diff_z_desc,
                         const void *input_diff_z,
                         const cnnlTensorDescriptor_t mean_var_desc,
                         const void *saved_mean,
                         const void *saved_invstd,
                         void *workspace,
                         size_t workspace_size,
                         const cnnlTensorDescriptor_t output_diff_x_desc,
                         void *output_diff_x,
                         const cnnlTensorDescriptor_t diff_scale_bias_desc,
                         void *output_diff_scale,
                         void *output_diff_bias);

// Group:BatchNormForwardTraining
/*!
 * @brief Returns in \p size the size of the MLU memory that is used to get
 *        extra space size in BatchNormForward operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the BatchNormForward operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of input tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is
 *   used in the BatchNormForward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetBatchNormForwardWorkspaceSize(cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t x_desc,
                                     size_t *workspace_size);

// Group:BatchNormForwardTraining
/*!
 * @brief Applies Batch Normalization for each channel across a batch of data in the training
 *        scenario.
 *
 * Batch Normalization is used in artificial intelligence, including but not limited to
 * ResNet (Deep Residual Network), Yolo (You Only Look Once) and R-CNN (Regions with CNN features).
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlBatchNormForwardTraining_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the batchnorm
 *   forward training operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] alpha, beta
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor \p x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \p x.
 * @param[in] filter_bias_mean_var_desc
 *   The descriptor of the \p filter, \p bias, \p saved_mean, \p saved_invstd, \p moving_mean and
 *   \p moving_var tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the input tensor \p filter.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the input tensor \p bias.
 * @param[in,out] moving_mean
 *   Input and Output. Pointer to the MLU memory that stores the tensor \p moving_mean, which is
 *   the moving average of mean computed over the (spatial+batch) dimensions of the input tensor
 *   \p x. The value of this pointer can be NULL.
 * @param[in,out] moving_var
 *   Input and Output. Pointer to the MLU memory that stores the tensor \p moving_var, which is the
 *   moving average of variance computed over the (spatial+batch) dimensions of the input tensor \p
 *   x. The value of this pointer can be NULL.
 * @param[in] eps
 *   Input. A float value added to the denominator for numerical stability.
 * @param[in] momentum
 *   Input. A float value used to do moving average of \p moving_mean and \p moving_var.
 * @param[in] z_desc
 *   Input. The descriptor of the output tensor \p z. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] z
 *   Output. Pointer to the MLU memory that stores the output tensor \p z.
 * @param[out] saved_mean
 *   Output. Pointer to the MLU memory that stores the output tensor \p saved_mean, which is
 *   current sample's mean computed over the (spatial+batch) dimensions. The value of this pointer can
 *   be NULL.
 * @param[out] saved_invstd
 *   Output. Pointer to the MLU memory that stores the output tensor \p saved_invstd, which is
 *   rsqrt of current sample's variance computed over the (spatial+batch) dimensions. The value of this
 *   pointer can be NULL.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "BatchNormForwardTraining Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *   input_tensor - filter_tensor - bias_tensor - moving_mean_tensor - moving_var_tensor -
 *   output_tensor - saved_mean_tensor - saved_invstd_tensor.
 *   - half - half - half - half - half - half - half - half
 *   - float - float - float - float - float - float - float - float
 *   - half - float - float - float - float - half - float - float
 *
 * @par Data Layout
 * - The supported data layouts of the input tensor, filter tensor, bias tensor, moving_mean tensor,
 *   moving_var tensor, output tensor, saved_mean tensor and saved_invstd tensor are as follows:
 *   - input tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - filter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - bias tensor: \p CNNL_LAYOUT_ARRAY.
 *   - moving_mean tensor: \p CNNL_LAYOUT_ARRAY.
 *   - moving_var tensor: \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *     The layout of the output tensor should be the same as input tensor.
 *   - saved_mean tensor: \p CNNL_LAYOUT_ARRAY.
 *   - saved_invstd tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - On MLU500 series, you can use only ::cnnlBatchNormForwardTraining_v2
 *   to perform the forward propagation of the batch normalization computation.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the batchnorm forward operation is as follows:
     @verbatim
      input five arrays by 1 * 2 * 3 * 2, 2, 2, 2 and 2
      --> x: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> filter: [0.5, 0.5]

      --> bias: [1.0, 1.0]

      --> moving_mean: [0.5, 0.5]

      --> moving_var: [1.0, 1.0]

      param:
        eps: 0.00001

      output three arrays by 1 * 2 * 3 * 2, 2 and 2
      --> z: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> saved_mean: [1.0, 1.0]

      --> saved_invstd: [316.221, 316.221]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d
 * - https://arxiv.org/abs/1502.03167
 * - Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,
 *   Sergey Ioffe, 2015.
 *
 */
CNNL_DEPRECATED_FOR(cnnlBatchNormForwardTraining_v2)
cnnlStatus_t CNNL_WIN_API
cnnlBatchNormForwardTraining(cnnlHandle_t handle,
                             const void *alpha,
                             const void *beta,
                             const cnnlTensorDescriptor_t x_desc,
                             const void *x,
                             const cnnlTensorDescriptor_t filter_bias_mean_var_desc,
                             const void *filter,
                             const void *bias,
                             void *moving_mean,
                             void *moving_var,
                             float eps,
                             float momentum,
                             const cnnlTensorDescriptor_t z_desc,
                             void *z,
                             void *saved_mean,
                             void *saved_invstd);

// Group:BatchNormForwardTrainingV2
/*!
 * @brief Applies Batch Normalization for each channel across a batch of data in the training
 *        scenario.
 *
 * Compared with ::cnnlBatchNormForwardTraining, this function allows you to choose whether to
 * perform add operations and activation operations after the batch normalization when
 * \p activation_desc is not NULL.
 *
 * Batch Normalization is used in artificial intelligence, including but not limited to
 * ResNet (Deep Residual Network), Yolo (You Only Look Once) and R-CNN (Regions with CNN features).
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlBatchNormForwardTraining_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the batchnorm
 *   forward training operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] activation_desc
 *   Input. The descriptor of the activation operation. For detailed information,
 *   see ::cnnlActivationDescriptor_t.
 * @param[in] mode
 *   Input. The mode of normalization. The \p mode must be \p CNNL_BATCHNORM_SPATIAL now.
 *   For detailed information, see ::cnnlBatchNormMode_t enum.
 * @param[in] bnOps
 *   Input. The method of activation. The supported combinations of \p bnOps and
 *   \p activation_desc->mode are:
 *   - \p bnOps is \p CNNL_BATCHNORM_OPS_BN and \p activation_desc->mode is
 *     \p CNNL_ACTIVATION_IDENTITY;
 *   - \p bnOps is \p CNNL_BATCHNORM_OPS_BN_ACTIVATION and \p activation_desc->mode is
 *     \p CNNL_ACTIVATION_RELU;
 *   - \p bnOps is \p CNNL_BATCHNORM_OPS_BN_ADD_ACTIVATION, \p activation_desc->mode is
 *     \p CNNL_ACTIVATION_RELU or \p CNNL_ACTIVATION_IDENTITY.
 *   For detailed information, see ::cnnlBatchNormOps_t enum.
 * @param[in] alpha
 *   Input. Reserved for future use. Set the value to NULL.
 * @param[in] beta
 *   Input. Reserved for future use. Set the value to NULL.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor \p x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \p x.
 * @param[in] z_desc
 *   Input. The descriptor of the input tensor \p z.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 *   Reserved for future use. Set the value to NULL.
 * @param[in] z
 *   Input. Pointer to the MLU memory that stores the input tensor \p z.
 *   The optional \p z_desc and \p z are only used when \p bnOps is
 *   \p CNNL_BATCHNORM_OPS_BN_ADD_ACTIVATION, otherwise should set it to NULL. When in use,
 *   \p z should have exactly the same dimensions as \p x and the final output \p y.
 * @param[in] filter_bias_mean_var_desc
 *   The descriptor of the \p filter, \p bias, \p saved_mean, \p saved_invstd, \p moving_mean and
 *   \p moving_var tensors. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the input tensor \p filter.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the input tensor \p bias.
 * @param[in,out] moving_mean
 *   Input and Output. Pointer to the MLU memory that stores the tensor \p moving_mean, which is
 *   the moving average of mean computed over the (spatial+batch) dimensions of the input tensor
 *   \p x. The value of this pointer can be NULL.
 * @param[in,out] moving_var
 *   Input and Output. Pointer to the MLU memory that stores the tensor \p moving_var, which is the
 *   moving average of variance computed over the (spatial+batch) dimensions of the input tensor \p
 *   x. The value of this pointer can be NULL.
 * @param[in] eps
 *   Input. A float value added to the denominator for numerical stability.
 * @param[in] momentum
 *   Input. A float value used to do moving average of \p moving_mean and \p moving_var.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor \p y. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor \p y.
 * @param[out] saved_mean
 *   Output. Pointer to the MLU memory that stores the output tensor \p saved_mean, which is
 *   current sample's mean computed over the (spatial+batch) dimensions. The value of this pointer can
 *   be NULL.
 * @param[out] saved_invstd
 *   Output. Pointer to the MLU memory that stores the output tensor \p saved_invstd, which is
 *   inversed standard deviation computed over the (spatial+batch) dimensions. The value of this
 *   pointer can be NULL.
 *   For variance, if the data type is half, make sure the accumulate value on the interval
 *   (-65504, 65504), otherwise, there will be an overflow risk.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for ::cnnlBatchNormForwardTrainingV2.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in ::cnnlBatchNormForwardTrainingV2.
 *   You can get the size of the workspace with the ::cnnlGetBatchNormForwardWorkspaceSize function.
 * @param[in] reservespace
 *   Input. Reserved for future use. Set the value to NULL.
 * @param[in] reservespace_size
 *   Input. Reserved for future use. Set the value to 0.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "BatchNormForwardTraining Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - By the order of x_tensor - z_tensor - filter_tensor - bias_tensor - moving_mean_tensor -
 *   moving_var_tensor - y_tensor - saved_mean_tensor - saved_invstd_tensor,
 *   the supported combinations of data types of this function are as follows:
 *   - half - half - half - half - half - half - half - half - half
 *   - float - float - float - float - float - float - float - float - float
 *   - half - half - float - float - float - float - half - float - float
 *
 * @par Data Layouts
 * - The supported data layouts of the \p x tensor, \p z tensor, \p filter tensor, \p bias tensor,
 *   \p moving_mean tensor, \p moving_var tensor, \p y tensor, \p saved_mean tensor and \p saved_invstd
 *   tensor are as follows:
 *   - x tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - z tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - filter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - bias tensor: \p CNNL_LAYOUT_ARRAY.
 *   - moving_mean tensor: \p CNNL_LAYOUT_ARRAY.
 *   - moving_var tensor: \p CNNL_LAYOUT_ARRAY.
 *   - y tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *     The layout of the output tensor should be the same as input tensor.
 *   - saved_mean tensor: \p CNNL_LAYOUT_ARRAY.
 *   - saved_invstd tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the batchnorm forward operation is as follows when \p bnOps is
 *   \p CNNL_BATCHNORM_OPS_BN and \p activation_desc->mode is \p CNNL_ACTIVATION_IDENTITY:
     @verbatim
      input five arrays by 1 * 2 * 3 * 2, 2, 2, 2 and 2
      --> x: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> filter: [0.5, 0.5]

      --> bias: [1.0, 1.0]

      --> moving_mean: [0.5, 0.5]

      --> moving_var: [1.0, 1.0]

      param:
        eps: 0.00001

      output three arrays by 1 * 2 * 3 * 2, 2 and 2
      --> y: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> saved_mean: [1.0, 1.0]

      --> saved_invstd: [316.221, 316.221]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d
 * - https://arxiv.org/abs/1502.03167
 * - Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,
 *   Sergey Ioffe, 2015.
 *
 */
CNNL_DEPRECATED_FOR(cnnlBatchNormForwardTraining_v2)
cnnlStatus_t CNNL_WIN_API
cnnlBatchNormForwardTrainingV2(cnnlHandle_t handle,
                               const cnnlActivationDescriptor_t activation_desc,
                               const cnnlBatchNormMode_t mode,
                               const cnnlBatchNormOps_t bnOps,
                               const void *alpha,
                               const void *beta,
                               const cnnlTensorDescriptor_t x_desc,
                               const void *x,
                               const cnnlTensorDescriptor_t z_desc,
                               const void *z,
                               const cnnlTensorDescriptor_t filter_bias_mean_var_desc,
                               const void *filter,
                               const void *bias,
                               void *moving_mean,
                               void *moving_var,
                               float eps,
                               float momentum,
                               const cnnlTensorDescriptor_t y_desc,
                               void *y,
                               void *saved_mean,
                               void *saved_invstd,
                               void *workspace,
                               size_t workspace_size,
                               void *reservespace,
                               size_t reservespace_size);

// Group:BatchNormForwardTraining
/*!
 * @brief Applies Batch Normalization for each channel across a batch of data in the training
 *        scenario.
 *
 * Compared with ::cnnlBatchNormForwardTraining, this function allows you to choose whether to
 * perform add operations and activation operations after the batch normalization when
 * \p activation_desc is not NULL.
 *
 * Batch Normalization is used in convolution network, including but not limited to
 * ResNet (Deep Residual Network), Yolo (You Only Look Once) and R-CNN (Regions with CNN features).
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the batchnorm
 *   forward training operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] activation_desc
 *   Input. The descriptor of the activation operation. For detailed information,
 *   see ::cnnlActivationDescriptor_t.
 * @param[in] mode
 *   Input. The mode of normalization. The \p mode must be \p CNNL_BATCHNORM_SPATIAL now.
 *   For detailed information, see ::cnnlBatchNormMode_t enum.
 * @param[in] bnOps
 *   Input. The method of activation. The supported combinations of \p bnOps and
 *   \p activation_desc->mode are:
 *   - \p bnOps is \p CNNL_BATCHNORM_OPS_BN and \p activation_desc->mode is
 *     \p CNNL_ACTIVATION_IDENTITY;
 *   - \p bnOps is \p CNNL_BATCHNORM_OPS_BN_ACTIVATION and \p activation_desc->mode is
 *     \p CNNL_ACTIVATION_RELU;
 *   - \p bnOps is \p CNNL_BATCHNORM_OPS_BN_ADD_ACTIVATION, \p activation_desc->mode is
 *     \p CNNL_ACTIVATION_RELU or \p CNNL_ACTIVATION_IDENTITY.
 *   For detailed information, see ::cnnlBatchNormOps_t enum.
 * @param[in] alpha
 *   Input. Reserved for future use. Set the value to NULL.
 * @param[in] beta
 *   Input. Reserved for future use. Set the value to NULL.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor \p x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \p x.
 * @param[in] z_desc
 *   Input. The descriptor of the input tensor \p z.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 *   Reserved for future use. Set the value to NULL.
 * @param[in] z
 *   Input. Pointer to the MLU memory that stores the input tensor \p z.
 *   The optional \p z_desc and \p z are only used when \p bnOps is
 *   \p CNNL_BATCHNORM_OPS_BN_ADD_ACTIVATION, otherwise should set it to NULL. When in use,
 *   \p z should have exactly the same dimensions as \p x and the final output \p y.
 * @param[in] filter_bias_mean_var_desc
 *   The descriptor of the \p filter, \p bias, \p saved_mean, \p saved_invstd, \p moving_mean and
 *   \p moving_var tensors. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the input tensor \p filter.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the input tensor \p bias.
 * @param[in,out] moving_mean
 *   Input and Output. Pointer to the MLU memory that stores the tensor \p moving_mean, which is
 *   the moving average of mean computed over the (spatial+batch) dimensions of the input tensor
 *   \p x. The value of this pointer can be NULL.
 * @param[in,out] moving_var
 *   Input and Output. Pointer to the MLU memory that stores the tensor \p moving_var, which is the
 *   moving average of variance computed over the (spatial+batch) dimensions of the input tensor \p
 *   x. The value of this pointer can be NULL.
 * @param[in] eps
 *   Input. A float value added to the denominator for numerical stability.
 * @param[in] momentum
 *   Input. A float value used to do moving average of \p moving_mean and \p moving_var.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor \p y. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor \p y.
 * @param[out] saved_mean
 *   Output. Pointer to the MLU memory that stores the output tensor \p saved_mean, which is
 *   current sample's mean computed over the (spatial+batch) dimensions. The value of this pointer can
 *   be NULL.
 * @param[out] saved_invstd
 *   Output. Pointer to the MLU memory that stores the output tensor \p saved_invstd, which is
 *   inversed standard deviation computed over the (spatial+batch) dimensions. The value of this
 *   pointer can be NULL.
 *   For variance, if the data type is half, make sure the accumulate value on the interval
 *   (-65504, 65504), otherwise, there will be an overflow risk.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for ::cnnlBatchNormForwardTraining_v2.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in ::cnnlBatchNormForwardTraining_v2.
 *   You can get the size of the workspace with the ::cnnlGetBatchNormForwardWorkspaceSize function.
 * @param[in] reservespace
 *   Input. Reserved for future use. Set the value to NULL.
 * @param[in] reservespace_size
 *   Input. Reserved for future use. Set the value to 0.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "BatchNormForwardTraining Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - By the order of x_tensor - z_tensor - filter_tensor - bias_tensor - moving_mean_tensor
 *   - moving_var_tensor - y_tensor - saved_mean_tensor - saved_invstd_tensor,
 *   the supported combinations of data types of this function are as follows:
 *   - half - half - half - half - half - half - half - half - half
 *   - float - float - float - float - float - float - float - float - float
 *   - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16
 *   - half - half - float - float - float - float - half - float - float
 *   - bfloat16 - bfloat16 - float - float - float - float - bfloat16 - float - float
 *
 *   bfloat16 is only supported on MLU500 Series.
 *
 * @par Data Layouts
 * - The supported data layouts of the \p x tensor, \p z_tensor, \p filter tensor, \p bias tensor,
 *   \p moving_mean tensor, \p moving_var tensor, \p y tensor, \p saved_mean tensor and
 *   \p saved_invstd tensor are as follows:
 *   - x tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - z tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - filter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - bias tensor: \p CNNL_LAYOUT_ARRAY.
 *   - moving_mean tensor: \p CNNL_LAYOUT_ARRAY.
 *   - moving_var tensor: \p CNNL_LAYOUT_ARRAY.
 *   - y tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *     The layout of the output tensor should be the same as input tensor.
 *   - saved_mean tensor: \p CNNL_LAYOUT_ARRAY.
 *   - saved_invstd tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the batchnorm forward operation is as follows when \p bnOps is
 *   \p CNNL_BATCHNORM_OPS_BN and \p activation_desc->mode is \p CNNL_ACTIVATION_IDENTITY:
     @verbatim
      input five arrays by 1 * 2 * 3 * 2, 2, 2, 2 and 2
      --> x: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> filter: [0.5, 0.5]

      --> bias: [1.0, 1.0]

      --> moving_mean: [0.5, 0.5]

      --> moving_var: [1.0, 1.0]

      param:
        eps: 0.00001

      output three arrays by 1 * 2 * 3 * 2, 2 and 2
      --> y: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> saved_mean: [1.0, 1.0]

      --> saved_invstd: [316.221, 316.221]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d
 * - https://arxiv.org/abs/1502.03167
 * - Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,
 *   Sergey Ioffe, 2015.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlBatchNormForwardTraining_v2(cnnlHandle_t handle,
                                const cnnlActivationDescriptor_t activation_desc,
                                const cnnlBatchNormMode_t mode,
                                const cnnlBatchNormOps_t bnOps,
                                const void *alpha,
                                const void *beta,
                                const cnnlTensorDescriptor_t x_desc,
                                const void *x,
                                const cnnlTensorDescriptor_t z_desc,
                                const void *z,
                                const cnnlTensorDescriptor_t filter_bias_mean_var_desc,
                                const void *filter,
                                const void *bias,
                                void *moving_mean,
                                void *moving_var,
                                float eps,
                                float momentum,
                                const cnnlTensorDescriptor_t y_desc,
                                void *y,
                                void *saved_mean,
                                void *saved_invstd,
                                void *workspace,
                                size_t workspace_size,
                                void *reservespace,
                                size_t reservespace_size);

// Group:BatchNormForwardInference
/*!
 * @brief Applies Batch Normalization for each channel across a batch of data in the inference
 *        scenario.
 *
 * Batch Normalization is used in artificial intelligence, including but not limited to
 * ResNet (Deep Residual Network), Yolo (You Only Look Once) and R-CNN (Regions with CNN features).
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the batchnorm
 *   forward inference operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] alpha, beta
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] x_desc
 *   The descriptor of the input tensor \p x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \p x.
 * @param[in] filter_bias_mean_var_desc
 *   The descriptor of the \p filter, \p bias, \p estimated_mean and \p estimated_var tensor. For
 *   detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the input tensor \p filter. The value of this
 *   pointer can be NULL.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the input tensor \p bias. The value of this
 *   pointer can be NULL.
 * @param[in] estimated_mean
 *   Input. Pointer to the MLU memory that stores the tensor \p estimated_mean, accumulated as \p
 *   moving_mean during the training phase from the ::cnnlBatchNormForwardTraining call.
 * @param[in] estimated_var
 *   Input. Pointer to the MLU memory that stores the tensor \p estimated_var, accumulated as \p
 *   moving_var during the training phase from the ::cnnlBatchNormForwardTraining call.
 * @param[in] eps
 *   Input. A float value added to the denominator for numerical stability.
 * @param[in] z_desc
 *   The descriptor of the batch normalization output tensor \p z. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] z
 *   Output. Pointer to the MLU memory that stores the output tensor \p z.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "BatchNormForwardInference Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - By the order of \p x tensor - \p filter tensor - \p bias tensor - \p estimated_mean tensor -
 *   \p estimated_var tensor - \p z tensor, the supported combinations of data types of this function
 *   are as follows:
 *
 *     - half - half - half - half - half - half
 *     - float - float - float - float - float - float
 *     - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16
 *     - half  - float - float - float - float - half
 *     - bfloat16 - float - float - float - float - bfloat16
 *     - uint8 - half - half - half - half - half
 *     - uint8 - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16
 *     - uint8 - float - float - float - float - float
 *     - uint8 - float - float - float - float - half
 *     - uint8 - float - float - float - float - bfloat16
 *
 *     bfloat16 is only supported on MLU500 Series.
 *
 * @par Data Layout
 * - The supported data layouts of the \p x tensor, \p filter tensor, \p bias tensor, \p estimated_mean
 *   tensor, \p estimated_var tensor and \p z tensor are as follows:
 *     - x tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *     - filter tensor: \p CNNL_LAYOUT_ARRAY.
 *     - bias tensor: \p CNNL_LAYOUT_ARRAY.
 *     - estimated_mean tensor: \p CNNL_LAYOUT_ARRAY.
 *     - estimated_var tensor: \p CNNL_LAYOUT_ARRAY.
 *     - z tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 * - The layout of the z tensor should be the same as the x tensor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the batchnorm inference operation is as follows:
     @verbatim
      input five arrays by 1 * 2 * 3 * 2, 2, 2, 2 and 2
      --> x: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> filter: [0.5, 0.5]

      --> bias: [1.0, 1.0]

      --> estimated_mean: [0.5, 0.5]

      --> estimated_var: [1.0, 1.0]

      param:
        eps: 0.00001

      output array by 1 * 2 * 3 * 2
      --> z: [[[[1.25, 1.25],[1.25, 1.25],[1.25, 1.25]],
               [[1.25, 1.25],[1.25, 1.25],[1.25, 1.25]]]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d
 * - https://arxiv.org/abs/1502.03167
 * - Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,
 *   Sergey Ioffe, 2015.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlBatchNormForwardInference(cnnlHandle_t handle,
                              const void *alpha,
                              const void *beta,
                              const cnnlTensorDescriptor_t x_desc,
                              const void *x,
                              const cnnlTensorDescriptor_t filter_bias_mean_var_desc,
                              const void *filter,
                              const void *bias,
                              const void *estimated_mean,
                              const void *estimated_var,
                              float eps,
                              const cnnlTensorDescriptor_t z_desc,
                              void *z);
// Group:BatchNormForwardInferenceV2
/*!
 * @brief Applies Batch Normalization with Relu activation for each channel across a batch of data
 * in the inference scenario. Compared with ::cnnlBatchNormForwardInference, this function allows
 * you to choose whether to perform activation operations after the batch normalization when
 * \p activation_desc is not Null.
 *
 * Batch Normalization is used in artificial intelligence, including but not limited to
 * ResNet (Deep Residual Network), Yolo (You Only Look Once) and R-CNN (Regions with CNN features).
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the batchnorm
 *   forward inference operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] activation_desc
 *   Input. The descriptor of the activation operation. For detailed information,
 *   see ::cnnlActivationDescriptor_t.
 * @param[in] mode
 *   Input. The mode of normalization. The \p mode must be \p CNNL_BATCHNORM_SPATIAL now.
 *   For detailed information, see ::cnnlBatchNormMode_t enum.
 * @param[in] bnops
 *   Input. The method of activation. The supported combinations of \p bnops and
 *   \p activation_desc->mode are:
 *   - \p bnops is \p CNNL_BATCHNORM_OPS_BN and \p activation_desc->mode is
 *     \p CNNL_ACTIVATION_IDENTITY;
 *   - \p bnops is \p CNNL_BATCHNORM_OPS_BN_ACTIVATION and \p activation_desc->mode is
 *     \p CNNL_ACTIVATION_RELU;
 *   - \p bnops is \p CNNL_BATCHNORM_OPS_BN_ADD_ACTIVATION, \p activation_desc->mode is
 *     \p CNNL_ACTIVATION_RELU or \p CNNL_ACTIVATION_IDENTITY.
 *   For detailed information, see ::cnnlBatchNormOps_t enum.
 * @param[in] alpha
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] beta
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] x_desc
 *   The descriptor of the input tensor \p x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \p x.
 * @param[in] filter_bias_mean_var_desc
 *   The descriptor of the \p filter, \p bias, \p estimated_mean and \p estimated_var tensor. For
 *   detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the input tensor \p filter. The value of this
 *   pointer can be NULL.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the input tensor \p bias. The value of this
 *   pointer can be NULL.
 * @param[in] y_desc
 *   Input. The descriptor of the input tensor \p y. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 *   Reserved for future use. Set the value to NULL.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the input tensor \p y.
 *   The optional \p y_desc and \p y are only used when \p bnops is
 *   \p CNNL_BATCHNORM_OPS_BN_ADD_ACTIVATION, otherwise should set it to NULL. When in use,
 *   \p y should have exactly the same dimensions as \p x and the final output \p z.
 * @param[in] estimated_mean
 *   Input. Pointer to the MLU memory that stores the tensor \p estimated_mean, accumulated as \p
 *   moving_mean during the training phase from the ::cnnlBatchNormForwardTraining call.
 * @param[in] estimated_var
 *   Input. Pointer to the MLU memory that stores the tensor \p estimated_var, accumulated as \p
 *   moving_var during the training phase from the ::cnnlBatchNormForwardTraining call.
 * @param[in] eps
 *   Input. A float value added to the denominator for numerical stability.
 * @param[in] z_desc
 *   The descriptor of the batch normalization output tensor \p z. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] z
 *   Output. Pointer to the MLU memory that stores the output tensor \p z.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "BatchNormForwardInference Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - By the order of \p x tensor - \p filter tensor - \p bias tensor - \p y tensor -
 *   \p estimated_mean tensor - \p estimated_var tensor - \p z tensor, the supported
 *   combinations of data types of this function are as follows:
 *   - half - half - half - half - half - half - half
 *   - float - float - float - float - float - float - float
 *   - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16
 *   - half - float - float - half - float - float - half
 *   - bfloat16 - float - float - bfloat16 - float - float - bfloat16
 *   - uint8 - half - half - NULL - half - half - half
 *   - uint8 - bfloat16 - bfloat16 - NULL - bfloat16 - bfloat16 - bfloat16
 *   - uint8 - float - float - NULL - float - float - float
 *   - uint8 - float - float - NULL - float - float - half
 *   - uint8 - float - float - NULL - float - float - bfloat16
 *
 *   bfloat16 is only supported on MLU500 series.
 *
 * @par Data Layout
 * - The supported data layouts of the \p x tensor, \p filter tensor, \p bias tensor, \p y tensor,
 *   \p estimated_mean tensor, \p estimated_var tensor and \p z tensor are as follows:
 *    - x tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *    - filter tensor: \p CNNL_LAYOUT_ARRAY.
 *    - bias tensor: \p CNNL_LAYOUT_ARRAY.
 *    - y tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *    - estimated_mean tensor: \p CNNL_LAYOUT_ARRAY.
 *    - estimated_var tensor: \p CNNL_LAYOUT_ARRAY.
 *    - z tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 * - The layout of the z tensor should be the same as the x tensor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the batchnorm inference operation is as follows:
     @verbatim
      input five arrays by 1 * 2 * 3 * 2, 2, 2, 2 and 2
      --> x: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> filter: [0.5, 0.5]

      --> bias: [1.0, 1.0]

      --> estimated_mean: [0.5, 0.5]

      --> estimated_var: [1.0, 1.0]

      param:
        eps: 0.00001
        bnops: CNNL_BATCHNORM_OPS_BN_ACTIVATION
        mode: CNNL_ACTIVATION_RELU

      output array by 1 * 2 * 3 * 2
      --> z: [[[[1.25, 1.25],[1.25, 1.25],[1.25, 1.25]],
               [[1.25, 1.25],[1.25, 1.25],[1.25, 1.25]]]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d
 * - https://arxiv.org/abs/1502.03167
 * - Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,
 *   Sergey Ioffe, 2015.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlBatchNormForwardInferenceV2(cnnlHandle_t handle,
                                const cnnlActivationDescriptor_t activation_desc,
                                const cnnlBatchNormMode_t mode,
                                const cnnlBatchNormOps_t bnops,
                                const void *alpha,
                                const void *beta,
                                const cnnlTensorDescriptor_t x_desc,
                                const void *x,
                                const cnnlTensorDescriptor_t filter_bias_mean_var_desc,
                                const void *filter,
                                const void *bias,
                                const cnnlTensorDescriptor_t y_desc,
                                const void *y,
                                const void *estimated_mean,
                                const void *estimated_var,
                                float eps,
                                const cnnlTensorDescriptor_t z_desc,
                                void *z);

// Group:SyncBatchNormStats
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the sync_batchnorm_stats operation.
 *
 * The size of the extra workspace is based on the given information of the sync_batchnorm_stats
 * operation, including the input tensor descriptor \p x_desc.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   sync_batchnorm_stats operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   sync_batchnorm_stats operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - This API is only used along with ::cnnlSyncBatchNormStats_v2. ::cnnlSyncBatchNormStats does
 *   not require this API.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetSyncBatchNormStatsWorkspaceSize(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t x_desc,
                                       size_t *workspace_size);

// Group:SyncBatchNormStats
/*!
 * @brief Computes the local mean and the local inverse standard deviation for each channel
 * across a batch of data in the training scenario.
 *
 * cnnlSyncBatchNormStats_v2 is used in convolution network, including but not limited to
 * ResNet (Deep Residual Network), Yolo (You Only Look Once) and R-CNN (Regions with CNN features).
 *
 * Compared with ::cnnlSyncBatchNormStats, this function allows you to allocate some extra
 * workspace as an input parameter. If you just set \p workspace to NULL and \p workspace_size
 * to 0, this function will perform as same as ::cnnlSyncBatchNormStats.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   sync_batchnorm_stats operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor \p x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \p x.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for
 *   ::cnnlSyncBatchNormStats_v2.
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the ::cnnlSyncBatchNormStats_v2. You can get the size of the workspace with
 *   the ::cnnlGetSyncBatchNormStatsWorkspaceSize function.
 * @param[in] eps
 *   Input. A float value added to the denominator for numerical stability.
 * @param[in] mean_desc
 *   Input. The descriptor of the output tensor \p mean. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] mean
 *   Output. Pointer to the MLU memory that stores the output tensor \p mean, which is the
 *   local mean.
 * @param[in] invstd_desc
 *   Input. The descriptor of the output tensor \p invstd. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] invstd
 *   Output. Pointer to the MLU memory that stores the output tensor \p invstd, which is the
 *   local inverse standard deviation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "SyncBatchnormStats Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *   x - eps - mean - invstd.
 *   - float - float - float - float.
 *   - half - float - float - float.
 *   - bfloat16 - float - float - float.
 * - The data type bfloat16 is only supported on MLU500 series.
 *
 * @par Data Layout
 * - The supported data layouts of the input tensors are shown as follows:
 *   - x: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 * - The layout of the output tensors are shown as follows:
 *   - mean tensor: \p CNNL_LAYOUT_ARRAY.
 *   - invstd tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - On MLU300 and eariler hardware platforms, both ::cnnlSyncBatchNormStats_v2 and
 *   ::cnnlSyncBatchNormStats can be used. On MLU500 series,
 *   ::cnnlSyncBatchNormStats_v2 should be used instead of ::cnnlSyncBatchNormStats.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the sync_batch_norm_stats operation is as follows:
     @verbatim
      input five arrays by 1 * 2 * 3 * 2
      --> x: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]
      param:
        eps: 0.00001
      output an array by 2
      --> mean: [1.0, 1.0]
      --> invstd: [316.221, 316.221]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/1.6.0/jit_builtin_functions.html?highlight=batch_norm_stats
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlSyncBatchNormStats_v2(cnnlHandle_t handle,
                                                    const cnnlTensorDescriptor_t x_desc,
                                                    const void *x,
                                                    void *workspace,
                                                    size_t workspace_size,
                                                    const float eps,
                                                    const cnnlTensorDescriptor_t mean_desc,
                                                    void *mean,
                                                    const cnnlTensorDescriptor_t invstd_desc,
                                                    void *invstd);

// Group:SyncBatchNormStats
/*!
 * @brief Computes the local mean and the local inverse standard deviation for each channel
 * across a batch of data in the training scenario.
 *
 * SyncBatchnormStats is used in CNN, including but not limited to
 * ResNet (Deep Residual Network), Yolo (You Only Look Once) and R-CNN (Regions with CNN features).
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlSyncBatchNormStats_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   SyncBatchnormStats operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor \p x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \p x.
 * @param[in] eps
 *   Input. A float value added to the denominator for numerical stability.
 * @param[in] mean_desc
 *   Input. The descriptor of the output tensor \p mean. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] mean
 *   Output. Pointer to the MLU memory that stores the output tensor \p mean, which is the
 *   local mean.
 * @param[in] invstd_desc
 *   Input. The descriptor of the output tensor \p invstd. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] invstd
 *   Output. Pointer to the MLU memory that stores the output tensor \p invstd, which is the
 *   local inverse standard deviation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "SyncBatchnormStats Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *   x - eps - mean - invstd.
 *   - float - float - float - float.
 *   - half - float - float - float.
 *   - bfloat16 - float - float - float.
 * - The data type bfloat16 is only supported on MLU500 series.
 *
 * @par Data Layout
 * - The supported data layouts of the input tensors are shown as follows:
 *   - x: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 * - The layout of the output tensors are shown as follows:
 *   - mean tensor: \p CNNL_LAYOUT_ARRAY.
 *   - invstd tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - On MLU500 series, ::cnnlSyncBatchNormStats cannot be used and you should
 *   use ::cnnlSyncBatchNormStats_v2 instead.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the sync_batch_norm_stats operation is as follows:
     @verbatim
      input five arrays by 1 * 2 * 3 * 2
      --> x: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]
      param:
        eps: 0.00001
      output an array by 2
      --> mean: [1.0, 1.0]
      --> invstd: [316.221, 316.221]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/1.6.0/jit_builtin_functions.html?highlight=batch_norm_stats
 *
 */
CNNL_DEPRECATED_FOR(cnnlSyncBatchNormStats_v2)
cnnlStatus_t CNNL_WIN_API cnnlSyncBatchNormStats(cnnlHandle_t handle,
                                                 const cnnlTensorDescriptor_t x_desc,
                                                 const void *x,
                                                 const float eps,
                                                 const cnnlTensorDescriptor_t mean_desc,
                                                 void *mean,
                                                 const cnnlTensorDescriptor_t invstd_desc,
                                                 void *invstd);

// Group:SyncBatchNormGatherStatsWithCounts
/*!
 * @brief Computes the global mean and the global inverse standard deviation across aggragation
 * of the local mean and local inverse standard deviation of multiple MLU devices.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   sync_batch_norm_gather_stats_with_counts operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] mean_all_desc
 *   Input. The descriptor of the input tensor \p mean_all. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] mean_all
 *   Input. Pointer to the MLU memory that stores the input tensor tensor \p mean_all, which is
 *   the local mean of multiple MLU devices.
 * @param[in] invstd_all_desc
 *   Input. The descriptor of the input tensor \p invstd_all. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] invstd_all
 *   Input. Pointer to the MLU memory that stores the input tensor tensor \n invstd_all, which
 *   is the local inverse standard deviation of multiple MLU devices.
 * @param[in] moving_mean_desc
 *   Input. The descriptor of the input tensor \p moving_mean. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in,out] moving_mean
 *   Input and Output. Pointer to the MLU memory that stores the input tensor \p moving_mean,
 *   which is the moving average of mean computed over the dimensions of the input tensor
 *   \p mean_all. The value of this pointer can be NULL.
 * @param[in] moving_var_desc
 *   Input. The descriptor of the input tensor \p moving_var. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in,out] moving_var
 *   Input and Output. Pointer to the MLU memory that stores the tensor \p moving_var, which is
 *   the moving average of inverse standard deviation computed over the dimensions of the input
 *   tensor \p invstd_all. The value of this pointer can be NULL.
 * @param[in] momentum
 *   Input. A float value used to do moving average of \p moving_mean and \p moving_var.
 * @param[in] eps
 *   Input. A float value added to the denominator for numerical stability.
 * @param[in] count_all_desc
 *   Input. The descriptor of the input tensor \p count_all. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] count_all
 *   Input. Pointer to the MLU memory that stores an array, which stores the total size of
 *   dimensions (except C dimension) of input for each MLU device.
 * @param[in] mean_desc
 *   Input. The descriptor of the output tensor \p mean. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] mean
 *   Output. Pointer to the MLU memory that stores the output tensor \p mean, which is the
 *   global mean.
 * @param[in] invstd_desc
 *   Input. The descriptor of the output tensor \p invstd. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] invstd
 *   Output. Pointer to the MLU memory that stores the output tensor \p invstd, which is the
 *   global inverse standard deviation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "SyncBatchnormGatherStatsWithCounts Operator" section in "Cambricon CNNL User Guide"
 *   for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown as the following order:
 *   mean_all - invstd_all - moving_mean - moving_var - momentum -  eps  - count_all - mean  - invstd.
 *   -  float  -   float   -   float     -  float     -  float   - float - float     - float -  float.
 *   -  float  -   float   -   half      -  half      -  float   - float - half      - float -  float.
 *   -  float  -   float   -   bfloat16  -  bfloat16  -  float   - float - bfloat16  - float -  float.
 * - The data type bfloat16 is only supported on MLU500 series.
 * - \p moving_mean in the input and output should have the same data type.
 * - \p moving_var in the input and output should have the same data type.
 *
 * @par Data Layout
 * - The supported data layouts of the input tensors are shown as follows:
 *   - mean_all tensor: \p CNNL_LAYOUT_NC.
 *   - invstd_all tensor: \p CNNL_LAYOUT_NC.
 *   - moving_mean tensor: \p CNNL_LAYOUT_ARRAY.
 *   - moving_var tensor: \p CNNL_LAYOUT_ARRAY.
 *   - momentum: Scalar.
 *   - eps: Scalar.
 *   - count_all tensor: \p CNNL_LAYOUT_ARRAY.
 * - The layouts of the output tensors are shown as the following:
 *   - mean tensor: \p CNNL_LAYOUT_ARRAY.
 *   - invstd tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - The input \p mean_all and the input \p invstd_all cannot be positive infinity or negative infinity
 *   at the same time on MLU300 series or above.
 * - \p mean_all, \p invstd_all, \p moving_mean, \p moving_var, \p mean and \p invstd support the mode of stride.
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the sync_batch_norm_gather_stats_with_counts operation is as follows:
     @verbatim
      --> mean_all: an array [8, 1024];
      --> invstd_all: an array [8, 1024];
      --> moving_mean: an array [1024];
      --> moving_var: an array [1024];
      --> count_all: an array [8];
      param:
      --> momentum: 0.1
      --> eps: 0.00001
      output:
      --> mean: an array [1024];
      --> invstd: [1024];
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/1.6.0/jit_builtin_functions.html?highlight=batch_norm_stats
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlSyncBatchNormGatherStatsWithCounts(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t mean_all_desc,
                                       const void *mean_all,
                                       const cnnlTensorDescriptor_t invstd_all_desc,
                                       const void *invstd_all,
                                       const cnnlTensorDescriptor_t moving_mean_desc,
                                       void *moving_mean,
                                       const cnnlTensorDescriptor_t moving_var_desc,
                                       void *moving_var,
                                       float momentum,
                                       float eps,
                                       const cnnlTensorDescriptor_t count_all_desc,
                                       const void *count_all,
                                       const cnnlTensorDescriptor_t mean_desc,
                                       void *mean,
                                       const cnnlTensorDescriptor_t invstd_desc,
                                       void *invstd);
// Group:SyncBatchNormElemt
/*!
 * @brief Applies Batch Normalization for each channel across a batch of data with the given mean,
 *        inverse variance and scaling factors.
 *
 * Batch Normalization is used in artificial intelligence, including but not limited to
 * ResNet (Deep Residual Network), Yolo (You Only Look Once) and R-CNN (Regions with CNN features).
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the sync batchnorm
 *   element operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   The descriptor of the input tensor \p x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \p x.
 * @param[in] mean_desc
 *   The descriptor of \p mean tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] mean
 *   Input. Pointer to the MLU memory that stores the tensor \p mean, which is computed over the
 *   batch and spatial dimensions by ::cnnlSyncBatchNormGatherStatsWithCounts.
 * @param[in] invstd_desc
 *   The descriptor of \p invstd tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] invstd
 *   Input. Pointer to the MLU memory that stores the tensor \p invstd, which is the inverse variance
 *   computed over the batch and spatial dimensions by ::cnnlSyncBatchNormGatherStatsWithCounts.
 * @param[in] filter_desc
 *   The descriptor of \p filter tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 *   The descriptor can be NULL when \p filter pointer is NULL.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the input tensor \p filter for affine transformation
 *   after batch normalization. The value of this pointer can be NULL.
 * @param[in] bias_desc
 *   The descriptor of \p bias tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 *   The descriptor can be NULL when \p bias pointer is NULL.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the input tensor \p bias for affine transformation
 *   after batch normalization. The value of this pointer can be NULL.
 * @param[in] y_desc
 *   The descriptor of the sync batch normalization output tensor \p y. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor \p y.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "SyncBatchNormForward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *   x_tensor - mean_tensor - invstd_tensor - filter_tensor - bias_tensor - y_tensor.
 *   - float - float - float - float - float - float.
 *   - half - float - float - float - float - half.
 *   - bfloat16 - float - float - float - float - bfloat16.
 * - The data type bfloat16 is only supported on MLU500 series.
 *
 * @par Data Layout
 * - The supported data layouts of \p x, \p mean, \p invstd, \p filter, \p bias and \p y are as follows:
 *   - x tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - mean tensor: \p CNNL_LAYOUT_ARRAY.
 *   - invstd tensor: \p CNNL_LAYOUT_ARRAY.
 *   - filter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - bias tensor: \p CNNL_LAYOUT_ARRAY.
 *   - y tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *     The layout of the \p y should be the same as \p x tensor.
 *
 * @note
 * - The \p mean, \p invstd, \p filter and \p bias must be 1D tensors and the length of their dimensions
 *   should be the same as the length of the lowest dimension of \p x.
 * - The length of each dimension of \p x and \p y must be the same.
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the sync batchnorm element operation is as follows:
     @verbatim
      input five arrays by 1 * 2 * 3 * 2, 2, 2, 2 and 2
      --> x: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> mean: [0.5, 0.5]

      --> invstd: [2.0, 2.0]

      --> filter: [0.5, 0.5]

      --> bias: [1.0, 1.0]

      output array by 1 * 2 * 3 * 2
      --> y: [[[[1.5, 1.5],[1.5, 1.5],[1.5, 1.5]],
               [[1.5, 1.5],[1.5, 1.5],[1.5, 1.5]]]]
     @endverbatim
 *
 * @par Reference
 * - Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,
 *   Sergey Ioffe, 2015.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlSyncBatchNormElemt(cnnlHandle_t handle,
                                                 const cnnlTensorDescriptor_t x_desc,
                                                 const void *x,
                                                 const cnnlTensorDescriptor_t mean_desc,
                                                 const void *mean,
                                                 const cnnlTensorDescriptor_t invstd_desc,
                                                 const void *invstd,
                                                 const cnnlTensorDescriptor_t filter_desc,
                                                 const void *filter,
                                                 const cnnlTensorDescriptor_t bias_desc,
                                                 const void *bias,
                                                 const cnnlTensorDescriptor_t y_desc,
                                                 void *y);

// Group:BatchNormBackward
/*!
 * @brief Applies to compute gradients of batch normalization for the training scenario.
 *
 * Batch Normalization is used in artificial intelligence, including but not limited to
 * ResNet (Deep Residual Network), Yolo (You Only Look Once) and R-CNN (Regions with CNN features).
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlBatchNormBackward_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the batchnorm
 *   backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] alpha_data_diff, beta_data_diff
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] alpha_param_diff, beta_param_diff
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] x_desc
 *   The descriptor of the input tensor \p x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \p x.
 * @param[in] diff_z_desc
 *   The descriptor of the backpropagated differential tensor \p diff_z. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] diff_z
 *   Input. Pointer to the MLU memory that stores the input tensor \p diff_z.
 * @param[in] filter_bias_mean_var_desc
 *   The descriptor of the \p filter, \p saved_invstd, \p saved_mean, \p diff_filter and \p
 *   diff_bias tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the input tensor \p filter.
 * @param[in] saved_mean
 *   Input. Pointer to the MLU memory that stores the input tensor \p saved_mean, computed during
 *   the forward phase from the ::cnnlBatchNormForwardTrainingV2 call. The value of this pointer can
 *   be NULL.
 * @param[in] saved_invstd
 *   Input. Pointer to the MLU memory that stores the input tensor \p saved_invstd, computed during
 *   the forward phase from the ::cnnlBatchNormForwardTrainingV2 call. The value of this pointer can
 *   be NULL.
 * @param[in] eps
 *   Input. A float value added to the denominator for numerical stability.
 * @param[in] diff_x_desc
 *   The descriptor of the result differential tensor \p diff_x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the output tensor \p diff_x.
 * @param[out] diff_filter
 *   Output. Pointer to the MLU memory that stores the output tensor \p diff_filter.
 * @param[out] diff_bias
 *   Output. Pointer to the MLU memory that stores the output tensor \p diff_bias.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "BatchNormBackward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *   input_tensor - diff_z_tensor - filter_tensor - saved_mean_tensor - saved_invstd_tensor -
 *   output_tensor - diff_filter_tensor - diff_bias_tensor.
 *   - half - half - half - half - half - half - half - half.
 *   - float - float - float - float - float - float - float - float.
 *   - half - half - float - float - float - half - float - float.
 *
 * @par Data Layout
 * - The supported data layouts of the input tensor, diff_z tensor, filter tensor, saved_mean
 *   tensor, saved_invstd tensor, output tensor, diff_filter tensor and diff_bias tensor are
 *   as follows:
 *   - input tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - diff_z tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *     The layout of the diff_z tensor should be the same as input tensor.
 *   - filter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - saved_mean tensor: \p CNNL_LAYOUT_ARRAY.
 *   - saved_invstd tensor: \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *     The layout of the output tensor should be the same as input tensor.
 *   - diff_filter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - diff_bias tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - On MLU500 series, you can use only ::cnnlBatchNormBackward_v2
 *   to perform the backward propagation of the batch normalization computation.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the batchnorm backward operation is as follows:
     @verbatim
      input five arrays by 1 * 2 * 3 * 2, 1 * 2 * 3 * 2, 2, 2 and 2
      --> x: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> diff_z: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
                    [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> filter: [0.5, 0.5]

      --> saved_mean: [1.0,1.0]

      --> saved_invstd: [0.0,0.0]

      param:
        eps: 0.00001

      output three arrays by 1 * 2 * 3 * 2, 2 and 2
      --> diff_x: [[[[0.0, 0.0],[0.0, 0.0],[0.0, 0.0]],
                    [[0.0, 0.0],[0.0, 0.0],[0.0, 0.0]]]]

      --> diff_filter: [0.0, 0.0]

      --> diff_bias: [12.0, 12.0]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d
 * - https://arxiv.org/abs/1502.03167
 * - Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,
 *   Sergey Ioffe, 2015.
 *
 */
CNNL_DEPRECATED_FOR(cnnlBatchNormBackward_v2)
cnnlStatus_t CNNL_WIN_API
cnnlBatchNormBackward(cnnlHandle_t handle,
                      const void *alpha_data_diff,
                      const void *beta_data_diff,
                      const void *alpha_param_diff,
                      const void *beta_param_diff,
                      const cnnlTensorDescriptor_t x_desc,
                      const void *x,
                      const cnnlTensorDescriptor_t diff_z_desc,
                      const void *diff_z,
                      const cnnlTensorDescriptor_t filter_bias_mean_var_desc,
                      const void *filter,
                      const void *saved_mean,
                      const void *saved_invstd,
                      float eps,
                      const cnnlTensorDescriptor_t diff_x_desc,
                      void *diff_x,
                      void *diff_filter,
                      void *diff_bias);

// Group:BatchNormBackward
/*!
 * @brief Returns in \p size the size of the MLU memory that is used to get
 *        extra space size in BatchNormBackward operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the BatchNormBackward operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of input tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is
 *   used in the BatchNormBackward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetBatchNormBackwardWorkspaceSize(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t x_desc,
                                      size_t *workspace_size);

// Group:BatchNormBackwardV2
/*!
 * @brief Applies to compute gradients of batch normalization for the training scenario.
 *
 * Batch Normalization is used in convolution network, including but not limited to
 * ResNet (Deep Residual Network), Yolo (You Only Look Once) and R-CNN (Regions with CNN features).
 * Compared with ::cnnlBatchNormBackward, this function can fuse add operations and relu operations.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlBatchNormBackward_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the batchnorm
 *   backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] activation_desc
 *   Input. The descriptor of the activation operation. For detailed information,
 *   see ::cnnlActivationDescriptor_t.
 * @param[in] mode
 *   Input. The mode of normalization. The \p mode must be \p CNNL_BATCHNORM_SPATIAL now.
 *   For detailed information, see ::cnnlBatchNormMode_t enum.
 * @param[in] bnOps
 *   Input. The method of activation. The supported combinations of \p bnOps and
 *   \p activation_desc->mode are:
 *   - \p bnOps is \p CNNL_BATCHNORM_OPS_BN and \p activation_desc->mode is
 *     \p CNNL_ACTIVATION_IDENTITY;
 *   - \p bnOps is \p CNNL_BATCHNORM_OPS_BN_ACTIVATION and \p activation_desc->mode is
 *     \p CNNL_ACTIVATION_RELU;
 *   - \p bnOps is \p CNNL_BATCHNORM_OPS_BN_ADD_ACTIVATION, \p activation_desc->mode is
 *     \p CNNL_ACTIVATION_RELU or \p CNNL_ACTIVATION_IDENTITY.
 *   For detailed information, see ::cnnlBatchNormOps_t enum.
 * @param[in] alpha_data_diff, beta_data_diff
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] alpha_param_diff, beta_param_diff
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] x_desc
 *   The descriptor of the input tensor \p x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \p x.
 * @param[in] y_desc
 *   Input. The descriptor of the input tensor \p y.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the input tensor \p y which is the output of
 *   cnnlBatchNormForwardTraining API. The optional \p y_desc and \p y are only used when \p bnOps is
 *   \p CNNL_BATCHNORM_OPS_BN_ADD_ACTIVATION or \p CNNL_BATCHNORM_OPS_BN_ACTIVATION, otherwise should
 *   set it to NULL. When in use, \p y should have exactly the same dimensions as \p x.
 * @param[in] diff_y_desc
 *   The descriptor of the backpropagated differential tensor \p diff_y. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] diff_y
 *   Input. Pointer to the MLU memory that stores the input tensor \p diff_y.
 * @param[in] filter_bias_mean_var_desc
 *   The descriptor of the \p filter, \p saved_invstd, \p saved_mean, \p diff_filter and \p
 *   diff_bias tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the input tensor \p filter.
 * @param[in] saved_mean
 *   Input. Pointer to the MLU memory that stores the input tensor \p saved_mean, computed during
 *   the forward phase from the ::cnnlBatchNormForwardTraining call. The value of this pointer can
 *   be NULL.
 * @param[in] saved_invstd
 *   Input. Pointer to the MLU memory that stores the input tensor \p saved_invstd, computed during
 *   the forward phase from the ::cnnlBatchNormForwardTraining call. The value of this pointer can
 *   be NULL.
 * @param[in] eps
 *   Input. A float value added to the denominator for numerical stability.
 * @param[in] diff_z_desc
 *   The descriptor of the result differential tensor \p diff_z. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] diff_z
 *   Output. Pointer to the MLU memory that stores the output tensor \p diff_z.
 *   The optional \p diff_z_desc and \p diff_z are only used when \p bnOps is
 *   \p CNNL_BATCHNORM_OPS_BN_ADD_ACTIVATION, otherwise should set it to NULL.
 *   When in use, \p diff_z should have exactly the same dimensions as \p x.
 * @param[in] diff_x_desc
 *   The descriptor of the result differential tensor \p diff_x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the output tensor \p diff_x.
 * @param[out] diff_filter
 *   Output. Pointer to the MLU memory that stores the output tensor \p diff_filter.
 * @param[out] diff_bias
 *   Output. Pointer to the MLU memory that stores the output tensor \p diff_bias.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlBatchNormBackwardV2. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the
 *   ::cnnlBatchNormBackwardV2. You can get the size of the workspace with the
 *   ::cnnlGetBatchNormBackwardWorkspaceSize function.
 * @param[in] reservespace
 *   Input. Reserved for future use. Set the value to NULL.
 * @param[in] reservespace_size
 *   Input. Reserved for future use. Set the value to 0.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "BatchNormBackward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *   x_tensor - y_tensor - diff_y_tensor - filter_tensor - saved_mean_tensor -
 *   saved_invstd_tensor - diff_z_tensor - diff_x_tensor - diff_filter_tensor - diff_bias_tensor.
 *   - half - half - half - half - half - half - half - half - half - half
 *   - float - float - float - float - float - float - float - float - float - float
 *   - half - half - half - float - float - float - half - half - float - float
 *
 * @par Data Layout
 * - The supported data layouts of the \p x tensor, \p y tensor, \p diff_y tensor, \p filter tensor,
 *   \p saved_mean tensor, \p saved_invstd tensor, \p diff_z tensor, \p diff_x tensor, \p diff_filter
 *   tensor and \p diff_bias tensor are as follows:
 *   - x tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - y tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - diff_y tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *     The layout of the diff_z tensor should be the same as input tensor.
 *   - filter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - saved_mean tensor: \p CNNL_LAYOUT_ARRAY.
 *   - saved_invstd tensor: \p CNNL_LAYOUT_ARRAY.
 *   - diff_z tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - diff_x tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *     The layout of the output tensor should be the same as input tensor.
 *   - diff_filter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - diff_bias tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the BatchNormBackward operation is as follows:
     @verbatim
      input five arrays by 1 * 2 * 3 * 2, 1 * 2 * 3 * 2, 2, 2 and 2
      --> x: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> y: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> diff_y: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
                    [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> filter: [0.5, 0.5]

      --> saved_mean: [1.0,1.0]

      --> saved_invstd: [0.0,0.0]

      param:
        eps: 0.00001

      output three arrays by 1 * 2 * 3 * 2, 2 and 2
      --> diff_z: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
                    [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> diff_x: [[[[0.0, 0.0],[0.0, 0.0],[0.0, 0.0]],
                    [[0.0, 0.0],[0.0, 0.0],[0.0, 0.0]]]]

      --> diff_filter: [0.0, 0.0]

      --> diff_bias: [12.0, 12.0]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d
 * - https://arxiv.org/abs/1502.03167
 * - Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,
 *   Sergey Ioffe, 2015.
 *
 */
CNNL_DEPRECATED_FOR(cnnlBatchNormBackward_v2)
cnnlStatus_t CNNL_WIN_API
cnnlBatchNormBackwardV2(cnnlHandle_t handle,
                        const cnnlActivationDescriptor_t activation_desc,
                        const cnnlBatchNormMode_t mode,
                        const cnnlBatchNormOps_t bnOps,
                        const void *alpha_data_diff,
                        const void *beta_data_diff,
                        const void *alpha_param_diff,
                        const void *beta_param_diff,
                        const cnnlTensorDescriptor_t x_desc,
                        const void *x,
                        const cnnlTensorDescriptor_t y_desc,
                        const void *y,
                        const cnnlTensorDescriptor_t diff_y_desc,
                        const void *diff_y,
                        const cnnlTensorDescriptor_t filter_bias_mean_var_desc,
                        const void *filter,
                        const void *saved_mean,
                        const void *saved_invstd,
                        float eps,
                        const cnnlTensorDescriptor_t diff_z_desc,
                        void *diff_z,
                        const cnnlTensorDescriptor_t diff_x_desc,
                        void *diff_x,
                        void *diff_filter,
                        void *diff_bias,
                        void *workspace,
                        size_t workspace_size,
                        void *reservespace,
                        size_t reservespace_size);

// Group:BatchNormBackward
/*!
 * @brief Applies to compute gradients of batch normalization for the training scenario.
 *
 * Batch Normalization is used in convolution network, including but not limited to
 * ResNet (Deep Residual Network), Yolo (You Only Look Once) and R-CNN (Regions with CNN features).
 * Compared with ::cnnlBatchNormBackward, this function can fuse add operations and relu operations.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the batchnorm
 *   backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] activation_desc
 *   Input. The descriptor of the activation operation. For detailed information,
 *   see ::cnnlActivationDescriptor_t.
 * @param[in] mode
 *   Input. The mode of normalization. The \p mode must be \p CNNL_BATCHNORM_SPATIAL now.
 *   For detailed information, see ::cnnlBatchNormMode_t enum.
 * @param[in] bnOps
 *   Input. The method of activation. The supported combinations of \p bnOps and
 *   \p activation_desc->mode are:
 *   - \p bnOps is \p CNNL_BATCHNORM_OPS_BN and \p activation_desc->mode is
 *     \p CNNL_ACTIVATION_IDENTITY;
 *   - \p bnOps is \p CNNL_BATCHNORM_OPS_BN_ACTIVATION and \p activation_desc->mode is
 *     \p CNNL_ACTIVATION_RELU;
 *   - \p bnOps is \p CNNL_BATCHNORM_OPS_BN_ADD_ACTIVATION, \p activation_desc->mode is
 *     \p CNNL_ACTIVATION_RELU or \p CNNL_ACTIVATION_IDENTITY.
 *   For detailed information, see ::cnnlBatchNormOps_t enum.
 * @param[in] alpha_data_diff, beta_data_diff
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] alpha_param_diff, beta_param_diff
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] x_desc
 *   The descriptor of the input tensor \p x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \p x.
 * @param[in] y_desc
 *   Input. The descriptor of the input tensor \p y.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the input tensor \p y which is the output of
 *   cnnlBatchNormForwardTraining API. The optional \p y_desc and \p y are only used when \p bnOps is
 *   \p CNNL_BATCHNORM_OPS_BN_ADD_ACTIVATION or \p CNNL_BATCHNORM_OPS_BN_ACTIVATION, otherwise should
 *   set it to NULL. When in use, \p y should have exactly the same dimensions as \p x.
 * @param[in] diff_y_desc
 *   The descriptor of the backpropagated differential tensor \p diff_y. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] diff_y
 *   Input. Pointer to the MLU memory that stores the input tensor \p diff_y.
 * @param[in] filter_bias_mean_var_desc
 *   The descriptor of the \p filter, \p saved_invstd, \p saved_mean, \p diff_filter and \p
 *   diff_bias tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the input tensor \p filter.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the input tensor \p bias.
 *   Reserved for future use. Now must set the value to NULL.
 * @param[in] saved_mean
 *   Input. Pointer to the MLU memory that stores the input tensor \p saved_mean, computed during
 *   the forward phase from the ::cnnlBatchNormForwardTraining_v2 call. The value of this pointer can
 *   be NULL.
 * @param[in] saved_invstd
 *   Input. Pointer to the MLU memory that stores the input tensor \p saved_invstd, computed during
 *   the forward phase from the ::cnnlBatchNormForwardTraining_v2 call. The value of this pointer can
 *   be NULL.
 * @param[in] eps
 *   Input. A float value added to the denominator for numerical stability.
 * @param[in] diff_z_desc
 *   The descriptor of the result differential tensor \p diff_z. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] diff_z
 *   Output. Pointer to the MLU memory that stores the output tensor \p diff_z.
 *   The optional \p diff_z_desc and \p diff_z are only used when \p bnOps is
 *   \p CNNL_BATCHNORM_OPS_BN_ADD_ACTIVATION, otherwise should set it to NULL.
 *   When in use, \p diff_z should have exactly the same dimensions as \p x.
 * @param[in] diff_x_desc
 *   The descriptor of the result differential tensor \p diff_x. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the output tensor \p diff_x.
 * @param[out] diff_filter
 *   Output. Pointer to the MLU memory that stores the output tensor \p diff_filter.
 * @param[out] diff_bias
 *   Output. Pointer to the MLU memory that stores the output tensor \p diff_bias.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlBatchNormBackward_v2. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the
 *   ::cnnlBatchNormBackward_v2. You can get the size of the workspace with the
 *   ::cnnlGetBatchNormBackwardWorkspaceSize function.
 * @param[in] reservespace
 *   Input. Reserved for future use. Set the value to NULL.
 * @param[in] reservespace_size
 *   Input. Reserved for future use. Set the value to 0.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "BatchNormBackward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *   x_tensor - y_tensor - diff_y_tensor - filter_tensor - bias_tensor - saved_mean_tensor -
 *   saved_invstd_tensor - diff_z_tensor - diff_x_tensor - diff_filter_tensor - diff_bias_tensor.
 *   - half - half - half - half - half - half - half - half - half - half - half
 *   - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16
 *   - float - float - float - float - float - float - float - float - float - float - float
 *   - half - half - half - float - float - float - float - half - half - float - float
 *   - bfloat16 - bfloat16 - bfloat16 - float - float - float - float - bfloat16 - bfloat16 - float - float
 *
 *   bfloat16 is only supported on MLU500 Series.
 *
 * @par Data Layout
 * - The supported data layouts of the \p x tensor, \p y tensor, \p diff_y tensor, \p filter tensor,
 *   \p bias tensor, \p saved_mean tensor, \p saved_invstd tensor, \p diff_z tensor, \p diff_x tensor,
 *   \p diff_filter tensor and \p diff_bias tensor are as follows:
 *   - x tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - y tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - diff_y tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *     The layout of the diff_z tensor should be the same as input tensor.
 *   - filter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - bias tensor: \p CNNL_LAYOUT_ARRAY.
 *   - saved_mean tensor: \p CNNL_LAYOUT_ARRAY.
 *   - saved_invstd tensor: \p CNNL_LAYOUT_ARRAY.
 *   - diff_z tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - diff_x tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *     The layout of the output tensor should be the same as input tensor.
 *   - diff_filter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - diff_bias tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the BatchNormBackward operation is as follows:
     @verbatim
      input five arrays by 1 * 2 * 3 * 2, 1 * 2 * 3 * 2, 2, 2 and 2
      --> x: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> y: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> diff_y: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
                    [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> filter: [0.5, 0.5]

      --> saved_mean: [1.0,1.0]

      --> saved_invstd: [0.0,0.0]

      param:
        eps: 0.00001

      output three arrays by 1 * 2 * 3 * 2, 2 and 2
      --> diff_z: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
                    [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> diff_x: [[[[0.0, 0.0],[0.0, 0.0],[0.0, 0.0]],
                    [[0.0, 0.0],[0.0, 0.0],[0.0, 0.0]]]]

      --> diff_filter: [0.0, 0.0]

      --> diff_bias: [12.0, 12.0]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d
 * - https://arxiv.org/abs/1502.03167
 * - Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,
 *   Sergey Ioffe, 2015.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlBatchNormBackward_v2(cnnlHandle_t handle,
                         const cnnlActivationDescriptor_t activation_desc,
                         const cnnlBatchNormMode_t mode,
                         const cnnlBatchNormOps_t bnOps,
                         const void *alpha_data_diff,
                         const void *beta_data_diff,
                         const void *alpha_param_diff,
                         const void *beta_param_diff,
                         const cnnlTensorDescriptor_t x_desc,
                         const void *x,
                         const cnnlTensorDescriptor_t y_desc,
                         const void *y,
                         const cnnlTensorDescriptor_t diff_y_desc,
                         const void *diff_y,
                         const cnnlTensorDescriptor_t filter_bias_mean_var_desc,
                         const void *filter,
                         const void *bias,
                         const void *saved_mean,
                         const void *saved_invstd,
                         float eps,
                         const cnnlTensorDescriptor_t diff_z_desc,
                         void *diff_z,
                         const cnnlTensorDescriptor_t diff_x_desc,
                         void *diff_x,
                         void *diff_filter,
                         void *diff_bias,
                         void *workspace,
                         size_t workspace_size,
                         void *reservespace,
                         size_t reservespace_size);

// Group:SyncBatchnormBackwardReduce
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the sync_batchnorm_backward_reduce operation.
 *
 * The size of the extra workspace is based on the given information of the
 * sync_batchnorm_backward_reduce operation, including the input tensor descriptor \p desc_x.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the mse_loss
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] desc_x
 *   Input. The descriptor of the input tensor. For detailed information, see
 * ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   sync_batchnorm_backward_reduce operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - This API is only used along with ::cnnlSyncBatchnormBackwardReduce_v2.
 *    ::cnnlSyncBatchnormBackwardReduce does not require this API.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetSyncBatchnormBackwardReduceWorkspaceSize(cnnlHandle_t handle,
                                                const cnnlTensorDescriptor_t desc_x,
                                                size_t *workspace_size);

// Group:SyncBatchnormBackwardReduce
/*!
 * @brief Applies Syncronized Batch Normalization Reduce operator to backwardly compute grad
 * filters, grad bias, sum_dy and sum_dy_xmu on each MLU device.
 *
 * Batch Normalization is used in convolution network, including but not limited to
 * ResNet (Deep Residual Network), Yolo (You Only Look Once) and R-CNN (Regions with CNN features).
 *
 * Compared with ::cnnlSyncBatchnormBackwardReduce, this function allows you to allocate some extra
 * workspace as an input parameter. If you just set \p workspace to NULL and \p workspace_size to 0,
 * this function will perform as same as ::cnnlSyncBatchnormBackwardReduce.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   sync_batchnorm_backward_reduce operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] desc_dz
 *   The descriptor of the input tensor \p dz. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] dz
 *   Input. Pointer to the MLU memory that stores the tensor \p dz, which denotes the partial
 *   derivative of batch normalization forward output.
 * @param[in] desc_x
 *   The descriptor of the input tensor \p x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \p x.
 * @param[in] desc_mean
 *   The descriptor of \p mean tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] mean
 *   Input. Pointer to the MLU memory that stores the tensor \p mean, which denotes the average
 *   result of input \p x.
 * @param[in] desc_invstd
 *   The descriptor of \p invstd tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] invstd
 *   Input. Pointer to the MLU memory that stores the tensor \p invstd, which denotes the inversed
 *   standard deviation of input \p x.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlSyncBatchnormBackwardReduce_v2.
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   ::cnnlSyncBatchnormBackwardReduce_v2. You can get the size of the workspace with
 *   the ::cnnlGetSyncBatchnormBackwardReduceWorkspaceSize function.
 * @param[out] desc_dfilter
 *   The descriptor of \p dfilters tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] dfilter
 *   Output. Pointer to the MLU memory that stores the input tensor \p dfilters, which denotes
 *   partial derivative of filter in sync batch normalization forward training. It will be computed
 *   only if Booleanvariable \p needs_input_grad1 is true.
 * @param[out] desc_dbias
 *   The descriptor of the sync batch normalization output tensor \p dbias. For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @param[out] dbias
 *   Output. Pointer to the MLU memory that stores the output tensor \p dbias, which denotes partial
 *   derivative of bias in sync batch normalization forward training. It will be computed
 *   only if \p needs_input_grad2 is true.
 * @param[out] desc_sum_dy
 *   The descriptor of the sync batch normalization output tensor \p sum_dy. For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @param[out] sum_dy
 *   Output. Pointer to the MLU memory that stores the output tensor \p sum_dy, which denotes the
 *   summation of dz and is also an intermediate variable to compute the partial derivative of
 *   input x. Moreover, it will be computed only if Boolean variable \p needs_input_grad0 is true.
 * @param[out] desc_sum_dy_xmu
 *   The descriptor of the sync batch normalization output tensor \p sum_dy_xmu. For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @param[out] sum_dy_xmu
 *   Output. Pointer to the MLU memory that stores the output tensor \p sum_dy_xmu, which denotes
 *   sum{dz(x-mean)}. It is also an intermediate variable to compute the partial derivative of
 *   input \p x. Moreover, it will be computed only if Boolean variable \p needs_input_grad0 is
 *   true.
 * @param[in] needs_input_grad0
 *   Input. A Boolean variable that determines whether to compute \p sum_dy and \p sum_dy_xmu.
 *   When \p needs_input_grad0 is true, \p sum_dy and \p sum_dy_xmu will be computed.
 *   When \p needs_input_grad0 is false, \p sum_dy and \p sum_dy_xmu will be NULL.
 * @param[in] needs_input_grad1
 *   Input. A Boolean variable that determines whether to compute \p dfilters.
 *   When \p needs_input_grad1 is true, \p dfilters will be computed.
 *   When \p needs_input_grad1 is false, \p dfilter will be NULL.
 * @param[in] needs_input_grad2
 *   Input. A Boolean variable that determines whether to compute \p dbias.
 *   When \p needs_input_grad2 is true, \p dbias will be computed.
 *   When \p needs_input_grad2 is false, \p dbias will be NULL.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "SyncBatchNormBackwardReduce Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *   dz_tensor - x_tensor - mean_tensor - invstd_tensor - dfilter_tensor - dbias_tensor -
 *   sum_dy_tensor - sum_dy_xmu_tensor.
 *   - float - float - float - float - float - float - float - float.
 *   - half - half - float - float - float - float - float - float.
 *   - bfloat16 - bfloat16 - float - float - float - float - float - float.
 * - The data type bfloat16 is only supported on MLU500 series.
 *
 * @par Data Layout
 * - The supported data layouts of \p dz, \p x, \p mean, \p invstd, \p dfilter, \p dbias, \p sum_dy
 *   and \p sum_dy_xmu are as follows:
 *   - dz tensor: \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NLC, \p CNNL_LAYOUT_NC.
 *   - x tensor: \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NLC, \p CNNL_LAYOUT_NC.
 *   - mean tensor: \p CNNL_LAYOUT_ARRAY.
 *   - invstd tensor: \p CNNL_LAYOUT_ARRAY.
 *   - dfilter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - dbias tensor: \p CNNL_LAYOUT_ARRAY.
 *   - sum_dy tensor: \p CNNL_LAYOUT_ARRAY.
 *   - sum_dy_xmu tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - The \p mean, \p invstd, \p dfilter, \p bias, \p sum_dy and \p sum_dy_xmu must be 1D tensors
 *   and the length of the dimensions of these tensors should be the same as the length of
 *   the lowest dimension of \p x.
 * - The length of each dimension of \p x and \p dz must be the same.
 * - On MLU300 and eariler hardware platforms, both ::cnnlSyncBatchnormBackwardReduce_v2 and
 *   ::cnnlSyncBatchnormBackwardReduce can be used. On MLU500 series,
 *   ::cnnlSyncBatchnormBackwardReduce_v2 should be used instead of ::cnnlSyncBatchnormBackwardReduce.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the sync batchnorm element operation is as follows:
     @verbatim
      input four arrays by 1 * 2 * 3 * 2, 2, 2, 2 and 2
      --> dz: [[[[6.0, 6.0],[6.0, 6.0],[6.0, 6.0]],
               [[6.0, 6.0],[6.0, 6.0],[6.0, 6.0]]]]

      --> x: [[[[3.0, 3.0],[3.0, 3.0],[3.0, 3.0]],
               [[3.0, 3.0],[3.0, 3.0],[3.0, 3.0]]]]

      --> mean: [1, 1]

      --> invstd: [0.8, 0.8]

      output array by 2
      --> dfilter: [57.6, 57.6]

      --> dbias: [36.0, 36.0]

      --> sum_dy: [36.0, 36.0]

      --> sum_dy_xmu: [72.0, 72.0]
     @endverbatim
 *
 * @par Reference
 * - Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,
 *   Sergey Ioffe, 2015.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlSyncBatchnormBackwardReduce_v2(cnnlHandle_t handle,
                                   const cnnlTensorDescriptor_t desc_dz,
                                   const void *dz,
                                   const cnnlTensorDescriptor_t desc_x,
                                   const void *x,
                                   const cnnlTensorDescriptor_t desc_mean,
                                   const void *mean,
                                   const cnnlTensorDescriptor_t desc_invstd,
                                   const void *invstd,
                                   void *workspace,
                                   size_t workspace_size,
                                   const cnnlTensorDescriptor_t desc_dfilter,
                                   void *dfilter,
                                   const cnnlTensorDescriptor_t desc_dbias,
                                   void *dbias,
                                   const cnnlTensorDescriptor_t desc_sum_dy,
                                   void *sum_dy,
                                   const cnnlTensorDescriptor_t desc_sum_dy_xmu,
                                   void *sum_dy_xmu,
                                   const bool needs_input_grad0,
                                   const bool needs_input_grad1,
                                   const bool needs_input_grad2);

// Group:SyncBatchnormBackwardReduce
/*!
 * @brief Applies Syncronized Batch Normalization Reduce operator to backwardly compute grad filters,
 * grad bias, sum_dy and sum_dy_xmu on each MLU device.
 *
 * Batch Normalization is used in CNN, including but not limited to
 * ResNet (Deep Residual Network), Yolo (You Only Look Once) and R-CNN (Regions with CNN features).
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlSyncBatchnormBackwardReduce_v2 instead, which does not support the \p prefer parameter.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   sync_batchnorm_backward_reduce operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] desc_dz
 *   The descriptor of the input tensor \p dz. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] dz
 *   Input. Pointer to the MLU memory that stores the tensor \p dz, which denotes the partial derivative of
 *   batch normalization forward output.
 * @param[in] desc_x
 *   The descriptor of the input tensor \p x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \p x.
 * @param[in] desc_mean
 *   The descriptor of \p mean tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] mean
 *   Input. Pointer to the MLU memory that stores the tensor \p mean, which denotes the average result of
 *   input \p x.
 * @param[in] desc_invstd
 *   The descriptor of \p invstd tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] invstd
 *   Input. Pointer to the MLU memory that stores the tensor \p invstd, which denotes the inversed standard deviation
 *   of input \p x.
 * @param[out] desc_dfilter
 *   The descriptor of \p dfilter tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] dfilter
 *   Output. Pointer to the MLU memory that stores the input tensor \p dfilter, which denotes partial derivative
 *   of filter in sync batch normalization forward training. It will be computed only if Boolean variable
 *   \p needs_input_grad1 is true.
 * @param[out] desc_dbias
 *   The descriptor of the sync batch normalization output tensor \p dbias. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] dbias
 *   Output. Pointer to the MLU memory that stores the output tensor \p dbias, which denotes partial derivative of
 *   bias in sync batch normalization forward training. It will be computed only if \p needs_input_grad2 is true.
 * @param[out] desc_sum_dy
 *   The descriptor of the sync batch normalization output tensor \p sum_dy. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] sum_dy
 *   Output. Pointer to the MLU memory that stores the output tensor \p sum_dy, which denotes the summation of dz
 *   and is also an intermediate variable to compute the partial derivative of input x. Moreover, it will be
 *   computed only if Boolean variable \p needs_input_grad0 is true.
 * @param[out] desc_sum_dy_xmu
 *   The descriptor of the sync batch normalization output tensor \p sum_dy_xmu. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] sum_dy_xmu
 *   Output. Pointer to the MLU memory that stores the output tensor \p sum_dy_xmu, which denotes sum{dz(x-mean)}.
 *   It is also an intermediate variable to compute the partial derivative of
 *   input \p x. Moreover, it will be computed only if Boolean variable \p needs_input_grad0 is true.
 * @param[in] needs_input_grad0
 *   Input. A Boolean variable that determines whether to compute \p sum_dy and \p sum_dy_xmu.
 *   When \p needs_input_grad0 is true, \p sum_dy and \p sum_dy_xmu will be computed.
 *   When \p needs_input_grad0 is false, \p sum_dy and \p sum_dy_xmu will be NULL.
 * @param[in] needs_input_grad1
 *   Input. A Boolean variable that determines whether to compute \p dfilters.
 *   When \p needs_input_grad1 is true, \p dfilters will be computed.
 *   When \p needs_input_grad1 is false, \p dfilter will be NULL.
 * @param[in] needs_input_grad2
 *   Input. A Boolean variable that determines whether to compute \p dbias.
 *   When \p needs_input_grad2 is true, \p dbias will be computed.
 *   When \p needs_input_grad2 is false, \p dbias will be NULL.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "SyncBatchNormBackwardReduce Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *   dz_tensor - x_tensor - mean_tensor - invstd_tensor - dfilter_tensor - dbias_tensor - sum_dy_tensor -
 *   sum_dy_xmu_tensor.
 *   - float - float - float - float - float - float - float - float.
 *   - half - half - float - float - float - float - float - float.
 *   - bfloat16 - bfloat16 - float - float - float - float - float - float.
 * - The data type bfloat16 is only supported on MLU500 series.
 *
 * @par Data Layout
 * - The supported data layouts of \p dz, \p x, \p mean, \p invstd, \p dfilter, \p dbias, \p sum_dy and
 *   \p sum_dy_xmu are as follows:
 *   - dz tensor: \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NLC, \p CNNL_LAYOUT_NC.
 *   - x tensor: \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NLC, \p CNNL_LAYOUT_NC.
 *   - mean tensor: \p CNNL_LAYOUT_ARRAY.
 *   - invstd tensor: \p CNNL_LAYOUT_ARRAY.
 *   - dfilter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - dbias tensor: \p CNNL_LAYOUT_ARRAY.
 *   - sum_dy tensor: \p CNNL_LAYOUT_ARRAY.
 *   - sum_dy_xmu tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - The \p mean, \p invstd, \p dfilter, \p bias, \p sum_dy and \p sum_dy_xmu must be 1D tensors and the length of the dimensions
 *   of these tensors should be the same as the length of the lowest dimension of \p x.
 * - The length of each dimension of \p x and \p dz must be the same.
 * - On MLU500 series, ::cnnlSyncBatchnormBackwardReduce cannot be used
 *   and you should use ::cnnlSyncBatchnormBackwardReduce_v2 instead.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the sync batchnorm element operation is as follows:
     @verbatim
      input four arrays by 1 * 2 * 3 * 2, 2, 2, 2 and 2
      --> dz: [[[[6.0, 6.0],[6.0, 6.0],[6.0, 6.0]],
               [[6.0, 6.0],[6.0, 6.0],[6.0, 6.0]]]]

      --> x: [[[[3.0, 3.0],[3.0, 3.0],[3.0, 3.0]],
               [[3.0, 3.0],[3.0, 3.0],[3.0, 3.0]]]]

      --> mean: [1, 1]

      --> invstd: [0.8, 0.8]

      output array by 2
      --> dfilter: [57.6, 57.6]

      --> dbias: [36.0, 36.0]

      --> sum_dy: [36.0, 36.0]

      --> sum_dy_xmu: [72.0, 72.0]
     @endverbatim
 *
 * @par Reference
 * - Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,
 *   Sergey Ioffe, 2015.
 *
 */
CNNL_DEPRECATED_FOR(cnnlSyncBatchnormBackwardReduce_v2)
cnnlStatus_t CNNL_WIN_API
cnnlSyncBatchnormBackwardReduce(cnnlHandle_t handle,
                                const cnnlTensorDescriptor_t desc_dz,
                                const void *dz,
                                const cnnlTensorDescriptor_t desc_x,
                                const void *x,
                                const cnnlTensorDescriptor_t desc_mean,
                                const void *mean,
                                const cnnlTensorDescriptor_t desc_invstd,
                                const void *invstd,
                                const cnnlTensorDescriptor_t desc_dfilter,
                                void *dfilter,
                                const cnnlTensorDescriptor_t desc_dbias,
                                void *dbias,
                                const cnnlTensorDescriptor_t desc_sum_dy,
                                void *sum_dy,
                                const cnnlTensorDescriptor_t desc_sum_dy_xmu,
                                void *sum_dy_xmu,
                                const bool needs_input_grad0,
                                const bool needs_input_grad1,
                                const bool needs_input_grad2);

// Group:FrozenBatchNormBackward
/*!
 * @brief Computes gradients of batch normalization for the fine-tuning scenario.
 *
 * Frozen Batch Normalization is used in artificial intelligence, including but not limited
 * to Fast-RCNN (Regions with CNN features).
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlFrozenBatchNormBackward_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the batchnorm
 *   backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   The descriptor of the input tensor \p x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \p x.
 * @param[in] diff_y_desc
 *   The descriptor of the backpropagated differential tensor \p diff_y. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] diff_y
 *   Input. Pointer to the MLU memory that stores the input tensor \p diff_y.
 * @param[in] filter_bias_mean_var_desc
 *   The descriptor of the \p filter, \p pop_mean, \p pop_var, \p diff_filter and \p
 *   diff_bias tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the input tensor \p filter.
 * @param[in] pop_mean
 *   Input. Pointer to the MLU memory that stores the mean of population.
 * @param[in] pop_var
 *   Input. Pointer to the MLU memory that stores the variance of population.
 * @param[in] eps
 *   Input. A float value added to the denominator for numerical stability.
 * @param[in] diff_x_desc
 *   The descriptor of the result differential tensor \p diff_x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the output tensor \p diff_x.
 * @param[out] diff_filter
 *   Output. Pointer to the MLU memory that stores the output tensor \p diff_filter.
 * @param[out] diff_bias
 *   Output. Pointer to the MLU memory that stores the output tensor \p diff_bias.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "FrozenBatchNormBackward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *   input_tensor - diff_y_tensor - filter_tensor - pop_mean_tensor - pop_var_tensor -
 *   output_tensor - diff_filter_tensor - diff_bias_tensor.
 *   - half - half - half - half - half - half - half - half
 *   - float - float - float - float - float - float - float - float
 *   - half - half - float - float - float - half - float - float
 *   - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16
 *   - bfloat16 - bfloat16 - float - float - float - bfloat16 - float - float
 * - The data type bfloat16 is only supported on MLU500 series.
 *
 * @par Data Layout
 * - The supported data layouts of the input tensor, diff_y tensor, filter tensor, pop_mean
 *   tensor, pop_var tensor, output tensor, diff_filter tensor and diff_bias tensor are
 *   as follows:
 *   - input tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - diff_y tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *     The layout of the diff_y tensor should be the same as input tensor.
 *   - filter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - pop_mean tensor: \p CNNL_LAYOUT_ARRAY.
 *   - pop_var tensor: \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *     The layout of the output tensor should be the same as input tensor.
 *   - diff_filter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - diff_bias tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - On MLU500 series, you can use only ::cnnlFrozenBatchNormBackward_v2
 *   to compute gradients of batch normalization for the fine-tuning scenario.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the frozen batchnorm backward operation is as follows:
     @verbatim
      input five arrays by 1 * 2 * 3 * 2, 1 * 2 * 3 * 2, 2, 2 and 2
      --> x: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> diff_y: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
                    [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> filter: [2.0, 2.0]

      --> pop_mean: [0.0,0.0]

      --> pop_var: [1.0,1.0]

      param:
        eps: 0.00001

      output three arrays by 1 * 2 * 3 * 2, 2 and 2
      --> diff_x: [[[[2.0, 2.0],[2.0, 2.0],[2.0, 2.0]],
                    [[2.0, 2.0],[2.0, 2.0],[2.0, 2.0]]]]

      --> diff_filter: [6.0, 6.0]

      --> diff_bias: [6.0, 6.0]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d
 * - https://arxiv.org/abs/1502.03167
 * - Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,
 *   Sergey Ioffe, 2015.
 *
 */
CNNL_DEPRECATED_FOR(cnnlFrozenBatchNormBackward_v2)
cnnlStatus_t CNNL_WIN_API
cnnlFrozenBatchNormBackward(cnnlHandle_t handle,
                            const cnnlTensorDescriptor_t x_desc,
                            const void *x,
                            const cnnlTensorDescriptor_t diff_y_desc,
                            const void *diff_y,
                            const cnnlTensorDescriptor_t filter_bias_mean_var_desc,
                            const void *filter,
                            const void *pop_mean,
                            const void *pop_var,
                            float eps,
                            const cnnlTensorDescriptor_t diff_x_desc,
                            void *diff_x,
                            void *diff_filter,
                            void *diff_bias);

// Group:FrozenBatchNormBackward
/*!
 * @brief Returns in \p size the size of the MLU memory that is used to get
 *        extra space size in FrozenBatchNormBackward operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the FrozenBatchNormBackward operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of input tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is
 *   used in the FrozenBatchNormBackward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetFrozenBatchNormBackwardWorkspaceSize(cnnlHandle_t handle,
                                            const cnnlTensorDescriptor_t x_desc,
                                            size_t *workspace_size);

// Group:FrozenBatchNormBackwardV2
/*!
 * @brief Computes gradients of batch normalization for the fine-tuning scenario.
 *
 * Frozen Batch Normalization is used in convolution network, including but not limited
 * to Fast-RCNN (Regions with CNN features). Compared with ::cnnlFrozenBatchNormBackward, this
 * function can fuse add operations and relu operations.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlFrozenBatchNormBackward_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the frozen
 *   batchnorm backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] activation_desc
 *   Input. The descriptor of the activation operation. For detailed information,
 *   see ::cnnlActivationDescriptor_t.
 * @param[in] mode
 *   Input. The mode of normalization. The \p mode must be \p CNNL_BATCHNORM_SPATIAL now.
 *   For detailed information, see ::cnnlBatchNormMode_t enum.
 * @param[in] bnOps
 *   Input. The method of activation. The supported combinations of \p bnOps and
 *   \p activation_desc->mode are:
 *   - \p bnOps is \p CNNL_BATCHNORM_OPS_BN and \p activation_desc->mode is
 *     \p CNNL_ACTIVATION_IDENTITY;
 *   - \p bnOps is \p CNNL_BATCHNORM_OPS_BN_ACTIVATION and \p activation_desc->mode is
 *     \p CNNL_ACTIVATION_RELU;
 *   - \p bnOps is \p CNNL_BATCHNORM_OPS_BN_ADD_ACTIVATION, \p activation_desc->mode is
 *     \p CNNL_ACTIVATION_RELU or \p CNNL_ACTIVATION_IDENTITY.
 *   For detailed information, see ::cnnlBatchNormOps_t enum.
 * @param[in] x_desc
 *   The descriptor of the input tensor \p x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \p x.
 * @param[in] y_desc
 *   Input. The descriptor of the input tensor \p y.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the input tensor \p y which is the output of
 *   cnnlBatchNormForwardInference API. The optional \p y_desc and \p y are only used when \p bnOps is
 *   \p CNNL_BATCHNORM_OPS_BN_ADD_ACTIVATION or \p CNNL_BATCHNORM_OPS_BN_ACTIVATION, otherwise should
 *   set it to NULL. When in use, \p y should have exactly the same dimensions as \p x.
 * @param[in] diff_y_desc
 *   The descriptor of the backpropagated differential tensor \p diff_y. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] diff_y
 *   Input. Pointer to the MLU memory that stores the input tensor \p diff_y.
 * @param[in] filter_bias_mean_var_desc
 *   The descriptor of the \p filter, \p pop_mean, \p pop_var, \p diff_filter and \p
 *   diff_bias tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the input tensor \p filter.
 * @param[in] pop_mean
 *   Input. Pointer to the MLU memory that stores the mean of population.
 * @param[in] pop_var
 *   Input. Pointer to the MLU memory that stores the variance of population.
 * @param[in] eps
 *   Input. A float value added to the denominator for numerical stability.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlFrozenBatchNormBackwardV2.
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   ::cnnlFrozenBatchNormBackwardV2. You can get the size of the workspace with
 *   the ::cnnlGetFrozenBatchNormBackwardWorkspaceSize function.
 * @param[in] diff_z_desc
 *   The descriptor of the result differential tensor \p diff_z. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] diff_z
 *   Output. Pointer to the MLU memory that stores the output tensor \p diff_z.
 *   The optional \p diff_z_desc and \p diff_z are only used when \p bnOps is
 *   \p CNNL_BATCHNORM_OPS_BN_ADD_ACTIVATION, otherwise should set it to NULL.
 *   When in use, \p diff_z should have exactly the same dimensions as \p x.
 * @param[in] diff_x_desc
 *   The descriptor of the result differential tensor \p diff_x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the output tensor \p diff_x.
 * @param[out] diff_filter
 *   Output. Pointer to the MLU memory that stores the output tensor \p diff_filter.
 * @param[out] diff_bias
 *   Output. Pointer to the MLU memory that stores the output tensor \p diff_bias.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "FrozenBatchNormBackward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *   x_tensor - y_tensor - diff_y_tensor - filter_tensor - pop_mean_tensor - pop_var_tensor -
 *   diff_z_tensor - diff_x_tensor - diff_filter_tensor - diff_bias_tensor.
 *   - half - half - half - half - half - half - half - half - half - half
 *   - float - float - float - float - float - float - float - float - float - float
 *   - half - half - half - float - float - float - half - half - float - float
 *   - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16
 *   - bfloat16 - bfloat16 - float - float - float - bfloat16 - float - float
 * - The data type bfloat16 is only supported on MLU500 series.
 *
 * @par Data Layout
 * - The supported data layouts of the \p x tensor, \p y tensor, \p diff_y tensor, \p filter tensor,
 *   \p pop_mean tensor, \p pop_var tensor, \p diff_z tensor, \p diff_x tensor, \p diff_filter tensor
 *   and \p diff_bias tensor are as follows:
 *   - x tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - y tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - diff_y tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *     The layout of the diff_y tensor should be the same as input tensor.
 *   - filter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - pop_mean tensor: \p CNNL_LAYOUT_ARRAY.
 *   - pop_var tensor: \p CNNL_LAYOUT_ARRAY.
 *   - diff_z tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - diff_x tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *     The layout of the output tensor should be the same as input tensor.
 *   - diff_filter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - diff_bias tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the FrozenBatchNormBackward operation is as follows:
     @verbatim
      input five arrays by 1 * 2 * 3 * 2, 1 * 2 * 3 * 2, 2, 2 and 2
      --> x: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> y: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> diff_y: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
                    [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> filter: [2.0, 2.0]

      --> pop_mean: [0.0,0.0]

      --> pop_var: [1.0,1.0]

      param:
        eps: 0.00001

      output three arrays by 1 * 2 * 3 * 2, 2 and 2
      --> diff_z: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
                    [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> diff_x: [[[[2.0, 2.0],[2.0, 2.0],[2.0, 2.0]],
                    [[2.0, 2.0],[2.0, 2.0],[2.0, 2.0]]]]

      --> diff_filter: [6.0, 6.0]

      --> diff_bias: [6.0, 6.0]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d
 * - https://arxiv.org/abs/1502.03167
 * - Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,
 *   Sergey Ioffe, 2015.
 *
 */
CNNL_DEPRECATED_FOR(cnnlFrozenBatchNormBackward_v2)
cnnlStatus_t CNNL_WIN_API
cnnlFrozenBatchNormBackwardV2(cnnlHandle_t handle,
                              const cnnlActivationDescriptor_t activation_desc,
                              const cnnlBatchNormMode_t mode,
                              const cnnlBatchNormOps_t bnOps,
                              const cnnlTensorDescriptor_t x_desc,
                              const void *x,
                              const cnnlTensorDescriptor_t y_desc,
                              const void *y,
                              const cnnlTensorDescriptor_t diff_y_desc,
                              const void *diff_y,
                              const cnnlTensorDescriptor_t filter_bias_mean_var_desc,
                              const void *filter,
                              const void *pop_mean,
                              const void *pop_var,
                              float eps,
                              void *workspace,
                              size_t workspace_size,
                              const cnnlTensorDescriptor_t diff_z_desc,
                              void *diff_z,
                              const cnnlTensorDescriptor_t diff_x_desc,
                              void *diff_x,
                              void *diff_filter,
                              void *diff_bias);

// Group:FrozenBatchNormBackward
/*!
 * @brief Computes gradients of batch normalization for the fine-tuning scenario.
 *
 * Frozen Batch Normalization is used in convolution network, including but not limited
 * to Fast-RCNN (Regions with CNN features). Compared with ::cnnlFrozenBatchNormBackward, this
 * function can fuse add operations and relu operations.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the frozen
 *   batchnorm backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] activation_desc
 *   Input. The descriptor of the activation operation. For detailed information,
 *   see ::cnnlActivationDescriptor_t.
 * @param[in] mode
 *   Input. The mode of normalization. The \p mode must be \p CNNL_BATCHNORM_SPATIAL now.
 *   For detailed information, see ::cnnlBatchNormMode_t enum.
 * @param[in] bnOps
 *   Input. The method of activation. The supported combinations of \p bnOps and
 *   \p activation_desc->mode are:
 *   - \p bnOps is \p CNNL_BATCHNORM_OPS_BN and \p activation_desc->mode is
 *     \p CNNL_ACTIVATION_IDENTITY;
 *   - \p bnOps is \p CNNL_BATCHNORM_OPS_BN_ACTIVATION and \p activation_desc->mode is
 *     \p CNNL_ACTIVATION_RELU;
 *   - \p bnOps is \p CNNL_BATCHNORM_OPS_BN_ADD_ACTIVATION, \p activation_desc->mode is
 *     \p CNNL_ACTIVATION_RELU or \p CNNL_ACTIVATION_IDENTITY.
 *   For detailed information, see ::cnnlBatchNormOps_t enum.
 * @param[in] x_desc
 *   The descriptor of the input tensor \p x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \p x.
 * @param[in] y_desc
 *   Input. The descriptor of the input tensor \p y.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the input tensor \p y which is the output of
 *   cnnlBatchNormForwardInference API. The optional \p y_desc and \p y are only used when \p bnOps is
 *   \p CNNL_BATCHNORM_OPS_BN_ADD_ACTIVATION or \p CNNL_BATCHNORM_OPS_BN_ACTIVATION, otherwise should
 *   set it to NULL. When in use, \p y should have exactly the same dimensions as \p x.
 * @param[in] diff_y_desc
 *   The descriptor of the backpropagated differential tensor \p diff_y. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] diff_y
 *   Input. Pointer to the MLU memory that stores the input tensor \p diff_y.
 * @param[in] filter_bias_mean_var_desc
 *   The descriptor of the \p filter, \p pop_mean, \p pop_var, \p diff_filter and \p
 *   diff_bias tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the input tensor \p filter.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the input tensor \p bias.
 *   Reserved for future use. Now must set the value to NULL.
 * @param[in] pop_mean
 *   Input. Pointer to the MLU memory that stores the mean of population.
 * @param[in] pop_var
 *   Input. Pointer to the MLU memory that stores the variance of population.
 * @param[in] eps
 *   Input. A float value added to the denominator for numerical stability.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlFrozenBatchNormBackward_v2.
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   ::cnnlFrozenBatchNormBackward_v2. You can get the size of the workspace with
 *   the ::cnnlGetFrozenBatchNormBackwardWorkspaceSize function.
 * @param[in] diff_z_desc
 *   The descriptor of the result differential tensor \p diff_z. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] diff_z
 *   Output. Pointer to the MLU memory that stores the output tensor \p diff_z.
 *   The optional \p diff_z_desc and \p diff_z are only used when \p bnOps is
 *   \p CNNL_BATCHNORM_OPS_BN_ADD_ACTIVATION, otherwise should set it to NULL.
 *   When in use, \p diff_z should have exactly the same dimensions as \p x.
 * @param[in] diff_x_desc
 *   The descriptor of the result differential tensor \p diff_x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the output tensor \p diff_x.
 * @param[out] diff_filter
 *   Output. Pointer to the MLU memory that stores the output tensor \p diff_filter.
 * @param[out] diff_bias
 *   Output. Pointer to the MLU memory that stores the output tensor \p diff_bias.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "FrozenBatchNormBackward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *   x_tensor - y_tensor - diff_y_tensor - filter_tensor - bias_tensor - pop_mean_tensor -
 *   pop_var_tensor - diff_z_tensor - diff_x_tensor - diff_filter_tensor - diff_bias_tensor.
 *   - half - half - half - half - half - half - half - half - half - half - half
 *   - float - float - float - float - float - float - float - float - float - float - float
 *   - half - half - half - float - float - float - float - half - half - float - float
 *   - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16 - bfloat16
 *   - bfloat16 - bfloat16 - bfloat16 - float - float - float - float - bfloat16 - bfloat16 - float - float
 * - The data type bfloat16 is only supported on MLU500 series and when the activation mode is CNNL_ACTIVATION_IDENTITY.
 *
 * @par Data Layout
 * - The supported data layouts of the \p x tensor, \p y tensor, \p diff_y tensor, \p filter tensor,
 *   \p bias tensor, \p pop_mean tensor, \p pop_var tensor, \p diff_z tensor, \p diff_x tensor,
 *   \p diff_filter tensor and \p diff_bias tensor are as follows:
 *   - x tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - y tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - diff_y tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *     The layout of the diff_y tensor should be the same as input tensor.
 *   - filter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - bias tensor: \p CNNL_LAYOUT_ARRAY.
 *   - pop_mean tensor: \p CNNL_LAYOUT_ARRAY.
 *   - pop_var tensor: \p CNNL_LAYOUT_ARRAY.
 *   - diff_z tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - diff_x tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *     The layout of the output tensor should be the same as input tensor.
 *   - diff_filter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - diff_bias tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the FrozenBatchNormBackward operation is as follows:
     @verbatim
      input five arrays by 1 * 2 * 3 * 2, 1 * 2 * 3 * 2, 2, 2 and 2
      --> x: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> y: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> diff_y: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
                    [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> filter: [2.0, 2.0]

      --> pop_mean: [0.0,0.0]

      --> pop_var: [1.0,1.0]

      param:
        eps: 0.00001

      output three arrays by 1 * 2 * 3 * 2, 2 and 2
      --> diff_z: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
                    [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> diff_x: [[[[2.0, 2.0],[2.0, 2.0],[2.0, 2.0]],
                    [[2.0, 2.0],[2.0, 2.0],[2.0, 2.0]]]]

      --> diff_filter: [6.0, 6.0]

      --> diff_bias: [6.0, 6.0]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d
 * - https://arxiv.org/abs/1502.03167
 * - Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,
 *   Sergey Ioffe, 2015.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlFrozenBatchNormBackward_v2(cnnlHandle_t handle,
                               const cnnlActivationDescriptor_t activation_desc,
                               const cnnlBatchNormMode_t mode,
                               const cnnlBatchNormOps_t bnOps,
                               const cnnlTensorDescriptor_t x_desc,
                               const void *x,
                               const cnnlTensorDescriptor_t y_desc,
                               const void *y,
                               const cnnlTensorDescriptor_t diff_y_desc,
                               const void *diff_y,
                               const cnnlTensorDescriptor_t filter_bias_mean_var_desc,
                               const void *filter,
                               const void *bias,
                               const void *pop_mean,
                               const void *pop_var,
                               float eps,
                               void *workspace,
                               size_t workspace_size,
                               const cnnlTensorDescriptor_t diff_z_desc,
                               void *diff_z,
                               const cnnlTensorDescriptor_t diff_x_desc,
                               void *diff_x,
                               void *diff_filter,
                               void *diff_bias);

// Group:SyncBatchNormBackwardElemt
/*!
 * @brief Computes the gradients of input in the training scenario.
 *
 * This function is used in artificial intelligence, including but not limited
 * to ResNet (Deep Residual Network), Yolo (You Only Look Once) and R-CNN (Regions with CNN features).
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   syncBatchNormBackwardElemt operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] diff_y_desc
 *   Input. The descriptor of the backpropagated differential tensor \p diff_y. For
 *   detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] diff_y
 *   Input. Pointer to the MLU memory that stores the backpropagated differential tensor.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor \p x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] mean_desc
 *   Input. The descriptor of the input tensor \p mean. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] mean
 *   Input. Pointer to the MLU memory that stores the global mean.
 * @param[in] invstd_desc
 *   Input. The descriptor of the input tensor \p invstd. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] invstd
 *   Input. Pointer to the MLU memory that stores the global inverse standard deviation.
 * @param[in] filter_desc
 *   Input. The descriptor of the input tensor \p filter. For detailed information, see
 *   ::cnnlTensorDescriptor_t. The descriptor can be NULL when \p filter pointer is NULL.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the input tensor \p filter for affine
 *   transformation after batch normalization. The value of this pointer can be NULL.
 * @param[in] mean_dy_desc
 *   Input. The descriptor of the input tensor \p mean_dy. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] mean_dy
 *   Input. Pointer to the MLU memory that stores the mean of diff_y.
 * @param[in] mean_dy_xmu_desc
 *   Input. The descriptor of the input tensor \p mean_dy_xmu. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] mean_dy_xmu
 *   Input. Pointer to the MLU memory that stores the mean of the result of diff_y * (x - mean).
 * @param[in] diff_x_desc
 *   Input. The descriptor of the output tensor \p diff_x. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the derivative of input.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "SyncBatchNormBackwardElemt Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below:
 *   - float(\p diff_y) - float(\p x) - float(\p mean) - float(\p invstd) - float(\p filter) -
 *     float(\p mean_dy) - float(\p mean_dy_xmu) - float(\p diff_x).
 *   - half(\p diff_y) - half(\p x) - float(\p mean) - float(\p invstd) - float(\p filter) -
 *     float(\p mean_dy) - float(\p mean_dy_xmu) - half(\p diff_x).
 *
 * @par Data Layout
 * - The supported data layouts of \p diff_y, \p x, \p mean, \p invstd, \p filter, \p mean_dy,
 *   \p mean_dy_xmu and \p diff_x are as follows:
 *   - diff_y tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and
 *     \p CNNL_LAYOUT_NLC.
 *   - x tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - mean tensor: \p CNNL_LAYOUT_ARRAY.
 *   - invstd tensor: \p CNNL_LAYOUT_ARRAY.
 *   - filter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - mean_dy tensor: \p CNNL_LAYOUT_ARRAY.
 *   - mean_dy_xmu tensor: \p CNNL_LAYOUT_ARRAY.
 *   - diff_x tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and
 *     \p CNNL_LAYOUT_NLC.
 * - The layouts of the \p diff_x \p x and \p diff_y should be the same.
 *
 * @note
 * - The \p mean, \p invstd, \p filter, \p mean_dy and \p mean_dy_xmu must be 1D tensors and the
 *   length of the dimension of these tensors should be the same as the length of the lowest
 *   dimension of \p x.
 * - The length of each dimension of \p diff_y, \p x and \p diff_x must be the same.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the sync_batch_norm_backward_elemt operation is as follows:
     @verbatim
      input seven arrays by 1, 1, 1, 1, 1, 1, 1 and 1
      --> diff_y: [[[[1.0]]]]
      --> x: [[[[2.0]]]]
      --> mean: [3.0]
      --> invstd: [4.0]
      --> filter: [5.0]
      --> mean_dy: [6.0]
      --> mean_dy_xmu: [7.0]

      output an array by 1
      --> mean: [[[[-8960.0]]]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/1.6.0/jit_builtin_functions.html?highlight=batch_norm_backward_elemt
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlSyncBatchNormBackwardElemt(cnnlHandle_t handle,
                               const cnnlTensorDescriptor_t diff_y_desc,
                               const void *diff_y,
                               const cnnlTensorDescriptor_t x_desc,
                               const void *x,
                               const cnnlTensorDescriptor_t mean_desc,
                               const void *mean,
                               const cnnlTensorDescriptor_t invstd_desc,
                               const void *invstd,
                               const cnnlTensorDescriptor_t filter_desc,
                               const void *filter,
                               const cnnlTensorDescriptor_t mean_dy_desc,
                               const void *mean_dy,
                               const cnnlTensorDescriptor_t mean_dy_xmu_desc,
                               const void *mean_dy_xmu,
                               const cnnlTensorDescriptor_t diff_x_desc,
                               void *diff_x);

// Group:SyncBatchNormBackwardElemtV2
/*!
 * @brief Computes the gradients of input in the training scenario.
 *
 * This function is used in ResNet (Deep Residual Network), Yolo (You Only Look Once) and
 * R-CNN (Regions with CNN features).
 *
 * Compared with ::cnnlSyncBatchNormBackwardElemt, this function first computes the intermediate
 * results mean_dy and mean_dy_xmu based on \p sum_dy, \p sum_dy_xmu and \p count, and then
 * computes the gradient of \p x with the intermediate results.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   syncBatchNormBackwardElemt operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] diff_y_desc
 *   Input. The descriptor of the backpropagated differential tensor \p diff_y. For
 *   detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] diff_y
 *   Input. Pointer to the MLU memory that stores the backpropagated differential tensor.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor \p x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] mean_desc
 *   Input. The descriptor of the input tensor \p mean. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] mean
 *   Input. Pointer to the MLU memory that stores the global mean.
 * @param[in] invstd_desc
 *   Input. The descriptor of the input tensor \p invstd. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] invstd
 *   Input. Pointer to the MLU memory that stores the global inverse standard deviation.
 * @param[in] filter_desc
 *   Input. The descriptor of the input tensor \p filter. For detailed information, see
 *   ::cnnlTensorDescriptor_t. The descriptor can be NULL when \p filter pointer is NULL.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the input tensor \p filter for affine
 *   transformation after batch normalization. The value of this pointer can be NULL.
 * @param[in] sum_dy_desc
 *   Input. The descriptor of the input tensor \p sum_dy. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] sum_dy
 *   Input. Pointer to the MLU memory that stores the sum of diff_y.
 * @param[in] sum_dy_xmu_desc
 *   Input. The descriptor of the input tensor \p sum_dy_xmu. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] sum_dy_xmu
 *   Input. Pointer to the MLU memory that stores the sum of the result of diff_y * (x - mean).
 * @param[in] count_desc
 *   Input. The descriptor of the input tensor \p count. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] count
 *   Input. Pointer to the MLU memory that stores the number of the high dimensions (the dimensions
 *   except the lowest dimension) of the input tensor \p x on all MLU devices.
 * @param[in] diff_x_desc
 *   Input. The descriptor of the output tensor \p diff_x. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the derivative of input.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "SyncBatchNormBackwardElemt_v2 Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below:
 *   diff_y - x - mean - invstd - filter - sum_dy - sum_dy_xmu - count - diff_x.
 *   - float - float - float - float - float - float - float - int32_t - float.
 *   - half - half - float - float - float - float - float - int32_t - half.
 *   - bfloat16 - bfloat16 - float - float - float - float - float - int32_t - bfloat16.
 * - The data type bfloat16 is only supported on MLU500 series.
 *
 * @par Data Layout
 * - The supported data layouts of \p diff_y, \p x, \p mean, \p invstd, \p filter, \p sum_dy,
 *   \p sum_dy_xmu and \p diff_x are as follows:
 *   - diff_y tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and
 *     \p CNNL_LAYOUT_NLC.
 *   - x tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - mean tensor: \p CNNL_LAYOUT_ARRAY.
 *   - invstd tensor: \p CNNL_LAYOUT_ARRAY.
 *   - filter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - sum_dy tensor: \p CNNL_LAYOUT_ARRAY.
 *   - sum_dy_xmu tensor: \p CNNL_LAYOUT_ARRAY.
 *   - diff_x tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and
 *     \p CNNL_LAYOUT_NLC.
 * - The layouts of the \p diff_x \p x and \p diff_y should be the same.
 *
 * @note
 * - The \p mean, \p invstd, \p filter, \p sum_dy and \p sum_dy_xmu must be 1D tensors and the
 *   length of the dimension of these tensors should be the same as the length of the lowest
 *   dimension of \p x.
 * - The length of each dimension of \p diff_y, \p x and \p diff_x must be the same.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the sync_batchnorm_backward_elemt_v2 operation is as follows:
     @verbatim
      input seven arrays by 1, 1, 1, 1, 1, 1, 1 and 1
      --> diff_y: [[[[1.0]]]]
      --> x: [[[[2.0]]]]
      --> mean: [3.0]
      --> invstd: [4.0]
      --> filter: [5.0]
      --> sum_dy: [6.0]
      --> sum_dy_xmu: [7.0]
      --> count: [1]

      output an array by 1
      --> mean: [[[[-8960.0]]]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/1.11.0/jit_builtin_functions.html?highlight=batch_norm_backward_elemt
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlSyncBatchNormBackwardElemtV2(cnnlHandle_t handle,
                                 const cnnlTensorDescriptor_t diff_y_desc,
                                 const void *diff_y,
                                 const cnnlTensorDescriptor_t x_desc,
                                 const void *x,
                                 const cnnlTensorDescriptor_t mean_desc,
                                 const void *mean,
                                 const cnnlTensorDescriptor_t invstd_desc,
                                 const void *invstd,
                                 const cnnlTensorDescriptor_t filter_desc,
                                 const void *filter,
                                 const cnnlTensorDescriptor_t sum_dy_desc,
                                 const void *sum_dy,
                                 const cnnlTensorDescriptor_t sum_dy_xmu_desc,
                                 const void *sum_dy_xmu,
                                 const cnnlTensorDescriptor_t count_desc,
                                 const void *count,
                                 const cnnlTensorDescriptor_t diff_x_desc,
                                 void *diff_x);

// Group:Square
/*!
 * @brief Computes the square value for every element of the input tensor \p x and returns in \p y.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the square operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Square Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float, bfloat16.
 *   - output tensor: half, float, bfloat16.
 *
 *   The bfloat16 data type is supported only on MLU500 series.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the square operation is as follows:
     @verbatim
      input arrays by 1 * 3 * 3 * 2 -->
          input: [[[[5, -11], [8, 1], [6, 4]],
                  [[3, 8], [2,6], [0, 6]],
                  [[8, 5], [7,4], [-9, 6]]]]

      output array by 1 * 3 * 3 * 2 -->
          output: [[[[25, 121], [64, 1], [36, 16]],
                   [[9, 64], [4,36], [0, 36]],
                   [[64, 25], [49,16], [81, 36]]]]
     @endverbatim
 *
 * @par Reference
 * - https://www.tensorflow.google.cn/api_docs/python/tf/math/square
 */
cnnlStatus_t CNNL_WIN_API cnnlSquare(cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t x_desc,
                                     const void *x,
                                     const cnnlTensorDescriptor_t y_desc,
                                     void *y);
// Group:SquaredDifference
/*!
 * @brief Computes the square of difference between two input tensors element-wise and returns the results in the output tensor \p output.
 *
 * This function may need extra MLU memory as the workspace. You can get the workspace size
 * with the ::cnnlGetSquaredDifferenceWorkspaceSize function.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   SquaredDifference operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  input1_desc
 *   Input. The descriptor of the first input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  input1
 *   Input. Pointer to the MLU memory that stores the shape of the first input tensor.
 * @param[in]  input2_desc
 *   Input. The descriptor of the second input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  input2
 *   Input. Pointer to the MLU memory that stores the shape of the second input tensor.
 * @param[in]  output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  output
 *   Output. Pointer to the MLU memory that stores the shape of the output tensor.
 * @param[in]  workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the SquaredDifference operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the SquaredDifference operation.
 *   You can get the size of the workspace with the ::cnnlGetSquaredDifferenceWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "SquaredDifference Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensors and output tensor must be the same.
 * - The data types of input and output tensors are as follows:
 *   - input tensor: half, float, bfloat16.
 *   - output tensor: half, float, bfloat16.
 *
 *   The bfloat16 data type is supported only on MLU500 series.
 *
 * @par Data Layout
 *  - The data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 *  - Tensors \p input1 and \p input2 support broadcasting and they should satisfy the broadcasting rules.
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlSquaredDifference function to perform the
 *   SquaredDifference operation.
 *
 * @note
 * - The inputs \p input1 and \p input2 are multi-dimensional arrays, supporting up to CNNL_DIM_MAX dimensions.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */

cnnlStatus_t CNNL_WIN_API cnnlSquaredDifference(cnnlHandle_t handle,
                                                const cnnlTensorDescriptor_t input1_desc,
                                                const void *input1,
                                                const cnnlTensorDescriptor_t input2_desc,
                                                const void *input2,
                                                const cnnlTensorDescriptor_t output_desc,
                                                void *output,
                                                void *workspace,
                                                size_t workspace_size);

// Group:SquaredDifference
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the SquaredDifference operation.
 *
 * The size of the extra workspace is based on the given information of the SquaredDifference operation,
 * including the input tensor descriptors \p input1_desc and \p input2_desc, and the output
 * tensor descriptor \p output_desc.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the SquaredDifference
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input1_desc
 *   Input. The descriptor of the first input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input2_desc
 *   Input. The descriptor of the second input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the SquaredDifference
 *   operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetSquaredDifferenceWorkspaceSize(cnnlHandle_t handle,
                                           const cnnlTensorDescriptor_t input1_desc,
                                           const cnnlTensorDescriptor_t input2_desc,
                                           const cnnlTensorDescriptor_t output_desc,
                                                             size_t *workspace_size);

/*!
 * @brief
 *
 * Enumeration variables describing the base that is used in the implementation
 * of the log function.
 *
 */
typedef enum {
  CNNL_LOG_E = 0,  /*!< The base e is used.*/
  CNNL_LOG_2 = 1,  /*!< The base 2 is used.*/
  CNNL_LOG_10 = 2, /*!< The base 10 is used.*/
} cnnlLogBase_t;

// Group:Log
/*!
 * @brief Computes logarithm of input tensor \p x, and returns the results in the output tensor \p y.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the log
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] base
 *    Input. A cnnlLogBase_t type value indicating which base (e, 2 or 10) to be used.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \p x.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor \p y.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Log Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensor and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 * - On MLU500 series, this function additionally supports the following data type combinations:
 *   - input tensor: bfloat16.
 *   - output tensor: bfloat16.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The data value of each element in the input tensor \p x must be greater than zero.
 *
 * @note
 * - The input tensor and output tensor must have the same shape.
 * - You can specify the stride of all dimensions for \p x_desc and \p y_desc
 *   with ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://tensorflow.google.cn/api_docs/python/tf/math/log
 */
cnnlStatus_t CNNL_WIN_API cnnlLog(cnnlHandle_t handle,
                                  const cnnlLogBase_t base,
                                  const cnnlTensorDescriptor_t x_desc,
                                  const void *x,
                                  const cnnlTensorDescriptor_t y_desc,
                                  void *y);

// Group:Log
/*!
 * @brief Computes logarithm of input tensor \p x, and returns the results in the output tensor \p y.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release. Use
 *   ::cnnlLog instead, which does not support the \p prefer parameter.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the log
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. The \p prefer modes defined in ::cnnlComputationPreference_t enum.
 * @param[in] base
 *    Input. A cnnlLogBase_t type value indicating which base (e, 2 or 10) to be used.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \p x.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor \p y.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Log Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensor and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 * - On MLU500 series, this function additionally supports the following data type combinations:
 *   - input tensor: bfloat16.
 *   - output tensor: bfloat16.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The data value of each element in the input tensor \p x must be greater than zero.
 *
 * @note
 * - The input tensor and output tensor must have the same shape.
 * - You can specify the stride of all dimensions for \p x_desc and \p y_desc
 *   with ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://tensorflow.google.cn/api_docs/python/tf/math/log
 */
CNNL_DEPRECATED_FOR(cnnlLog)
cnnlStatus_t CNNL_WIN_API cnnlLog_v2(cnnlHandle_t handle,
                                     const cnnlComputationPreference_t prefer,
                                     const cnnlLogBase_t base,
                                     const cnnlTensorDescriptor_t x_desc,
                                     const void *x,
                                     const cnnlTensorDescriptor_t y_desc,
                                     void *y);
// Group:Rsqrt
/*!
 * @brief Computes the reciprocal of the square root on input tensor \p x, and returns the results
 *        in the output tensor \p y.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the rsqrt
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. The pointer of output data.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Rsqrt Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float, bfloat16.
 *   - output tensor: half, float, bfloat16.
 *
 *   bfloat16 is only supported on MLU500 Series.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor must have the same shape.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.google.cn/api_docs/python/tf/math/rsqrt
 *
 * @note
 * - You can specify the stride of all dimensions for \p x_desc and \p y_desc
 *   with ::cnnlSetTensorDescriptorEx.
 */
cnnlStatus_t CNNL_WIN_API cnnlRsqrt(cnnlHandle_t handle,
                                    const cnnlTensorDescriptor_t x_desc,
                                    const void *x,
                                    const cnnlTensorDescriptor_t y_desc,
                                    void *y);
// Group:Rsqrt
/*!
 * @brief Computes the reciprocal root square on input tensor \p x, and returns the results in the output tensor \p y.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlRsqrt instead, which does not support the \p prefer parameter.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the rsqrt
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. The \p prefer modes defined in ::cnnlComputationPreference_t enum.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. The pointer of output data.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Rsqrt Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float, bfloat16.
 *   - output tensor: half, float, bfloat16.
 *
 *   bfloat16 is only supported on MLU500 Series.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor must have the same shape.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.google.cn/api_docs/python/tf/math/rsqrt
 *
 * @note
 * - You can specify the stride of all dimensions for \p x_desc and \p y_desc
 *   with ::cnnlSetTensorDescriptorEx.
 */
CNNL_DEPRECATED_FOR(cnnlRsqrt)
cnnlStatus_t CNNL_WIN_API cnnlRsqrt_v2(cnnlHandle_t handle,
                                       const cnnlComputationPreference_t prefer,
                                       const cnnlTensorDescriptor_t x_desc,
                                       const void *x,
                                       const cnnlTensorDescriptor_t y_desc,
                                       void *y);
// Group:BatchMatMul
/*!
 * @brief Computes the batch matrices multiplication of a batch of matrices, then returns the
 * results in the output tensor \p c. For more information about quantization, see "Cambricon CNNL User Guide".
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlBatchMatMulEx instead.
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   batch matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] is_transa
 *   Input. Boolean value indicating whether \p a matrix is transposed.
 * @param[in] is_transb
 *   Input. Boolean value indicating whether \p b matrix is transposed.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of the left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. Pointer to the MLU memory that stores the left matrix.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of the right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. Pointer to the MLU memory that stores the right matrix.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] c
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "BatchMatMul Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for input tensor
 *   \p a, \p b and output tensor \p c.
 *   - \p a offchip data type: int8, int16, int31, half, float.
 *   - \p b offchip data type: int8, int16, int31, half, float.
 *   - \p c offchip data type: half, float.
 *   - \p c onchip data type: half, float.
 *
 * @note
 * - Not supported on 520 platforms.
 * - The combinations of the data types should satisfy the following rules:
 *   - The \p c offchip data type for operation computing and onchip data type must be
 *     float when \p a or \p b data type is int31.
 *   - \p c offchip data type must be the same as \p c onchip data type.
 *   - \p c offchip data type and \p b offchip data type must be half when \p a offchip
 *     data type is half.
 *   - \p c offchip data type and \p b offchip data type must be float when \p a offchip
 *     data type is float.
 *   - The hardware platform must be MLU300 series when \p a offchip data type is half or float.
 *
 * @par Scale Limitation
 * - The input tensors and output tensor must meet the following requirements:
 *   - The inputs \p a and \p b are multi-dimensional arrays, the shape must be no less than
 *     two dimensions and no more than \p CNNL_DIM_MAX dimensions.
 *   - The last two dimensions of the \p a and \p b must be the number of rows and the number
 *     of columns for matrix multiplication respectively.
 *   - The number of columns of \p a matrix must be equal to the number of rows of \p b matrix
 *     after both inputs have performed the transpose operations according to parameters.
 *
 * @par API Dependency
 * - Before calling this function to implement batch matrix multiplication, you need to prepare
 *   all the parameters passed to this function. See each parameter description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, matrix \p a does not need to transpose
 *   and matrix \p b needs to transpose.
 * - If \p a and \p b are two-dimensional tensors, for best practices, it is recommended to call
 *   ::cnnlMatMul.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      is_transa:                    false
      is_transb:                    false
      Dimension of input tensor a:  [64, 99, 128]
      Dimension of input tensor b:  [64, 128, 256]
      Dimension of output tensor c: [64, 99, 256]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/torch.html?highlight=bmm#torch.bmm
 */
CNNL_DEPRECATED_FOR(cnnlBatchMatMulEx)
cnnlStatus_t CNNL_WIN_API cnnlBatchMatMul(cnnlHandle_t handle,
                                          const bool is_transa,
                                          const bool is_transb,
                                          const cnnlTensorDescriptor_t a_desc,
                                          const void *a,
                                          const cnnlTensorDescriptor_t b_desc,
                                          const void *b,
                                          const cnnlTensorDescriptor_t c_desc,
                                          void *c);
// Group:StrideBatchMatMul
/*!
 * @brief Computes the batch matrices multiplication of a batch of matrices, and then returns the
 * results in the output tensor \p c. Input matrices \p a, \p b and output matrix \p c for each
 * instance of the batch are located at fixed offsets in number of elements from their locations
 * in the previous instance.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlStrideBatchMatMul_v3 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   batch matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] is_transa
 *   Input. Boolean value indicating whether \p a matrix is transposed.
 * @param[in] is_transb
 *   Input. Boolean value indicating whether \p b matrix is transposed.
 * @param[in] m
 *   Input. Row of \p a matrix when \p a matrix is not transposed.
 * @param[in] n
 *   Input. Column of \p b matrix when \p b matrix is not transposed.
 * @param[in] k
 *   Input. Column of \p a matrix when \p a matrix is not transposed.
 * @param[in] batch_size
 *   Input. Number of batch for matrices multiplication.
 * @param[in] alpha
 *   Input. Scalar used for multiplication.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of the left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. Pointer to the MLU memory that stores the matrix \p a, which corresponds to the first instance of the batch.
 *   If \p is_transa is false, the dimension of \p a is lda * m with \p lda equal to or greater than \p k,
 *   otherwise, the dimension is lda * k with \p lda equal to or greater than \p m.
 * @param[in] lda
 *   Input. Leading dimension of two-dimensional array used to store the matrix a[i].
 * @param[in] stride_a
 *   Input. An integer value that gives the offset in number of elements between a[i] and a[i + 1].
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of the right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. Pointer to the MLU memory that stores the matrix \p b, which corresponds to the first instance of the batch.
 *   If \p is_transb is false, the dimension of \p b is ldb * k with \p ldb equal to or greater than \p n,
 *   otherwise, the dimension is ldb * n  with \p ldb equal to or greater than \p k.
 * @param[in] ldb
 *   Input. Leading dimension of two-dimensional array used to store the matrix b[i].
 * @param[in] stride_b
 *   Input. An integer value that gives the offset in number of elements between b[i] and b[i + 1].
 * @param[in] beta
 *   Input. Scalar used for multiplication. Now that beta equals 0 is only supported.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] c
 *   Output. Pointer to the MLU memory that stores the matrix \p c, which corresponds to the first instance of the batch,
 *   with dimensions ldc * m with ldc >= n.
 * @param[in] ldc
 *   Input. Leading dimension of two-dimensional array used to store the matrix c[i].
 * @param[in] stride_c
 *   Input. An integer value that gives the offset in number of elements between c[i] and c[i + 1].
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "StrideBatchMatMul Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for input tensor
 *   \p a, \p b and output tensor \p c.
 *   - \p a, \p b, \p c offchip data type, \p c onchip data type: half, half, half, half.
 *   - \p a, \p b, \p c offchip data type, \p c onchip data type: half, half, half, float.
 *   - \p a, \p b, \p c offchip data type, \p c onchip data type: bfloat16, bfloat16, bfloat16, float.
 *   - \p a, \p b, \p c offchip data type, \p c onchip data type: float, float, float, float.
 *
 * @note
 * - The following conditions must be satisfied:
 *   - \p a, \p b and \p c are arrays of pointers to matrices stored in row-major format.
 *   - The hardware platform must be MLU300 series or above.
 *
 * @par Scale Limitation
 * - The input tensors and output tensor must meet the following requirements:
 *   - Total number of elements in the input tensor \p a must be greater than or equal to
 *     (batch_size - 1) * stride_a + lda * ((is_transa ? k : m) - 1) + (is_transa ? m : k).
 *   - Total number of elements in the input tensor \p b must be greater than or equal to
 *     (batch_size - 1) * stride_b + ldb * ((is_transb ? n : k) - 1) + (is_transb ? k : n).
 *   - Total number of elements in the output tensor \p c must be greater than or equal to
 *     (batch_size - 1) * stride_c + ldc * (m - 1) + n.
 *
 * @par API Dependency
 * - Before calling this function to implement batch matrix multiplication, you need to prepare
 *   all the parameters passed to this function. See each parameter description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, matrix \p a does not need to transpose,
 *   and matrix \p b needs to transpose.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      is_transa:  false
      is_transb:  false
      m:  80
      n:  80
      k:  64
      lda:  64
      ldb:  80
      ldc:  80
      batch_size: 64
      stride_a: 15360
      stride_b: 15360
      stride_c: 19200
      alpha:  1
      beta: 0
      Dimension of input tensor a:  [972800]
      Dimension of input tensor b:  [972800]
      Dimension of output tensor c: [1216000]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/torch.html?highlight=bmm#torch.bmm
 */
CNNL_DEPRECATED_FOR(cnnlStrideBatchMatMul_v3)
cnnlStatus_t CNNL_WIN_API cnnlStrideBatchMatMul(cnnlHandle_t handle,
                                                const bool is_transa,
                                                const bool is_transb,
                                                const int  m,
                                                const int  n,
                                                const int  k,
                                                const int batch_size,
                                                const float alpha,
                                                const cnnlTensorDescriptor_t a_desc,
                                                const void *a,
                                                const int lda,
                                                const int64_t stride_a,
                                                const cnnlTensorDescriptor_t b_desc,
                                                const void *b,
                                                const int ldb,
                                                const int64_t stride_b,
                                                const float beta,
                                                const cnnlTensorDescriptor_t c_desc,
                                                void *c,
                                                const int ldc,
                                                const int64_t stride_c);

/*! The descriptor of the stride batch matrix multiplication computation algorithm.
 *
 *  You need to call the ::cnnlCreateStrideBatchMatMulAlgo function to create a descriptor.
 *  Also, you need to destroy the Cambricon CNNL context at the end with the
 * ::cnnlDestroyStrideBatchMatMulAlgo function.
 */
typedef struct cnnlStrideBatchMatMulAlgoStruct *cnnlStrideBatchMatMulAlgo_t;
/*! The descriptor of the stride batch matrix multiplication function that holds max_batch_dim,
 *  allow_tf32, and other attributes defined in ::cnnlStrideBatchMatMulDescAttribute_t.
 *
 *  You need to call the ::cnnlCreateStrideBatchMatMulDescriptor function to create a descriptor,
 *  and call the ::cnnlSetStrideBatchMatMulDescAttr function to set the information of the stride
 *  batch matrix multiplication to the descriptor. Also, you need to destroy
 *  the Cambricon CNNL context at the end with the ::cnnlDestroyStrideBatchMatMulDescriptor function.
 */
typedef struct cnnlStrideBatchMatMulStruct *cnnlStrideBatchMatMulDescriptor_t;
/*! The descriptor of the stride batch matrix multiplication that holds the preferences for
 * ::cnnlStrideBatchMatMulHeuristicResult_t
 *  configuration.
 */
typedef struct cnnlStrideBatchMatMulPrefer *cnnlStrideBatchMatMulPrefer_t;

// Group:StrideBatchMatMul
/*!
 * @brief Creates a descriptor pointed by \p stride_bmm_desc for a stride batch matrix
 * multiplication operation
 *        and allocates memory for holding the information about the operation.
 *        The information is defined in ::cnnlStrideBatchMatMulDescriptor_t.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlCreateStrideBatchMatMulDescriptor instead.
 *
 * @param[out] stride_bmm_desc
 *   Output. A host pointer to the stride batch matrix multiplication descriptor that holds
 * information about the stride batch matrix
 *   multiplication operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetStrideBatchMatMulDescAttr function to
 * initialize
 *   and set the information to the stride batch matrix multiplication descriptor.
 * - You need to call the ::cnnlStrideBatchMatMulDescDestroy function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlCreateStrideBatchMatMulDescriptor)
cnnlStatus_t CNNL_WIN_API
cnnlStrideBatchMatMulDescCreate(cnnlStrideBatchMatMulDescriptor_t *stride_bmm_desc);

// Group:StrideBatchMatMul
/*!
 * @brief Creates a descriptor pointed by \p stride_bmm_desc for a stride batch matrix
 *        multiplication operation and allocates memory for holding the information about the operation.
 *        The information is defined in ::cnnlStrideBatchMatMulDescriptor_t.
 *
 * @param[out] stride_bmm_desc
 *   Output. A host pointer to the stride batch matrix multiplication descriptor that holds
 * information about the stride batch matrix
 *   multiplication operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetStrideBatchMatMulDescAttr function to
 * initialize
 *   and set the information to the stride batch matrix multiplication descriptor.
 * - You need to call the ::cnnlDestroyStrideBatchMatMulDescriptor function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCreateStrideBatchMatMulDescriptor(cnnlStrideBatchMatMulDescriptor_t *stride_bmm_desc);

// Group:StrideBatchMatMul
/*!
 * @brief Destroys stride batch matrix multiplication descriptor \p stride_bmm_desc
 *        that was previously created with ::cnnlStrideBatchMatMulDescCreate.
 *
 * The stride batch matrix multiplication descriptor is defined in
 * ::cnnlStrideBatchMatMulDescriptor_t
 * and holds the information about the stride batch matrix multiplication operation.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlDestroyStrideBatchMatMulDescriptor instead.
 *
 * @param[in] stride_bmm_desc
 *   Input. The stride batch matrix multiplication descriptor to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlDestroyStrideBatchMatMulDescriptor)
cnnlStatus_t CNNL_WIN_API
cnnlStrideBatchMatMulDescDestroy(cnnlStrideBatchMatMulDescriptor_t stride_bmm_desc);

// Group:StrideBatchMatMul
/*!
 * @brief Destroys stride batch matrix multiplication descriptor \p stride_bmm_desc
 *        that was previously created with ::cnnlCreateStrideBatchMatMulDescriptor.
 *
 * The stride batch matrix multiplication descriptor is defined in
 * ::cnnlStrideBatchMatMulDescriptor_t
 * and holds the information about the stride batch matrix multiplication operation.
 *
 * @param[in] stride_bmm_desc
 *   Input. The stride batch matrix multiplication descriptor to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlDestroyStrideBatchMatMulDescriptor(cnnlStrideBatchMatMulDescriptor_t stride_bmm_desc);

// Group:StrideBatchMatMul
/*!
 * @brief Initializes descriptor \p stride_bmm_desc
 * that was previously created with ::cnnlCreateStrideBatchMatMulDescriptor and sets
 * the information about the stride batch matrix multiplication operation to the stride batch matrix
 * multiplication to descriptor \p stride_bmm_desc.
 *
 * @param[in] stride_bmm_desc
 *   Input. The descriptor of the stride batch matrix multiplication operation. For detailed
 *   information, see ::cnnlStrideBatchMatMulDescriptor_t.
 * @param[in] attr
 *   Input. Attributes of the stride batch matrix multiplication descriptor to be set. For detailed
 *   information, see ::cnnlStrideBatchMatMulDescAttribute_t.
 * @param[out] buf
 *   Output. A host pointer to the attribute value set by this function.
 * @param[in] size_in_bytes
 *   Input. Buffer in bytes for verification.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetStrideBatchMatMulDescAttr(cnnlStrideBatchMatMulDescriptor_t stride_bmm_desc,
                                 cnnlStrideBatchMatMulDescAttribute_t attr,
                                 const void *buf,
                                 size_t size_in_bytes);

// Group:StrideBatchMatMul
/*!
 * @brief Returns the pointer to \p buf and size of the buffer \p size_written of the attributes
 * retrieved with the given stride batch matrix multiplication descriptor \p stride_bmm_desc and
 * attribute \p attr.
 *
 * You can set the attributes in the stride batch matrix multiplication descriptor based on the
 * return value
 * of this function.
 *
 * @param[in] stride_bmm_desc
 *   Input. The descriptor of the stride batch matrix multiplication operation. For detailed
 *   information, see ::cnnlStrideBatchMatMulDescriptor_t.
 * @param[in] attr
 *   Input. Attributes of stride batch matrix multiplication descriptor to be retrieved.
 * @param[out] buf
 *   Output. A host pointer to the attribute value to be retrieved by this function.
 * @param[in] size_in_bytes
 *   Input. Buffer in bytes for verification, which is used to check whether the memory size is the same as \p size_written.
 * @param[out] size_written
 *   Output. A host pointer to the number of bytes actually written to the buffer.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetStrideBatchMatMulDescAttr(const cnnlStrideBatchMatMulDescriptor_t stride_bmm_desc,
                                 cnnlStrideBatchMatMulDescAttribute_t attr,
                                 void *buf,
                                 size_t size_in_bytes,
                                 size_t *size_written);

// Group:StrideBatchMatMul
/*!
 * @brief Creates a descriptor pointed by \p algo for a stride batch matrix multiplication
 * algorithm
 *        and allocates memory for holding the information about the algorithm.
 *        The information is defined in ::cnnlStrideBatchMatMulAlgo_t.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlCreateStrideBatchMatMulAlgo instead.
 *
 * @param[out] algo
 *   Output. A host pointer to the descriptor that holds
 *           information about the stride batch matrix multiplication algorithm.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - You need to call the ::cnnlStrideBatchMatMulAlgoDestroy function at the end to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlCreateStrideBatchMatMulAlgo)
cnnlStatus_t CNNL_WIN_API
cnnlStrideBatchMatMulAlgoCreate(cnnlStrideBatchMatMulAlgo_t *algo);

// Group:StrideBatchMatMul
/*!
 * @brief Creates a descriptor pointed by \p algo for a stride batch matrix multiplication
 *        algorithm and allocates memory for holding the information about the algorithm.
 *        The information is defined in ::cnnlStrideBatchMatMulAlgo_t.
 *
 * @param[out] algo
 *   Output. A host pointer to the descriptor that holds
 *           information about the stride batch matrix multiplication algorithm.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call the ::cnnlDestroyStrideBatchMatMulAlgo function at the end to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCreateStrideBatchMatMulAlgo(cnnlStrideBatchMatMulAlgo_t *algo);

// Group:StrideBatchMatMul
/*!
 * @brief Destroys stride batch matrix multiplication algorithm descriptor \p algo
 *        that was previously created with ::cnnlStrideBatchMatMulAlgoCreate.
 *
 * The descriptor is defined in ::cnnlStrideBatchMatMulAlgo_t and holds the information
 *     about the stride batch matrix multiplication algorithm.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlDestroyStrideBatchMatMulAlgo instead.
 *
 * @param[in] algo
 *   Input. The stride batch matrix multiplication algorithm descriptor to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlDestroyStrideBatchMatMulAlgo)
cnnlStatus_t CNNL_WIN_API
cnnlStrideBatchMatMulAlgoDestroy(cnnlStrideBatchMatMulAlgo_t algo);

// Group:StrideBatchMatMul
/*!
 * @brief Destroys stride batch matrix multiplication algorithm descriptor \p algo
 *        that was previously created with ::cnnlCreateStrideBatchMatMulAlgo.
 *
 * The descriptor is defined in ::cnnlStrideBatchMatMulAlgo_t and holds the information
 *     about the stride batch matrix multiplication algorithm.
 *
 * @param[in] algo
 *   Input. The stride batch matrix multiplication algorithm descriptor to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlDestroyStrideBatchMatMulAlgo(cnnlStrideBatchMatMulAlgo_t algo);

/*! The descriptor of the stride batch matrix multiplication that holds the configured stride batch
 * matrix multiplication
 *  algorithm descriptor and its runtime properties.
 *
 *  You need to call the ::cnnlCreateStrideBatchMatMulHeuristicResult function to create a
 * descriptor.
 *  Also, you need to destroy the Cambricon CNNL context at the end with the
 * ::cnnlDestroyStrideBatchMatMulHeuristicResult function.
 */
typedef struct cnnlStrideBatchMatMulHeuristicResult *cnnlStrideBatchMatMulHeuristicResult_t;

// Group:StrideBatchMatMul
/*!
 * @brief Creates a descriptor pointed by \p result for a stride batch matrix multiplication
 * heuristic result
 *        and allocates memory for the result. The result is defined in
 * ::cnnlStrideBatchMatMulHeuristicResult_t.
 *
 * @param[out] result
 *   Output. A host pointer to the descriptor of the stride batch matrix multiplication.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call the ::cnnlDestroyStrideBatchMatMulHeuristicResult function to destroy the
 * descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t
cnnlCreateStrideBatchMatMulHeuristicResult(cnnlStrideBatchMatMulHeuristicResult_t *result);

// Group:StrideBatchMatMul
/*!
 * @brief Destroys a stride batch matrix multiplication heuristic result that was previously created
 * with ::cnnlCreateStrideBatchMatMulHeuristicResult.
 *
 * @param[in] result
 *   Input. The stride batch matrix multiplication heuristic result to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t
cnnlDestroyStrideBatchMatMulHeuristicResult(cnnlStrideBatchMatMulHeuristicResult_t result);

// Group:StrideBatchMatMul
/*!
 * @brief Gets the stride batch matrix multiplication algorithm and workspace size from heuristic
 * result,
 *        that was previously obtained with ::cnnlGetStrideBatchMatMulAlgoHeuristic_v2.
 *
 * @param[in] result
 *   Input. The stride batch matrix multiplication heuristic result obtained by
 *   ::cnnlGetStrideBatchMatMulAlgoHeuristic_v2.
 *
 * @param[out] algo
 *   Output. The stride batch matrix multiplication algorithm.
 *
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in
 *   the stride batch matrix multiplication operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t
cnnlGetStrideBatchMatMulHeuristicResult(cnnlStrideBatchMatMulHeuristicResult_t result,
                                        cnnlStrideBatchMatMulAlgo_t *algo,
                                        size_t *workspace_size);

// Group:StrideBatchMatMul
/*!
 * @brief Retrieves the possible algorithms that can be used in the stride batch matrix
 * multiplication.
 *        The output is placed in \p result_array[] in the order of increasing estimated compute time.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGetStrideBatchMatMulAlgoHeuristic_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to the Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   stride batch matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] stride_bmm_desc
 *   Input. The descriptor of the stride batch matrix multiplication operation. For detail
 * information,
 *   see ::cnnlStrideBatchMatMulDescriptor_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of the left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of the right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 * ::cnnlTensorDescriptor_t.
 * @param[in] d_desc
 *   Input. The descriptor of the input tensor used in out-of-place stride batch matrix
 *          multiplication.
 *   Not supported currently and should be set to NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.

 * @param[in] is_trans_a
 *   Input. Boolean value indicating whether left matrix is transposed.
 * @param[in] is_trans_b
 *   Input. Boolean value indicating whether right matrix is transposed.
 * @param[in] is_trans_c
 *   Input. Boolean value indicating whether output matrix is transposed.
 *   Not supported currently and should be set to false.
 * @param[in] alpha
 *   Input. A host pointer to scaling factor of tensor \p a. The default value is 1.0.
 * @param[in] beta
 *   Input. A host pointer to scaling factor of tensor \p d. The default value is 0.0.
 *   Not supported currently and should be set to 0.0.
 * @param[in] m
 *   Input. Row of \p a matrix when \p a matrix is not transposed.
 * @param[in] n
 *   Input. Column of \p b matrix when \p b matrix is not transposed.
 * @param[in] k
 *   Input. Column of \p a matrix when \p a matrix is not transposed.
 * @param[in] lda
 *   Input. Leading dimension of two-dimensional array used to store the matrix a[i].
 * @param[in] ldb
 *   Input. Leading dimension of two-dimensional array used to store the matrix b[i].
 * @param[in] ldc
 *   Input. Leading dimension of two-dimensional array used to store the matrix c[i].
 * @param[in] batch_size
 *   Input. An array that stores the number of each batch dimension. \p batch_size must be
 *   one dimension currently, and the array must has only one element. From tensor perspective,
 *   the batch dimensions must be continuous.
 * @param[in] stride_a
 *   Input. An array that gives the offsets in number of elements between a[i] and a[i + 1].
 *   The stride must be one dimension currently, and \p stride_a must has only one element.
 * @param[in] stride_b
 *   Input. An array that gives the offsets in number of elements between b[i] and b[i + 1].
 *   The stride must be one dimension currently, and \p stride_b must has only one element.
 * @param[in] stride_c
 *   Input. An array that gives the offsets in number of elements between c[i] and c[i + 1].
 *   The stride must be one dimension currently, and \p stride_c must has only one element.
 * @param[in] preference
 *   Input. The descriptor of the stride batch matrix multiplication that holds the preferences for
 *   ::cnnlStrideBatchMatMulHeuristicResult_t configuration. Currently it is not supported and should be set
 * to NULL.
 * @param[in] requested_algo_count
 *   Input. The number of requested algorithms, which is the maximum number of algorithms to be returned.
 *   Currently this value supports only 1.
 * @param[out] result_array
 *   Output. Array containing the algorithm heuristics and associated runtime characteristics
 *           in the order of increasing estimated compute time.
 * @param[out] return_algo_count
 *   Output. A host pointer to the number of algorithms returned by this function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlGetStrideBatchMatMulAlgoHeuristic_v2)
cnnlStatus_t
cnnlGetStrideBatchMatMulAlgoHeuristic(cnnlHandle_t handle,
                                      cnnlStrideBatchMatMulDescriptor_t stride_bmm_desc,
                                      cnnlTensorDescriptor_t a_desc,
                                      cnnlTensorDescriptor_t b_desc,
                                      cnnlTensorDescriptor_t c_desc,
                                      cnnlTensorDescriptor_t d_desc,
                                      const bool is_trans_a,
                                      const bool is_trans_b,
                                      const bool is_trans_c,
                                      const void *alpha,
                                      const void *beta,
                                      const int32_t m,
                                      const int32_t n,
                                      const int32_t k,
                                      const int32_t lda,
                                      const int32_t ldb,
                                      const int32_t ldc,
                                      const int32_t batch_size[],
                                      const int64_t stride_a[],
                                      const int64_t stride_b[],
                                      const int64_t stride_c[],
                                      cnnlStrideBatchMatMulPrefer_t preference,
                                      int requested_algo_count,
                                      cnnlStrideBatchMatMulHeuristicResult_t *result_array,
                                      int *return_algo_count);

// Group:StrideBatchMatMul
/*!
 * @brief Retrieves the possible algorithms that can be used in the stride batch matrix
 * multiplication. The output is placed in \p result_array[] in the order of increasing estimated compute time.
 *
 * @param[in] handle
 *   Input. Handle to the Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   stride batch matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] stride_bmm_desc
 *   Input. The descriptor of the stride batch matrix multiplication operation. For detail
 * information,
 *   see ::cnnlStrideBatchMatMulDescriptor_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of the left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of the right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the residual tensor used in out-of-place stride batch matrix multiplication.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] d_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] is_trans_a
 *   Input. Boolean value indicating whether left matrix is transposed.
 * @param[in] is_trans_b
 *   Input. Boolean value indicating whether right matrix is transposed.
 * @param[in] alpha
 *   Input. A host pointer to scaling factor of tensor \p a. The default value is 1.0.
 * @param[in] beta
 *   Input. A host pointer to scaling factor of tensor \p d. The default value is 0.0.
 *   Not supported currently and should be set to 0.0.
 * @param[in] m
 *   Input. Row of \p a matrix when \p a matrix is not transposed.
 * @param[in] n
 *   Input. Column of \p b matrix when \p b matrix is not transposed.
 * @param[in] k
 *   Input. Column of \p a matrix when \p a matrix is not transposed.
 * @param[in] lda
 *   Input. Leading dimension of two-dimensional array used to store the matrix a[i].
 * @param[in] ldb
 *   Input. Leading dimension of two-dimensional array used to store the matrix b[i].
 * @param[in] ldc
 *   Input. Leading dimension of two-dimensional array used to store the matrix c[i].
 * @param[in] ldd
 *   Input. Leading dimension of two-dimensional array used to store the matrix d[i].
 * @param[in] batch_size
 *   Input. An array that stores the number of each batch dimension. \p batch_size must be
 *   one dimension currently, and the array must has only one element. From tensor perspective,
 *   the batch dimensions must be continuous.
 * @param[in] stride_a
 *   Input. An array that gives the offsets in number of elements between a[i] and a[i + 1].
 *   The stride must be one dimension currently, and \p stride_a must have only one element.
 * @param[in] stride_b
 *   Input. An array that gives the offsets in number of elements between b[i] and b[i + 1].
 *   The stride must be one dimension currently, and \p stride_b must have only one element.
 * @param[in] stride_c
 *   Input. An array that gives the offsets in number of elements between c[i] and c[i + 1].
 *   The stride must be one dimension currently, and \p stride_c must have only one element.
 * @param[in] stride_d
 *   Input. An array that gives the offsets in number of elements between d[i] and d[i + 1].
 *   The stride must be one dimension currently, and \p stride_d must have only one element.
 * @param[in] preference
 *   Input. The descriptor of the stride batch matrix multiplication that holds the preferences for
 *   ::cnnlStrideBatchMatMulHeuristicResult_t configuration. Currently it is not supported and should be set
 *   to NULL.
 * @param[in] requested_algo_count
 *   Input. The number of requested algorithms, which is the maximum number of algorithms to be returned.
 *   Currently this value supports only 1.
 * @param[out] result_array
 *   Output. Array containing the algorithm heuristics and associated runtime characteristics
 *           in the order of increasing estimated compute time.
 * @param[out] return_algo_count
 *   Output. A host pointer to the number of algorithms returned by this function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t
cnnlGetStrideBatchMatMulAlgoHeuristic_v2(cnnlHandle_t handle,
                                         const cnnlStrideBatchMatMulDescriptor_t stride_bmm_desc,
                                         const cnnlTensorDescriptor_t a_desc,
                                         const cnnlTensorDescriptor_t b_desc,
                                         const cnnlTensorDescriptor_t c_desc,
                                         const cnnlTensorDescriptor_t d_desc,
                                         const bool is_trans_a,
                                         const bool is_trans_b,
                                         const void *alpha,
                                         const void *beta,
                                         const int32_t m,
                                         const int32_t n,
                                         const int32_t k,
                                         const int32_t lda,
                                         const int32_t ldb,
                                         const int32_t ldc,
                                         const int32_t ldd,
                                         const int32_t batch_size[],
                                         const int64_t stride_a[],
                                         const int64_t stride_b[],
                                         const int64_t stride_c[],
                                         const int64_t stride_d[],
                                         cnnlStrideBatchMatMulPrefer_t preference,
                                         int requested_algo_count,
                                         cnnlStrideBatchMatMulHeuristicResult_t *result_array,
                                         int *return_algo_count);

// Group:StrideBatchMatMul
/*!
 * @brief Computes the batch matrices multiplication of a batch of matrices, and then returns the
 * results in the output tensor \p c. Input matrices \p a, \p b, \p d and output matrix \p c for each
 * instance of the batch are located at fixed offsets in number of elements from their locations
 * in the previous instance.
 *
 * The main differences between ::cnnlStrideBatchMatMul_v2 and ::cnnlStrideBatchMatMul are:
 * - ::cnnlStrideBatchMatMul_v2 supports multiple batches, so parameters \p batch_size, \p stride_a, \p stride_b
 *   and \p stride_c are given with the form of array instead of integer.
 * - ::cnnlStrideBatchMatMul_v2 calculates c = alpha * a * b + beta * d instead of c = alpha * a * b.
 * - ::cnnlStrideBatchMatMul_v2 can use workspace.
 * - TensorFloat-32 mode is supported in ::cnnlStrideBatchMatMul_v2.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlStrideBatchMatMul_v3 instead.
 *
 * @param[in] handle
 *   Input. Handle to the Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   batch matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] stride_bmm_desc
 *   Input. The descriptor of the stride batch matrix multiplication operations. For detail
 information,
 *   see ::cnnlStrideBatchMatMulDescriptor_t.
 * @param[in] algo
 *   Input. A host pointer to the most suitable algorithm to compute the stride batch matrix
 multiplication.
 * @param[in] is_trans_a
 *   Input. Boolean value indicating whether \p a matrix is transposed.
 * @param[in] is_trans_b
 *   Input. Boolean value indicating whether \p b matrix is transposed.
 * @param[in] is_trans_c
 *   Input. Boolean value indicating whether \p c matrix is transposed. Currently it can only be set to false.
 * @param[in] m
 *   Input. Row of \p a matrix when \p a matrix is not transposed.
 * @param[in] n
 *   Input. Column of \p b matrix when \p b matrix is not transposed.
 * @param[in] k
 *   Input. Column of \p a matrix when \p a matrix is not transposed.
 * @param[in] batch_size
 *   Input. An array that stores the number of each batch dimension. \p batch_size must be
 *   one dimension currently, and the array must has only one element. From tensor perspective,
 *   the batch dimensions must be continuous.
 * @param[in] alpha
 *   Input. A host pointer to scaling factor of tensor \p a. The default value is 1.0.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of the left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. Pointer to the MLU memory that stores the matrix \p a, which corresponds to the first
 instance of the batch.
 *   If \p is_transa is false, the dimension of \p a is lda * m with \p lda,
 *   which should be equal to or greater than \p k.
 *   Otherwise, the dimension is lda * k with \p lda, which should be equal to or greater than \p m.
 * @param[in] lda
 *   Input. Leading dimension of two-dimensional array used to store the matrix a[i].
 * @param[in] stride_a
 *   Input. An array that gives the offsets in number of elements between a[i] and a[i + 1].
 *   The stride must be one dimension currently, and the array \p stride_a must have only one element.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of the right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. Pointer to the MLU memory that stores the matrix \p b, which corresponds to the first
 instance of the batch.
 *   If \p is_transb is false, the dimension of \p b is ldb * k with \p ldb,
 *   which should be equal to or greater than \p n.
 *   Otherwise, the dimension is ldb * n  with \p ldb, which should be equal to or greater than \p k.
 * @param[in] ldb
 *   Input. Leading dimension of two-dimensional array used to store the matrix b[i].
 * @param[in] stride_b
 *   Input. An array that gives the offsets in number of elements between b[i] and b[i + 1].
 *   The stride must be one dimension currently, and the array \p stride_b must have only one element.
 * @param[in] beta
 *   Input. A host pointer to scaling factor of tensor \p d. The default value is 0.0.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] c
 *   Output. Pointer to the MLU memory that stores the matrix \p c, which corresponds to the first
 instance of the batch,
 *   with dimensions ldc * m where ldc >= n.
 * @param[in] ldc
 *   Input. Leading dimension of two-dimensional array used to store the matrix c[i] and d[i].
 * @param[in] stride_c
 *   Input. An array that gives the offsets in number of elements between c[i] and c[i + 1], d[i] and d[i + 1].
 *   The stride must be one dimension currently, and the array \p stride_c must have only one element.
 * @param[in] d_desc
 *   Input. The descriptor of the input tensor used in out-of-place stride batch matrix
 multiplication. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] d
 *   Input. Pointer to the MLU memory that stores the input \p d.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the stride batch matrix
 multiplication
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the \p workspace in bytes. You can get the size of the workspace with the
 * ::cnnlGetStrideBatchMatMulAlgoHeuristic and ::cnnlGetStrideBatchMatMulHeuristicResult functions in turn.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "StrideBatchMatMul Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for input tensor
 *   \p a, \p b and output tensor \p c.
 *   - \p a, \p b, \p c offchip data type, \p c onchip data type: half, half, half, half.
 *   - \p a, \p b, \p c offchip data type, \p c onchip data type: half, half, half, float.
 *   - \p a, \p b, \p c offchip data type, \p c onchip data type: bfloat16, bfloat16, bfloat16, float.
 *   - \p a, \p b, \p c offchip data type, \p c onchip data type: float, float, float, float.
 * - When the input data type is half or bfloat16, and the output is INF on MLU300 series, unexpected
 *   saturated data may be returned.
 *
 * @note
 * - The following conditions must be satisfied:
 *   - \p a, \p b and \p c are arrays of pointers to matrices stored in row-major format.
 *   - The hardware platform must be MLU300 series or above.
 *
 * @par Scale Limitation
 * - The input tensors and output tensor must meet the following requirements:
 *   - Total number of elements in the input tensor \p a must be greater than or equal to
 *     (batch_size - 1) * stride_a + lda * ((is_transa ? k : m) - 1) + (is_transa ? m : k).
 *   - Total number of elements in the input tensor \p b must be greater than or equal to
 *     (batch_size - 1) * stride_b + ldb * ((is_transb ? n : k) - 1) + (is_transb ? k : n).
 *   - Total number of elements in the output tensor \p c must be greater than or equal to
 *     (batch_size - 1) * stride_c + ldc * (m - 1) + n.
 *
 * @par API Dependency
 * - Before calling this function to implement batch matrix multiplication, you need to prepare
 *   all the parameters passed to this function. See each parameter description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, matrix \p a does not need to transpose,
 *   and matrix \p b needs to transpose.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      is_transa:  false
      is_transb:  false
      is_transc:  false
      m:  80
      n:  80
      k:  64
      lda:  64
      ldb:  80
      ldc:  80
      batch_size: 64
      stride_a: 15360
      stride_b: 15360
      stride_c: 19200
      alpha:  1
      beta: 0
      Dimension of input tensor a:  [972800]
      Dimension of input tensor b:  [972800]
      Dimension of output tensor c: [1216000]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/torch.html?highlight=bmm#torch.bmm
 */
CNNL_DEPRECATED_FOR(cnnlStrideBatchMatMul_v3)
cnnlStatus_t CNNL_WIN_API
cnnlStrideBatchMatMul_v2(cnnlHandle_t handle,
                         cnnlStrideBatchMatMulDescriptor_t stride_bmm_desc,
                         cnnlStrideBatchMatMulAlgo_t algo,
                         const bool is_trans_a,
                         const bool is_trans_b,
                         const bool is_trans_c,
                         const int32_t m,
                         const int32_t n,
                         const int32_t k,
                         const int32_t batch_size[],
                         const void *alpha,
                         cnnlTensorDescriptor_t a_desc,
                         const void *a,
                         const int32_t lda,
                         const int64_t stride_a[],
                         const cnnlTensorDescriptor_t b_desc,
                         const void *b,
                         const int32_t ldb,
                         const int64_t stride_b[],
                         const void *beta,
                         const cnnlTensorDescriptor_t c_desc,
                         void *c,
                         const int32_t ldc,
                         const int64_t stride_c[],
                         const cnnlTensorDescriptor_t d_desc,
                         void *d,
                         void *workspace,
                         size_t workspace_size);

// Group:StrideBatchMatMul
/*!
 * @brief Computes the batch matrices multiplication of a batch of matrices, and then returns the
 * results in the output tensor \p d. Input matrices \p a, \p b, \p c and output matrix \p d for each
 * instance of the batch are located at fixed offsets in number of elements from their locations
 * in the previous instance.
 *
 * The main differences between ::cnnlStrideBatchMatMul_v3 and ::cnnlStrideBatchMatMul_v2 are:
 * - The meanings of parameters \p c and \p d are interchanged: For ::cnnlStrideBatchMatMul_v3,
 *   \p c represents residual, and \p d represents output.
 * - Add parameters \p ldd and \p stride_d for output.
 * - \p is_trans_c is deleted.
 * - Fix point input is supported.
 * - Residual \p c supports the following shapes:
 *   [batch_size, m, n], [batch_size, n] (\p ldc == 0), [m, n] (\p stride_c == {0}), [n] (\p ldc == 0 && \p stride_c == {0}).
 *
 * @param[in] handle
 *   Input. Handle to the Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   batch matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] stride_bmm_desc
 *   Input. The descriptor of the stride batch matrix multiplication operations. For detail
 information,
 *   see ::cnnlStrideBatchMatMulDescriptor_t.
 * @param[in] algo
 *   Input. A host pointer to the most suitable algorithm to compute the stride batch matrix
 multiplication.
 * @param[in] is_trans_a
 *   Input. Boolean value indicating whether \p a matrix is transposed.
 * @param[in] is_trans_b
 *   Input. Boolean value indicating whether \p b matrix is transposed.
 * @param[in] m
 *   Input. Row of \p a matrix when \p a matrix is not transposed.
 * @param[in] n
 *   Input. Column of \p b matrix when \p b matrix is not transposed.
 * @param[in] k
 *   Input. Column of \p a matrix when \p a matrix is not transposed.
 * @param[in] batch_size
 *   Input. An array that stores the number of each batch dimension. \p batch_size must be
 *   one dimension currently, and the array must has only one element. From tensor perspective,
 *   the batch dimensions must be continuous.
 * @param[in] alpha
 *   Input. A host pointer to scaling factor of tensor \p a. The default value is 1.0.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of the left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. Pointer to the MLU memory that stores the matrix \p a, which corresponds to the first
 instance of the batch.
 *   If \p is_transa is false, the dimension of \p a is lda * m with \p lda,
 *   which should be equal to or greater than \p k.
 *   Otherwise, the dimension is lda * k with \p lda, which should be equal to or greater than \p m.
 * @param[in] lda
 *   Input. Leading dimension of two-dimensional array used to store the matrix a[i].
 * @param[in] stride_a
 *   Input. An array that gives the offsets in number of elements between a[i] and a[i + 1].
 *   The stride must be one dimension currently, and the array \p stride_a must have only one element.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of the right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. Pointer to the MLU memory that stores the matrix \p b, which corresponds to the first
 instance of the batch.
 *   If \p is_transb is false, the dimension of \p b is ldb * k with \p ldb,
 *   which should be equal to or greater than \p n.
 *   Otherwise, the dimension is ldb * n  with \p ldb, which should be equal to or greater than \p k.
 * @param[in] ldb
 *   Input. Leading dimension of two-dimensional array used to store the matrix b[i].
 * @param[in] stride_b
 *   Input. An array that gives the offsets in number of elements between b[i] and b[i + 1].
 *   The stride must be one dimension currently, and the array \p stride_b must have only one element.
 * @param[in] beta
 *   Input. A host pointer to scaling factor of tensor \p d. The default value is 0.0.
 * @param[in] c_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] c
 *   Input. Pointer to the MLU memory that stores the input tensor \p c in out-of-place matrix multiplication
 *   where d = alpha * a * b + beta * c, or pointer to the MLU memory that stores the output tensor \p d in in-place
 *   matrix multiplication where c == d = alpha * a * b + beta * c.
 * @param[in] ldc
 *   Input. Leading dimension of two-dimensional array used to store the matrix c[i].
 * @param[in] stride_c
 *   Input. An array that gives the offsets in number of elements between c[i] and c[i + 1].
 *   The stride must be one dimension currently, and the array \p stride_c must have only one element.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the stride batch matrix
 *   multiplication operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the \p workspace in bytes. You can get the size of the workspace with the
 *   ::cnnlGetStrideBatchMatMulAlgoHeuristic_v2 and ::cnnlGetStrideBatchMatMulHeuristicResult functions in turn.
 * @param[in] d_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] d
 *   output. Pointer to the MLU memory that stores the output \p d.
 * @param[in] ldd
 *   Input. Leading dimension of two-dimensional array used to store the matrix d[i].
 * @param[in] stride_d
 *   Input. An array that gives the offsets in number of elements between d[i] and d[i + 1].
 *   The stride must be one dimension currently, and the array \p stride_d must have only one element.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH,
 *   ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @par Formula
 * - See "StrideBatchMatMul Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are as follows in the order of
 *   \p a data type - \p b data type - \p d offchip data type - \p d onchip data type:
 *   - int8 - int8 - half - float
 *   - int8 - int8 - float - float
 *   - int16 - int16 - half - float
 *   - int16 - int16 - float - float
 *   - half - half - half - half
 *   - half - half - half - float
 *   - half - half - float - float
 *   - bfloat16 - bfloat16 - bfloat16 - float
 *   - bfloat16 - bfloat16 - float - float
 *   - float - float - float - float
 * - When the input data type is half or bfloat16, and the output is INF on MLU300 series, unexpected
 *   saturated data may be returned.
 *
 * @note
 * - The following conditions must be satisfied:
 *   - \p a, \p b, \p c and \p d are arrays of pointers to matrices stored in row-major format.
 *   - The hardware platform must be MLU300 series or above.
 *
 * @par Scale Limitation
 * - The input tensors and output tensor must meet the following requirements:
 *   - Total number of elements in the input tensor \p a must be greater than or equal to
 *     (batch_size - 1) * stride_a + lda * ((is_transa ? k : m) - 1) + (is_transa ? m : k).
 *   - Total number of elements in the input tensor \p b must be greater than or equal to
 *     (batch_size - 1) * stride_b + ldb * ((is_transb ? n : k) - 1) + (is_transb ? k : n).
 *   - Total number of elements in the input tensor \p c must be greater than or equal to
 *     (batch_size - 1) * stride_c + ldc * (m - 1) + n.
 *   - Total number of elements in the output tensor \p d must be greater than or equal to
 *     (batch_size - 1) * stride_d + ldd * (m - 1) + n.
 *
 * @par API Dependency
 * - Before calling this function to implement batch matrix multiplication, you need to prepare
 *   all the parameters passed to this function. See each parameter description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, matrix \p a does not need to transpose,
 *   and matrix \p b needs to transpose.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      is_trans_a:  false
      is_trans_b:  false
      m:  80
      n:  80
      k:  64
      lda:  64
      ldb:  80
      ldc:  80
      batch_size: 64
      stride_a: 15360
      stride_b: 15360
      stride_c: 19200
      ldd: 100
      stride_d: 8000
      alpha:  1
      beta: 0
      Dimension of input tensor a:  [972800]
      Dimension of input tensor b:  [972800]
      Dimension of input tensor c: [1216000]
      Dimension of input tensor d: [512000]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/torch.html?highlight=bmm#torch.bmm
 */
cnnlStatus_t CNNL_WIN_API
cnnlStrideBatchMatMul_v3(cnnlHandle_t handle,
                         cnnlStrideBatchMatMulDescriptor_t stride_bmm_desc,
                         cnnlStrideBatchMatMulAlgo_t algo,
                         const bool is_trans_a,
                         const bool is_trans_b,
                         const int32_t m,
                         const int32_t n,
                         const int32_t k,
                         const int32_t batch_size[],
                         const void *alpha,
                         const cnnlTensorDescriptor_t a_desc,
                         const void *a,
                         const int32_t lda,
                         const int64_t stride_a[],
                         const cnnlTensorDescriptor_t b_desc,
                         const void *b,
                         const int32_t ldb,
                         const int64_t stride_b[],
                         const void *beta,
                         const cnnlTensorDescriptor_t c_desc,
                         void *c,
                         const int32_t ldc,
                         const int64_t stride_c[],
                         void *workspace,
                         size_t workspace_size,
                         const cnnlTensorDescriptor_t d_desc,
                         void *d,
                         const int32_t ldd,
                         const int64_t stride_d[]);

// Group:BatchMatMulBCast
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra workspace
 * to optimize the batch matrix multiplication with broadcasting operation.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlGetBatchMatMulHeuristicResult instead.
 *
 * The size of the extra workspace is based on the given information of the batch matrix multiplication
 * with broadcasting operation, including input tensor descriptor of left matrix \p a_desc, input tensor descriptor of
 * right matrix \p b_desc and output tensor descriptor \p c_desc.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the batch
 *   matrix multiplication with broadcasting operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of the left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of the right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the input tensor of output matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in
 *   the matrix multiplication operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlGetBatchMatMulHeuristicResult)
cnnlStatus_t CNNL_WIN_API cnnlGetBatchMatMulBCastWorkspaceSize(cnnlHandle_t handle,
                                                               const cnnlTensorDescriptor_t a_desc,
                                                               const cnnlTensorDescriptor_t b_desc,
                                                               const cnnlTensorDescriptor_t c_desc,
                                                               size_t *workspace_size);

// Group:BatchMatMulBCast
/*!
 * @brief Gets the matrix multiplication algorithm and workspace size from heuristic result,
 *        that was previously selected with ::cnnlGetBatchMatMulAlgoHeuristic.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlGetBatchMatMulExHeuristicResult instead.
 *
 * @param[in] result
 *   Input. The matrix multiplication heuristic result obtained by ::cnnlGetBatchMatMulAlgoHeuristic.
 *
 * @param[out] algo
 *   Output. The matrix multiplication algorithm.
 *
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in
 *   the matrix multiplication operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlGetBatchMatMulExHeuristicResult)
cnnlStatus_t cnnlGetBatchMatMulHeuristicResult(cnnlMatMulHeuristicResult_t result,
                                               cnnlMatMulAlgo_t algo,
                                               size_t *workspace_size);

// Group:BatchMatMulEx
/*!
 * @brief Gets the matrix multiplication algorithm and workspace size from heuristic result,
 *        which was previously selected with ::cnnlGetBatchMatMulExAlgoHeuristic.
 *
 * @param[in] result
 *   Input. The matrix multiplication heuristic result obtained by ::cnnlGetBatchMatMulExAlgoHeuristic.
 *
 * @param[out] algo
 *   Output. The matrix multiplication algorithm.
 *
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in
 *   the matrix multiplication operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t cnnlGetBatchMatMulExHeuristicResult(cnnlMatMulHeuristicResult_t result,
                                                 cnnlMatMulAlgo_t algo,
                                                 size_t *workspace_size);


// Group:BatchMatMulBCast
/*!
 * @brief Retrieves the possible algorithms can be used in the matrix multiplication.
 *        The output is placed in result_array[] in the order of increasing estimated compute time.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlGetBatchMatMulExAlgoHeuristic instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the batch
 *   matrix multiplication with broadcasting operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] bmm_bcast_desc
 *   Input. The descriptor of the matrix multiplication operations. For detail information,
 *   see ::cnnlMatMulDescriptor_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of the left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of the right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] preference
 *   Input. The descriptor of the matrix multiplication that holds the preferences for cnnlMatMulHeuristicResult_t
 *   configuration. Currently it is not supported and should be set to NULL.
 * @param[in] requested_algo_count
 *   Input. The number of requested algorithms. The maximum number of algorithms to be returned.
 *   Currently this value only supports 1.
 * @param[out] result_array
 *   Output. Array containing the algorithm heuristics and associated runtime characteristics, returned by this function,
 *   in the order of increasing estimated compute time.
 * @param[out] return_algo_count
 *   Output. A host pointer to the number of algorithms returned by this function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - Currently the maximum number of algorithms \p requested_algo_count only supports 1.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlGetBatchMatMulExAlgoHeuristic)
cnnlStatus_t CNNL_WIN_API
             cnnlGetBatchMatMulAlgoHeuristic(cnnlHandle_t handle,
                                             const cnnlMatMulDescriptor_t bmm_bcast_desc,
                                             const cnnlTensorDescriptor_t a_desc,
                                             const cnnlTensorDescriptor_t b_desc,
                                             const cnnlTensorDescriptor_t c_desc,
                                             const cnnlMatMulPrefer_t preference,
                                             int requested_algo_count,
                                             cnnlMatMulHeuristicResult_t *result_array,
                                             int *return_algo_count);

// Group:BatchMatMulEx
/*!
 * @brief Retrieves the possible algorithms that can be used in the matrix multiplication.
 *        The output is placed in result_array[] in the order of increasing estimated compute time.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the batch
 *   matrix multiplication with broadcasting operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] bmm_ex_desc
 *   Input. The descriptor of the matrix multiplication operations. For detail information,
 *   see ::cnnlMatMulDescriptor_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of the left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of the right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] preference
 *   Input. The descriptor of the matrix multiplication that holds the preferences for
 *   ::cnnlMatMulHeuristicResult_t configuration. Currently it is not supported and should be set to NULL.
 * @param[in] requested_algo_count
 *   Input. The number of requested algorithms, which is the maximum number of algorithms to be returned.
 *   Currently this value only supports 1.
 * @param[out] result_array
 *   Output. Array containing the algorithm heuristics and associated runtime characteristics, returned by this function,
 *   in the order of increasing estimated compute time.
 * @param[out] return_algo_count
 *   Output. A host pointer to the number of algorithms returned by this function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - Currently the maximum number of algorithms \p requested_algo_count only supports 1.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */

cnnlStatus_t CNNL_WIN_API
             cnnlGetBatchMatMulExAlgoHeuristic(cnnlHandle_t handle,
                                               const cnnlMatMulDescriptor_t bmm_ex_desc,
                                               const cnnlTensorDescriptor_t a_desc,
                                               const cnnlTensorDescriptor_t b_desc,
                                               const cnnlTensorDescriptor_t c_desc,
                                               const cnnlMatMulPrefer_t preference,
                                               int requested_algo_count,
                                               cnnlMatMulHeuristicResult_t *result_array,
                                               int *return_algo_count);

// Group:BatchMatMulBCast
/*!
 * @brief Computes the batch matrix multiplication with broadcasting operation,
 * then returns the results in the output tensor \p c. For more information about quantization,
 * see "Cambricon CNNL User Guide".
 *
 * This function may need extra MLU memory as the workspace to improve the batch matrix multiplication
 * with broadcasting performance. You can get the workspace size with the
 * ::cnnlGetBatchMatMulAlgoHeuristic function.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlBatchMatMulEx instead.
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the batch
 *   matrix multiplication with broadcasting operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] is_transa
 *   Input. Boolean value indicating whether \p a matrix is transposed.
 * @param[in] is_transb
 *   Input. Boolean value indicating whether \p b matrix is transposed.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of the left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. Pointer to the MLU memory that stores the left matrix.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of the right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. Pointer to the MLU memory that stores the right matrix.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] c
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the batch matrix multiplication
 *   with broadcasting operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the batch matrix multiplication
 *   with broadcasting operation. You can get the size of the workspace with the
 *   ::cnnlGetBatchMatMulBCastWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "BatchMatMulBCast Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for input tensor \p a, \p b and
 *   output tensor \p c.
 *   - \p a: int8, int16.
 *   - \p b: int8, int16.
 *   - \p c: half, float.
 * - This function supports the combinations of the following data types for input tensor \p a, \p b and
 *   output tensor \p c on MLU300 series or above.
 *   - \p a, \p b, \p c: half, half, half.
 *   - \p a, \p b, \p c: float, float, float.
 *
 * @note
 * - The combinations of the data types should satisfy the following rules:
 *   - The number of dimensions is no more than \p CNNL_DIM_MAX.
 *   - \p c offchip data type must be the same as \p c onchip data type.
 *
 * @par Scale Limitation
 * - The input tensors and output tensor must meet the following requirements:
 *   - \p a and \p b must have no less than two dimensions, and the last two dimensions of \p a and \p b
 *     are matrix multiplication compatible.
 *   - The number of columns of \p a matrix must be equal to the number of rows of \p b matrix after both
 *     inputs have performed the transpose operations according to parameters. With the exception of the
 *     last two dimensions, the other dimensions need to satisfy the broadcasting rules.
 *
 * @par API Dependency
 * - Before calling this function to implement batch matrix multiplication with broadcasting, you need to prepare
 *   all the parameters passed to this function. See each parameter description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, matrix \p a does not need to transpose and matrix \p b
 *   needs to transpose.
 * - If \p a and \p b do not need broadcasting, for best practices, it is recommended to call ::cnnlBatchMatMul.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      transa:                    false
      transb:                    false
      Dimension of input tensor a:  [99, 128]
      Dimension of input tensor b:  [64, 128, 256]
      Dimension of output tensor c: [64, 99, 256]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/torch.html?highlight=bmm#torch.bmm
 */
CNNL_DEPRECATED_FOR(cnnlBatchMatMulEx)
cnnlStatus_t CNNL_WIN_API cnnlBatchMatMulBCast(cnnlHandle_t handle,
                                               const bool is_transa,
                                               const bool is_transb,
                                               const cnnlTensorDescriptor_t a_desc,
                                               const void *a,
                                               const cnnlTensorDescriptor_t b_desc,
                                               const void *b,
                                               void *workspace,
                                               size_t workspace_size,
                                               const cnnlTensorDescriptor_t c_desc,
                                               void *c);

// Group:BatchMatMulBCast
/*!
 * @brief Computes the batch matrix multiplication with broadcasting operation,
 * and then returns the results in the output tensor \p c. For more information about quantization,
 * see "Cambricon CNNL User Guide".
 *
 * Compared with ::cnnlBatchMatMulBCast, it supports the use of \p bmm_bcast_desc
 * to pass parameters like ::CNNL_MATMUL_DESC_TRANSA.
 *
 * This function needs extra MLU memory as the workspace to improve the batch matrix multiplication
 * with broadcasting performance. You can get the workspace size with the
 * ::cnnlGetBatchMatMulAlgoHeuristic and ::cnnlGetBatchMatMulHeuristicResult functions in turn.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlBatchMatMulEx instead.
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the batch
 *   matrix multiplication with broadcasting operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] bmm_bcast_desc
 *   Input. The descriptor of the batch matrix multiplication with broadcasting operation.
 *   For detail information, see ::cnnlMatMulDescriptor_t.
 * @param[in] algo
 *   Input. A host pointer to the most suitable algorithm to compute the batch matrix multiplication
 *   with broadcasting.
 * @param[in] alpha
 *   Input. A host pointer to the scaling factor of tensor \p a.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of the left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. Pointer to the MLU memory that stores the left matrix.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of the right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. Pointer to the MLU memory that stores the right matrix.
 * @param[in] beta
 *   Input. A host pointer to the scaling factor of tensor \p c. \p beta must be zero when the dim
 *   stride of tensor \p a, \p b or \p c is not default and tensor \p c is transposed.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in,out] c
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the batch matrix multiplication
 *   with broadcasting operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the batch matrix
 *   multiplication with broadcasting operation. You can get the size of the workspace with
 *   the ::cnnlGetBatchMatMulAlgoHeuristic and ::cnnlGetBatchMatMulHeuristicResult functions in turn.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "BatchMatMulBCast Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \p a, \p b and output tensor \p c.
 *   - The offchip data type of \p a: half, float, int8, int16.
 *   - The onchip data type of \p a: half, float, int8, int16.
 *   - The offchip data type of \p b: half, float, int8, int16.
 *   - The onchip data type of \p b: half, float, int8, int16.
 *   - The offchip data type of \p c: half, float, int8, int16.
 *   - The onchip data type of \p c: half, float.
 * - This function also supports the combinations of the following data types for input tensor \p a, \p b and
 *   output tensor \p c on MLU500 series.
 *   - \p a, \p b, \p c offchip data type, \p c onchip data type: half, half, half, half.
 *   - \p a, \p b, \p c offchip data type, \p c onchip data type: half, half, half, float.
 *   - \p a, \p b, \p c offchip data type, \p c onchip data type: bfloat16, bfloat16, bfloat16, float.
 *   - \p a, \p b, \p c offchip data type, \p c onchip data type: float, float, float, float.
 * - When the input data type is half or bfloat16, and the output is INF on MLU300 series, unexpected
 *   saturated data may be returned.
 * - The following data type combinations are deprecated:
 *   - \p a data type is half and \p a onchip data type is int16.
 *   - \p b data type is half and \p b onchip data type is int16.
 *   - \p c onchip data type is half and \p c data type is int16.
 *
 * @note
 * - The data type of \p c must be floating-point when beta is not zero.
 * - The data type bitwidth of \p c onchip data type for operation computing is not shorter than
 *   \p c offchip data type.
 * - The onchip data types of \p a and \p b must be equal to the offchip data type of \p c
 *   when the offchip data type of \p c is int8 or int16.
 * - The offchip data types of \p a and \p b must be equal to the offchip data type of \p c
 *   when the offchip data type of \p a is floating point and the onchip data type of \p a is
 *   CNNL_DTYPE_INVALID or the onchip data type of \p a is the same as the offchip data type
 *   of \p a.
 * - The offchip data types of \p a and \p b must be then same when they are both floating
 *   point. The bfloat16 data type is supported only on MLU500 series.
 * - The onchip data types of \p a and \p b must be both fixed point on MLU200, CE3226 series.
 * - If \p a offchip data type is integer, it should be the same as \p a onchip data type.
 * - The floating point of \p a, \p b and \p c under the same combination need to be consistent.
 * - The number of dimensions is no more than \p CNNL_DIM_MAX.
 * - Strides are supported when data types of tensors \p a, \p b and \p c are half or float or bfloat16.
 *   You can specify the stride of all dimensions for \p a_desc, \p b_desc, \p c_desc with ::cnnlSetTensorDescriptorEx.
 *   Once the strides of all tensors are set, the lowest 3 dims of \p a_desc are [batch_size, m, k],
 *   the lowest 3 dims of \p b_desc are [batch_size, k, n], and the lowest 3 dims of \p c_desc are [batch_size, m, n].
 *   The last value of \p c_desc->stride should be equal to \p 1, or it can be equal to 0 when the last dim of \p c
 *   is 1, because transpose operation of \p c is not supported when beta is not zero. cnnlBatchMatMulBCast_v2 does
 *   not support the following conditions:
 *   - The dim number of \p c is large than 3 and \p c is not contiguous.
 *   - Both m and n is not contiguous.
 * - If you want to use stride feature, CNNL_MATMUL_USE_STRIDE attribute of \p bmm_bcast_desc should be specified as \p true
 *   with ::cnnlSetMatMulDescAttr. When stride is enabled, transpose information of \p a and \p b will be neglected,
 *   because they may be in conflict with tensors' stride values.
 *
 * @par Scale Limitation
 * - The input tensors and output tensor must meet the following requirements:
 *   - \p a and \p b must have no less than two dimensions, and the last two dimensions of \p a and \p b
 *     are matrix multiplication compatible.
 *   - The number of columns of \p a matrix must be equal to the number of rows of \p b matrix after both
 *     inputs have performed the transpose operations according to parameters. With the exception of the
 *     last two dimensions, the other dimensions need to satisfy the broadcasting rules.
 *   - The maximum dimension of \p a should be less than or equal to INT_MAX.
 *   - The maximum dimension of \p b should be less than or equal to INT_MAX.
 *   - The maximum dimension of \p c should be less than or equal to INT_MAX.
 *   - The batch size of \p a should be less than or equal to INT_MAX.
 *   - The batch size of \p b should be less than or equal to INT_MAX.
 *   - The batch size of \p c should be less than or equal to INT_MAX.
 *   - The lda of \p a should be less than or equal to INT_MAX.
 *   - The lda of \p b should be less than or equal to INT_MAX.
 *   - The lda of \p c should be less than or equal to INT_MAX.
 *
 * @par API Dependency
 * - Before calling this function to implement batch matrix multiplication with broadcasting, you need to prepare
 *   all the parameters passed to this function. See each parameter description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, matrix \p a does not need to transpose and matrix \p b
 *   needs to transpose.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      CNNL_MATMUL_DESC_TRANSA:      false
      CNNL_MATMUL_DESC_TRANSB:      false
      Dimension of input tensor a:  [99, 128]
      Dimension of input tensor b:  [64, 128, 256]
      Dimension of output tensor c: [64, 99, 256]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/torch.html?highlight=bmm#torch.bmm
 */
CNNL_DEPRECATED_FOR(cnnlBatchMatMulEx)
cnnlStatus_t CNNL_WIN_API cnnlBatchMatMulBCast_v2(cnnlHandle_t handle,
                                                  const cnnlMatMulDescriptor_t bmm_bcast_desc,
                                                  cnnlMatMulAlgo_t  algo,
                                                  const void *alpha,
                                                  const cnnlTensorDescriptor_t a_desc,
                                                  const void *a,
                                                  const cnnlTensorDescriptor_t b_desc,
                                                  const void *b,
                                                  const void *beta,
                                                  const cnnlTensorDescriptor_t c_desc,
                                                  void *c,
                                                  void *workspace,
                                                  size_t workspace_size);

// Group:cnnlBatchMatMulEx
/*!
 * @brief Computes the batch matrix multiplication with broadcasting operation,
 * and then returns the results in the output tensor \p c. For more information about quantization,
 * see "Cambricon CNNL User Guide".
 *
 * This function needs extra MLU memory as the workspace to improve the operation performance. You
 * can get the workspace size with the ::cnnlGetBatchMatMulExAlgoHeuristic and
 * ::cnnlGetBatchMatMulExHeuristicResult functions in turn.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the batch
 *   matrix multiplication with broadcasting operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] bmm_ex_desc
 *   Input. The descriptor of the batch matrix multiplication with broadcasting operation.
 *   For detail information, see ::cnnlMatMulDescriptor_t.
 * @param[in] algo
 *   Input. A host pointer to the most suitable algorithm to compute the batch matrix multiplication
 *   with broadcasting.
 * @param[in] alpha
 *   Input. A host pointer to the scaling factor of tensor \p a.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of the left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. Pointer to the MLU memory that stores the left matrix.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of the right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. Pointer to the MLU memory that stores the right matrix.
 * @param[in] beta
 *   Input. A host pointer to the scaling factor of tensor \p c. \p beta must be zero when the dim
 *   stride of tensor \p a, \p b or \p c is not default and tensor \p c is transposed.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in,out] c
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the batch matrix multiplication
 *   with broadcasting operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the batch matrix
 *   multiplication with broadcasting operation. You can get the size of the workspace with
 *   the ::cnnlGetBatchMatMulAlgoHeuristic and ::cnnlGetBatchMatMulHeuristicResult functions in turn.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "BatchMatMulBCast Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \p a, \p b and output tensor \p c.
 *   - The offchip data type of \p a: half, float, int8, int16.
 *   - The onchip data type of \p a: half, float, int8, int16.
 *   - The offchip data type of \p b: half, float, int8, int16.
 *   - The onchip data type of \p b: half, float, int8, int16.
 *   - The offchip data type of \p c: half, float, int8, int16.
 *   - The onchip data type of \p c: half, float.
 * - This function also supports the combinations of the following data types for input tensor \p a, \p b and
 *   output tensor \p c on MLU500 series.
 *   - \p a, \p b, \p c offchip data type, \p c onchip data type: half, half, half, half.
 *   - \p a, \p b, \p c offchip data type, \p c onchip data type: half, half, half, float.
 *   - \p a, \p b, \p c offchip data type, \p c onchip data type: bfloat16, bfloat16, bfloat16, float.
 *   - \p a, \p b, \p c offchip data type, \p c onchip data type: float, float, float, float.
 * - When the input data type is half or bfloat16, and the output is INF on MLU300 series, unexpected
 *   saturated data may be returned.
 * - The following data type combinations are deprecated:
 *   - \p a data type is half and \p a onchip data type is int16.
 *   - \p b data type is half and \p b onchip data type is int16.
 *   - \p c onchip data type is half and \p c data type is int16.
 *
 * @note
 * - The data type of \p c must be floating-point when beta is not zero.
 * - The data type bitwidth of \p c onchip data type for operation computing is not shorter than
 *   \p c offchip data type.
 * - The onchip data types of \p a and \p b must be equal to the offchip data type of \p c
 *   when the offchip data type of \p c is int8 or int16.
 * - The offchip data types of \p a and \p b must be equal to the offchip data type of \p c
 *   when the offchip data type of \p a is floating point and the onchip data type of \p a is
 *   CNNL_DTYPE_INVALID, or the onchip data type of \p a is the same as the offchip data type
 *   of \p a.
 * - The offchip data types of \p a and \p b must be the same when they are both floating
 *   point. The bfloat16 data type is supported only on MLU500 series.
 * - The onchip data types of \p a and \p b must be both fixed point on MLU200, CE3226 series.
 * - If \p a offchip data type is integer, it should be the same as \p a onchip data type.
 * - The floating point of \p a, \p b and \p c under the same combination need to be consistent.
 * - The number of dimensions is no more than \p CNNL_DIM_MAX.
 * - Strides will be not supported in the future.
 *
 * @par Scale Limitation
 * - The input tensors and output tensor must meet the following requirements:
 *   - \p a and \p b must have no less than two dimensions, and the last two dimensions of \p a and \p b
 *     are matrix multiplication compatible.
 *   - The number of columns of \p a matrix must be equal to the number of rows of \p b matrix after both
 *     inputs have performed the transpose operations according to parameters. With the exception of the
 *     last two dimensions, the other dimensions need to satisfy the broadcasting rules.
 *   - The maximum dimension of \p a should be less than or equal to INT_MAX.
 *   - The maximum dimension of \p b should be less than or equal to INT_MAX.
 *   - The maximum dimension of \p c should be less than or equal to INT_MAX.
 *   - The batch size of \p a should be less than or equal to INT_MAX.
 *   - The batch size of \p b should be less than or equal to INT_MAX.
 *   - The batch size of \p c should be less than or equal to INT_MAX.
 *   - The lda of \p a should be less than or equal to INT_MAX.
 *   - The lda of \p b should be less than or equal to INT_MAX.
 *   - The lda of \p c should be less than or equal to INT_MAX.
 *
 * @par API Dependency
 * - Before calling this function to implement batch matrix multiplication with broadcasting, you need to prepare
 *   all the parameters passed to this function. See each parameter description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, matrix \p a does not need to transpose and matrix \p b
 *   needs to transpose.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      CNNL_MATMUL_DESC_TRANSA:      false
      CNNL_MATMUL_DESC_TRANSB:      false
      Dimension of input tensor a:  [99, 128]
      Dimension of input tensor b:  [64, 128, 256]
      Dimension of output tensor c: [64, 99, 256]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/torch.html?highlight=bmm#torch.bmm
 */
cnnlStatus_t CNNL_WIN_API cnnlBatchMatMulEx(cnnlHandle_t handle,
                                            const cnnlMatMulDescriptor_t bmm_ex_desc,
                                            cnnlMatMulAlgo_t  algo,
                                            const void *alpha,
                                            const cnnlTensorDescriptor_t a_desc,
                                            const void *a,
                                            const cnnlTensorDescriptor_t b_desc,
                                            const void *b,
                                            const void *beta,
                                            const cnnlTensorDescriptor_t c_desc,
                                            void *c,
                                            void *workspace,
                                            size_t workspace_size);

// Group:BatchMatMulBCast
/*!
 * @brief Creates a descriptor pointed by \p bmm_bcast_desc for a batch matrix multiplication with
 *        broadcasting operation, and allocates memory for holding the information about the batch
 *        matrix multiplication with broadcasting operation. The information is defined in
 *        ::cnnlBatchMatMulBCastDescriptor_t.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[out] bmm_bcast_desc
 *   Output. A host pointer to the batch matrix multiplication with broadcasting descriptor that holds
 *   information about the batch matrix multiplication with broadcasting operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetBatchMatMulBCastDescAttr function to initialize
 *   and set the information to the batch matrix multiplication with broadcasting descriptor.
 * - You need to call the ::cnnlBatchMatMulBCastDescDestroy function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlBatchMatMulBCastDescCreate(cnnlBatchMatMulBCastDescriptor_t *bmm_bcast_desc);

// Group:BatchMatMulBCast
/*!
 * @brief Destroys a batch matrix multiplication with broadcasting descriptor \p bmm_bcast_desc
 *        that was previously created with ::cnnlBatchMatMulBCastDescCreate.
 *
 * The batch matrix multiplication with broadcasting descriptor is defined in ::cnnlBatchMatMulBCastDescriptor_t
 * and holds the information about the batch matrix multiplication with broadcasting operation.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] bmm_bcast_desc
 *   Input. The batch matrix multiplication with broadcasting descriptor to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlBatchMatMulBCastDescDestroy(cnnlBatchMatMulBCastDescriptor_t bmm_bcast_desc);

// Group:BatchMatMulBCast
/*!
 * @brief Initializes the batch matrix multiplication with broadcasting descriptor \p bmm_bcast_desc
 * that was previously created with the ::cnnlBatchMatMulBCastDescCreate function, and sets
 * the information about the batch matrix multiplication with broadcasting operation to the batch matrix
 * multiplication with broadcasting descriptor \p bmm_bcast_desc. The information includes the attribute \p attr
 * defined in ::cnnlBatchMatMulBCastDescAttribute_t, the host pointer \p buf to the attribute value,
 * and the size of buffer for verification.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in,out] bmm_bcast_desc
 *   Input/output. The descriptor of the batch matrix multiplication with broadcasting operation. For detailed
 *   information, see ::cnnlBatchMatMulBCastDescriptor_t.
 * @param[in] attr
 *   Input. Attribute of batch matrix multiplication with broadcasting descriptor to be set. For detailed
 *   information, see ::cnnlBatchMatMulBCastDescAttribute_t.
 * @param[in] buf
 *   Input. A host pointer to the attribute value set by this function.
 * @param[in] size_in_bytes
 *   Input. Buffer in bytes for verification.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlSetBatchMatMulBCastDescAttr(const cnnlBatchMatMulBCastDescriptor_t bmm_bcast_desc,
                                const cnnlBatchMatMulBCastDescAttribute_t attr,
                                const void *buf,
                                size_t size_in_bytes);

// Group:BatchMatMulBCast
/*!
 * @brief Returns the pointer to the \p buf and size of the buffer \p size_written of the attribute
 * retrieved with the given batch matmul multiplication with broadcasting descriptor \p bmm_bcast_desc,
 * attribute \p attr.
 *
 * You can set the attribute in the batch matrix multiplication with broadcasting descriptor based on
 * the return value of this function.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] bmm_bcast_desc
 *   Input. The descriptor of the batch matrix multiplication with broadcasting operation. For detailed
 *   information, see ::cnnlBatchMatMulBCastDescriptor_t.
 * @param[in] attr
 *   Input. Attribute of batch matrix multiplication with broadcasting descriptor to be retrieved.
 * @param[out] buf
 *   Output. A host pointer to the attribute value to be retrieved by this function.
 * @param[in] size_in_bytes
 *   Input. Buffer in bytes for verification, which is used to check whether the memory size is the same as \p size_written.
 * @param[out] size_written
 *   Output. A host pointer to the number of bytes actually written to the buffer.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlGetBatchMatMulBCastDescAttr(const cnnlBatchMatMulBCastDescriptor_t bmm_bcast_desc,
                                cnnlBatchMatMulBCastDescAttribute_t attr,
                                void *buf,
                                size_t size_in_bytes,
                                size_t *size_written);

// Group:BatchMatMulBCast
/*!
 * @brief Creates a descriptor pointed by \p algo for a batch matrix multiplication with broadcasting algorithm,
 *        and allocates memory for holding the information about the algorithm.
 *        The information is defined in ::cnnlBatchMatMulBCastAlgo_t. For more information about descriptor,
 *        see "Cambricon CNNL User Guide".
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[out] algo
 *   Output. A host pointer to the batch matrix multiplication with broadcasting algorithm that holds information about
 *   the batch matrix multiplication with broadcasting algorithm.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlGetQuantizeBatchMatMulBCastAlgorithm function to initialize
 *   and set the information to the batch matrix multiplication with broadcasting algorithm.
 * - You need to call the ::cnnlBatchMatMulBCastAlgoDestroy function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlBatchMatMulBCastAlgoCreate(cnnlBatchMatMulBCastAlgo_t *algo);

// Group:BatchMatMulBCast
/*!
 * @brief Destroys a batch matrix multiplication with broadcasting algorithm descriptor \p algo
 *        that was previously created with ::cnnlBatchMatMulBCastAlgoCreate.
 *
 * The batch matrix multiplication with broadcasting descriptor is defined in ::cnnlBatchMatMulBCastAlgo_t
 * and holds the information about the batch matrix multiplication with broadcasting algorithm.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] algo
 *   Input. The batch matrix multiplication with broadcasting algorithm descriptor to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlBatchMatMulBCastAlgoDestroy(cnnlBatchMatMulBCastAlgo_t algo);

// Group:BatchMatMulBCast
/*!
 * @brief Returns the most suited batch matrix multiplication with broadcasting algorithm that can be used
 * in the operation.
 *
 * The returned batch matrix multiplication is chosen from all the supported batch matrix with
 * broadcasting algorithms defined in ::cnnlBatchMatMulBCastAlgo_t and is based on the given batch matrix
 * multiplication with broadcasting descriptor \p bmm_bcast_desc, tensor descriptor of left matrix \p a_desc,
 * tensor descriptor of right matrix \p b_desc, tensor descriptor of output matrix \p c_desc, and batch
 * matrix multiplication with broadcasting algorithm \p preference.
 *
 * The computing performance options \p preference is defined in the ::cnnlBatchMatMulBCastPreference_t
 * enum, and only supports the high speed mode.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the batch
 *   matrix multiplication with broadcasting operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] bmm_bcast_desc
 *  Input. The descriptor of the batch matrix multiplication with broadcasting operation. For detailed
 *  information, see ::cnnlBatchMatMulBCastDescriptor_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor descriptor of the left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor descriptor of the right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the input tensor descriptor of output matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] preference
 *   Input. The options for implementing the batch matrix multiplication with broadcasting operation to
 * get better performance. This parameter only supports CNNL_BMM_BCAST_FASTEST now.
 * @param[out] algo
 *   Output. A host pointer to the returned algorithm that is best suited for computing the batch matrix multiplication
 *   with broadcasting. The algorithms are defined in the ::cnnlBatchMatMulBCastAlgo_t enum.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlGetQuantizeBatchMatMulBCastAlgorithm(cnnlHandle_t handle,
                                         const cnnlBatchMatMulBCastDescriptor_t bmm_bcast_desc,
                                         const cnnlTensorDescriptor_t a_desc,
                                         const cnnlTensorDescriptor_t b_desc,
                                         const cnnlTensorDescriptor_t c_desc,
                                         cnnlBatchMatMulBCastPreference_t preference,
                                         cnnlBatchMatMulBCastAlgo_t *algo);

// Group:BatchMatMulBCast
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra workspace
 * to optimize the batch matrix multiplication with broadcasting operation.
 *
 * The size of the extra workspace is based on the given information of the batch matrix multiplication
 * with broadcasting operation, including the batch matrix multiplication with broadcasting descriptor
 * \p bmm_bcast_desc, input tensor descriptor of left matrix \p a_desc, input tensor descriptor of
 * right matrix \p b_desc, output tensor descriptor \p c_desc, and the batch matrix multiplication
 * with broadcasting algorithm \p algo. For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the batch
 *   matrix multiplication with broadcasting operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] bmm_bcast_desc
 *   Input. The descriptor of the batch matrix multiplication with broadcasting operations. For detail information,
 *   see ::cnnlBatchMatMulBCastDescriptor_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of the left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of the right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the input tensor of output matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] algo
 *   Input. The algorithm used to compute the batch matrix multiplication with broadcasting. The algorithms are defined
 *   in the ::cnnlBatchMatMulBCastAlgo_t enum. You can get the best suited algorithm with the
 *   ::cnnlGetQuantizeBatchMatMulBCastAlgorithm function.
 * @param[out] workspace_size
 *   Output. Pointer to the MLU memory that stores the returned size of the extra workspace in bytes.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called after ::cnnlGetQuantizeBatchMatMulBCastAlgorithm function. You also need to
 *   call the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions to create and set
 *   tensor descriptors \p a_desc, \p b_desc, \p c_desc before calling this function.
 * - The allocated extra workspace should be passed to the ::cnnlQuantizeBatchMatMulBCast function to
 *   performs the batch matrix multiplication with broadcasting operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlGetQuantizeBatchMatMulBCastWorkspaceSize(
                          cnnlHandle_t handle,
                          const cnnlBatchMatMulBCastDescriptor_t bmm_bcast_desc,
                          const cnnlTensorDescriptor_t a_desc,
                          const cnnlTensorDescriptor_t b_desc,
                          const cnnlTensorDescriptor_t c_desc,
                          cnnlBatchMatMulBCastAlgo_t algo,
                          size_t *workspace_size);

// Group:BatchMatMulBCast
/*!
 * @brief Quantizes data type of input tensor \p a and \p b, and computes the batch matrix
 * multiplication with broadcasting operation, then returns the results in the output tensor \p c.
 * For more information about quantization, see "Cambricon CNNL User Guide".
 *
 * This function needs extra MLU memory as the workspace to improve the batch matrix multiplication
 * with broadcasting performance. You can get the workspace size with the
 * ::cnnlGetQuantizeBatchMatMulBCastWorkspaceSize function. The batch matrix multiplication with broadcasting
 * is computed based on the algorithm set in \p algo. You can call the
 * ::cnnlGetQuantizeBatchMatMulBCastAlgorithm function to get the most suited algorithm.
 *
 * The scaling factors \p alpha and \p beta are not supported currently.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlBatchMatMulEx instead.
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the batch
 *   matrix multiplication with broadcasting operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] bmm_bcast_desc
 *   Input. The descriptor of the batch matrix multiplication with broadcasting operations.
 *   For detail information, see ::cnnlBatchMatMulBCastDescriptor_t.
 * @param[in] alpha, beta
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of the left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] a_position
 *   Input. Pointer to the MLU memory associated tensor \p a quantization param \p position.
 * @param[in] a_scale
 *   Input. Pointer to the MLU memory associated tensor \p a quantization param \p scale.
 *   The value of this parameter can be NULL.
 * @param[in] a_offset
 *   Input. Pointer to the MLU memory associated tensor \p a quantization param \p offset.
 *   The value of this parameter can be NULL.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of the right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] b_position
 *   Input. Pointer to the MLU memory associated tensor \p b quantization param \p position.
 * @param[in] b_scale
 *   Input. Pointer to the MLU memory associated tensor \p b quantization param \p scale.
 *   The value of this parameter can be NULL.
 * @param[in] b_offset
 *   Input. Pointer to the MLU memory associated tensor \p b quantization param \p offset.
 *   The value of this parameter can be NULL.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] c
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] algo
 *   Input. The algorithm used to compute the batch matrix multiplication with broadcasting.
 *   The algorithms are defined in the ::cnnlBatchMatMulBCastAlgo_t enum. You can get the best
 *   suited algorithm with the ::cnnlGetQuantizeBatchMatMulBCastAlgorithm function.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the batch matrix multiplication
 *   with broadcasting operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the batch matrix multiplication
 *   with broadcasting operation. You can get the size of the workspace with the
 *   ::cnnlGetQuantizeBatchMatMulBCastWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for input tensor \p a, \p b and
 *   output tensor \p c.
 *   - \p a ddr data type: half, float.
 *   - \p a onchip data type: int8, int16, int31.
 *   - \p b ddr data type: half, float.
 *   - \p b onchip data type: int8, int16, int31.
 *   - \p c ddr offchip data type: half, float.
 *   - The data type for operation computing: half, float.
 * - \p a ddr data type should be the same as \p b ddr data type.
 * - \p a ddr data type can be combined with any onchip data type.
 * - \p b ddr data type can be combined with any onchip data type.
 *
 * @note
 * - The combinations of the data types should satisfy the following rules:
 *   - The data type bitwidth for operation computing is not shorter than \p c ddr data type.
 *   - The data type for operation computing must be float when onchip data type is int31.
 *   - The number of dimensions is no more than \p CNNL_DIM_MAX.
 * - This function does not support offline asymmetric quantization currently.
 *
 * @par Scale Limitation
 * - The input tensors and output tensor must meet the following requirements:
 *   - \p a and \p b must have no less than two dimensions, and the last two dimensions of \p a and \p b
 *     are matrix multiplication compatible.
 *   - The number of columns of \p a matrix must be equal to the number of rows of \p b matrix after both
 *     inputs have performed the transpose operations according to parameters. With the exception of the last
 *     two dimensions, the other dimensions need to satisfy the broadcasting rules.
 *
 * @par API Dependency
 * - Before calling this function to implement batch matrix multiplication with broadcasting, you need to prepare
 *   all the parameters passed to this function. See each parameter description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, matrix \p a does not need to transpose and matrix \p b
 *   needs to transpose.
 * - If \p a and \p b do not need broadcasting, for best practices, it is recommended to call ::cnnlQuantizeBatchMatMul.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      transa:                    false
      transb:                    false
      Dimension of input tensor a:  [99, 128]
      Dimension of input tensor b:  [64, 128, 256]
      Dimension of output tensor c: [64, 99, 256]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/torch.html?highlight=bmm#torch.bmm
 */
CNNL_DEPRECATED_FOR(cnnlBatchMatMulEx)
cnnlStatus_t CNNL_WIN_API
cnnlQuantizeBatchMatMulBCast(cnnlHandle_t handle,
                             const cnnlBatchMatMulBCastDescriptor_t bmm_bcast_desc,
                             const void *alpha,
                             const cnnlTensorDescriptor_t a_desc,
                             const void *a,
                             const void *a_position,
                             const void *a_scale,
                             const void *a_offset,
                             const cnnlTensorDescriptor_t b_desc,
                             const void *b,
                             const void *b_position,
                             const void *b_scale,
                             const void *b_offset,
                             const void *beta,
                             const cnnlTensorDescriptor_t c_desc,
                             void *c,
                             cnnlBatchMatMulBCastAlgo_t algo,
                             void *workspace,
                             size_t workspace_size);

// Group:OpTensor
/*!
 * @brief Creates a descriptor pointed by \p op_tensor_desc for an OpTensor operation,
 * and allocates memory for holding the information about the ::cnnlOpTensor operation.
 * The information is defined in ::cnnlOpTensorDescriptor_t. For more information about
 * descriptor, see "Cambricon CNNL User Guide".
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlOpTensor instead.
 *
 * @param[out] op_tensor_desc
 *   Output. A host pointer to the OpTensor descriptor that holds information about the
 *   OpTensor operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 *  - After calling this function, you can call the ::cnnlSetOpTensorDescriptor function
 *    to initialize and set the information to the OpTensor descriptor.
 *  - You need to call the ::cnnlDestroyOpTensorDescriptor function to destroy the
 *    descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */

cnnlStatus_t CNNL_WIN_API cnnlCreateOpTensorDescriptor(cnnlOpTensorDescriptor_t *op_tensor_desc);

// Group:OpTensor
/*!
 * @brief Initializes the OpTensor descriptor \p op_tensor_desc that was previously created
 * with the ::cnnlCreateOpTensorDescriptor function, and sets the information about the
 * OpTensor operation to the OpTensor descriptor \p op_tensor_desc. The information
 * includes the operation of the OpTensor \p op_tensor_op, the data type of operation
 * \p op_tensor_comp_type, the NaN propagation policy \p op_tensor_nan_opt.
 *
 * @param[in,out] op_tensor_desc
 *   Input/output. The descriptor of the OpTensor operation. For detailed information,
 *   see ::cnnlOpTensorDescriptor_t.
 * @param[in] op_tensor_op
 *   Input. The specific operation for the OpTensor operation. For detailed information,
 *   see ::cnnlOpTensorDesc_t.
 * @param[in] op_tensor_comp_type
 *   Input. The data type of operation. For detailed information, see ::cnnlDataType_t.
 * @param[in] op_tensor_nan_opt
 *   Input. The NaN propagation policy. For detailed information,
 *   see ::cnnlNanPropagation_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */

cnnlStatus_t CNNL_WIN_API cnnlSetOpTensorDescriptor(cnnlOpTensorDescriptor_t op_tensor_desc,
                                                    cnnlOpTensorDesc_t op_tensor_op,
                                                    cnnlDataType_t op_tensor_comp_type,
                                                    cnnlNanPropagation_t op_tensor_nan_opt);

// Group:OpTensor
/*!
 * @brief Returns the information that is included in the OpTensor descriptor
 * \p op_tensor_desc. The operation type will be returned in the \p op_tensor_op,
 * the data type of operation will be returned in the \p op_tensor_comp_type,
 * the NaN propagation policy will be rerurned in the \p op_tensor_nan_opt.
 *
 * @param[in] op_tensor_desc
 *   Input. The descriptor of the OpTensor operation. For detailed information,
 *   see ::cnnlOpTensorDescriptor_t.
 * @param[out] op_tensor_op
 *   Output. The specific operation for the OpTensor operation. For detailed information,
 *   see ::cnnlOpTensorDesc_t.
 * @param[out] op_tensor_comp_type
 *   Output. The data type of operation. For detailed information, see ::cnnlDataType_t.
 * @param[out] op_tensor_nan_opt
 *   Output. The NaN propagation policy. For detailed information,
 *   see ::cnnlNanPropagation_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */

cnnlStatus_t CNNL_WIN_API cnnlGetOpTensorDescriptor(const cnnlOpTensorDescriptor_t op_tensor_desc,
                                                    cnnlOpTensorDesc_t *op_tensor_op,
                                                    cnnlDataType_t *op_tensor_comp_type,
                                                    cnnlNanPropagation_t *op_tensor_nan_opt);

// Group:OpTensor
/*!
 * @brief Destroys a descriptor \p op_tensor_desc that was previously created with the
 * ::cnnlCreateOpTensorDescriptor.
 *
 * @param[in]  op_tensor_desc
 *   Input. The OpTensor descriptor to be destroyed.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */

cnnlStatus_t CNNL_WIN_API cnnlDestroyOpTensorDescriptor(cnnlOpTensorDescriptor_t op_tensor_desc);

// Group:OpTensor
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory in bytes that is used as
 * an extra workspace to optimize the ::cnnlOpTensor operation.
 *
 * The size of the extra workspace is based on the given information of the input and output
 * tensor descriptors, \p a_desc, \p b_desc, and \p c_desc.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlGetOpTensorWorkspaceSize instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] a_desc
 *   Input. The descriptor of the first input tensor a. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the second input tensor b. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the tensor c used as input and output. For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is
 *   used in ::cnnlOpTensor operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */

cnnlStatus_t CNNL_WIN_API cnnlGetOpTensorWorkspaceSize(cnnlHandle_t handle,
                                                       const cnnlTensorDescriptor_t a_desc,
                                                       const cnnlTensorDescriptor_t b_desc,
                                                       const cnnlTensorDescriptor_t c_desc,
                                                       size_t *workspace_size);

// Group:OpTensor
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory in bytes that is used as
 * an extra workspace to optimize the ::cnnlOpTensor operation.
 *
 * The size of the extra workspace is based on the given information of the input and output
 * tensor descriptors, \p a_desc, \p b_desc, and \p c_desc.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] op_tensor_desc
 *   Input. The specific operation performed in the function. The operations are defined
 *   in the ::cnnlOpTensorDesc_t enum.
 * @param[in] alpha1
 *   Input. A host pointer to scaling factor of the tensor a.
 * @param[in] a_desc
 *   Input. The descriptor of the first input tensor a. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. A device pointer to the MLU memory that stores the first input tensor a.
 * @param[in] alpha2
 *   Input. A host pointer to scaling factor of the tensor b.
 * @param[in] b_desc
 *   Input. The descriptor of the second input tensor b. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. A device pointer to the MLU memory that stores the second input tensor b.
 * @param[in] beta
 *   Input. A host pointer to scaling factor of the tensor c.
 * @param[in] c_desc
 *   Input. The descriptor of the tensor c used as input and output. For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @param[in, out] c
 *   Input, Output. A device pointer to the MLU memory that stores the tensor c.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is
 *   used in ::cnnlOpTensor operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
CNNL_DEPRECATED_FOR(cnnlGetOpTensorWorkspaceSize)
cnnlStatus_t CNNL_WIN_API
cnnlGetOpTensorWorkspaceSize_v2(cnnlHandle_t handle,
                                const cnnlOpTensorDescriptor_t op_tensor_desc,
                                const void *alpha1,
                                const cnnlTensorDescriptor_t a_desc,
                                const void *a,
                                const void *alpha2,
                                const cnnlTensorDescriptor_t b_desc,
                                const void *b,
                                const void *beta,
                                const cnnlTensorDescriptor_t c_desc,
                                void *c,
                                size_t *workspace_size);

// Group:OpTensor
/*!
 * @brief Implements the basic operation that are widely used in artificial intelligence with the
 * following formula:
 *
 * c = op(alpha1[0] * a, alpha2[0] * b) + beta[0] * c
 *
 * where op is the basic operation defined by ::cnnlOpTensorDesc_t enum and is indicated
 * by the descriptor ::cnnlOpTensorDescriptor_t to represent the type of \p op_tensor_desc.
 * The \p a, \p b, and \p c are tensors and \p alpha1, \p alpha2, and \p beta are the
 * scaling factors used in the operation.
 *
 * This function may need extra MLU memory as the workspace to support broadcast operation.
 * You can get the size of the workspace with ::cnnlGetOpTensorWorkspaceSize. For more information
 * about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   optensor operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] op_tensor_desc
 *   Input. The specific operation performed in the function. The operations are defined
 *   in the ::cnnlOpTensorDesc_t enum.
 * @param[in] alpha1
 *   Input. A host pointer to scaling factor of tensor a.
 * @param[in] a_desc
 *   Input. The descriptor of the first input tensor a. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. A device pointer to the MLU memory that stores the first input tensor a.
 * @param[in] alpha2
 *   Input. A host pointer to scaling factor of tensor b.
 * @param[in] b_desc
 *   Input. The descriptor of the second input tensor b. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. A device pointer to the MLU memory that stores the second input tensor b.
 * @param[in] workspace
 *   Input. A device pointer to the MLU memory that is used as an extra workspace for this
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in this
 *   operation. You can get the size of workspace with ::cnnlGetOpTensorWorkspaceSize.
 * @param[in] beta
 *   Input. A host pointer to scaling factor of tensor c.
 * @param[in] c_desc
 *   Input. The descriptor of the tensor c used as input and output. For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @param[in, out] c
 *   Input, Output. A device pointer to the MLU memory that stores the tensor c.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "OpTensor Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - By the order of \p a - \p b - \p c - \p alpha1 - \p alpha2 - \p beta, the supported data
 *   types are as follows:
 *   - float - float - float - float* - float* - float*
 *   - int32 - int32 - int32 - int32* - int32* - int32*
 *   - half  - half  - half  - float* - float* - float*
 *   - float - half  - float - float* - float* - float*
 *   - half  - float - float - float* - float* - float*
 *   - half  - float - half  - float* - float* - float*
 *   - float - half  - half  - float* - float* - float*
 *   - float - int32 - float - float* - float* - float*
 *   - int32 - float - float - float* - float* - float*
 *   - half  - int32 - half  - float* - float* - float*
 *   - int32 - half  - half  - float* - float* - float*
 *   - bfloat16  - bfloat16  - bfloat16  - float* - float* - float*
 *   - float - bfloat16  - float - float* - float* - float*
 *   - bfloat16  - float - float - float* - float* - float*
 *   - bfloat16  - float - bfloat16  - float* - float* - float*
 *   - float - bfloat16  - bfloat16  - float* - float* - float*
 *   - bfloat16  - int32 - bfloat16  - float* - float* - float*
 *   - int32 - bfloat16  - bfloat16  - float* - float* - float*
 *   - complex_float - complex_float  - complex_float  - float* - float* - float*
 *   - int64 - int64 - int64 - int64* - int64* - int64*
 *   - int64 - int32 - int64 - int64* - int64* - int64*
 *   - int32 - int64 - int64 - int64* - int64* - int64*
 *   - int64 - float - float - float* - float* - float*
 *   - float - int64 - float - float* - float* - float*
 *
 * @par Data Layout
 * - Data layouts of all input tensors and output tensor must be the same.
 *
 * @par Scale Limitation
 * - Broadcasting semantics is supported. Its validation must conform to following rules:
 *   - Two tensors are "broadcastable" if the following rules hold:
 *     - Each of tensor \p a and tensor \p b has at least one dimension.
 *     - When iterating over the dimension sizes, starting at the trailing dimension, the
 *       dimension sizes must either be equal, one of them is 1, or one of them does not exit.
 *   - Right examples
 *     @verbatim
         1. same shapes are always broadcastable
         shape of input tensor a: [2, 3, 4]
         shape of input tensor b: [2, 3, 4]

         2. can line up to trailing dimensions
         a and b are broadcastable because:
         1st trailing dimension: both have size 1
         2nd trailing dimension: b has size 1
         3rd trailing dimension: a size == b size
         4th trailing dimension: b dimension doesn't exit
         shape of input tensor a: [7, 3, 5, 1]
         shape of input tensor b: [   3, 1, 1]
       @endverbatim

     - Error examples
       @verbatim
         1. a and b are not broadcastable, because in the 3rd trailing dimension 6 != 3
         shape of input tensor a: [7, 6, 5, 1]
         shape of input tensor b: [   3, 1, 1]

         2. a and b are not broadcastable, because when iterating from the trailing dimension,
            the last dimension 2 != 6
         shape of input tensor a: [7, 6, 5, 2]
         shape of input tensor b: [   6]
       @endverbatim

 *   - You need to calculate the correct dimension of \p c in advance.
 *   - The number of dimensions is no more than \p CNNL_DIM_MAX.
 * - When the data type of \p input is int32, the intermediate result of \p input cannot
 *   exceed the value range of the corresponding data type.
 *
 * @par API Dependency
 * - Before calling this function to perform optensor operation, you need to get the size
 *   of workspace with ::cnnlGetOpTensorWorkspaceSize.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the dimensions of tensor \p a,
 *   \p b and \p c to be the same, and the value of scaling factors \p alpha1 equals 1,
 *   \p alpha2 equals 1, beta equals 0.
 *
 * @note
 * - You can specify the stride of all dimensions for a_desc, b_desc and c_desc with
 *   ::cnnlSetTensorDescriptorEx.
 * - When the data type is set to int64, \p alpha1, \p alpha2 and \p beta should be in range of
 *   [-140737488355328, 140737488355328).
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
       Input tensor a  :   [[1, 2, 3],
                            [4, 5, 6],
                            [7, 8, 9]]

       alpha1          :   1

       Input tensor b  :   [[1, 2, 3],
                            [4, 5, 6],
                            [7, 8, 9]]

       alpha2          :   2

       Input tensor c  :   [[1, 2, 3],
                            [4, 5, 6],
                            [7, 8, 9]]

       beta            :    0

       Output tensor c :   [[3,  6,  9],
                            [12, 15, 18],
                            [21, 24, 27]]
     @endverbatim
 *
 */

cnnlStatus_t CNNL_WIN_API cnnlOpTensor(cnnlHandle_t handle,
                                       const cnnlOpTensorDescriptor_t op_tensor_desc,
                                       const void *alpha1,
                                       const cnnlTensorDescriptor_t a_desc,
                                       const void *a,
                                       const void *alpha2,
                                       const cnnlTensorDescriptor_t b_desc,
                                       const void *b,
                                       void *workspace,
                                       size_t workspace_size,
                                       const void *beta,
                                       const cnnlTensorDescriptor_t c_desc,
                                       void *c);

// Group:AssignAdd
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the ::cnnlAssignAdd operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor \p a. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the tensor c used as input and output. For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is
 *   used in ::cnnlAssignAdd operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */

cnnlStatus_t CNNL_WIN_API cnnlGetAssignAddWorkspaceSize(cnnlHandle_t handle,
                                                        const cnnlTensorDescriptor_t a_desc,
                                                        const cnnlTensorDescriptor_t c_desc,
                                                        size_t *workspace_size);

// Group:AssignAdd
/*!
 * @brief Implements the basic operation that are widely used in artificial intelligence with the
 * following formula:
 *
 * c = alpha[0] * a + beta[0] * c
 *
 * where \p a and \p c are tensors, and \p alpha and \p beta are the scaling factors used
 * in this operation.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlOpTensor instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] alpha
 *   Input. A host pointer to scaling factor of tensor \p a.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor \p a. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. A device pointer to the MLU memory that stores the input tensor a.
 * @param[in] workspace
 *   Input. A device pointer to the MLU memory that is used as an extra workspace for this
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in this
 *   operation. You can get the size of workspace with the ::cnnlGetAssignAddWorkspaceSize
 *   function.
 * @param[in] beta
 *   Input. A host pointer to scaling factor of tensor c.
 * @param[in] c_desc
 *   Input. The descriptor of the tensor c used as input and output. For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @param[in, out] c
 *   Input, Output. A device pointer to the MLU memory that stores the tensor c.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "AssignAdd Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The combinations of the data types for all tensors \p a and \p c must be half-half or
 *   float-float. On MLU500 series, this function additionally supports bfloat16-bfloat16.
 * - \p alpha, \p beta: float*.
 *
 * @par Data Layout
 * - Data layouts of input tensor and output tensor must be the same.
 *
 * @par Scale Limitation
 * - The corresponding dimensions of input and output tensors should satisfy the following
 *   condition:
 *   - Each dimension of the input tensor \p a must match the corresponding dimension of
 *     the destination tensor \p c.
 *   - The number of dimensions is no more than \p CNNL_DIM_MAX.
 *
 * @par API Dependency
 * - Before calling this function to perform operation, you need to get the size
 *   of workspace by ::cnnlGetAssignAddWorkspaceSize.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the dimensions of tensor \p a,
 *   \p c to be the same, and the value of scaling factors \p alpha equals 1, \p beta
 *   equals 0 or 1.
 *
 * @note
 * - This function supports tensor broadcasting from \p a to \p c, and does not support that from \p c to \p a.
 * - On MLU200 series, when the data type is float16, \p a, \p c, \p a * \p alpha, \p c * \p beta and result \p c must be within [-65504, 65504].
 *   When the data type is float32, \p a, \p c, \p a * \p alpha, \p c * \p beta and result \p c must be within [-3.4 * e38, 3.4 * e38].
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
       Input tensor  a :   [[1, 2, 3]]

       alpha           :   2

       Input tensor  c :   [[1, 2, 3],
                            [4, 5, 6],
                            [7, 8, 9]]

       beta            :   1

       Output tensor   :   [[3, 6,  9],
                            [6, 9,  12],
                            [9, 12, 15]]
     @endverbatim
 *
 */

CNNL_DEPRECATED_FOR(cnnlOpTensor)
cnnlStatus_t CNNL_WIN_API cnnlAssignAdd(cnnlHandle_t handle,
                                        const void *alpha,
                                        const cnnlTensorDescriptor_t a_desc,
                                        const void *a,
                                        void *workspace,
                                        size_t workspace_size,
                                        const void *beta,
                                        const cnnlTensorDescriptor_t c_desc,
                                        void *c);

// Group:AssignSub
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the ::cnnlAssignSub operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor \p a. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the tensor c used as input and output. For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is
 *   used in ::cnnlAssignSub operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */

cnnlStatus_t CNNL_WIN_API cnnlGetAssignSubWorkspaceSize(cnnlHandle_t handle,
                                                        const cnnlTensorDescriptor_t a_desc,
                                                        const cnnlTensorDescriptor_t c_desc,
                                                        size_t *workspace_size);

// Group:AssignSub
/*!
 * @brief Implements the basic operation that are widely used in artificial intelligence with the
 * following formula:
 *
 * c = beta[0] * c - alpha[0] * a
 *
 * where \p a and \p c are tensors, and \p alpha and \p beta are the scaling factors
 * used in the operation.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlOpTensor instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] alpha
 *   Input. A host pointer to scaling factor of tensor a. The data type of alpha should be
 *   float*.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor \p a. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. A device pointer to the MLU memory that stores the input tensor a.
 * @param[in] workspace
 *   Input. A device pointer to the MLU memory that is used as an extra workspace for this
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in this
 *   operation. You can get the size of workspace with the ::cnnlGetAssignSubWorkspaceSize
 *   function.
 * @param[in] beta
 *   Input. A host pointer to scaling factor of tensor c. The data type of beta should be
 *   float*.
 * @param[in] c_desc
 *   Input. The descriptor of the tensor c used as input and output. For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @param[in, out] c
 *   Input, Output. A device pointer to the MLU memory that stores the tensor c.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "AssignSub Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The combinations of the data types for all tensors \p a and \p c must be half-half or
 *   float-float. On MLU500 series, this function additionally supports bfloat16-bfloat16.
 * - \p alpha, \p beta: float*.
 *
 * @par Data Layout
 * - Data layouts of input tensor and output tensor must be the same.
 *
 * @par Scale Limitation
 * - The corresponding dimensions of input and output tensors should satisfy the following
 *   condition:
 *   - Each dimension of the input tensor \p a must match the corresponding dimension of
 *     the destination tensor \p c.
 *   - The number of dimensions is no more than \p CNNL_DIM_MAX.
 *
 * @par API Dependency
 * - Before calling this function to perform this operation, you need to get the size of
 *   workspace by ::cnnlGetAssignSubWorkspaceSize.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the dimensions of tensor \p a,
 *   \p c to be the same, and the value of scaling factors \p alpha equals 1, \p beta
 *   equals 0 or 1.
 *
 * @note
 * - This function supports tensor broadcasting from \p a to \p c, and does not support that from \p c to \p a.
 * - On MLU200 series, when the data type is float16, \p a, \p c, \p a * \p alpha, \p c * \p beta and result \p c must be within [-65504,65504].
 *   When the data type is float32, \p a, \p c, \p a * \p alpha, \p c * \p beta and result \p c must be within [-3.4 * e38, 3.4 * e38].
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
       Input tensor  a :   [[1, 2, 3]]

       alpha           :   1

       Input tensor  c :   [[1, 2, 3],
                            [4, 5, 6],
                            [7, 8, 9]]

       beta            :   1

       Output tensor   :   [[0, 0, 0],
                            [3, 3, 3],
                            [6, 6, 6]]
     @endverbatim
 *
 */
CNNL_DEPRECATED_FOR(cnnlOpTensor)
cnnlStatus_t CNNL_WIN_API cnnlAssignSub(cnnlHandle_t handle,
                                        const void *alpha,
                                        const cnnlTensorDescriptor_t a_desc,
                                        const void *a,
                                        void *workspace,
                                        size_t workspace_size,
                                        const void *beta,
                                        const cnnlTensorDescriptor_t c_desc,
                                        void *c);

/******************************************************************************
 * Cambricon CNNL OP: AssignTo
 ******************************************************************************/

// Group:AssignTo
/*!
 * @brief Assigns input tensor \p input to output tensor \p output starting from the
 * position \p start along a dimension \p axis, and the output tensor \p output is reused as
 * the assigned tensor.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlCopy_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the assignto operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] axis
 *   Input. The dimension of input tensor along which to be assigned.
 * @param[in] start
 *   Input. The starting position of assigned dimension of the input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \p input and output tensor
 *   \p output. Data type of input tensor and output tensor should be the same.
 *   - input tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *
 * @par Scale Limitation
 * - The parameters should satisfy the following conditions:
 *
 *   - The number of input dimensions must be equal to the number of output dimensions.
 *   - All dimensions except \p axis of input and output tensors must be the same.
 *     The input is assignment tensor, the output is assigned tensor.
 *   - The dimension of input tensor on \p axis plus \p start must be less than or equal to
 *     the dimension of output tensor on \p axis.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, sets \p axis to 0.
 *
 * @note
 * - The data type of input should be the same as output.
 * - This operation is not supported on the 1V platforms.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the assignto operation is as follows:
   @verbatim
   input: assignment tensor shape (2,3)
          --> [[2,3,4],[5,6,7]]

   axis: 0

   start: 1

   output: assigned tensor shape (3,3)
           --> [[1,1,1],[1,1,1],[1,1,1]]

   Then we will get the output:

   output: tensor shape (3,3) --> [[1,1,1],[2,3,4],[5,6,7]]
   @endverbatim
 *
 */
CNNL_DEPRECATED_FOR(cnnlCopy_v2)
cnnlStatus_t CNNL_WIN_API cnnlAssignTo(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t input_desc,
                                       const void *input,
                                       size_t axis,
                                       unsigned start,
                                       const cnnlTensorDescriptor_t output_desc,
                                       void *output);

// Group:AddN
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the ::cnnlAddN_v2 operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in ::cnnlAddN_v2
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] inputs_desc[]
 *   Input. An array of descriptors for all input tensors. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] inputs_num
 *   Input. The number of tensors in array inputs[].
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is
 *   used in ::cnnlAddN operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlGetAddNWorkspaceSize(cnnlHandle_t handle,
                                                   const cnnlTensorDescriptor_t inputs_desc[],
                                                   uint32_t inputs_num,
                                                   const cnnlTensorDescriptor_t output_desc,
                                                   size_t *workspace_size);
// Group:AddN
/*!
 * @brief Computes the sum of input tensors.
 *
 * AddN operation is wildly used in artificial intelligence as a kind of basic mathematical
 * operations. Also, this operation is supported in almost all common frameworks, like
 * PyTorch and TensorFlow.
 * Compared with ::cnnlAddN, this function supports multidirectional broadcasting of input tensors.
 *
 * This function may need extra MLU memory as the workspace to support multidirectional broadcasting.
 * You can get the workspace size with the ::cnnlGetAddNWorkspaceSize
 * function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] inputs_desc[]
 *   Input. An array of descriptors for all input tensors. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] inputs[]
 *   Input. An array of device pointers to the MLU memory for the input tensors.
 * @param[in] inputs_num
 *   Input. The number of tensors in array inputs[].
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. A device pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. A device pointer to the MLU memory that is used as an extra workspace for this
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in this
 *   operation. You can get the size of workspace with the ::cnnlGetAddNWorkspaceSize
 *   function.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "AddN Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par API Dependency
 * - Before calling this function to perform AddN operation, you need to get the size
 *   of workspace by the ::cnnlGetAddNWorkspaceSize function.
 *
 * @par Data Type
 * - This function supports the following data types for input and output tensors.
 * Note that the data type of output should be the same as input.
 *   - input tensor: float, half, int32, int16, int8, uint8.
 *   - output tensor: float, half, int32, int16, int8, uint8.
 * - On MLU500 series, this function additionally supports the following data type combinations:
 *   - input tensor: bfloat16.
 *   - output tensor: bfloat16.

 *
 * @par Scale Limitation
 * - The maximum dimension of both input and output tensors is 8.
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
       Input tensor  1 :   [[1, 2, 3]]


       Input tensor  2 :   [[1],
                            [4],
                            [7]]

       Input tensor  3 :   [[1, 2, 3],
                            [4, 5, 6],
                            [7, 8, 9]]

       Input num       :   3

       Output tensor   :   [[3,  5,  7],
                            [9, 11, 13],
                            [15,17, 19]]
     @endverbatim
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlAddN_v2(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t inputs_desc[],
                                      const void *const inputs[],
                                      uint32_t inputs_num,
                                      const cnnlTensorDescriptor_t output_desc,
                                      void *output,
                                      void * workspace,
                                      size_t workspace_size);
// Group:AddN
/*!
 * @brief Computes the sum of input tensors.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release. Use
 *   ::cnnlAddN_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] inputs_desc[]
 *   Input. An array of descriptor for the all input tensors. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] inputs[]
 *   Input. An array of device pointers to the MLU memory for the all input tensors.
 * @param[in] inputs_num
 *   Input. The num of tensors in array inputs[].
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. A device pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "AddN Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input and output tensors.
 *   - input tensor: float, half.
 *   - output tensor: float, half.
 *   Note that the data type of output should be the same as inputs.
 * - On MLU500 series, this function additionally supports the following data type combinations:
 *   - input tensor: bfloat16.
 *   - output tensor: bfloat16.
 *
 * @par Data Layout
 * - Data layouts of all input tensors and output tensor must be the same.
 *
 * @par Scale Limitation
 * - The dimensions of input tensors and output tensor must be the same.
 * - The shape of input tensors and output tensor must be the same.
 * - The number of input tensors must be greater than or equal to one.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
       Input tensor  1 :   [[1, 2, 3],
                            [4, 5, 6],
                            [7, 8, 9]]

       Input tensor  2 :   [[1, 2, 3],
                            [4, 5, 6],
                            [7, 8, 9]]

       Input tensor  3 :   [[1, 2, 3],
                            [4, 5, 6],
                            [7, 8, 9]]

       Input num       :   3

       Output tensor   :   [[3,  6,  9],
                            [12, 15, 18],
                            [21, 24, 27]]
     @endverbatim
 *
 */

CNNL_DEPRECATED_FOR(cnnlAddN_v2)
cnnlStatus_t CNNL_WIN_API cnnlAddN(cnnlHandle_t handle,
                                   const cnnlTensorDescriptor_t inputs_desc[],
                                   const void *const inputs[],
                                   uint32_t inputs_num,
                                   const cnnlTensorDescriptor_t output_desc,
                                   void *output);
// Group:MulN
/*!
 * @brief Implements the equation:
 *
 * c = product(inputs)
 *
 * MulN operation is wildly used in artificial intelligence as a kind of basic mathematical
 * operations. Also, this operation is supported in almost all common frameworks, like
 * PyTorch and TensorFlow.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] inputs_desc[]
 *   Input. An array of descriptor for the all input tensors. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] inputs[]
 *   Input. An array of device pointers to the MLU memory for the all input tensors.
 * @param[in] inputs_num
 *   Input. The number of tensors in array inputs[].
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. A device pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "MulN Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input and output tensors.
 *   - input tensor: float, half.
 *   - output tensor: float, half.
 *   Note that the data type of output should be the same as inputs.
 *
 * @par Data Layout
 * - Data layouts of all input tensors and output tensor must be the same.
 *
 * @par Scale Limitation
 * - The dimensions of input tensors and output tensor must be the same.
 * - The shape of input tensors and output tensor must be the same.
 * - The number of input tensors must be greater than or equal to one.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
       Input tensor  1 :   [[1, 2, 3],
                            [4, 5, 6],
                            [7, 8, 9]]

       Input tensor  2 :   [[1, 2, 3],
                            [4, 5, 6],
                            [7, 8, 9]]

       Input tensor  3 :   [[1, 2, 3],
                            [4, 5, 6],
                            [7, 8, 9]]

       Input num       :   3

       Output tensor   :   [[1,   8,   27],
                            [64,  125, 216],
                            [343, 512, 729]]
     @endverbatim
 *
 */

cnnlStatus_t CNNL_WIN_API cnnlMulN(cnnlHandle_t handle,
                                   const cnnlTensorDescriptor_t inputs_desc[],
                                   const void *const inputs[],
                                   uint32_t inputs_num,
                                   const cnnlTensorDescriptor_t output_desc,
                                   void *output);

// Group:Floor
/*!
 * @brief Computes floor on input tensor \p x, and returns the results in the output tensor \p y.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the floor
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. The pointer of output data.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Floor Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 * - On MLU500 series, this function additionally supports the following data type combinations:
 *   - input tensor: bfloat16.
 *   - output tensor: bfloat16.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.google.cn/api_docs/python/tf/math/floor
 *
 * @note
 * - You can specify the stride of all dimensions for \p x_desc and \p y_desc with
 *   ::cnnlSetTensorDescriptorEx.
 */
cnnlStatus_t CNNL_WIN_API cnnlFloor(cnnlHandle_t handle,
                                    const cnnlTensorDescriptor_t x_desc,
                                    const void *x,
                                    const cnnlTensorDescriptor_t y_desc,
                                    void *y);

// Group:WeightNormBackward
/*!
 *  @brief Performs the backward filter normalization operator computation.
 *
 *  filter normalization is a reparameterization that decouples the magnitude of
 *  the filter tensor from its direction. The parameter filter in network layer is
 *  replaced by magnitude (g) and direction (v).
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] diff_w_desc
 *   Input. Descriptor of \p diff_w that is gradient of filter in network layer.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] diff_w
 *   Input. Pointer to the MLU memory that stores the \p diff_w tensor.
 * @param[in] v_desc
 *   Input. Descriptor of \p v that is direction of filter in network layer.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] v
 *   Input. Pointer to the MLU memory that stores the \p v tensor.
 * @param[in] g_desc
 *   Input. Descriptor of \p g that is magnitude of filter in network layer.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] g
 *   Input. Pointer to the MLU memory that stores the \p g tensor.
 * @param[in] norm_recip_desc
 *   Input. Descriptor of \p norm_recip that is intermediate data fro
 *   weightnorm forward. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] norm_recip
 *   Input. Pointer to the MLU memory that stores the \p norm_recip tensor.
 * @param[in] axis
 *   Input.  The dimension of filter to normalize in network layer.
 * @param[in] diff_v_desc
 *   Input. Descriptor of \p diff_v that gradient of \p v.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] diff_v
 *   Output. Pointer to the MLU memory that stores the \p diff_v tensor.
 * @param[in] diff_g_desc
 *   Input. Descriptor of \p diff_g that is gradient of \p g.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] diff_g
 *   Output. Pointer to the MLU memory that stores the \p diff_g tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - Data type of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float, bfloat16.
 *   - output tensor: half, float, bfloat16.
 *
 *   The bfloat16 data type is supported only on MLU500 series.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the filter normalization backward operation is as follows:
     @verbatim
       diff_w : tensor with dimension of  [2, 3, 4, 9]
       v : tensor with dimension of  [2, 3, 4, 9]
       g : tensor with dimension of  [2, 1, 1, 1]
       norm_recip : tensor with dimension of  [2, 1, 1, 1]
       axis: 0
       Then we will get the output:
       diff_v : tensor with dimension of  [2, 3, 4, 9]
       diff_g : tensor with dimension of  [2, 1, 1, 1]
      @endverbatim
 *
 * @par Reference
 * - http://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/WeightNorm.cpp
 * - http://arxiv.org/abs/1602.07868v3
 */
cnnlStatus_t CNNL_WIN_API
cnnlWeightNormBackward(cnnlHandle_t handle,
                       const cnnlTensorDescriptor_t diff_w_desc,
                       const void *diff_w,
                       const cnnlTensorDescriptor_t v_desc,
                       const void *v,
                       const cnnlTensorDescriptor_t g_desc,
                       const void *g,
                       const cnnlTensorDescriptor_t norm_recip_desc,
                       const void *norm_recip,
                       const int axis,
                       const cnnlTensorDescriptor_t diff_v_desc,
                       void *diff_v,
                       const cnnlTensorDescriptor_t diff_g_desc,
                       void *diff_g);

// Group:Exp
/*!
 * @brief Computes exponetial of input tensor \p x, and returns the results in the output tensor \p y.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the exp
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \p x.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor \p y.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Exp Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensor and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 * - On MLU500 series, this function additionally supports the following data type combinations:
 *   - input tensor: bfloat16.
 *   - output tensor: bfloat16.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - To achieve high-accuracy output data, the data value of each element in the input tensor \p x
 *   is recommended to be in [-10.0,10.0] for all data types.
 *
 * @note
 * - The input tensor \p x and output tensor \p y must have the same shape.
 * - You can specify the stride of all dimensions for \p x_desc and \p y_desc
 *   with ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://tensorflow.google.cn/api_docs/python/tf/math/exp
 */
cnnlStatus_t CNNL_WIN_API cnnlExp(cnnlHandle_t handle,
                                  const cnnlTensorDescriptor_t x_desc,
                                  const void *x,
                                  const cnnlTensorDescriptor_t y_desc,
                                  void *y);

// Group:Exp
/*!
 * @brief Computes exponetial of input tensor \p x, and returns the results in the output tensor \p y.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release. Use
 *   ::cnnlExp instead, which does not support the \p prefer parameter.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the exp
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. The \p prefer modes defined in ::cnnlComputationPreference_t enum.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \p x.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor \p y.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Exp Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensor and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 * - On MLU500 series, this function additionally supports the following data type combinations:
 *   - input tensor: bfloat16.
 *   - output tensor: bfloat16.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - To achieve high-accuracy output data, the data value of each element in the input tensor \p x
 *   is recommended to be in [-10.0,10.0] for all data types.
 *
 * @note
 * - The input tensor \p x and output tensor \p y must have the same shape.
 * - You can specify the stride of all dimensions for \p x_desc and \p y_desc
 *   with ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://tensorflow.google.cn/api_docs/python/tf/math/exp
 */
CNNL_DEPRECATED_FOR(cnnlExp)
cnnlStatus_t CNNL_WIN_API cnnlExp_v2(cnnlHandle_t handle,
                                     const cnnlComputationPreference_t prefer,
                                     const cnnlTensorDescriptor_t x_desc,
                                     const void *x,
                                     const cnnlTensorDescriptor_t y_desc,
                                     void *y);

// Group:Expm1
/*!
 * @brief Computes the exponential of the elements of input tensor \p x minus one.
 *
 * This function allows you to choose whether to perform exp operation
 * with high precision algorithm or ultra high precision.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlExpm1_v2 instead, which removes the \p prefer parameter.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   expm1 operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. The \p prefer modes defined in ::cnnlComputationPreference_t enum.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \p x.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor \p y.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Expm1 Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float, bfloat16.
 *   - output tensor: half, float, bfloat16.
 *
 *   The bfloat16 data type is supported only on MLU500 series.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The input tensor \p x and output tensor \p y must have the same shapes.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://tensorflow.google.cn/api_docs/python/tf/math/expm1
 */
CNNL_DEPRECATED_FOR(cnnlExpm1_v2)
cnnlStatus_t CNNL_WIN_API cnnlExpm1(cnnlHandle_t handle,
                                    const cnnlComputationPreference_t prefer,
                                    const cnnlTensorDescriptor_t x_desc,
                                    const void *x,
                                    const cnnlTensorDescriptor_t y_desc,
                                    void *y);

// Group:Expm1
/*!
 * @brief Computes the exponetial of the elements of input tensor \p x minus one.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   expm1 operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \p x.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor \p y.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Expm1 Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float, bfloat16.
 *   - output tensor: half, float, bfloat16.
 *
 *   The bfloat16 data type is supported only on MLU500 series.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The input tensor \p x and output tensor \p y must have the same shapes.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://tensorflow.google.cn/api_docs/python/tf/math/expm1
 */
cnnlStatus_t CNNL_WIN_API cnnlExpm1_v2(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t x_desc,
                                       const void *x,
                                       const cnnlTensorDescriptor_t y_desc,
                                       void *y);

// Group:Sqrt
/*!
 * @brief Computes the square root on input tensor \p x, and returns the results in the output tensor \p y.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the sqrt
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Sqrt Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported data type combinations of input tensor and output tensor are as follows:
 *   - half - half.
 *   - float - float.
 *   - int32 - float.
 * - On MLU500 series, this function additionally supports another data type combinations:
 *   - bfloat16 - bfloat16.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.

 * @par Reference
 * - https://www.tensorflow.google.cn/api_docs/python/tf/math/sqrt
 *
 * @note
 * - You can specify the stride of all dimensions for \p x_desc and \p y_desc
 *   with ::cnnlSetTensorDescriptorEx.
 */
cnnlStatus_t CNNL_WIN_API cnnlSqrt(cnnlHandle_t handle,
                                   const cnnlTensorDescriptor_t x_desc,
                                   const void *x,
                                   const cnnlTensorDescriptor_t y_desc,
                                   void *y);
// Group:Sqrt
/*!
 * @brief Computes square root on input tensor \p x, and returns the results in the output tensor \p y.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlSqrt instead, which does not support the \p prefer parameter.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the sqrt
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. The \p prefer modes defined in ::cnnlComputationPreference_t enum.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Sqrt Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported data type combinations of input tensor and output tensor are as follows:
 *   - half - half.
 *   - float - float.
 *   - int32 - float.
 * - On MLU500 series, this function additionally supports another data type combinations:
 *   - bfloat16 - bfloat16.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.

 * @par Reference
 * - https://www.tensorflow.google.cn/api_docs/python/tf/math/sqrt
 *
 * @note
 * - The \p prefer mode ::CNNL_COMPUTATION_ULTRAHIGH_PRECISION is not supported currently.
 * - Stride is supported when the input type is half or float, but not supported when the input type is int32.
 * - You can specify the stride of all dimensions for \p x_desc and \p y_desc
 *   with ::cnnlSetTensorDescriptorEx.
 */
CNNL_DEPRECATED_FOR(cnnlSqrt)
cnnlStatus_t CNNL_WIN_API cnnlSqrt_v2(cnnlHandle_t handle,
                                      const cnnlComputationPreference_t prefer,
                                      const cnnlTensorDescriptor_t x_desc,
                                      const void *x,
                                      const cnnlTensorDescriptor_t y_desc,
                                      void *y);
// Group:Reciprocal
/*!
 * @brief Computes the reciprocal of the elements of input tensor \p input.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   reciprocal operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH,
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Reciprocal Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensor and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 * - On MLU500 series or above, this function additionally supports bfloat16 data type.
 *   - input tensor: bfloat16.
 *   - output tensor: bfloat16.
 *
 * @par Data Layout
 * - The data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - When the data type of input \p input is float, the range of input is [-2e6, -0.00391] or [0.00391, 2e6].
 *   When the data type of input \p input is half, the range of input is [-65504, -0.00391] or [0.00391, 65504].
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 *  - The example of the reciprocal operation is as follows:
     @verbatim
     input one array by 2 * 2 --> input: [[5, 2], [3, 4]]

     output array by 2 * 2 --> output: [[0.2, 0.5], [0.3333, 0.25]]
     @endverbatim
 *
 * @par Reference
 * - https://tensorflow.google.cn/api_docs/python/tf/math/reciprocal
 */
cnnlStatus_t CNNL_WIN_API cnnlReciprocal(cnnlHandle_t handle,
                                         const cnnlTensorDescriptor_t input_desc,
                                         const void * input,
                                         const cnnlTensorDescriptor_t output_desc,
                                         void * output);

/******************************************************************************
 * Cambricon CNNL OP: SoftsignForward
 ******************************************************************************/

// Group:SoftsignForward
/*!
 * @brief Applys the softsign function on input.
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   softsignforward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "SoftsignForward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensor and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - When the data type of \p input is float and platforms is MLU200 series,
 *   the range of input is [-2e6, 2e6].
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 *  - The example of the softsign_forward operation is as follows:
     @verbatim
     input one array by 2 * 2 --> input: [[-4., 1.], [0., 3.]]

     output array by 2 * 2 --> output: [[-0.8, 0.5], [0., 0.75]]
     @endverbatim
 *
 * @par Reference
 * - https://tensorflow.org/api_docs/python/tf/nn/softsign
 */

cnnlStatus_t CNNL_WIN_API cnnlSoftsignForward(cnnlHandle_t handle,
                                              const cnnlTensorDescriptor_t x_desc,
                                              const void *x,
                                              const cnnlTensorDescriptor_t y_desc,
                                              void *y);

/******************************************************************************
 * Cambricon CNNL OP: ComplexAbs
 ******************************************************************************/
// Group:ComplexAbs
/*!
 * @brief Computes the modulus of complex numbers.
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   ComplexAbs operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor of complex numbers.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "ComplexAbs Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: complex_float.
 *   - output tensor: float.
 *
 * @par Scale Limitation
 * - On MLU200 series, when the data type of \p input is complex_float, The modulus range of
 *   input is [1e-5, 800].
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 *  - The example of the complex_abs operation is as follows:
     @verbatim
     input one array by 2 * 2 --> input: [[(-4.,-3.), (1.,0.)], [(0.,0.), (-3.,4.)]]

     output array by 2 * 2 --> output: [[5., 1.], [0., 5.]]
     @endverbatim
 *
 * @par Reference
 * - https://tensorflow.org/api_docs/python/tf/raw_ops/ComplexAbs
 */

cnnlStatus_t cnnlComplexAbs(const cnnlHandle_t handle,
                            const cnnlTensorDescriptor_t x_desc,
                            const void *x,
                            const cnnlTensorDescriptor_t y_desc,
                            void *y);

/******************************************************************************
 * Cambricon CNNL OP: SoftsignGrad
 ******************************************************************************/
// Group:SoftsignGrad
/*!
 * @brief Computes the softsign gradients of softsign operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   softsignGrad operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] gradients_desc
 *   Input. The descriptor of the gradients tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] gradients
 *   Input. Pointer to the MLU memory that stores the gradients tensor.
 * @param[in] features_desc
 *   Input. The descriptor of the features tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] features
 *   Input. Pointer to the MLU memory that stores the features tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "SoftsignGrad Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of gradients tensor and output tensor must be the same.
 * - The supported data types of gradients and output tensors are as follows:
 *   - gradients tensor: half, float.
 *   - features tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The data layout of the gradients tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The range of inputs are [-1.41e3, 1.41e3] on the MLU200.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 *  - The example of the SoftsignGrad operation is as follows:
     @verbatim
     gradients array by 2 * 2 --> input: [[1, -2], [3, -4]]
     features array by 2 * 2 --> input: [[-4, 1], [0, 3]]

     output array by 2 * 2 --> output: [[0.04, -0.5], [3, -0.25]]
     @endverbatim
 *
 * @par Reference
 * - https://tensorflow.org/api_docs/python/tf/nn/softsigngrad
 */
cnnlStatus_t CNNL_WIN_API cnnlSoftsignGrad(cnnlHandle_t handle,
                                           const cnnlTensorDescriptor_t gradients_desc,
                                           const void *gradients,
                                           const cnnlTensorDescriptor_t features_desc,
                                           const void *features,
                                           const cnnlTensorDescriptor_t output_desc,
                                           void *output);

// Group:Div
/*!
 * @brief Computes division on input tensor \p x and \p y, and returns the results
 *        in the output tensor \p output.
 *
 * This function may need extra MLU memory as the workspace to improve the division performance.
 * You can get the workspace size with the ::cnnlGetDivWorkspaceSize
 * function.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlDiv_v3 instead, which supports the \p div_desc parameter.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the division
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the dividend tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the dividend tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the divisor tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the divisor tensor.
 * @param[in] z_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] z
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the division operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the division operation.
 *   You can get the size of the workspace with the ::cnnlGetDivWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Div Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported data types of input and output tensors are as follows:
 *   - input1 tensor: half, float, float, int64, int64.
 *   - input2 tensor: half, float, int64, float, int64.
 *   - output tensor: half, float, float, float, float.
 * - On MLU500 series, this function additionally supports bfloat16 data type.
 *
 * @par Scale Limitation
 * - According to the rule of tensor broadcasting, the parameters should satisfy the following
 *   conditions.
 *
 *   Take the lowest dimension for example, c1_dim and c2_dim represent the dimensions of \p x
 *   and \p y, c3_dim represents the dimension of \p z:
 *
 *   min(c1_dim, c2_dim) == 1 or c1_dim == c2_dim
 *
 *   max(c1_dim, c2_dim) == c3_dim
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlDiv function to perform the division operation.
 *
 * @note
 * - The inputs \p x and \p y are multi-dimensional arrays, supporting up to CNNL_DIM_MAX dimensions.
 * - On MLU200 series, when input \p y data type is float, \p y data range is [-1e10,-1e-20] & [1e-20,1e10],
 *   and when \p y data type is half, \p y data range is [-65504,-1e-4] & [1e-4,65504].
 * - You can specify the stride of all dimensions for x_desc, y_desc and z_desc with ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.google.cn/api_docs/python/tf/math/divide
 */
CNNL_DEPRECATED_FOR(cnnlDiv_v3)
cnnlStatus_t CNNL_WIN_API cnnlDiv(cnnlHandle_t handle,
                                  const cnnlTensorDescriptor_t x_desc,
                                  const void *x,
                                  const cnnlTensorDescriptor_t y_desc,
                                  const void *y,
                                  void *workspace,
                                  size_t workspace_size,
                                  const cnnlTensorDescriptor_t z_desc,
                                  void *z);

// Group:Div
/*!
 * @brief Computes division on input tensor \p x and \p y, and returns the results
 *        in the output tensor \p output.
 *
 * This function may need extra MLU memory as the workspace to improve the division performance.
 * You can get the workspace size with the ::cnnlGetDivWorkspaceSize
 * function.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlDiv_v3 instead, which does not support the \p prefer parameter and supports the
 *   \p div_desc parameter.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the division
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. The \p prefer modes defined in ::cnnlComputationPreference_t enum.
 * @param[in] x_desc
 *   Input. The descriptor of the dividend tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the dividend tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the divisor tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the divisor tensor.
 * @param[in] z_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] z
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the division operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the division operation.
 *   You can get the size of the workspace with the ::cnnlGetDivWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Div Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported data types of input and output tensors are as follows:
 *   - input1 tensor: half, float, float, int64, int64.
 *   - input2 tensor: half, float, int64, float, int64.
 *   - output tensor: half, float, float, float, float.
 * - On MLU500 series, this function additionally supports bfloat16 data type.
 *
 * @par Scale Limitation
 * - According to the rule of tensor broadcasting, the parameters should satisfy the following
 *   conditions.
 *
 *   Take the lowest dimension for example, c1_dim and c2_dim represent the dimensions of \p x
 *   and \p y, c3_dim represents the dimension of \p z:
 *
 *   min(c1_dim, c2_dim) == 1 or c1_dim == c2_dim
 *
 *   max(c1_dim, c2_dim) == c3_dim
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlDiv_v2 function to perform the division operation.
 *
 * @note
 * - The inputs \p x and \p y are multi-dimensional arrays, supporting up to CNNL_DIM_MAX dimensions.
 * - On MLU200 series, when the \p prefer is ::CNNL_COMPUTATION_FAST and input \p y data type is float, \p y data range
 *   is [-1e10,-1e-20] & [1e-20,1e10].
 * - On MLU200 series, when the \p prefer is ::CNNL_COMPUTATION_HIGH_PRECISION and \p y data type is half, \p y data
 *   range is [-65504,-1e-4] & [1e-4,65504].
 * - You can specify the stride of all dimensions for x_desc, y_desc and z_desc with ::cnnlSetTensorDescriptorEx.
 * - On MLU300 series or above, \p prefer does not work currently. No matter what \p prefer is, the calculation method is the same.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.google.cn/api_docs/python/tf/math/divide
 */
CNNL_DEPRECATED_FOR(cnnlDiv_v3)
cnnlStatus_t CNNL_WIN_API cnnlDiv_v2(cnnlHandle_t handle,
                                     cnnlComputationPreference_t prefer,
                                     const cnnlTensorDescriptor_t x_desc,
                                     const void *x,
                                     const cnnlTensorDescriptor_t y_desc,
                                     const void *y,
                                     void *workspace,
                                     size_t workspace_size,
                                     const cnnlTensorDescriptor_t z_desc,
                                     void *z);

// Group:Div
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the division operation.
 *
 * The size of the extra workspace is based on the given information of the division operation,
 * including the input tensor descriptors \p x_desc and \p y_desc, and the output
 * tensor descriptor \p z_desc. For more information about the workspace, see "Cambricon
 * CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the division
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the dividend tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] y_desc
 *   Input. The descriptor of the divisor tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] z_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the division
 *   operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - According to the rule of tensor broadcasting, the parameters should satisfy the following
 *   conditions.
 *
 *   Take the lowest dimension for example, c1_dim and c2_dim represent the dimensions of \p x
 *   and \p y, c3_dim represents the dimension of \p z:
 *
 *   min(c1_dim, c2_dim) == 1 or c1_dim == c2_dim
 *
 *   max(c1_dim, c2_dim) == c3_dim
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetDivWorkspaceSize(cnnlHandle_t handle,
                                                  const cnnlTensorDescriptor_t x_desc,
                                                  const cnnlTensorDescriptor_t y_desc,
                                                  const cnnlTensorDescriptor_t z_desc,
                                                  size_t *workspace_size);

// Group:Div
/*!
 * @brief Initializes the div descriptor \p div_desc, and sets the information
 * about the division operation to the div descriptor \p div_desc. The
 * information includes the attribute \p attr defined in ::cnnlDivDescAttribute_t,
 * the host pointer \p buf to the attribute value, and the size of buffer for verification.
 *
 * @param[in,out] div_desc
 *   Input/output. The descriptor of the division operation. For detailed
 *   information, see ::cnnlDivDescriptor_t.
 * @param[in] attr
 *   Input. Attribute of the div descriptor to be set. For detailed
 *   information, see ::cnnlDivDescAttribute_t.
 * @param[in] buf
 *   Input. A host pointer to the attribute value that is set by this function.
 * @param[in] size_in_bytes
 *   Input. Buffer in bytes for verification.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetDivDescAttr(cnnlDivDescriptor_t div_desc,
                                             cnnlDivDescAttribute_t attr,
                                             const void *buf,
                                             size_t size_in_bytes);

// Group:Div
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the division operation.
 *
 * The size of the extra workspace is based on the given information of the division operation,
 * including the input tensor descriptors \p x_desc and \p y_desc, and the output
 * tensor descriptor \p z_desc. For more information about the workspace, see "Cambricon
 * CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the division
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] div_desc
 *   Input. The descriptor of the division operation. For detailed information, see ::cnnlDivDescriptor_t.
 * @param[in] x_desc
 *   Input. The descriptor of the dividend tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] y_desc
 *   Input. The descriptor of the divisor tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] z_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the division
 *   operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - According to the rule of tensor broadcasting, the parameters should satisfy the following
 *   conditions.
 *
 *   Take the lowest dimension for example, c1_dim and c2_dim represent the dimensions of \p x
 *   and \p y, c3_dim represents the dimension of \p z:
 *
 *   min(c1_dim, c2_dim) == 1 or c1_dim == c2_dim
 *
 *   max(c1_dim, c2_dim) == c3_dim
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetDivWorkspaceSize_v2(cnnlHandle_t handle,
                                                     const cnnlDivDescriptor_t div_desc,
                                                     const cnnlTensorDescriptor_t x_desc,
                                                     const cnnlTensorDescriptor_t y_desc,
                                                     const cnnlTensorDescriptor_t z_desc,
                                                     size_t *workspace_size);

// Group:Div
/*!
 * @brief Creates a descriptor pointed by \p div_desc for a division operation, and allocates
 *        memory for holding the information about the division operation. The information is
 *        defined in ::cnnlDivDescriptor_t. For more information about descriptor,
 *        see "Cambricon CNNL User Guide".
 *
 * @param[out] div_desc
 *   Output. A host pointer to the div descriptor that holds information about the
 *   div operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - After calling this function, you need to call the ::cnnlSetDivDescAttr function to
 *   initialize and set the information to the div descriptor.
 * - At the end, you need to call the ::cnnlDestroyDivDescriptor function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCreateDivDescriptor(cnnlDivDescriptor_t *div_desc);

// Group:Div
/*!
 * @brief Destroys a div descriptor \p div_desc that was previously created with the
 *        ::cnnlCreateDivDescriptor function.
 *
 * The div descriptor is defined in ::cnnlDivDescriptor_t and holds the information
 * about the division operation.
 *
 * @param[in] div_desc
 *   Input. The div descriptor to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - You need to call this function after calling the ::cnnlDiv_v3 function.
 *   Otherwise, ::CNNL_STATUS_BAD_PARAM is returned.
 * - This function should be called to destroy the div descriptor.
 *   Otherwise, the memory leak may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlDestroyDivDescriptor(cnnlDivDescriptor_t div_desc);

// Group:Div
/*!
 * @brief Computes division operations for different modes on input tensors \p x and \p y,
 *        and returns the results in the output tensor \p output.
 *
 * This function may need extra MLU memory as the workspace to improve the division performance.
 * You can get the workspace size with the ::cnnlGetDivWorkspaceSize_v2
 * function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the division
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] div_desc
 *   Input. The descriptor of the division operation. For detailed information,
 *   see ::cnnlDivDescriptor_t.
 * @param[in] x_desc
 *   Input. The descriptor of the dividend tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the dividend tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the divisor tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the divisor tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the division operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the division operation.
 *   You can get the size of the workspace with the ::cnnlGetDivWorkspaceSize_v2 function.
 * @param[in] z_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] z
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Div Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported data types of input and output tensors are as follows:
 *   - When the \p mode is \p CNNL_DIV_TRUE:
 *     - x: half, float, float, int64, int64.
 *     - y: half, float, int64, float, int64.
 *     - z: half, float, float, float, float.
 *   - When the \p mode is \p CNNL_DIV_FLOOR:
 *     - x: half, float, int32, float, int64, int64.
 *     - y: half, float, int32, int64, float, int64.
 *     - z: half, float, int32, float, float, int64.
 *   - When the \p mode is \p CNNL_DIV_TRUNC:
 *     - x: half, float, int32, float, int64, int64.
 *     - y: half, float, int32, int64, float, int64.
 *     - z: half, float, int32, float, float, int64.
 * - On MLU500 series, this function additionally supports bfloat16 data type.
 *
 * @par Scale Limitation
 * - According to the rule of tensor broadcasting, the parameters should satisfy the following
 *   conditions.
 *
 *   Take the lowest dimension for example, c1_dim and c2_dim represent the dimensions of \p x
 *   and \p y, c3_dim represents the dimension of \p z:
 *
 *   min(c1_dim, c2_dim) == 1 or c1_dim == c2_dim
 *
 *   max(c1_dim, c2_dim) == c3_dim
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlDiv_v3 function to perform the division operation.
 *
 * @note
 * - The inputs \p x and \p y are multi-dimensional arrays, supporting up to CNNL_DIM_MAX dimensions.
 * - You can specify the stride of all dimensions for \p x_desc, \p y_desc and \p z_desc with ::cnnlSetTensorDescriptorEx when
 *   the \p mode is \p CNNL_DIV_TRUE.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.div.html
 */
cnnlStatus_t CNNL_WIN_API cnnlDiv_v3(cnnlHandle_t handle,
                                     const cnnlDivDescriptor_t div_desc,
                                     const cnnlTensorDescriptor_t x_desc,
                                     const void *x,
                                     const cnnlTensorDescriptor_t y_desc,
                                     const void *y,
                                     void *workspace,
                                     size_t workspace_size,
                                     const cnnlTensorDescriptor_t z_desc,
                                     void *z);

// Group:DivNoNan
/*!
 * @brief Computes a division of \p x and \p y, and returns zero if input \p y is zero.
 *
 * This function needs extra MLU memory as the workspace to improve the
 * performance. You can get the workspace size
 * with the ::cnnlGetDivNoNanWorkspaceSize function. The input and output tensors
 * are multi-dimensional arrays, supporting up to CNNL_DIM_MAX dimensions.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the divnonan operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor which is a dividend.
 * @param[in] y_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the input tensor which is a divisor.
 * @param[out] z_desc
 *   Output. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] z
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   divnonan operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the divnonan operation. You can get the size of the workspace with
 *   the ::cnnlGetDivNoNanWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Data Type
 * - Data type of input tensors and output tensor should be the same.
 *   The supported data types of input and output tensors are as follows:
 *   - input tensors: half, float.
 *   - output tensors: half, float.
 *
 * @note
 * - The inputs \p x and \p y are multi-dimensional arrays, supporting up to
 *   CNNL_DIM_MAX dimensions.
 * - When input \p y data type is float, \p y data range is [-1e10, -1e-10] & [1e-10,1e10].
 *   when \p y data type is half, the range is [-500, -1e-3] & [1e-3, 500].
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Scale Limitation
 * - According to the rule of tensor broadcast, the parameters should satisfy the following
 *   conditions.
 *
 *   Take the lowest dimension for example:
 *
 *   min(x_dim, y_dim) == 1 or x_dim == y_dim
 *
 *   max(x_dim, y_dim) == z_dim
 *
 *   Where x_dim, and y_dim are the number of dimensions in the input tensor \p x and
 *   \p y of the divnonan operation.
 *
 * @par API Dependency
 * - Before calling this function to implement the divnonan operation, you need to prepare
 *   all the parameters passed to this function. See each parameter description for details.
 *
 * @par Reference
 * - https://www.tensorflow.google.cn/api_docs/python/tf/math/divide.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlDivNoNan(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t x_desc,
                                       const void *x,
                                       const cnnlTensorDescriptor_t y_desc,
                                       const void *y,
                                       void *workspace,
                                       size_t workspace_size,
                                       const cnnlTensorDescriptor_t z_desc,
                                       void *z);

// Group:DivNoNan
/*!
 * @brief Computes a division of \p x and \p y, and returns zero if input \p y is zero.
 *
 * This function needs extra MLU memory as the workspace to improve the
 * performance. You can get the workspace size
 * with the ::cnnlGetDivNoNanWorkspaceSize function. The input and output tensors
 * are multi-dimensional arrays, supporting up to CNNL_DIM_MAX dimensions.
 *
 * @deprecated
 *  This function is deprecated and will be removed in future release.
 *   Use ::cnnlDivNoNan instead, which does not support the \p prefer parameter.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the divnonan operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. The \p prefer modes defined in ::cnnlComputationPreference_t enum.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor which is a dividend.
 * @param[in] y_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the input tensor which is a divisor.
 * @param[out] z_desc
 *   Output. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] z
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   divnonan operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the divnonan operation. You can get the size of the workspace with
 *   the ::cnnlGetDivNoNanWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *   x - y - z.
 *   - half - half - half.
 *   - float - float - float.
 *   - bfloat16 - bfloat16 - bfloat16.
 * - The data type bfloat16 is only supported on MLU500 series.
 *
 * @note
 * - The inputs \p x and \p y are multi-dimensional arrays, supporting up to
 *   CNNL_DIM_MAX dimensions.
 * - When input \p y data type is float, \p y data range is [-1e10, -1e-10] & [1e-10,1e10].
 *   when \p y data type is half, the range is [-500, -1e-3] & [1e-3, 500].
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Scale Limitation
 * - According to the rule of tensor broadcast, the parameters should satisfy the following
 *   conditions.
 *
 *   Take the lowest dimension for example:
 *
 *   min(x_dim, y_dim) == 1 or x_dim == y_dim
 *
 *   max(x_dim, y_dim) == z_dim
 *
 *   Where x_dim, and y_dim are the number of dimensions in the input tensor \p x and
 *   \p y of the divnonan operation.
 *
 * @par API Dependency
 * - Before calling this function, you need to prepare
 *   all the parameters passed to this function. See each parameter description for details.
 *
 * @par Reference
 * - https://www.tensorflow.google.cn/api_docs/python/tf/math/divide
 *
 */
CNNL_DEPRECATED_FOR(cnnlDivNoNan)
cnnlStatus_t CNNL_WIN_API cnnlDivNoNan_v2(cnnlHandle_t handle,
                                          cnnlComputationPreference_t prefer,
                                          const cnnlTensorDescriptor_t x_desc,
                                          const void *x,
                                          const cnnlTensorDescriptor_t y_desc,
                                          const void *y,
                                          void *workspace,
                                          size_t workspace_size,
                                          const cnnlTensorDescriptor_t z_desc,
                                          void *z);

// Group:DivNoNan
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace
 * to optimize the divnonan operation.
 *
 * The size of the extra workspace is based on the given information of the divnonan
 * operation, including the input tensor descriptor \p x_desc, \p y_desc, output tensor
 * descriptor \p z_desc. For more information about the workspace,
 * see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the convolution operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] y_desc
 *   Input. The descriptor of the input tensor which is a dividend. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] z_desc
 *   Output. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in
 *   the divnonan operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetDivNoNanWorkspaceSize(cnnlHandle_t handle,
                                                       const cnnlTensorDescriptor_t x_desc,
                                                       const cnnlTensorDescriptor_t y_desc,
                                                       const cnnlTensorDescriptor_t z_desc,
                                                       size_t *workspace_size);
// Group:PowR
/*!
 * @brief Computes the power of each element in input tensor \p x with exponent tensor
 *        \p y, and returns the result in the output tensor \p z.
 *
 * This function may need extra MLU memory as the workspace to improve the powr
 * performance. You can get the workspace size with the
 * ::cnnlGetPowRWorkspaceSize function.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlPow_v2 instead, which supports the \p prefer parameter that sets the computing with
 *   faster algorithm or higher precision and supports value of \p x less than 0.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   the powr operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the exponent tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the exponent tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the powr
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the powr
 *   operation. You can get the size of the workspace with the
 *   ::cnnlGetPowRWorkspaceSize function.
 * @param[in] z_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] z
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par API Dependency
 * - You need to call the ::cnnlGetPowRWorkspaceSize function to allocate extra
 *   workspace for \p workspace.
 *
 * @par Formula
 * - See "PowR Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor \p x, exponent tensor \p y and output tensor \p z must be
 *   the same.
 * - The supported data types of input, exponent and output tensors are as follows:
 *   - input tensor: half, float.
 *   - exponent tensor: half, float.
 *   - output tensor: half, float.
 * - On MLU500 series or above, the additional supported data types of input, exponent
 *   and output tensor are as follows:
 *   - input tensor \p x: bfloat16.
 *   - exponent tensor \p y: bfloat16.
 *   - output tensor \p z: bfloat16.
 *
 * @par Scale Limitation
 * - The data range of the input tensor \p x and exponent tensor \p y should satisfy the
 *   following conditions:
 *   - On MLU200 series:
 *     - input tensor \p x >= 0.
 *     - -15.5 < y * log(x) < 15.5.
 *   - On MLU300 series and CE3226:
 *     - input tensor \p x >= 0.
 *     - if \p x = inf or -inf, \p y != 0, where inf represents infinity.
 *     - if \p x = 1, \p y != nan.
 * - According to the rule of tensor broadcast, the parameters should satisfy the following
 *   conditions. For example, c1_dim, c2_dim, and c3_dim represent the lowest dimension of
 *   \p input1, \p input2, \p output respectively:
 *   - min(c1_dim, c2_dim) == 1 or c1_dim == c2_dim.
 *   - max(c1_dim, c2_dim) == c3_dim.
 *
 * @note
 * - The input tensor \p x and exponent tensor \p y are multi-dimensional arrays, supporting
 *   up to \p CNNL_DIM_MAX dimensions.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.google.cn/api_docs/python/tf/math/pow
 */
CNNL_DEPRECATED_FOR(cnnlPow_v2)
cnnlStatus_t CNNL_WIN_API cnnlPowR(cnnlHandle_t handle,
                                   const cnnlTensorDescriptor_t x_desc,
                                   const void *x,
                                   const cnnlTensorDescriptor_t y_desc,
                                   const void *y,
                                   void *workspace,
                                   size_t workspace_size,
                                   const cnnlTensorDescriptor_t z_desc,
                                   void *z);
// Group:PowR
/*!
 * @brief Computes the power of each element in input tensor \p x with exponent tensor
 *        \p y, and returns the result in the output tensor \p z.
 *
 * This function may need extra MLU memory as the workspace to improve the powr
 * performance. You can get the workspace size with the
 * ::cnnlGetPowRWorkspaceSize function.
 *
 * @deprecated
 *  This function is deprecated and will be removed in future release.
 *    Use ::cnnlPow_v2 instead, which supports value of \p x less than 0.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   the powr operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. The algorithm used to compute the output. For detailed information,
 *   see ::cnnlComputationPreference_t. The default value of this parameter is
 *   ::CNNL_COMPUTATION_HIGH_PRECISION.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the exponent tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the exponent tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the powr
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the powr
 *   operation. You can get the size of the workspace with the
 *   ::cnnlGetPowRWorkspaceSize function.
 * @param[in] z_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] z
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par API Dependency
 * - You need to call the ::cnnlGetPowRWorkspaceSize function to allocate extra
 *   workspace for \p workspace.
 *
 * @par Formula
 * - See "PowR Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor \p x, exponent tensor \p y and output tensor \p z must be
 *   the same.
 * - The supported data types of input, exponent and output tensors are as follows:
 *   - input tensor: half, float.
 *   - exponent tensor: half, float.
 *   - output tensor: half, float.
 * - On MLU500 series, the additional supported data types of input, exponent
 *   and output tensor are as follows:
 *   - input tensor \p x: bfloat16.
 *   - exponent tensor \p y: bfloat16.
 *   - output tensor \p z: bfloat16.
 *
 * @par Scale Limitation
 * - The data range of the input tensor \p x and exponent tensor \p y should satisfy the
 *   following conditions:
 *   - On MLU200 series:
 *     - input tensor \p x >= 0.
 *     - -15.5 < y * log(x) < 15.5.
 *   - On MLU300 series and CE3226:
 *     - input tensor \p x >= 0.
 *     - if \p x = inf or -inf, \p y != 0, where inf represents infinity.
 *     - if \p x = 1, \p y != nan.
 * - According to the rule of tensor broadcast, the parameters should satisfy the following
 *   conditions. For example, c1_dim, c2_dim, and c3_dim represent the lowest dimension of
 *   \p input1, \p input2, \p output respectively:
 *   - min(c1_dim, c2_dim) == 1 or c1_dim == c2_dim.
 *   - max(c1_dim, c2_dim) == c3_dim.
 *
 * @note
 * - The input tensor \p x and exponent tensor \p y are multi-dimensional arrays, supporting
 *   up to \p CNNL_DIM_MAX dimensions.
 * - On MLU300 series or above, \p prefer does not work currently. No matter what \p prefer is, the calculation method is the same.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.google.cn/api_docs/python/tf/math/pow
 */
CNNL_DEPRECATED_FOR(cnnlPow_v2)
cnnlStatus_t CNNL_WIN_API cnnlPowR_v2(cnnlHandle_t handle,
                                      cnnlComputationPreference_t prefer,
                                      const cnnlTensorDescriptor_t x_desc,
                                      const void *x,
                                      const cnnlTensorDescriptor_t y_desc,
                                      const void *y,
                                      void *workspace,
                                      size_t workspace_size,
                                      const cnnlTensorDescriptor_t z_desc,
                                      void *z);
// Group:PowR
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 *        workspace to optimize the powr operation.
 *
 * The size of the extra workspace is based on the given information of the powr operation,
 * including the input tensor descriptors \p x_desc and \p y_desc, and the output tensor
 * descriptor \p z_desc.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   the powr operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] y_desc
 *   Input. The descriptor of the exponent tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] z_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used
 *   in the powr operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetPowRWorkspaceSize(cnnlHandle_t handle,
                                                   const cnnlTensorDescriptor_t x_desc,
                                                   const cnnlTensorDescriptor_t y_desc,
                                                   const cnnlTensorDescriptor_t z_desc,
                                                   size_t *workspace_size);
// Group:Pow
/*!
 * @brief Computes the power of each element in input tensor \p x with exponent tensor
 *        \p y, and returns the result in the output tensor \p z.
 *
 * Compared with ::cnnlPowR and ::cnnlPowR_v2, this function has no limitations on the
 * \p x value. Compared with ::cnnlPowN and ::cnnlPowN_v2, the data type of \p y is
 * float or half.
 *
 * This function may need extra MLU memory as the workspace to improve the power
 * performance. You can get the workspace size with the
 * ::cnnlGetPowWorkspaceSize function.
 *
 * @deprecated
 * This function is depreceted and will be removed in future release.
 *   Use ::cnnlPow_v2 instead, which removes the \p prefer parameter.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   the power operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. The algorithm used to compute the output. For detailed information,
 *   see ::cnnlComputationPreference_t. The default value of this parameter is
 *   ::CNNL_COMPUTATION_HIGH_PRECISION.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory or host memory that stores the base tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the exponent tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory or host memory that stores the exponent tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the power
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the power
 *   operation. You can get the size of the workspace with the
 *   ::cnnlGetPowWorkspaceSize function.
 * @param[in] z_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] z
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par API Dependency
 * - You need to call the ::cnnlGetPowWorkspaceSize function to allocate extra
 *   workspace for \p workspace.
 *
 * @par Formula
 * - See "Pow Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor \p x and output tensor \p z must be the same.
 * - This function supports the combinations of the following data types for input tensor \p x,
 *   exponent tensor \p y and output tensor \p z on all hardware platforms.
 *   - \p x, \p y, \p z data type: half, half, half.
 *   - \p x, \p y, \p z data type: float, float, float.
 *   - \p x, \p y, \p z data type: half, int16, half.
 *   - \p x, \p y, \p z data type: float, int16, float.
 * - On MLU500 series, the additional supported data types of input tensor \p x,
 *   exponent tensor \p y, and output tensor \p z are as follows:
 *   - \p x, \p y, \p z data type: bfloat16, bfloat16, bfloat16.
 *   - \p x, \p y, \p z data type: bfloat16, int16, bfloat16.
 *
 * @par Scale Limitation
 * - The data range of the input tensor \p x and exponent tensor \p y should satisfy the
 *   following conditions:
 *   - On MLU200 series:
 *     - -15.5 < y * log(|x|) < 15.5.
 *   - On MLU300 series, MLU500 series, CE3226 and 1V,
 *     - If the data type of \p y is float or bfloat16, the value of \p y should be in range of
 *       [\f$-2^{24}\f$, \f$2^{24}\f$].
 *     - If the data type of \p y is half, the value of \p y should be in range of [-65504.0,65504.0].
 *     - If \p x = inf or -inf, \p y != 0, where inf represents infinity.
 *     - If \p x = 1, \p y != nan.
 * - According to the rule of tensor broadcast, the parameters should satisfy the following
 *   conditions. For example, c1_dim, c2_dim, and c3_dim represent the lowest dimension of
 *   \p input1, \p input2, \p output respectively:
 *   - min(c1_dim, c2_dim) == 1 or c1_dim == c2_dim.
 *   - max(c1_dim, c2_dim) == c3_dim.
 *
 * @note
 * - The base tensor \p x and exponent tensor \p y are multi-dimensional arrays, supporting
 *   up to \p CNNL_DIM_MAX dimensions.
 * - This function does not support that both tensors \p x and \p y are on host memory.
 * - When the dimension of input tensor \p x or \p y is 0, the dimensions of other input tensor
 *   \p y or \p x and output tensor \p z must be the same.
 * - On MLU300 series or above, \p prefer does not work currently. No matter what \p prefer is, the calculation method is the same.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.google.cn/api_docs/python/tf/math/pow
 */
CNNL_DEPRECATED_FOR(cnnlPow_v2)
cnnlStatus_t CNNL_WIN_API cnnlPow(cnnlHandle_t handle,
                                  cnnlComputationPreference_t prefer,
                                  const cnnlTensorDescriptor_t x_desc,
                                  const void *x,
                                  const cnnlTensorDescriptor_t y_desc,
                                  const void *y,
                                  void *workspace,
                                  size_t workspace_size,
                                  const cnnlTensorDescriptor_t z_desc,
                                  void *z);

// Group:Pow
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 *        workspace to optimize the pow operation.
 *
 * The size of the extra workspace is based on the given information of the pow operation,
 * including the input tensor descriptors \p x_desc and \p y_desc, and the output tensor
 * descriptor \p z_desc. For more information about the workspace, see "Cambricon CNNL
 * User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   the pow operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] y_desc
 *   Input. The descriptor of the exponent tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] z_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used
 *   in the pow operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, CNNL_STATUS_BAD_PARAM
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetPowWorkspaceSize(cnnlHandle_t handle,
                                                  const cnnlTensorDescriptor_t x_desc,
                                                  const cnnlTensorDescriptor_t y_desc,
                                                  const cnnlTensorDescriptor_t z_desc,
                                                  size_t *workspace_size);

// Group:Pow
/*!
 * @brief Computes the power of each element in input tensor \p x with exponent tensor
 *        \p y, and returns the result in the output tensor \p z.
 *
 * Compared with ::cnnlPowR and ::cnnlPowR_v2, this function has no limitations on the
 * \p x value. Compared with ::cnnlPowN and ::cnnlPowN_v2, the data type of \p y is
 * float or half. Compared with ::cnnlPow, this function removes the \p prefer parameter.
 *
 * This function may need extra MLU memory as the workspace to improve the power
 * performance. You can get the workspace size with the
 * ::cnnlGetPowWorkspaceSize function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   the power operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory or host memory that stores the base tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the exponent tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory or host memory that stores the exponent tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the power
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the power
 *   operation. You can get the size of the workspace with the
 *   ::cnnlGetPowWorkspaceSize function.
 * @param[in] z_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] z
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par API Dependency
 * - You need to call the ::cnnlGetPowWorkspaceSize function to allocate extra
 *   workspace for \p workspace.
 *
 * @par Formula
 * - See "Pow Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor \p x and output tensor \p z must be the same.
 * - This function supports the combinations of the following data types for input tensor \p x,
 *   exponent tensor \p y and output tensor \p z on all hardware platforms.
 *   - \p x, \p y, \p z data type: half, half, half.
 *   - \p x, \p y, \p z data type: float, float, float.
 *   - \p x, \p y, \p z data type: half, int16, half.
 *   - \p x, \p y, \p z data type: float, int16, float.
 * - On MLU500 series, the additional supported data types of input tensor \p x,
 *   exponent tensor \p y, and output tensor \p z are as follows:
 *   - \p x, \p y, \p z data type: bfloat16, bfloat16, bfloat16.
 *   - \p x, \p y, \p z data type: bfloat16, int16, bfloat16.
 *
 * @par Scale Limitation
 * - The data range of the input tensor \p x and exponent tensor \p y should satisfy the
 *   following conditions:
 *   - On MLU200 series:
 *     - -15.5 < y * log(|x|) < 15.5.
 *   - On MLU300 series, MLU500 series, CE3226 and 1V,
 *     - If the data type of \p y is float or bfloat16, the value of \p y should be in range of
 *       [\f$-2^{24}\f$, \f$2^{24}\f$].
 *     - If the data type of \p y is half, the value of \p y should be in range of [-65504.0,65504.0].
 *     - If \p x = inf or -inf, \p y != 0, where inf represents infinity.
 *     - If \p x = 1, \p y != nan.
 * - According to the rule of tensor broadcast, the parameters should satisfy the following
 *   conditions. For example, c1_dim, c2_dim, and c3_dim represent the lowest dimension of
 *   \p input1, \p input2, \p output respectively:
 *   - min(c1_dim, c2_dim) == 1 or c1_dim == c2_dim.
 *   - max(c1_dim, c2_dim) == c3_dim.
 *
 * @note
 * - The base tensor \p x and exponent tensor \p y are multi-dimensional arrays, supporting
 *   up to \p CNNL_DIM_MAX dimensions.
 * - This function does not support that both tensors \p x and \p y are on host memory.
 * - When the dimension of input tensor \p x or \p y is 0, the dimensions of other input tensor
 *   \p y or \p x and output tensor \p z must be the same.
 *
 * @par Requirements
 * - None.
 *
 *@par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.google.cn/api_docs/python/tf/math/pow
 */
cnnlStatus_t CNNL_WIN_API cnnlPow_v2(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t x_desc,
                                      const void *x,
                                      const cnnlTensorDescriptor_t y_desc,
                                      const void *y,
                                      void *workspace,
                                      size_t workspace_size,
                                      const cnnlTensorDescriptor_t z_desc,
                                      void *z);

// Group:Polygamma
/*!
 * @brief Computes the \f${order}^{th}\f$ derivative of the digamma function of each element in input
 *        tensor \p input with each element in \p order, and returns the result in output tensor \p output.
 *
 * This function may need an extra MLU memory as the workspace. You can get the size of the workspace
 * \p workspace_size with the ::cnnlGetPolygammaWorkspaceSize function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   the polygamma operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] order_desc
 *   Input. The descriptor of the order tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] order
 *   Input. Pointer to the MLU memory or host memory that stores the order tensor.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory or host memory that stores the input tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the polygamma
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the polygamma
 *   operation.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par API Dependency
 * - You need to call the ::cnnlGetPolygammaWorkspaceSize function to allocate an extra
 *   workspace.
 *
 * @par Formula
 * - See "Polygamma Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor \p input and output tensor \p output must be the same.
 * - This function supports the combinations of the following data types for input tensor \p order,
 *   input tensor \p input and output tensor \p output on all hardware platforms:
 *   - \p order, \p input, \p output : int32, half, half.
 *   - \p order, \p input, \p output : int32, float, float.
 *   - \p order, \p input, \p output : float, float, float.
 *
 * @par Scale Limitation
 * - The data range of the input tensor \p order should satisfy the
 *   following conditions if data type is int32:
 *   - order >=0.
 *
 * @note
 * - This function does not support that \p input is on host memory.
 * - When the dimension of input tensor \p order is 0, dimensions of input tensor
 *   \p input and output tensor \p output must be the same.
 * - When the data type of \p order is float, this operation supports tensor broadcasting
 *   and \p order, \p input and \p output must be broadcastable.
 * - When the data type of \p order is int32, you can specify the strides of all dimensions for
 *   \p input_desc and \p output_desc with ::cnnlSetTensorDescriptorEx.
 * - Input tensors \p order, \p input and output tensor \p output are multi-dimensional arrays, supporting
 *   up to \p CNNL_DIM_MAX dimensions.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.polygamma.html
 * - https://www.tensorflow.google.cn/api_docs/python/tf/math/polygamma
 */
cnnlStatus_t CNNL_WIN_API cnnlPolygamma(cnnlHandle_t handle,
                                        const cnnlTensorDescriptor_t order_desc,
                                        const void *order,
                                        const cnnlTensorDescriptor_t input_desc,
                                        const void *input,
                                        void *workspace,
                                        size_t workspace_size,
                                        const cnnlTensorDescriptor_t output_desc,
                                        void *output);

// Group:Polygamma
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 *        workspace to optimize the polygamma operation.
 *
 * The size of the extra workspace is based on the given information of the polygamma operation,
 * including the input tensor descriptors \p order_desc and \p input_desc, and the output tensor
 * descriptor \p output_desc. For more information about the workspace, see "Cambricon CNNL
 * User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   the pow operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] order_desc
 *   Input. The descriptor of the order tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used
 *   in the polygamma operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetPolygammaWorkspaceSize(cnnlHandle_t handle,
                                                        const cnnlTensorDescriptor_t order_desc,
                                                        const cnnlTensorDescriptor_t input_desc,
                                                        const cnnlTensorDescriptor_t output_desc,
                                                        size_t *workspace_size);

// Group:Scatter
/*!
 * @brief Reduces all values from \p src tensor into \p input at the indices specified in the \p
 * index tensor along a given axis \p dim. For each value in \p src, its output index is specified
 * by its index in \p src for dimensions outside of \p dim and by the corresponding value in \p index
 * for dimension \p dim. The applied reduction is defined via the ::cnnlScatterMode_t.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlScatter_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the scatter operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] dim
 *   Input. The axis along which to index. Different value of \p dim corresponds to different formula. The axis can be negative.
 *   See "Scatter Operator" section in "Cambricon CNNL User Guide" for details.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input data. \p input supports up to \p CNNL_DIM_MAX dimensions.
 * @param[in] index_desc
 *   Input. The descriptor of the index tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] index
 *   Input. Pointer to the MLU memory that stores the index data. The index value of each output data in one of the dimensions is equal to the
 *   corresponding value from \p index tensor. \p index supports up to \p CNNL_DIM_MAX dimensions.
 *   The dimension is determined by the different \p dim value.
 * @param[in] src_desc
 *   Input. The descriptor of the \p src tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] src
 *   Input. Pointer to the MLU memory that stores the src data. All values from tensor \p src will be used to replace or add input value according to \p index.
 *   \p src data supports up to \p CNNL_DIM_MAX dimensions.
 * @param[out] output_desc
 *   Output. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output data.
 * @param[in] mode
 *   Input. Scatter mode. Whether to perform one of the replace, add, max or min operations defined in ::cnnlScatterMode_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @par Formula
 * - See "Scatter Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The byte width of a data type can be got with the ::cnnlGetSizeOfDataType function.
 * - The data type of \p input, \p src and \p output must be the same.
 * - The replace operation supports the following data widths for input tensors \p input, \p src, \p index, and output tensor \p output.
 *   - input tensor: 1-byte, 2-byte, 4-byte, 8-byte.
 *   - index tensor: int32, int64.
 *   - src tensor: 1-byte, 2-byte, 4-byte, 8-byte.
 *   - output tensor: 1-byte, 2-byte, 4-byte, 8-byte.
 *
 * - The add/max/min operation supports the following data types for input tensors \p input, \p src, \p index,
 *   and output tensor \p output.
 *   - input: int32, half, float, bfloat16.
 *   - index: int32, int64.
 *   - src: int32, half, float, bfloat16.
 *   - output: int32, half, float, bfloat16.
 *
 *   The bfloat16 data type is supported only on MLU500 series.
 *
 * @par Scale Limitation
 * - The \p input, \p index, \p src (if it is a tensor) and \p output should all have the same number of dimensions.
 * - On the 1V series, the shape of \p src (if it is a tensor) tensor should be equal to \p index tensor.
 *   On other platforms, the shape of \p src (if it is a tensor) tensor should be greater than or equal to \p index tensor.
 * - The shape of \p input tensor and \p output tensor must be the same.
 * - The size of \p index tensor is equal to or less than \p input tensor for all dimensions except for the \p dim dimension.
 * - The \p index value must be greater than or equal to 0.
 * - The output tensor with stride feature is not supported currently.
 *
 * @note
 * - Currently, \p input, \p index, \p src and \p output support up to \p CNNL_DIM_MAX dimensions.
 * When indices are not unique, the behavior is non-deterministic (one of the values from \p src will be picked arbitarily) and
 * the gradient will be incorrect (it will be propagated to all locations in the source that correspond to the same index).
 * - You can specify the stride of all dimensions for input_desc, index_desc,
 *   src_desc, and output_desc with ::cnnlSetTensorDescriptorEx except on the 1V.
 * - The \p src can be either a tensor or a scalar:
 *   - When the total element number of \p src is more than 1, it is treated as a tensor.
 *   - When the total element number of \p src is equal to 1, it is treated as a scalar.
 * - In add operation, accuracy problem may occur when the inputs are accumulated on the same coordinate too many times (for example, more
 *   than 800 times).
 * - When \p input or \p src contains NaN:
 *   - On MLU300 series:
 *     - NaN is supported only in replace or add operation.
 *   - On MLU500 series:
 *     - NaN is supported in replace, add, max or min operation.
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the scatter or scatter_add operation is as follows:
   @verbatim

   input: [[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]]

   index: [[0, 1, 2, 0, 0], [2, 0, 0, 1, 2]]

   src: [[0., 1., 2., 3., 4.], [5., 6., 7., 8., 9.]]

   dimension: 0

   mode: CNNL_SCATTER

   output: [[0., 6., 7., 3., 4.], [0., 1., 0., 8., 0.], [5., 0., 2., 0., 9.]]

   input: [[0., 0., 0., 0.], [0., 0., 0., 0.]]

   index: [[2], [3]]

   src: 1.23

   dimension: 1

   mode: CNNL_SCATTER

   output: [[0., 0., 1.23, 0.], [0., 0., 0., 1.23]]

   input: [[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]]

   index: [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]

   src: [[0.3747, 0.2888, 0.5314, 0.5101, 0.5372], [0.1934, 0.0481, 0.7520, 0.8824, 0.3281]]

   dimension: 0

   mode: CNNL_SCATTER_ADD

   output: [[0.5681, 0.3368, 1.2834, 1.3943, 0.8653], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]]

   input: [5., 4., 3., 2.]

   index: [0, 1, 0, 1, 2, 1]

   src: [1., 2., 3., 4., 5., 6.]

   dimension: 0

   mode: CNNL_SCATTER_MAX

   output: [5., 6., 5., 2.]

   input: [5., 4., 3., 2.]

   index: [0, 1, 0, 1, 2, 1]

   src: [1., 2., 3., 4., 5., 6.]

   dimension: 0

   mode: CNNL_SCATTER_MIN

   output: [1., 2., 3., 2.]
   @endverbatim

 * @par Reference
 * - https://pytorch.org/docs/stable/tensors.html?highlight=scatter_#torch.Tensor.scatter_
 *
 */
CNNL_DEPRECATED_FOR(cnnlScatter_v2)
cnnlStatus_t CNNL_WIN_API cnnlScatter(cnnlHandle_t handle,
                                      const int dim,
                                      const cnnlTensorDescriptor_t input_desc,
                                      const void *input,
                                      const cnnlTensorDescriptor_t index_desc,
                                      const void *index,
                                      const cnnlTensorDescriptor_t src_desc,
                                      const void *src,
                                      const cnnlTensorDescriptor_t output_desc,
                                      void *output,
                                      cnnlScatterMode_t mode);

// Group:Scatter
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the scatter operation.
 *
 * The size of the extra workspace is based on the given information of the scatter operation,
 * including the index tensor descriptor \p index_desc. For more information about the workspace,
 * see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   scatter operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] index_desc
 *   Input. The descriptor of the input index tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   scatter operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetScatterWorkspaceSize(cnnlHandle_t handle,
                                                      const cnnlTensorDescriptor_t index_desc,
                                                      size_t *size);

// Group:Scatter
/*!
 * @brief Reduces all values from \p src tensor into \p input at the indices specified in the \p
 * index tensor along a given axis \p dim. For each value in \p src, its output index is specified
 * by its index in \p src for dimensions outside of \p dim and by the corresponding value in \p index
 * for dimension \p dim. The applied reduction is defined via the ::cnnlScatterMode_t.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the scatter operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] dim
 *   Input. The axis along which to index. Different value of \p dim corresponds to different formula. The axis can be negative.
 *   See "Scatter Operator" section in "Cambricon CNNL User Guide" for details.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input data. \p input supports up to \p CNNL_DIM_MAX dimensions.
 * @param[in] index_desc
 *   Input. The descriptor of the index tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] index
 *   Input. Pointer to the MLU memory that stores the index data. The index value of each output data in one of the dimensions is equal to the
 *   corresponding value from \p index tensor. \p index supports up to \p CNNL_DIM_MAX dimensions.
 *   The dimension is determined by the different \p dim value.
 * @param[in] src_desc
 *   Input. The descriptor of the \p src tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] src
 *   Input. Pointer to the MLU memory that stores the src data. All values from tensor \p src will be used to replace or add input value according to \p index.
 *   \p src data supports up to \p CNNL_DIM_MAX dimensions.
 * @param[in] workspace
 *   Input. A device pointer to the MLU memory that is used as an extra workspace for this
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in this
 *   operation. You can get the workspace size with the ::cnnlGetScatterWorkspaceSize
 *   function.
 * @param[out] output_desc
 *   Output. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output data.
 * @param[in] mode
 *   Input. Scatter mode. Whether to perform one of the replace, add, max or min operations defined in ::cnnlScatterMode_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "Scatter Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The byte width of a data type can be got with the ::cnnlGetSizeOfDataType function.
 * - The data type of \p input, \p src and \p output must be the same.
 * - The replace operation supports the following data widths for input tensors \p input, \p src, \p index, and output tensor \p output.
 *   - input tensor: 1-byte, 2-byte, 4-byte, 8-byte.
 *   - index tensor: int32, int64.
 *   - src tensor: 1-byte, 2-byte, 4-byte, 8-byte.
 *   - output tensor: 1-byte, 2-byte, 4-byte, 8-byte.
 *
 * - The add/max/min operation supports the following data types for input tensors \p input, \p src, \p index,
 *   and output tensor \p output.
 *   - input: int32, half, float, bfloat16.
 *   - index: int32, int64.
 *   - src: int32, half, float, bfloat16.
 *   - output: int32, half, float, bfloat16.
 *
 *   The bfloat16 data type is supported only on MLU500 series.
 *
 * @par Scale Limitation
 * - The \p input, \p index, \p src (if it is a tensor) and \p output should all have the same number of dimensions.
 * - On the 1V series, the shape of \p src (if it is a tensor) tensor should be equal to \p index tensor.
 *   On other platforms, the shape of \p src (if it is a tensor) tensor should be greater than or equal to \p index tensor.
 * - The shape of \p input tensor and \p output tensor must be the same.
 * - The size of \p index tensor is equal to or less than \p input tensor for all dimensions except for the \p dim dimension.
 * - The \p index value must be greater than or equal to 0.
 * - The output tensor with stride feature is not supported currently.
 *
 * @note
 * - Currently, \p input, \p index, \p src and \p output support up to \p CNNL_DIM_MAX dimensions.
 * When indices are not unique, the behavior is non-deterministic (one of the values from \p src will be picked arbitarily) and
 * the gradient will be incorrect (it will be propagated to all locations in the source that correspond to the same index).
 * - You can specify the stride of all dimensions for input_desc, index_desc,
 *   src_desc, and output_desc with ::cnnlSetTensorDescriptorEx except on the 1V.
 * - The \p src can be either a tensor or a scalar:
 *   - When the total element number of \p src is more than 1, it is treated as a tensor.
 *   - When the total element number of \p src is equal to 1, it is treated as a scalar.
 * - In add operation, accuracy problem may occur when the inputs are accumulated on the same coordinate too many times(for example, more
 *   than 800 times).
 * - When \p input or \p src contains NaN:
 *   - On MLU300 series:
 *     - NaN is supported only in replace or add operation.
 *   - On MLU500 series:
 *     - NaN is supported in replace, add, max or min operation.
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the scatter or scatter_add operation is as follows:
   @verbatim

   input: [[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]]

   index: [[0, 1, 2, 0, 0], [2, 0, 0, 1, 2]]

   src: [[0., 1., 2., 3., 4.], [5., 6., 7., 8., 9.]]

   dimension: 0

   mode: CNNL_SCATTER

   output: [[0., 6., 7., 3., 4.], [0., 1., 0., 8., 0.], [5., 0., 2., 0., 9.]]

   input: [[0., 0., 0., 0.], [0., 0., 0., 0.]]

   index: [[2], [3]]

   src: 1.23

   dimension: 1

   mode: CNNL_SCATTER

   output: [[0., 0., 1.23, 0.], [0., 0., 0., 1.23]]

   input: [[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]]

   index: [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]

   src: [[0.3747, 0.2888, 0.5314, 0.5101, 0.5372], [0.1934, 0.0481, 0.7520, 0.8824, 0.3281]]

   dimension: 0

   mode: CNNL_SCATTER_ADD

   output: [[0.5681, 0.3368, 1.2834, 1.3943, 0.8653], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]]

   input: [5., 4., 3., 2.]

   index: [0, 1, 0, 1, 2, 1]

   src: [1., 2., 3., 4., 5., 6.]

   dimension: 0

   mode: CNNL_SCATTER_MAX

   output: [5., 6., 5., 2.]

   input: [5., 4., 3., 2.]

   index: [0, 1, 0, 1, 2, 1]

   src: [1., 2., 3., 4., 5., 6.]

   dimension: 0

   mode: CNNL_SCATTER_MIN

   output: [1., 2., 3., 2.]
   @endverbatim

 * @par Reference
 * - https://pytorch.org/docs/stable/tensors.html?highlight=scatter_#torch.Tensor.scatter_
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlScatter_v2(cnnlHandle_t handle,
                                         const int dim,
                                         const cnnlTensorDescriptor_t input_desc,
                                         const void *input,
                                         const cnnlTensorDescriptor_t index_desc,
                                         const void *index,
                                         const cnnlTensorDescriptor_t src_desc,
                                         const void *src,
                                         void *workspace,
                                         const size_t workspace_size,
                                         const cnnlTensorDescriptor_t output_desc,
                                         void *output,
                                         cnnlScatterMode_t mode);

// Group:BiasAdd
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the ::cnnlBiasAdd operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor \p a. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the tensor c used as input and output. For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is
 *   used in ::cnnlBiasAdd operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */

cnnlStatus_t CNNL_WIN_API cnnlGetBiasAddWorkspaceSize(cnnlHandle_t handle,
                                                      const cnnlTensorDescriptor_t a_desc,
                                                      const cnnlTensorDescriptor_t c_desc,
                                                      size_t *workspace_size);

// Group:BiasAdd
/*!
 * @brief Implements the BiasAdd operation that is widely used in artificial intelligence with the
 * following formula:
 *
 * c = alpha[0] * a + beta[0] * c
 *
 * Where, \p a and \p c are tensors, and \p alpha and \p beta are the scaling factors
 * used in the operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] alpha
 *   Input. A host pointer to the scaling factor of tensor \p a.
 * @param[in] a_desc
 *   Input. The descriptor of input tensor \p a. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. A device pointer to the MLU memory that stores input tensor a.
 * @param[in] workspace
 *   Input. A device pointer to the MLU memory that is used as an extra workspace for this
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in this
 *   operation. You can get the workspace size with the ::cnnlGetBiasAddWorkspaceSize
 *   function.
 * @param[in] beta
  *   Input. A host pointer to the scaling factor of tensor \p c.
 * @param[in] c_desc
 *   Input. The descriptor of the tensor \p c used as input and output. For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @param[in, out] c
 *   Input, Output. A device pointer to the MLU memory that stores tensor \p c.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "BiasAdd Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The data type combinations for tensors \p a and \p c must be half-half,
 *   float-float, bfloat16-bfloat16 and int32-int32.
 * - \p alpha, \p beta: If the data type of tensors is float, half or bfloat16, the data type of
 *   \p alpha and \p beta should be float*. If the data type of tensors is int32, the
 *   data type of \p alpha and \p beta should be int*.
 * - The bfloat16 data type is supported only on MLU500 series.
 *
 * @par Data Layout
 * - Data layouts of input tensor and output tensor must be the same.
 *
 * @par Scale Limitation
 * - The input \p a tensor has only one dimension and the length of this dimension is the
 *   same as the lowest dimension of \p c. In short, A_DIM[0] = C_DIM[C_DIM_NUM - 1].
 *   A_DIM[0] means the length of the only one dimension of \p a tensor, and
 *   C_DIM[C_DIM_NUM - 1] means the length of the last dimension of \p c tensor.
 * - The number of dimensions is no more than \p CNNL_DIM_MAX.
 * - When the data type of \p input is int32, the intermediate result of \p input cannot
 *   exceed the value range of the corresponding data type.
 *
 * @par API Dependency
 * - Before calling this function to perform this operation, you need to get the size of
 *   workspace by calling the ::cnnlGetBiasAddWorkspaceSize function.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the value of scaling factors
 *   \p alpha to 1, and \p beta to 0 or 1.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
       Input tensor  a :   [[1, 2, 3]]

       alpha           :   1

       Input tensor  c :   [[1, 2, 3],
                            [4, 5, 6],
                            [7, 8, 9]]

       beta            :   1

       Output tensor   :   [[2, 4, 6],
                            [5, 7, 9],
                            [8, 10, 12]]
     @endverbatim
 *
 */

cnnlStatus_t CNNL_WIN_API cnnlBiasAdd(cnnlHandle_t handle,
                                      const void *alpha,
                                      const cnnlTensorDescriptor_t a_desc,
                                      const void *a,
                                      void *workspace,
                                      size_t workspace_size,
                                      const void *beta,
                                      const cnnlTensorDescriptor_t c_desc,
                                      void *c);

// Group:BiasAddBackward
/*!
 * @brief Computes the bias add gradient with respect to the bias, which is the sum
 * of every element belonging to the same feature map across all of the images of
 * the input tensor. Therefore, the number of elements produced is equal to the
 * number of features maps of the input tensor.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlBiasAddBackward_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] diff_y_desc
 *   Input. Descriptor of input tensor \p diff_y.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] diff_y
 *   Input. Pointer to the MLU memory that stores the \p diff_y tensor.
 * @param[in] axis
 *   Input. \p axis is used to determine the channel dimension of the feature map.
 * @param[in] diff_bias_desc
 *   Input. Descriptor of output tensor \p diff_bias.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] diff_bias
 *   Output. Pointer to the MLU memory that stores the \p diff_bias tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "BiasAddBackward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - None.
 *
 * @par Scale Limitation
 * - None.
 *
 * @note
 * - Input tensor (\p diff_y) only supports 2D~5D.
 * - \p axis must be in range of [1, diff_y_desc.dim-1].
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://tensorflow.org/api_docs/cc/class/tensorflow/ops/bias-add-grad
 */
CNNL_DEPRECATED_FOR(cnnlBiasAddBackward_v2)
cnnlStatus_t CNNL_WIN_API cnnlBiasAddBackward(cnnlHandle_t handle,
                                              const cnnlTensorDescriptor_t diff_y_desc,
                                              const void *diff_y,
                                              int axis,
                                              const cnnlTensorDescriptor_t diff_bias_desc,
                                              void *diff_bias);

// Group:BiasAddBackward
/*!
 * @brief Computes the bias add gradient with respect to the bias, which is the sum
 *        of every element belonging to the same feature map across all of the images
 *        of the input tensor. Therefore, the number of elements produced is equal to
 *        the number of features maps of the input tensor.
 *
 * Compared with ::cnnlBiasAddBackward, ::cnnlBiasAddBackward_v2 needs extra
 * workspace. You can get the workspace size with
 * the ::cnnlGetBiasAddBackwardWorkspaceSize function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the ::cnnlBiasAddBackward_v2 operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] diff_y_desc
 *   Input. Descriptor of input tensor \p diff_y.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] diff_y
 *   Input. Pointer to the MLU memory that stores the \p diff_y tensor.
 * @param[in] axis
 *   Input. \p axis is used to determine the channel dimension of the feature map.
 * @param[in] diff_bias_desc
 *   Input. Descriptor of output tensor \p diff_bias.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] diff_bias
 *   Output. Pointer to the MLU memory that stores the \p diff_bias tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlBiasAddBackward_v2 operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the ::cnnlBiasAddBackward_v2 operation. You can get the size of the workspace with
 *   the ::cnnlGetBiasAddBackwardWorkspaceSize function.

 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "BiasAddBackward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float, bfloat16.
 *   - output tensor: half, float, bfloat16.
 *
 *   The bfloat16 data type is supported only on MLU500 series.
 *
 * @par Data Layout
 * - None.
 *
 * @par Scale Limitation
 * - None.
 *
 * @note
 * - Input tensor (\p diff_y) only supports 2D~5D.
 * - \p axis must be in range of [1, diff_y_desc.dim-1].
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://tensorflow.org/api_docs/cc/class/tensorflow/ops/bias-add-grad
 */
cnnlStatus_t CNNL_WIN_API cnnlBiasAddBackward_v2(cnnlHandle_t handle,
                                                 const cnnlTensorDescriptor_t diff_y_desc,
                                                 const void *diff_y,
                                                 int axis,
                                                 const cnnlTensorDescriptor_t diff_bias_desc,
                                                 void *diff_bias,
                                                 void *workspace,
                                                 size_t workspace_size);
// Group:BiasAddBackward
/*!
 * @brief Returns in \p workspace_size_inbytes the size of the MLU memory that
 *        is used to get extra space size in BiasAddBackward operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the BiasAddBackward operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] diff_y_desc
 *   Input. The descriptor of input tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] diff_bias_desc
 *   Input. The descriptor of output tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] axis
 *   Input. \p axis is used to determine the channel dimension of the feature map.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is
 *   used in the BiasAddBackward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "BiasAddBackward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetBiasAddBackwardWorkspaceSize(cnnlHandle_t handle,
                                    const cnnlTensorDescriptor_t diff_y_desc,
                                    const cnnlTensorDescriptor_t diff_bias_desc,
                                    const int axis,
                                    size_t *workspace_size);

// Group:Gather
/*!
 * @brief Gathers values along an axis specified by \p dim.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGather_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the gather
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] dim
 *   Input. An integer value specifying an axis along which to gather values.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] index_desc
 *   Input. The descriptor of the index tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] index
 *   Input. Pointer to the MLU memory that stores the index of each element of \p output in
 *   the corresponding dimension of input tensor \p input.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH,
 *   ::CNNL_STATUS_NOT_SUPPORTED, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @par Formula
 * - See "Gather Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The (I/O)function supports the following data widths for \p input and \p output tensors.
 *   - input tensor: 1-byte, 2-byte, 4-byte, 8-byte.
 *   - index tensor: int32, int64.
 *   - output tensor: 1-byte, 2-byte, 4-byte, 8-byte.
 *
 *   The byte width of a data type can be got with the ::cnnlGetSizeOfDataType function.
 *
 *   Note that the data type of input tensor \p input and output tensor \p output must be the same.
 *
 * @par Data Layout
 * - The supported data layouts of the input tensor, index tensor and output tensor are
 *   as follows:
 *   - input tensor: \p CNNL_LAYOUT_ARRAY.
 *   - index tensor: \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The input tensor, index tensor and dimension must meet the following requirements:
 *   - They must have the same number of dimensions.
 *   - input tensor: On 1V series, the shape of \p input tensor should be equal to \p index tensor's
 *     all dimensions except for \p dim dimension. On other platforms, the shape of \p input tensor should be
 *     greater than or equal to \p index tensor's all dimensions except for \p dim dimension.
 *   - index tensor: The shape of index tensor should be the same as that of output tensor.
 *   - dim: It should be greater than or equal to negative number of \p input dimensions,
 *     and less than or equal to number of \p input dimensions -1.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the gather operation is as follows:
     @verbatim
     input two arrays both by 1 * 2 * 2 --> input: [[[1., 2.], [3., 4.]]]

     --> index: [[[0, 0], [1, 0]]]

     param:
       dim: 2

     output array by 1 * 2 * 2 --> output: [[[1., 1.], [4., 3.]]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.gather.html
 */
CNNL_DEPRECATED_FOR(cnnlGather_v2)
cnnlStatus_t CNNL_WIN_API cnnlGather(cnnlHandle_t handle,
                                     const int dim,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const void *input,
                                     const cnnlTensorDescriptor_t index_desc,
                                     const void *index,
                                     const cnnlTensorDescriptor_t output_desc,
                                     void *output);

// Group:Gather
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the gather operation.
 *
 * The size of the extra workspace is based on the given information of the gather operation,
 * including the index tensor descriptor \p index_desc. For more information about the workspace,
 * see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   gather operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] index_desc
 *   Input. The descriptor of index tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] dim
 *   Input. An integer value specifying an axis along which to gather values.
 * @param[out] size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   gather operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetGatherWorkspaceSize(cnnlHandle_t handle,
                                                     const cnnlTensorDescriptor_t input_desc,
                                                     const cnnlTensorDescriptor_t index_desc,
                                                     const cnnlTensorDescriptor_t output_desc,
                                                     const int dim,
                                                     size_t *size);

// Group:Gather
/*!
 * @brief Gathers values along an axis specified by \p dim.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the gather
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] dim
 *   Input. An integer value that determines the axis to gather value along.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] index_desc
 *   Input. The descriptor of the index tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] index
 *   Input. Pointer to the MLU memory that stores the index of each element of \p output in
 *   the corresponding dimension of input tensor \p input.
 * @param[in] workspace
 *   Input. A device pointer to the MLU memory that is used as an extra workspace for this
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in this
 *   operation. You can get the workspace size with the ::cnnlGetGatherWorkspaceSize
 *   function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH,
 *   ::CNNL_STATUS_NOT_SUPPORTED, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @par Formula
 * - See "Gather Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The (I/O)function supports the following data widths for \p input and \p output tensors.
 *   - input tensor: 1-byte, 2-byte, 4-byte, 8-byte.
 *   - index tensor: int32, int64.
 *   - output tensor: 1-byte, 2-byte, 4-byte, 8-byte.
 *
 *   The byte width of a data type can be got with the ::cnnlGetSizeOfDataType function.
 *
 *   Note that the data type of input tensor \p input and output tensor \p output must be the same.
 *
 * @par Data Layout
 * - The supported data layouts of the input tensor, index tensor and output tensor are
 *   as follows:
 *   - input tensor: \p CNNL_LAYOUT_ARRAY.
 *   - index tensor: \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The input tensor, index tensor and dimension must meet the following requirements:
 *   - The input tensor, index tensor and output tensor should have the same number of dimensions.
 *   - input tensor: On the 1V series, the shape of \p input tensor should be equal to \p index tensor's
 *     all dimensions except for \p dim dimension. On other platforms, the shape of \p input tensor should be
 *     greater than or equal to \p index tensor's all dimensions except for \p dim dimension.
 *   - index tensor: The shape of index tensor should be the same as output tensor.
 *   - dim: It should be greater than or equal to negative number of \p input dimensions,
 *     and less than or equal to number of \p input dimensions -1.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the gather operation is as follows:
     @verbatim
     input two arrays both by 1 * 2 * 2 --> input: [[[1., 2.], [3., 4.]]]

     --> index: [[[0, 0], [1, 0]]]

     param:
       dim: 2

     output array by 1 * 2 * 2 --> output: [[[1., 1.], [4., 3.]]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.gather.html
 */
cnnlStatus_t CNNL_WIN_API cnnlGather_v2(cnnlHandle_t handle,
                                        const int dim,
                                        const cnnlTensorDescriptor_t input_desc,
                                        const void *input,
                                        const cnnlTensorDescriptor_t index_desc,
                                        const void *index,
                                        void *workspace,
                                        const size_t workspace_size,
                                        const cnnlTensorDescriptor_t output_desc,
                                        void *output);
// Group:Cast
/*!
 *  @brief Converts \p input from original type to target type,
 *  and returns in \p output. This operation supports data type conversion in any dimensions,
 *  The supported data type conversions are defined in ::cnnlCastDataType_t. The shape of \p input
 *  and \p output must be consistent.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *    queues in the cast operation. For detailed information,
 *    see ::cnnlHandle_t.
 *  @param[in] input_desc
 *    Input. The descriptor of the input tensor. For detailed information, see
 *    ::cnnlTensorDescriptor_t.
 *  @param[in]  input
 *    Input. Pointer to the MLU memory that stores the input tensor.
 *  @param[in] cast_type
 *    Input. The cast_type used to specify the conversion data types from one to another.
 *    The \p cast_type are defined the ::cnnlCastDataType_t.
 *  @param[in] output_desc
 *    Input. The descriptor of the output tensor. For detailed information, see
 *    ::cnnlTensorDescriptor_t.
 *  @param[out] output
 *    Output. Pointer to the MLU memory that stores the output tensor.
 *  @par Return
 *    - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH,
 *      ::CNNL_STATUS_NOT_SUPPORTED
 *
 *  @par Scale Limitation
 *    - The input tensor and output tensor have the same shape.
 *    - The total number of dimensions of input tensor should be less than CNNL_DIM_MAX
 *      when tensor is discontinuous.
 *
 *  @note
 *    - This operation only supports conversion data types defined in ::cnnlCastDataType_t.
 *    You can specify the stride of all dimensions for input_desc and output_desc with
 *    ::cnnlSetTensorDescriptorEx.
 *    - The following items in ::cnnlCastDataType_t are only supported on MLU500 series:
 *
 *      CNNL_CAST_BFLOAT16_TO_FLOAT, CNNL_CAST_FLOAT_TO_BFLOAT16, CNNL_CAST_BFLOAT16_TO_BOOL,
 *      CNNL_CAST_BOOL_TO_BFLOAT16, CNNL_CAST_INT64_TO_BFLOAT16, CNNL_BFLOAT16_TO_INT64,
 *      CNNL_CAST_BFLOAT16_TO_INT32, CNNL_CAST_INT16_TO_BFLOAT16, CNNL_CAST_BFLOAT16_TO_INT16,
 *      CNNL_CAST_INT8_TO_BFLOAT16, CNNL_CAST_BFLOAT16_INT8, CNNL_CAST_UINT8_TO_BFLOAT16 and
 *      CNNL_CAST_BFLOAT16_TO_UINT8, CNNL_CAST_COMPLEX_FLOAT_TO_BFLOAT16, CNNL_CAST_COMPLEX_HALF_TO_BFLOAT16,
 *      CNNL_CAST_BFLOAT16_TO_COMPLEX_HALF and CNNL_CAST_BFLOAT16_TO_COMPLEX_FLOAT.
 *    - The following items in ::cnnlCastDataType_t are not supported on 1V:
 *
 *      CNNL_CAST_DOUBLE_TO_FLOAT, CNNL_CAST_FLOAT_TO_DOUBLE, CNNL_CAST_FLOAT_TO_INT64,
 *      CNNL_CAST_INT64_TO_FLOAT, CNNL_CAST_INT64_TO_HALF and CNNL_CAST_DOUBLE_TO_COMPLEX_FLOAT.
 *
 *  @par Requirements
 *  - None.
 *  @par Reference
 *  - https://tensorflow.org/api_docs/cc/class/tensorflow/ops/cast
 */
cnnlStatus_t CNNL_WIN_API cnnlCastDataType(cnnlHandle_t handle,
                                           const cnnlTensorDescriptor_t input_desc,
                                           const void *input,
                                           const cnnlCastDataType_t cast_type,
                                           const cnnlTensorDescriptor_t output_desc,
                                           void *output);

// Group:PowN
/*!
 * @brief Computes the power of each element in input tensor \p x with exponent tensor
 *        \p y, and returns the result in the output tensor \p z.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlPow_v2 instead, which supports the \p prefer parameter to set the computing with
 *   faster algorithm or higher precision and supports data type of \p y to set float or half.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   pown operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the exponent tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the exponent tensor.
 * @param[in] z_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] z
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "PowN Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor \p x and output tensor \p z must be the same.
 * - The supported data types of input, exponent and output tensors are as follows:
 *   - input tensor \p x: half, float.
 *   - exponent tensor \p y: int16.
 *   - output tensor \p z: half, float.
 * - On MLU500 series, the additional supported data types of input, exponent
 *   and output tensor are as follows:
 *   - input tensor \p x: bfloat16.
 *   - exponent tensor \p y: int16.
 *   - output tensor \p z: bfloat16.
 *
 * @par Scale Limitation
 * - The data range of the input tensor \p x and exponent tensor \p y should satisfy the
 *   following conditions:
 *   - On MLU200 series:
 *     -15.5 < y*log(|x|) < 15.5.
 *   - On MLU300 series and CE3226:
 *     - if \p x = inf or -inf, \p y != 0, where inf represents infinity.
 *
 * @note
 * - The input tensor \p x and exponent tensor \p y do not support broadcast.
 * - You can specify the stride of all dimensions for x_desc, y_desc and z_desc with
 *   ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.google.cn/api_docs/python/tf/math/pow
 */
CNNL_DEPRECATED_FOR(cnnlPow_v2)
cnnlStatus_t CNNL_WIN_API cnnlPowN(cnnlHandle_t handle,
                                   const cnnlTensorDescriptor_t x_desc,
                                   const void *x,
                                   const cnnlTensorDescriptor_t y_desc,
                                   const void *y,
                                   const cnnlTensorDescriptor_t z_desc,
                                   void *z);

// Group:PowN
/*!
 * @brief Computes the power of each element in input tensor \p x with exponent tensor
 *        \p y, and returns the result in the output tensor \p z.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlPow_v2 instead, which supports data type of \p y to set float or half.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   pown operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. The algorithm used to compute the output. For detailed information,
 *   see ::cnnlComputationPreference_t. The default value of this parameter is
 *   ::CNNL_COMPUTATION_HIGH_PRECISION.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the exponent tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the exponent tensor.
 * @param[in] z_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] z
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "PowN Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor \p x and output tensor \p z must be the same.
 * - The supported data types of input, exponent and output tensors are as follows:
 *   - input tensor \p x: half, float.
 *   - exponent tensor \p y: int16.
 *   - output tensor \p z: half, float.
 * - On MLU500 series, the additional supported data types of input, exponent
 *   and output tensor are as follows:
 *   - input tensor \p x: bfloat16.
 *   - exponent tensor \p y: int16.
 *   - output tensor \p z: bfloat16.
 *
 * @par Scale Limitation
 * - The data range of the input tensor \p x and exponent tensor \p y should satisfy the
 *   following conditions:
 *   - On MLU200 series:
 *     -15.5 < y*log(|x|) < 15.5.
 *   - On MLU300 series and CE3226:
 *     - if \p x = inf or -inf, \p y != 0, where inf represents infinity.
 *
 * @note
 * - The input tensor \p x and exponent tensor \p y do not support broadcast.
 * - You can specify the stride of all dimensions for x_desc, y_desc and z_desc with
 *   ::cnnlSetTensorDescriptorEx.
 * - On MLU300 series or above, \p prefer does not work currently. No matter what \p prefer is, the calculation method is the same.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.google.cn/api_docs/python/tf/math/pow
 */
CNNL_DEPRECATED_FOR(cnnlPow_v2)
cnnlStatus_t CNNL_WIN_API cnnlPowN_v2(cnnlHandle_t handle,
                                      cnnlComputationPreference_t prefer,
                                      const cnnlTensorDescriptor_t x_desc,
                                      const void *x,
                                      const cnnlTensorDescriptor_t y_desc,
                                      const void *y,
                                      const cnnlTensorDescriptor_t z_desc,
                                      void *z);

// Group:BitCompute
/*!
 * @brief  Returns in \p workspace_size the size of the MLU memory in bytes that is
 *         used as an extra workspace to optimize the ::cnnlBitCompute_v2 operation.
 *
 * The size of the extra workspace is based on the given information of the input
 * and output tensor descriptors, \p a_desc, \p b_desc, and \p c_desc. For more
 * information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  input1_desc
 *   Input. The descriptor of the first input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  input2_desc
 *   Input. The descriptor of the second input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is
 *   used in ::cnnlBitCompute_v2.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlBitCompute_v2 function
 *   to perform the bitwise operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetBitComputeWorkspaceSize(cnnlHandle_t handle,
                                                         const cnnlTensorDescriptor_t input1_desc,
                                                         const cnnlTensorDescriptor_t input2_desc,
                                                         const cnnlTensorDescriptor_t output_desc,
                                                         size_t *workspace_size);

// Group:BitCompute
/*!
 * @brief Performs bitwise operation \p optype between two input tensors \p input1 and
 *        \p input2, and returns the results in the output tensor \p output.
 *
 * Bitwise operation is widely used in image processing. It is
 * supported in PyTorch and TensorFlow.
 *
 * The ::cnnlBitCompute has the following limitations on input shapes.
 * For each dimension of the input1 and input2, the size of the dimension can be the same,
 * and these same dimensions should be consecutive, other dimensions of input2 should be 1.
 * This function has less limitations on input shapes. For each dimension of the two input
 * tensors, the size of the dimension should be the same or one of them should equal to 1.
 *
 * This function supports partial in-place operation, which means that the first input
 * tensor \p input1 and the output tensor \p output can be the same one.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the BitCompute operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] optype
 *   Input. The specific bitwise operation performed in the function. The operations are
 *   defined in the ::cnnlBitComputeOp_t enum.
 * @param[in]  input1_desc
 *   Input. The descriptor of the first input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  input1
 *   Input. Pointer to the MLU memory or host memory that stores the first input tensor.
 * @param[in]  input2_desc
 *   Input. The descriptor of the second input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  input2
 *   Input. Pointer to the MLU memory or host memory that stores the second input tensor.
 * @param[out]  output_desc
 *   Output. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out]  output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in]  workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   logic operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in]  workspace_size
 *   The size of the extra workspace in bytes that needs to be used in the logic operation.
 *   You can get the size of the workspace with the ::cnnlGetBitComputeWorkspaceSize function.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Bit Compute" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input and output tensor.
 *   - input: uint8, bool, int8, uint16, int16, uint32, int32, uint64, int64, half, float, bfloat16.
 *   - output: uint8, bool, int8, uint16, int16, uint32, int32, uint64, int64, half, float, bfloat16.
 *
 * @par Scale Limitation
 * - The input \p optype ::CNNL_BLEFT_SHIFT_OP, ::CNNL_BRIGHT_SHIFT_OP, ::CNNL_BLEFT_SHIFT_OP_V2 and
 *   ::CNNL_BRIGHT_SHIFT_OP_V2 are not supported on MLU 200, and 1V series.
 *
 * @par API Dependency
 * - Before calling this function to perform Bitwise operation, you need to get the
 *   size of workspace by the ::cnnlGetBitComputeWorkspaceSize function.
 *
 * @note
 * - The data type of output tensor must be the same as that of the input tensor.
 * - Only half, float and bfloat16 are supported and bool is not supported when \p optype is
 *   ::CNNL_BLEFT_SHIFT_OP, ::CNNL_BRIGHT_SHIFT_OP, ::CNNL_BLEFT_SHIFT_OP_V2 or ::CNNL_BRIGHT_SHIFT_OP_V2.
 * - Uint16 and uint32 are not supported when \p optype is ::CNNL_BLEFT_SHIFT_OP_V2 or
 *   ::CNNL_BRIGHT_SHIFT_OP_V2.
 * - Uint64 is not supported when \p optype is ::CNNL_BLEFT_SHIFT_OP, ::CNNL_BRIGHT_SHIFT_OP,
 *   ::CNNL_BLEFT_SHIFT_OP_V2, ::CNNL_BRIGHT_SHIFT_OP_V2 or ::CNNL_BNOT_OP.
 * - Int64 is not supported when \p optype is ::CNNL_BLEFT_SHIFT_OP or ::CNNL_BRIGHT_SHIFT_OP.
 * - Bfloat16 is only supported on MLU500 platform now.
 * - \p input2_desc and \p intput2 can be NULL when \p optype is ::CNNL_BNOT_OP.
 * - This function does not support that both tensors \p x and \p y are on host memory.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the bitwise operation is as follows:
     @verbatim
      bitcompute_op: CNNL_CYCLE_BAND_OP
      input two arrays by 2 * 3 * 3, 2 * 1 * 3
      --> input1: [[[1, 2, 3], [2, 3, 4], [3, 4, 5]],
                  [[4, 5, 6], [5, 6, 7], [6, 7, 8]]]

      --> input2: [[[-1, -2, -1]], [[-3, -4, -5]]]

      output array by 2 * 3 * 3
      --> output: [[[1, 2, 3], [2, 2, 4], [3, 4, 5]],
                  [[4, 4, 2], [5, 4, 3], [4, 4, 8]]]
     @endverbatim
 */
cnnlStatus_t CNNL_WIN_API cnnlBitCompute_v2(cnnlHandle_t handle,
                                          const cnnlBitComputeOp_t optype,
                                          const cnnlTensorDescriptor_t input1_desc,
                                          const void * input1,
                                          const cnnlTensorDescriptor_t input2_desc,
                                          const void * input2,
                                          const cnnlTensorDescriptor_t output_desc,
                                          void * output,
                                          void * workspace,
                                          size_t workspace_size);

// Group:BitCompute
/*!
 * @brief Performs the bitwise operation between two input tensors \p input1 and
 *        \p input2, and returns the results in the output tensor \p output.
 *
 * Bitwise operation is widely used in image processing. It is
 * supported in PyTorch.
 *
 * This function supports partial in-place operation, which means that the first input tensor \p input1 and
 * the output tensor \p output can be the same one.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlBitCompute_v2 instead. This function has the following limitations on input
 *   shapes: for each dimension of the input1 and input2, the size of the dimension can be the
 *   same, and these same dimensions should be consecutive, other dimensions of input2 should be 1.
 *   The new function ::cnnlBitCompute_v2 has less limitations on input shapes. For each dimension
 *   of the two input tensors, the size of the dimension should be the same or one of them should
 *   equal to 1.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the BitCompute_v2 operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] bitcompute_op
 *   Input. The specific bitwise operation performed in the function. The operations are
 *   defined in the ::cnnlBitComputeOp_t enum.
 * @param[in]  input1_desc
 *   Input. The descriptor of the first input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  input1
 *   Input. Pointer to the MLU memory that stores the first input tensor.
 * @param[in]  input2_desc
 *   Input. The descriptor of the second input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  input2
 *   Input. Pointer to the MLU memory that stores the second input tensor.
 * @param[out]  output_desc
 *   Output. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out]  output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Bit Compute" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input and output tensor.
 *   - input: uint8, bool, int8, int16, int32.
 *   - output: uint8, bool, int8, int16, int32.
 *   Note that the data type of output tensor must be the same as the input tensor.
 *
 * @par Scale Limitation
 * - The input \p optype ::CNNL_BLEFT_SHIFT_OP, ::CNNL_BRIGHT_SHIFT_OP, ::CNNL_BLEFT_SHIFT_OP_V2
 *   and ::CNNL_BRIGHT_SHIFT_OP_V2 are not supported.
 *
 * @note
 * - Not supported on 520 platforms.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the bitwise operation is as follows:
     @verbatim
      bitcompute_op: CNNL_CYCLE_BAND_OP
      input two arrays by 2 * 3 * 3, 2 * 1 * 1
      --> input1: [[[1, 2, 3], [1, 2, 3], [1, 2, 3]],
                  [[2, 3, 4], [2, 3, 4], [2, 3, 4]]]

      --> input2: [[[2]], [[3]]]

      output array by 2 * 3 * 3
      --> output: [[[0, 2, 2], [0, 2, 2], [0, 2, 2]],
                  [[2, 3, 0], [2, 3, 0], [2, 3, 0]]]
     @endverbatim
 */

CNNL_DEPRECATED_FOR(cnnlBitCompute_v2)
cnnlStatus_t CNNL_WIN_API cnnlBitCompute(cnnlHandle_t handle,
                                         const cnnlBitComputeOp_t bitcompute_op,
                                         const cnnlTensorDescriptor_t input1_desc,
                                         const void *input1,
                                         const cnnlTensorDescriptor_t input2_desc,
                                         const void *input2,
                                         const cnnlTensorDescriptor_t output_desc,
                                         void *output);

// Group:CycleOp
/*!
 * @brief Performs cycle element-wise operation based on the operation type defined in
 *        ::cnnlCycleOp_t between two input \p input1 and \p input2, and returns the results
 *        in the output tensor \p output.
 *
 * Cycle operations are wildly used in artificial intelligence as a kind of basic mathematical operations.
 * Also, they are supported in almost all common frameworks, like PyTorch and TensorFlow.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlOpTensor, ::cnnlMaximum or ::cnnlLogicOp instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the cycle operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  cycle_optype
 *   Input. The specific cycle operation performed in the function. The operations are defined in
 *   the ::cnnlCycleOp_t enum.
 * @param[in]  input1_desc
 *   Input. The descriptor of the first input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  input1
 *   Input. Pointer to the MLU memory that stores the first input tensor. \p Input1 is the larger
 *   input of two inputs, the size of \p input1 should be multi times of the size of \p input2
 * @param[in]  input2_desc
 *   Input. The descriptor of the second input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  input2
 *   Input. Pointer to the MLU memory that stores the second input tensor. \p Input2 is the smaller
 *   input of two inputs.
 * @param[out]  output_desc
 *   Output. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out]  output
 *  Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Data Type
 * - This function supports the following data types for input and output tensors.
 *   - input: float, half, int32, bool.
 *   - output: float, half, int32, bool.
 *   Note that the data type of output tensor must be the same as the input tensor expected
 *   when out type is bool.
 * - Data type of input as bool only for \p CNNL_CYCLE_OR, \p CNNL_CYCLE_AND, \p CNNL_CYCLE_XOR,
 *   \p CNNL_CYCLE_EQUAL and \p CNNL_CYCLE_NEQUAL.
 * - Data type of output as bool only for \p CNNL_CYCLE_OR, \p CNNL_CYCLE_AND, \p CNNL_CYCLE_XOR,
 *   \p CNNL_CYCLE_LESS, \p CNNL_CYCLE_LESS_EQUAL, \p CNNL_CYCLE_EQUAL, \p CNNL_CYCLE_NEQUAL,
 *   \p CNNL_CYCLE_GREATER and \p CNNL_CYCLE_GREATER_EQUAL.
 *
 * @par Limitations
 * - For each dimension of the \p input1 and \p input2, the size of the dimension can be the same,
 * and these same dimensions should be consecutive, other dimensions of \p input2 should be 1.
 *
 * @note
 * - When \p input1 or \p input2 contains NaN:
 *   If \p cycle_optype is \p CNNL_CYCLE_NEQUAL, \p output is positive saturation value on MLU200 series
 *   and \p output is 0 on MLU300 series and CE3226.
 *   If \p cycle_optype is \p CNNL_CYCLE_MIN_EQUAL or \p CNNL_CYCLE_MAX_EQUAL,
 *   \p output is saturation value on MLU200 series and \p output is the value in \p input1 on MLU300 series and CE3226.
 *
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the cycle operation is as follows:
     @verbatim
      optype: CNNL_CYCLE_ADD

      input two arrays by 2 * 3 * 3, 2 * 1 * 1
      --> input1: [[[1, 2, 3], [1, 2, 3], [1, 2, 3]],
                  [[2, 3, 4], [2, 3, 4], [2, 3, 4]]]

      --> input2: [[[2]], [[3]]]

      output array by 2 * 3 * 3
      --> output: [[[3, 4, 5], [3, 4, 5], [3, 4, 5]],
                  [[5, 6, 7], [5, 6, 7], [5, 6, 7]]]
     @endverbatim
 */
CNNL_DEPRECATED_FOR(cnnlOpTensor cnnlMaximum cnnlLogicOp)
cnnlStatus_t CNNL_WIN_API cnnlCycleOp(cnnlHandle_t handle,
                                      const cnnlCycleOp_t cycle_optype,
                                      const cnnlTensorDescriptor_t input1_desc,
                                      const void *input1,
                                      const cnnlTensorDescriptor_t input2_desc,
                                      const void *input2,
                                      const cnnlTensorDescriptor_t output_desc,
                                      void *output);

// Group:Erf
/*!
 * @brief Computes gauss error function (erf) of input tensor \p input, and returns the results in the
 *        output tensor \p output. To set the computing with faster algorithm or higher precision,
 *        call ::cnnlErf_v2.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlErf_v2 instead, which supports the \p prefer parameter to set the computing
 *   with faster algorithm or higher precision.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the erf
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Erf Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float, bfloat16 (bfloat16 is only supported on MLU500 series).
 *   - output tensor: half, float, bfloat16 (bfloat16 is only supported on MLU500 series).
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor have the same shape, and the input tensor must meet
 *   the following input data range:
 *   - float: [-3.4e+38,3.4e+38].
 *   - half:  [-65504,-1e-3] || [1e-3,65504].

 * @note
 * - You can specify the stride of all dimensions for input_desc and output_desc with
 *   ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://tensorflow.google.cn/api_docs/python/tf/math/erf
 */
CNNL_DEPRECATED_FOR(cnnlErf_v2)
cnnlStatus_t CNNL_WIN_API cnnlErf(cnnlHandle_t handle,
                                  const cnnlTensorDescriptor_t input_desc,
                                  const void *input,
                                  const cnnlTensorDescriptor_t output_desc,
                                  void *output);

// Group:Erf
/*!
 * @brief Computes gauss error function (erf) of input tensor \p input, and returns the results in the
 *        output tensor \p output.
 *
 * Compared with ::cnnlErf, this function allows you to choose whether to perform erf operation
 * with faster algorithm or higher precision.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the erf
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. The algorithm used to compute the output. For detailed information,
 *   see ::cnnlComputationPreference_t. The default value of this parameter is
 *   ::CNNL_COMPUTATION_HIGH_PRECISION.
 *   When the type of input data is float, it is recommended to set \p prefer to
 *   ::CNNL_COMPUTATION_FAST; when the type of input data is half, it is recommended to set
 *   \p prefer to ::CNNL_COMPUTATION_HIGH_PRECISION.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Data Type
 * - Data type of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float, bfloat16.
 *   - output tensor: half, float, bfloat16.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor have the same shape, and the input tensor must meet
 *   the following input data range:
 *   - float: [-3.4e+38, 3.4e+38].
 *   - half:  [-65504,-1e-3] || [1e-3,65504].

 * @note
 * - You can specify the stride of all dimensions for input_desc and output_desc with
 *   ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://tensorflow.google.cn/api_docs/python/tf/math/erf
 */
cnnlStatus_t CNNL_WIN_API cnnlErf_v2(cnnlHandle_t handle,
                                     const cnnlComputationPreference_t prefer,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const void *input,
                                     const cnnlTensorDescriptor_t output_desc,
                                     void *output);

// Group:Erfinv
/*!
 * @brief Computes inverse error function value(erfinv) of input tensor \p input, and returns the results in the
 *        output tensor \p output.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the erfinv
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. The algorithm used to compute the output. For detailed information,
 *   see ::cnnlComputationPreference_t. The default value of this parameter is
 *   ::CNNL_COMPUTATION_HIGH_PRECISION.
 *   On MLU200 series, it is recommended to set \p prefer to
 *   ::CNNL_COMPUTATION_FAST.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Data Type
 * - Data types of input and output tensors should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float, bfloat16.
 *   - output tensor: half, float, bfloat16.
 *
 *   The bfloat16 data type is supported only on MLU500 series.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor have the same shape.
 *
 * @note
 * - The value of \p input should be in range of (-1, 1), otherwise the corresponding \p output
 *   will be NaN.*
 * - Not supported on 520 platforms.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://tensorflow.google.cn/api_docs/python/tf/math/erfinv
 */
cnnlStatus_t CNNL_WIN_API cnnlErfinv(cnnlHandle_t handle,
                                     const cnnlComputationPreference_t prefer,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const void *input,
                                     const cnnlTensorDescriptor_t output_desc,
                                     void *output);
// Group:Gru
/*!
 * @brief Executes the GRU network described by \p gru_desc with inputs \p input, \p state, filter
 *        \p filter, biases \p bias, output \p output, and \p state_output. \p workspace is
 *        required for intermediate results. You can get the size of the workspace
 *        \p workspace_size by the ::cnnlGetGruWorkspaceSize function.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGru_v2 instead, which supports input sequence data and more parameters in
 *   \p gru_desc to set direction, layer number, filter layout and input layout.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the GRU operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] gru_desc
 *   Input. The descriptor of the GRU operation. For detailed information,
 *   see ::cnnlGruDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] state_desc
 *   Input. The descriptor of the state tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] state
 *   Input. Pointer to the MLU memory that stores the state tensor, which stores the hidden state.
 * @param[in] filter_desc
 *   Input. An array that stores the descriptors of filter tensor, and the number of descriptors is
 *   determined by the information described in \p gru_desc. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores an array of pointers of filter
 *   tensors. The element order of this filter array must be consistent with the element order of the
 *   ::cnnlTensorDescriptor_t array.
 * @param[in] bias_desc
 *   Input. An array that stores the descriptors of bias tensor, and the number of descriptors is
 *   determined by the information described in \p gru_desc. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores an array of pointers of bias
 *   tensors. The element order of this bias array must be consistent with the element order of the
 *   ::cnnlTensorDescriptor_t array.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that stores the workspace tensor, which is used as an extra
 *   workspace for the GRU operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the GRU operation. You can get the size of the workspace with
 *   the ::cnnlGetGruWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] state_output_desc
 *   Input. The descriptor of the state_output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] state_output
 *   Output. Pointer to the MLU memory that stores the state_output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "GRU Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for the input tensor, state tensor,
 *   filter tensor, bias tensor, output tensor, and state_output tensor:
 *   - input tensor: half, float.
 *   - state tensor: half, float.
 *   - filter tensor: int8, int16.
 *   - bias tensor: half, float.
 *   - output tensor: half, float.
 *   - state_output tensor: half, float.
 * - The data types of \p input, \p state, \p bias, \p output, and \p state_output must be the
 *   same.
 *
 * @par Data Layout
 * - None.
 *
 * @par Scale Limitation
 * - The input tensor, and state tensor must meet the following requirements:
 *   - input tensor: \p batch > 0, \p time > 0, \p input_size > 0.
 *   - state tensor: \p batch > 0, \p hidden_size > 0.
 * - The batch number of input tensor must be the same as the batch number of state tensor.
 *
 * @par API Dependency
 * - You need to call the ::cnnlCreateGruDescriptor and ::cnnlSetGruDescriptor functions
 *   to create and set the descriptors \p gru_desc before calling this function.
 * - Before performing the operation, you need to get the size of workspace by the
 *   ::cnnlGetGruWorkspaceSize function and pass the allocated extra workspace to the
 *   ::cnnlGru function.
 *
 * @note
 * - The shape of the input tensor, state tensor, filter tensor, bias tensor, output tensor and
 *   state_output tensor are as follows:
 *   - input tensor: When \p dim is equal to 3, shape is [\p batch, \p time, \p input_size].
 *   - state tensor: When \p dim is equal to 2, shape is
 *     [\p batch * \p num_layer in \p gru_desc, \p hidden_size].
 *     - If \p is_bidirectional in \p gru_desc is true, \p hidden_size needs to be doubled.
 *   - filter tensor: When \p dim is equal to 2, for the filter of input, the shape is
 *     [\p hidden_size, \p input_size]; for the filter of state, the shape is
 *     [\p hidden_size, \p hidden_size].
 *   - bias tensor: When \p dim is equal to 2, the shape is [1, \p hidden_size].
 *   - output tensor: When \p dim is equal to 3, the shape is [\p batch, \p time, \p hidden_size].
 *     - If \p is_bidirectional in \p gru_desc is true, \p hidden_size needs to be doubled.
 *   - state_output tensor: When \p dim is equal to 2, the shape is [\p batch, \p hidden_size].
 *     - If \p is_bidirectional in \p gru_desc is true, \p hidden_size needs to be doubled.
 *
 * @par Requirements
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/nn.html#gru
 */
CNNL_DEPRECATED_FOR(cnnlGru_v2)
cnnlStatus_t CNNL_WIN_API cnnlGru(cnnlHandle_t handle,
                                  const cnnlGruDescriptor_t gru_desc,
                                  const cnnlTensorDescriptor_t input_desc,
                                  const void *input,
                                  const cnnlTensorDescriptor_t state_desc,
                                  const void *state,
                                  const cnnlTensorDescriptor_t filter_desc[],
                                  const void **filter,
                                  const cnnlTensorDescriptor_t bias_desc[],
                                  const void **bias,
                                  void *workspace,
                                  size_t workspace_size,
                                  const cnnlTensorDescriptor_t output_desc,
                                  void *output,
                                  const cnnlTensorDescriptor_t state_output_desc,
                                  void *state_output);

// Group:Gru
/*!
 * @brief Executes the GRU network described by \p gru_desc with inputs \p input, \p state,
 *        sequence lengths \p sequence_lens, filter \p filter, biases \p bias, output \p output
 *        and \p state_output. \p workspace is required for intermediate results. Users can get the
 *        size of the workspace \p workspace_size by the ::cnnlGetGruWorkspaceSize_v2 function.
 *
 * Compared with ::cnnlGru, this function allows users to set direction, filter layout,
 * input layout, and layer number.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the GRU operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] gru_desc
 *   Input. The descriptor of the GRU operation. For detailed information,
 *   see ::cnnlGruDescriptor_t.
 * @param[in] dev_seq_lengths
 *   Input. A copy of \p seqLengthArray set in \p input_desc or \p output_desc RNN data descriptor.
 *   The dev_seq_lengths array must be stored in MLU memory.
 * @param[in] extra_input_ptr
 *   Input. Pointer to the MLU memory that stores the extra input data.
 *   You need to copy the extra input data to MLU from the host that is
 *   initialized with ::cnnlInitGruExtraInput.
 * @param[in] input_desc
 *   Input. The descriptor of the input sequence tensor. For detailed information,
 *   see ::cnnlSeqDataDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input sequence tensor.
 * @param[in] state_desc
 *   Input. The descriptor of the state tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] state
 *   Input. Pointer to the MLU memory that stores the state tensor, which stores the hidden state.
 * @param[in] sequence_lens_desc
 *   Input. The descriptor of the sequence_lens tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] sequence_lens
 *   Input. Pointer to the MLU memory that stores the sequence_lens tensor,
 *   which stores the sequence lengths of input tensor.
 * @param[in] filter_desc
 *   Input. An array that stores the descriptors of filter tensor, and the number of descriptors is
 *   determined by the information described in \p gru_desc. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores an array of pointers of filter
 *   tensors. The element order of this filter array must be consistent with the element order of the
 *   ::cnnlTensorDescriptor_t array.
 * @param[in] bias_desc
 *   Input. An array that stores the descriptors of bias tensor, and the number of descriptors is
 *   determined by the information described in \p gru_desc. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores an array of pointers of bias
 *   tensors. The element order of this bias array must be consistent with the element order of the
 *   ::cnnlTensorDescriptor_t array.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that stores the workspace tensor, which is used as an extra
 *   workspace for the GRU operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the GRU operation. You can get the size of the workspace with
 *   the ::cnnlGetGruWorkspaceSize_v2 function.
 * @param[in] output_desc
 *   Input. The descriptor of the output sequence tensor. For detailed information,
 *   see ::cnnlSeqDataDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output sequence tensor.
 * @param[in] state_output_desc
 *   Input. The descriptor of the state_output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] state_output
 *   Output. Pointer to the MLU memory that stores the state_output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH,
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "GRU Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for the input tensor, state tensor, filter
 *   tensor, bias tensor, output tensor, and state_output tensor:
 *   - input tensor: half, float.
 *   - state tensor: half, float.
 *   - filter tensor: int8, int16, half, float.
 *   - bias tensor: half, float.
 *   - output tensor: half, float.
 *   - state_output tensor: half, float.
 * - The data types of \p input, \p state, \p bias, \p output, and \p state_output must be the
 *   same.
 * - The data types of \p input, \p state, \p filter, \p bias, \p output, and \p state_output must
 *   be the same when data type of \p filter is half or float.
 *
 * @par Data Layout
 * - The supported data layouts of the input tensor, and output tensor are as follows:
 *   - input sequence data: ::CNNL_SEQDATA_NTC, ::CNNL_SEQDATA_TNC, ::CNNL_SEQDATA_TNC_PACKED.
 *   - output sequence data: ::CNNL_SEQDATA_NTC, ::CNNL_SEQDATA_TNC, ::CNNL_SEQDATA_TNC_PACKED.
 *
 * @par Scale Limitation
 * - The input tensor, and state tensor must meet the following requirements:
 *   - input tensor: \p batch > 0, \p time > 0, \p input_size > 0.
 *   - state tensor: \p batch > 0, \p hidden_size > 0.
 *   - If input tensor data type is half, then \p hidden_size < 128.
 *   - If \p hidden_size > 1024, then \p time <= 3.
 *   - On MLU200 series, if \p hidden_size > 256, then \p time <= 10.
 *   - On MLU200 series, if \p hidden_size > 128, then \p time <= 16.
 * - The input and sequence_lens tensor, \p num_layer, \p state_layout, \p algo,
 *   \p and multi_layer_mode must meet the following requirements:
 *   - If \p algo is \p CNNL_GRU_ALGO_V1, then
 *     layout of input and output tensors cannot be ::CNNL_SEQDATA_TNC_PACKED,
 *     sequence_lens tensor must be NULL, \p direction cannot be ::CNNL_GRU_BACKWARD,
 *     and \p multi_layer_mode cannot be ::CNNL_GRU_MODE_V2.
 *   - If \p layout of input and output tensors is ::CNNL_SEQDATA_TNC_PACKED, then
 *     sequence_lens tensor must be NULL, \p multi_layer_mode cannot be ::CNNL_GRU_MODE_V1.
 *     and \p state_layout cannot be ::CNNL_GRU_LNDC.
 *   - If sequence_lens tensor is not NULL, then \p num_layer must be 1.
 * - The batch number of input tensor must be the same as the batch number of state tensor.
 *
 * @par API Dependency
 * - You need to call the ::cnnlCreateGruDescriptor and ::cnnlSetGruDescriptor_v2 functions
 *   to create and set the GRU descriptors \p gru_desc before calling this function.
 * - Before performing the operation, you need to get the size of workspace by the
 *   ::cnnlGetGruWorkspaceSize_v2 function and pass the allocated extra workspace to the
 *   ::cnnlGru_v2 function.
 *
 * @note
 * - The shape of the input tensor, state tensor, filter tensor, bias tensor, output tensor and
 *   state_output tensor are as follows:
 *   - input sequence data: When \p dim is equal to 3, the shape is as follows:
 *     - If \p version in \p gru_desc is 1, the shape is [\p batch, \p time, \p input_size].
 *     - If \p version in \p gru_desc is 2 and layout is ::CNNL_SEQDATA_NTC, the shape
 *       is [\p batch, \p time, \p input_size].
 *     - If \p version in \p gru_desc is 2 and layout is ::CNNL_SEQDATA_TNC, the shape
 *       is [\p time, \p batch, \p input_size].
 *   - state tensor: When \p dim is equal to 4, the shape is
 *     [\p num_layer, \p batch, 1, \p hidden_size].
 *     - If \p version in \p gru_desc is 1 and \p is_bidirectional in \p gru_desc is true, the
 *       third dim value needs to be 2.
 *     - If \p version in \p gru_desc is 2 and \p direction in \p gru_desc is
 *       ::CNNL_GRU_BIDIRECTIONAL, the third dim value needs to be 2.
 *   - filter tensor: When \p dim is equal to 2, for the filter of input, the shape is
 *     [\p hidden_size, \p input_size]; for the filter of state, the shape is
 *     [\p hidden_size, \p hidden_size].
 *   - bias tensor: When \p dim is equal to 2, the shape is [1, \p hidden_size].
 *   - output sequence data: When \p dim is equal to 3, the shape is as follows:
 *     - If \p version in \p gru_desc is 1, the shape is [\p batch, \p time, \p hidden_size].
 *     - If \p version in \p gru_desc is 2 and layout is ::CNNL_SEQDATA_NTC, the shape
 *       is [\p batch, \p time, \p hidden_size].
 *     - If \p version in \p gru_desc is 2 and layout is ::CNNL_SEQDATA_TNC, the shape
 *       is [\p time, \p batch, \p hidden_size].
 *     - If \p version in \p gru_desc is 1 and \p is_bidirectional in \p gru_desc is true,
 *       \p hidden_size needs to be doubled.
 *     - If \p version in \p gru_desc is 2 and \p direction in \p gru_desc is
 *       ::CNNL_GRU_BIDIRECTIONAL, \p hidden_size needs to be doubled.
 *   - state_output tensor: When \p dim is equal to 4, the shape is
 *     [\p num_layer, \p batch, 1, \p hidden_size].
 *     - If \p version in \p gru_desc is 1 and \p is_bidirectional in \p gru_desc is true, the
 *       third dim value needs to be doubled.
 *     - If \p version in \p gru_desc is 2 and \p direction in \p gru_desc is
 *       ::CNNL_GRU_BIDIRECTIONAL, the third dim value needs to be doubled.
 *
 * @par Requirements
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/nn.html#gru
 */
cnnlStatus_t CNNL_WIN_API cnnlGru_v2(cnnlHandle_t handle,
                                     const cnnlGruDescriptor_t gru_desc,
                                     const int dev_seq_lengths[],
                                     const void *extra_input_ptr,
                                     const cnnlSeqDataDescriptor_t input_desc,
                                     const void * input,
                                     const cnnlTensorDescriptor_t state_desc,
                                     const void * state,
                                     const cnnlTensorDescriptor_t sequence_lens_desc,
                                     const void * sequence_lens,
                                     const cnnlTensorDescriptor_t filter_desc[],
                                     const void ** filter,
                                     const cnnlTensorDescriptor_t bias_desc[],
                                     const void ** bias,
                                     void * workspace,
                                     size_t workspace_size,
                                     const cnnlSeqDataDescriptor_t output_desc,
                                     void * output,
                                     const cnnlTensorDescriptor_t state_output_desc,
                                     void * state_output);

// Group:Gru
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace
 *        to optimize the ::cnnlGru function.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlGetGruWorkspaceSize_v3 instead, which accepts filter tensor descriptor to avoid unnecessary workspace.
 *
 * The size of the extra workspace is based on the given information of the GRU operation,
 * including the input tensor descriptor \p input_desc, state tensor descriptor \p state_desc,
 * and operation descriptor \p gru_desc. For more information about the workspace,
 * see "Cambricon CNNL User Guide".
 *
 * Compared with ::cnnlGetGruWorkspaceSize_v2, this function accepts input tensor.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the GRU operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] state_desc
 *   Input. The descriptor of the state tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] gru_desc
 *   Input.The descriptor of the GRU operation. For detailed information,
 *   see ::cnnlGruDescriptor_t.
 * @param[out] size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in
 *   the GRU operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions
 *   to create and set the tensor descriptors \p input_desc and \p state_desc
 *   before calling this function.
 * - You need to call the ::cnnlCreateGruDescriptor and ::cnnlSetGruDescriptor functions
 *   to create and set the descriptors \p gru_desc before calling this function.
 * - The allocated extra workspace should be passed to the ::cnnlGru function
 *   to perform the GRU operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 */

CNNL_DEPRECATED_FOR(cnnlGetGruWorkspaceSize_v3)
cnnlStatus_t CNNL_WIN_API cnnlGetGruWorkspaceSize(cnnlHandle_t handle,
                                                  const cnnlTensorDescriptor_t input_desc,
                                                  const cnnlTensorDescriptor_t state_desc,
                                                  const cnnlGruDescriptor_t gru_desc,
                                                  size_t *size);

// Group:Gru
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace
 *        to optimize the ::cnnlGru_v2 function.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlGetGruWorkspaceSize_v3 instead, which accepts filter tensor descriptor to avoid unnecessary workspace.
 *
 * The size of the extra workspace is based on the given information of the GRU operation,
 * including the input tensor descriptor \p input_desc, state tensor descriptor \p state_desc,
 * and operation descriptor \p gru_desc. For more information about the workspace,
 * see "Cambricon CNNL User Guide".
 *
 * Compared with ::cnnlGetGruWorkspaceSize, this function accepts input sequence data.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the GRU operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input sequence data. For detailed information,
 *   see ::cnnlSeqDataDescriptor_t.
 * @param[in] state_desc
 *   Input. The descriptor of the state tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] gru_desc
 *   Input.The descriptor of the GRU operation. For detailed information,
 *   see ::cnnlGruDescriptor_t.
 * @param[out] size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in
 *   the GRU operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call the ::cnnlCreateSeqDataDescriptor and ::cnnlSetSeqDataDescriptor functions
 *   to create and set the sequence data descriptors \p input_desc
 *   before calling this function.
 * - You need to call the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions
 *   to create and set the tensor descriptors \p state_desc
 *   before calling this function.
 * - You need to call the ::cnnlCreateGruDescriptor and ::cnnlSetGruDescriptor_v2 functions
 *   to create and set the GRU descriptors \p gru_desc before calling this function.
 * - The allocated extra workspace should be passed to the ::cnnlGru_v2 function
 *   to perform the GRU operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlGetGruWorkspaceSize_v3)
cnnlStatus_t CNNL_WIN_API cnnlGetGruWorkspaceSize_v2(cnnlHandle_t handle,
                                                     const cnnlSeqDataDescriptor_t input_desc,
                                                     const cnnlTensorDescriptor_t state_desc,
                                                     const cnnlGruDescriptor_t gru_desc,
                                                     size_t *size);

// Group:Gru
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace
 *        to optimize the ::cnnlGru_v2 function.
 *
 * The size of the extra workspace is based on the given information of the GRU operation,
 * including the input tensor descriptor \p input_desc, state tensor descriptor \p state_desc,
 * and GRU descriptor \p gru_desc. For more information about the workspace,
 * see "Cambricon CNNL User Guide".
 *
 * Compared with ::cnnlGetGruWorkspaceSize_v2, this function accepts filter tensor descriptor to avoid unnecessary workspace.
 *
 * @param[in] handle
 *   Input. Handle to a CNNL context that is used to manage MLU devices and
 *   queues in the GRU operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input sequence data. For detailed information,
 *   see ::cnnlSeqDataDescriptor_t.
 * @param[in] state_desc
 *   Input. The descriptor of the state tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] filter_desc
 *   Input. An array that stores the descriptors of filter tensor, and the number of descriptors is
 *   determined by the information described in \p gru_desc. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] gru_desc
 *   Input.The descriptor of the GRU operation. For detailed information,
 *   see ::cnnlGruDescriptor_t.
 * @param[out] size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in
 *   the GRU operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call the ::cnnlCreateSeqDataDescriptor and ::cnnlSetSeqDataDescriptor functions
 *   to create and set the sequence data descriptors \p input_desc
 *   before calling this function.
 * - You need to call the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions
 *   to create and set the tensor descriptors \p state_desc
 *   before calling this function.
 * - You need to call the ::cnnlCreateGruDescriptor and ::cnnlSetGruDescriptor_v2 functions
 *   to create and set the GRU descriptors \p gru_desc before calling this function.
 * - The allocated extra workspace should be passed to the ::cnnlGru_v2 function
 *   to perform the GRU operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetGruWorkspaceSize_v3(cnnlHandle_t handle,
                                                     const cnnlSeqDataDescriptor_t input_desc,
                                                     const cnnlTensorDescriptor_t state_desc,
                                                     const cnnlTensorDescriptor_t filter_desc[],
                                                     const cnnlGruDescriptor_t gru_desc,
                                                     size_t *size);

// Group:Gru
/*!
 * @brief Creates a descriptor pointed by \p desc for a GRU operation,
 *        and allocates memory for holding the information about the GRU operation.
 *        The information is defined in ::cnnlGruDescriptor_t. For more
 *        information about descriptor, see "Cambricon CNNL User Guide".
 *
 * @param[out] desc
 *  Output. A host pointer to the GRU descriptor that holds information about the GRU operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_NOT_INITIALIZED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetGruDescriptor function to initialize
 *   and set the information to the GRU descriptor.
 * - You need to call the ::cnnlDestroyGruDescriptor function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateGruDescriptor(cnnlGruDescriptor_t *desc);

// Group:Gru
/*!
 * @brief Initializes the GRU descriptor \p gru_desc that was previously created
 *        by the ::cnnlCreateGruDescriptor function, and sets the information
 *        about the GRU operation to the GRU descriptor \p gru_desc.
 *        The information includes the algorithm \p algo defined in ::cnnlGruAlgo_t,
 *        \p is_bidirectional to determine whether the GRU operation is bidirectional,
 *        and \p num_layer the number of layer.
 *
 * Compared with ::cnnlSetGruDescriptor_v2, this function only supports
 * to set \p algo, \p is_bidirectional, and \p num_layer parameters.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlSetGruDescriptor_v2 instead, which use \p direction
 *   instead of \p is_bidirectional, support to set more parameters,
 *   include \p direction, \p filter_order, \p activation_zr, \p activation_n,
 *   \p linear_before_reset, \p alpha_zr, \p alpha_n, \p beta_zr, \p beta_n, and \p clip.
 *
 * @param[in,out] gru_desc
 *  Input/output. The descriptor of the GRU operation. For detailed information,
 *  see ::cnnlGruDescriptor_t.
 * @param[in] algo
 *   Input. The algorithm used to compute GRU, defined in ::cnnlGruAlgo_t.
 * @param[in] is_bidirectional
 *   Input. Determines whether the GRU is bidirectional or not.
 * @param[in] num_layer
 *   Input. The number of layers of the GRU operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - The \p num_layer must be larger than 0.
 *
 * @par Requirements
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlSetGruDescriptor_v2)
cnnlStatus_t CNNL_WIN_API cnnlSetGruDescriptor(cnnlGruDescriptor_t gru_desc,
                                               cnnlGruAlgo_t algo,
                                               bool is_bidirectional,
                                               int num_layer);

// Group:Gru
/*!
 * @brief Initializes the GRU descriptor \p gru_desc that was previously created
 *        by the ::cnnlCreateGruDescriptor function, and sets the information
 *        about the GRU operation to the GRU descriptor \p gru_desc.
 *        The information includes the algorithm \p algo defined in ::cnnlGruAlgo_t,
 *        the recurrence pattern \p direction, and \p num_layer the number of layer.
 *
 * Compared with ::cnnlSetGruDescriptor, this function use \p direction
 * instead of \p is_bidirectional, support to set more parameters,
 * include \p direction, \p filter_order, \p activation_zr, \p activation_n,
 * \p linear_before_reset, \p alpha_zr, \p alpha_n, \p beta_zr, \p beta_n, and \p clip.
 *
 * @param[in,out] gru_desc
 *  Input/output. The descriptor of the GRU operation. For detailed information,
 *  see ::cnnlGruDescriptor_t.
 * @param[in] algo
 *   Input. The algorithm used to compute GRU, defined in ::cnnlGruAlgo_t.
 * @param[in] multi_layer_mode
 *   Input. The mode used to compute multi-layer bidirectional GRU, defined in ::cnnlGruMode_t.
 * @param[in] direction
 *   Input. Indicate the recurrence pattern.
 * @param[in] filter_order
 *   Input. Indicate the filter order of the gates.
 * @param[in] activation_zr
 *   Input. An array that is used to specify activation function for update and reset gates.
 *   The function must be one of supported activation functions: Sigmoid, Tanh and Relu.
 *   If \p direction is set to ::CNNL_GRU_FORWARD or ::CNNL_GRU_BACKWARD, one
 *   activation function is needed. If \p direction is set to ::CNNL_GRU_BIDIRECTIONAL,
 *   two activation functions are needed.
 *   If Tanh or Relu is used, the precision will be low when sequence time is too large.
 *   When it is null, it means that default activation Sigmoid is used.
 * @param[in] activation_n
 *   Input. An array that is used to specify activation function for hidden gate.
 *   The function must be one of supported activation functions: Sigmoid, Tanh and Relu.
 *   If \p direction is set to ::CNNL_GRU_FORWARD or ::CNNL_GRU_BACKWARD, one
 *   activation function is needed. If \p direction is set to ::CNNL_GRU_BIDIRECTIONAL,
 *   two activation functions are needed.
 *   If Relu is used, the precision will be low when sequence time is too large.
 *   When it is null, it means that default activation Tanh is used.
 * @param[in] state_layout
 *   Input. The layout of \p state and \p state_output in ::cnnlGru_v2.
 * @param[in] math_prec
 *   Input. This parameter is not supported currently.
 * @param[in] linear_before_reset
 *   Input. A Boolean value indicating whether to apply linear transformation before multiplying
 *   reset gate when computing output of hidden gate.
 * @param[in] alpha_zr
 *   Input. Not implemented.
 * @param[in] alpha_n
 *   Input. Not implemented.
 * @param[in] beta_zr
 *   Input. Not implemented.
 * @param[in] beta_n
 *   Input. Not implemented.
 * @param[in] clip
 *   Input. An array with one value that is used to define a value range to clip the elements of an
 *   input tensor within the defined range[-clip, clip] before performing activation.
 *   When it is null, it means that \p clip is not enabled.
 * @param[in] num_layer
 *   Input. The number of layers of the GRU operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - If \p algo is ::CNNL_GRU_ALGO_V1, \p direction must be ::CNNL_GRU_FORWARD,
 *   \p filter_order must be ::CNNL_GRU_RZN, \p num_layer must be 1,
 *   and other parameters will be ignored except these three parameters.
 * - If \p direction is ::CNNL_GRU_BACKWARD, \p num_layer must be 1.
 * - If \p activation_zr is not Sigmoid or \p activation_n is not Tanh, filter data type
 *   cannot be half.
 * - If \p linear_before_reset is false, the size of input channel or output channel
 *   should not be too large, e.g. For example, the size of input channel or output channel should
 *   be less than 209 when filter data type is float on MLU370x4.
 * - If \p linear_before_reset is false, only float-point data type is supported in filter tensor.
 * - The \p clip must be larger than 0.
 * - The \p num_layer must be larger than 0.
 *
 * @par Requirements
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetGruDescriptor_v2(cnnlGruDescriptor_t gru_desc,
                                                  cnnlGruAlgo_t algo,
                                                  cnnlGruMode_t multi_layer_mode,
                                                  cnnlGruDir_t direction,
                                                  cnnlGruWeightOrder_t filter_order,
                                                  cnnlActivationMode_t activation_zr[],
                                                  cnnlActivationMode_t activation_n[],
                                                  cnnlGruLayout_t state_layout,
                                                  cnnlDataType_t math_prec,
                                                  bool linear_before_reset,
                                                  float alpha_zr[],
                                                  float alpha_n[],
                                                  float beta_zr[],
                                                  float beta_n[],
                                                  float clip[],
                                                  int num_layer);

// Group:Gru
/*!
 * @brief Destroys a GRU descriptor \p desc that was previously created with the
 *        ::cnnlCreateGruDescriptor function.
 *
 * The GRU descriptor is defined in ::cnnlGruDescriptor_t
 * and holds the information about the GRU operation.
 *
 *
 * @param[in] desc
 *   Input. The GRU descriptor to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - You need to call this function after calling the ::cnnlGru function.
 * - This function should be called to destroy the GRU descriptor.
 *   Otherwise, the memory leak may occur.
 *
 * @par Requirements
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyGruDescriptor(cnnlGruDescriptor_t desc);

// Group:Gru
/*!
 * @brief Returns in \p extra_input_size the size of the MLU memory and host memory that is used as an extra
 * input data to optimize the GRU operation. You need to allocate memory both on host and MLU based on
 * the size returned in \p extra_input_size.
 *
 * The size of extra input data is based on the given information of the GRU
 * operation, including Gru descriptor \p gru_desc.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the GRU operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] gru_desc
 *   Input.The descriptor of the GRU operation. For detailed information,
 *   see ::cnnlGruDescriptor_t.
 * @param[out] extra_input_size
 *   Output. A host pointer to the returned size of the extra input data in bytes
 *   that is used in the GRU operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - The parameter \p extra_input_size cannot be nullptr.
 *
 * @par API Dependency
 * - Before calling this function,
 *   you need to call ::cnnlCreateGruDescriptor and ::cnnlSetGruDescriptor functions to create and
 *   set up the GRU operation descriptors \p gru_desc.
 * - After calling this function, you need to call ::cnnlInitGruExtraInput to initialize the memory on host.
 * - The allocated extra input should be passed to the ::cnnlGru_v2 function
 *   to perform the GRU operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetGruExtraInputSize(cnnlHandle_t handle,
                                                   const cnnlGruDescriptor_t gru_desc,
                                                   size_t *extra_input_size);

// Group:GRU
/*!
 * @brief Initializes the extra input data space \p extra_input_host_ptr on host.
 * Compared with ::cnnlInitGruExtraInput, ::cnnlInitGRUWeightPositionAndScale
 * obtains position and scale from \p weight_position_ptr and \p weight_scale_ptr.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the GRU forward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] gru_desc
 *   Input. The descriptor of the GRU operation. For detailed information,
 *   see ::cnnlGruDescriptor_t.
 * @param[in] weight_num
 *   Input. The number of filters used in the GRU operation.
 * @param[in] weight_position_ptr
 *   Input. The pointer to position for filter data address on the host memory.
 * @param[in] weight_scale_ptr
 *   Input. The pointer to scale for filter data address on the host memory.
 * @param[in] weight_offset_ptr
 *   Input. The pointer to offset for filter data address on the host memory.
 * @param[out]  extra_input_host_ptr
 *   Output. Pointer to the host memory that is used as an extra input space
 *   in the GRU forward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - The parameter \p extra_input_host_ptr is a pointer and should not be NULL.
 *
 * @par API Dependency
 * - Before calling this function, you need to get the size of the extra input data with
 *   ::cnnlGetGruExtraInputSize, and make sure that the memory of the extra input data is allocated.
 * - The allocated extra input should be passed to the ::cnnlGru_v2 function
 *   to perform the RNN operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlInitGRUWeightPositionAndScale(cnnlHandle_t handle,
                                  const cnnlGruDescriptor_t gru_desc,
                                  const int weight_num,
                                  void *weight_position_ptr,
                                  void *weight_scale_ptr,
                                  void *weight_offset_ptr,
                                  void *extra_input_host_ptr);

// Group:Gru
/*!
 * @brief Initializes the extra input data space \p extra_input_host_ptr on host.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlInitGRUWeightPositionAndScale instead, which obtains position and scale
 * from \p weight_position_ptr and \p weight_scale_ptr.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the GRU operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] gru_desc
 *   Input.The descriptor of the GRU operation. For detailed information,
 *   see ::cnnlGruDescriptor_t.
 * @param[in] filter_desc[]
 *   Input. The descriptor of the filter tensor array. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] extra_input_host_ptr
 *   Output. Pointer to the host memory that is used as an extra input space
 *   for the GRU operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - The parameter \p extra_input_host_ptr cannot be nullptr.
 *
 * @par API Dependency
 * - Before calling this function,
 *   The ::cnnlCreateGruDescriptor and ::cnnlSetGruDescriptor functions are used to create and
 *   set up a complete GRU operation descriptors \p gru_desc.
 *   The ::cnnlCreateTensorDescriptor
 *   and ::cnnlSetTensorDescriptorPositionAndScale functions are used to create and set
 *   the tensor descriptor array \p filter_desc[].
 * - The allocated extra input should be passed to the ::cnnlGru_v2 function
 *   to perform the GRU operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlInitGRUWeightPositionAndScale)
cnnlStatus_t CNNL_WIN_API cnnlInitGruExtraInput(cnnlHandle_t handle,
                                                const cnnlGruDescriptor_t gru_desc,
                                                const cnnlTensorDescriptor_t filter_desc[],
                                                void *extra_input_host_ptr);

// Group:LSTM
/*!
 * @brief Computes the forward process of LSTM network in the inference scenario. This function uses
 *        the input data \p input, \p state, \p control, \p filter, \p bias, according to the specific
 *        network structure, and writes the calculation result into the output memory \p output,
 *        \p state_output, \p control_output.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlRNNForwardInference instead.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
 *          For detailed information, see ::cnnlHandle_t.
 * @param[in]  input_desc
 *   Input. The descriptor of input tensor.
 * @param[in]  input
 *   Input. Pointer to MLU memory that stores input data.
 * @param[in]  state_desc
 *   Input. The descriptor of state tensor.
 * @param[in]  state
 *   Input. Pointer to MLU memory that stores state data.
 * @param[in]  control_desc
 *   Input. The descriptor of control tensor.
 * @param[in]  control
 *   Input. Pointer to MLU memory that stores control data.
 * @param[in]  filter_desc
 *   Input. The descriptor array of filter tensor.
 * @param[in]  filter
 *   Input. Pointer to MLU memory that stores filter data.
 * @param[in]  bias_desc
 *   Input. The descriptor array of bias tensor.
 * @param[in]  bias
 *   Input. Pointer to MLU memory that stores bias data.
 * @param[in]  forget
 *   Input. The value of forget.
 * @param[in]  workspace
 *   Input. Pointer to MLU workspace memory.
 * @param[in]  workspace_size
 *   Input. The size of the workspace in bytes.
 * @param[in]  output_desc
 *   Input. The descriptor of output tensor.
 * @param[out]  output
 *   Input. Pointer to MLU memory that stores output data.
 * @param[in]  state_output_desc
 *   Input. The descriptor of output state tensor.
 * @param[out]  state_output
 *   Input. Pointer to MLU memory that stores output state data.
 * @param[in]  control_output_desc
 *   Input. The descriptor of output control tensor.
 * @param[out]  control_output
 *   Input. Pointer to the MLU memory that stores output control data.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for the input, filter
 *   and output tensors.
 *   - input tensor: float, half
 *   - filter tensor: int8, int16
 *   - output tensor: float, half
 *
 * @par Scale Limitation
 * - The sum of \p hidden size and \p input size should be smaller than 2048 in int16 mode and
 *   should be smaller than 4096 in int8 mode.
 * - The sum of \p hidden size and \p state size should be smaller than 2048 in int16 mode and
 *   should be smaller than 4096 in int8 mode.
 *
 * @par API Dependency
 * - None.
 *
 * @par Requirements
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlRNNForwardInference)
cnnlStatus_t CNNL_WIN_API cnnlLSTM(cnnlHandle_t handle,
                                   const cnnlTensorDescriptor_t input_desc,
                                   const void *input,
                                   const cnnlTensorDescriptor_t state_desc,
                                   const void *state,
                                   const cnnlTensorDescriptor_t control_desc,
                                   const void *control,
                                   const cnnlTensorDescriptor_t filter_desc[],
                                   void **filter,
                                   const cnnlTensorDescriptor_t bias_desc[],
                                   void **bias,
                                   const float forget,
                                   void *workspace,
                                   size_t workspace_size,
                                   const cnnlTensorDescriptor_t output_desc,
                                   void *output,
                                   const cnnlTensorDescriptor_t state_output_desc,
                                   void *state_output,
                                   const cnnlTensorDescriptor_t control_output_desc,
                                   void *control_output);

// Group:Attention
/*!
 * @brief Creates the attention operation.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in]  handle
 *   Input. A handle struct with information that all you need in this operation.
 * @param[in] attention_desc
 *   Input. Description of attention operation, containing algorithm.
 * @param[in]  input_desc
 *   Input. The descriptor array of input tensor, containing dimensions and data type of all input data.
 * @param[in]  input
 *   Input. The pointer to data address of input.
 * @param[in]  filter_desc
 *   Input. The descriptor array of filter tensor, containing dimensions and data type of all
 *  filter.
 * @param[in]  filter
 *   Input. The pointer to data address of filter.
 * @param[in]  bias_desc
 *   Input. The descriptor array of filter tensor, containing dimensions and data type of all bias.
 * @param[in]  bias
 *   Input. The pointer to data address of bias.
 * @param[in]  workspace
 *   Input. The pointer to data address of workspace.
 * @param[in]  workspace_size
 *   Input. The value of workspace_size.
 * @param[in]  output_desc
 *   Input. The descriptor of output tensor, containing dimensions and data type of output data.
 * @param[in]  output
 *   Input. The pointer to data address of output.
 * @param[in]  output_score_desc
 *   Input. The descriptor of output_score tensor, containing dimensions and data type of output_score data.
 * @param[in]  output_score
 *   Input. The pointer to data address of output_score.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for the input, output_score,
 *   filter and output tensors.
 *   - input: half, float.
 *   - output_score: half, float.
 *   - filter: int8, int16
 *   - bias: half, float.
 *   - output: half, float.
 *
 * @par Scale Limitation
 *  - batch > 0, time > 0, channel > 0, units > 0
 *
 * @note
 *  - The data type of output and input tensors should be the same.
 *  - The layout of filter is NHWC.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlAttention(cnnlHandle_t handle,
                                        const cnnlAttentionDescriptor_t attention_desc,
                                        const cnnlTensorDescriptor_t input_desc[],
                                        const void **input,
                                        const cnnlTensorDescriptor_t filter_desc[],
                                        void **filter,
                                        const cnnlTensorDescriptor_t bias_desc[],
                                        void **bias,
                                        void *workspace,
                                        size_t workspace_size,
                                        const cnnlTensorDescriptor_t output_desc,
                                        void *output,
                                        const cnnlTensorDescriptor_t output_score_desc,
                                        void *output_score);

// Group:Attention
/*!
 * @brief Gets the extra space size needed in cnnlAttention operation, according to the
 * input parameters.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in]  handle
 *   Input.A handle struct holding all the information used in this operation.
 * @param[in]  input_desc
 *   Input. The descriptor array of input tensor, containing dimensions and data type of all input data.
 * @param[in] attention_desc
 *   Input. Description of attention operation, containing algorithm.
 * @param[out]  size
 *   Output. Extra space size needed in the cnnlAttention operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlGetAttentionWorkspaceSize(cnnlHandle_t handle,
                                                   const cnnlTensorDescriptor_t *input_desc,
                                                   const cnnlAttentionDescriptor_t attention_desc,
                                                   size_t *size);

// Group:Attention
/*!
 * @brief Creates a descriptor of attention and allocates memory for it.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[out] desc
 *  Output. Descriptor of the attention operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlCreateAttentionDescriptor(cnnlAttentionDescriptor_t *desc);

// Group:Attention
/*!
 * @brief Assigns attention descriptor with parameters.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in,out] attention_desc
 *   Input/output. Description of attention operation, containing algorithm.
 * @param[in] algo
 *   Input. The algorithm to use to compute attention.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlSetAttentionDescriptor(cnnlAttentionDescriptor_t attention_desc,
                                                     cnnlAttentionAlgo_t algo);

// Group:Attention
/*!
 * @brief Destroy the attention descriptor.
 *
 * This function needs to be called after cnnlAttention().
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] desc
 *   Input. Description of attention operation, containing algorithm.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlDestroyAttentionDescriptor(cnnlAttentionDescriptor_t desc);

/*****************************************************************************
 * Cambricon CNNL OP: MultiHeadAttn
 *****************************************************************************/
/*!
 * @brief Enumeration variables describing the attention modes of multi-head attention.
 * NOBIAS and BIAS modes are used to distinguish whether extra biases are needed after the linear
 * projection of queries, keys, values and outputs. When extra biases are configured, biases are
 * broadcasted along the column dimension and added to the projection results.
 *
 * It is deprecated and will be removed in future release.
 */
typedef enum {
  ALL_TO_ONE_BIAS = 0,
  /*!< All beams of queries map to a single beam of keys-values with extra projection biases.
   * This is used when the beam size of keys and values is equal to 1 and the beam size of
   * queries is greater than 1.*/
  ALL_TO_ONE_NOBIAS = 1,
  /*!< All beams of queries map to a single beam of keys-values without extra projection biases.
   * This is used when the beam size of keys and values is equal to 1 and the beam size of
   * queries is greater than 1.*/
  ONE_TO_ONE_BIAS = 2,
  /*!< All beams of queries map to the corresponding beams of keys and values with extra projection
   * biases. The beam size of keys and values is equal to the beam size of queries.*/
  ONE_TO_ONE_NOBIAS = 3,
  /*!< All beams of queries map to the corresponding beams of keys and values without extra
   * projection biases. The beam size of keys and values is equal to the beam size of queries.*/
} cnnlAttnMode;

// Group:MultiHeadAttnForward
 /*!
 * @brief Creates a descriptor pointed by \p desc for a multi-head attention forward or backward
 * operation, and allocates memory for holding the information about the multi-head attention
 * operation. The information is defined in ::cnnlMultiHeadAttnDescriptor_t.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[out] desc
 *  Output. A host pointer to the multi-head attention descriptor that containing information about
 *  the multi-head attention operation. For detailed information, see ::cnnlMultiHeadAttnDescriptor_t.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetMultiHeadAttnDescriptor function to
 * initialize and set the information to the multihead_attn descriptor.
 * - You need to call the ::cnnlDestroyMultiHeadAttnDescriptor function to destroy the descriptor.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlCreateMultiHeadAttnDescriptor(cnnlMultiHeadAttnDescriptor_t *desc);

// Group:MultiHeadAttnForward
/*!
 * @brief Initializes the multi-head attention descriptor \p desc that was previously created with
 * ::cnnlCreateMultiHeadAttnDescriptor function, and sets the information about the multi-head
 * attention forward and backward operation to the multi-head attention descriptor \p desc.
 * The information includes \p attn_mode, \p nheads, \p sm_scaler, \p dtype,
 * \p compute_type, \p attn_dropout, \p post_dropout,
 * \p q_size, \p k_size, \p v_size, \p q_proj_size, \p k_proj_size,
 * \p v_proj_size, \p o_proj_size,
 * \p qo_max_seq_length, \p kv_max_seq_length, \p max_batch_size and \p max_beam_size.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in,out] desc
 *   Input/output. The descriptor of multi-head attention operation. For detailed information,
 *   see ::cnnlMultiHeadAttnDescriptor_t.
 * @param[in] attn_mode
 *   Input. Attention mode including the query map and extra biases. See ::cnnlAttnMode.
 * @param[in] nheads
 *   Input. The number of attention heads.
 * @param[in] sm_scaler
 *   Input. The coefficient for softmax smoothing.
 * @param[in] dtype
 *   Input. The data type of inputs, outputs and filter. See ::cnnlDataType_t.
 * @param[in] compute_type
 *   Input. The data type for computing all matrix multiplications in the
 *   multi-head attention operation.
 * @param[in] attn_dropout
 *   Input. The dropout rate of the dropout layer after softmax output.
 *   Dropout is used only in the training mode,
 *   and it is should be zero in the inferencing mode.
 * @param[in] post_dropout
 *   Input. The dropout rate of the dropout layer after multi-head attention output
 *   and before residual connection. Dropout is used only in the training mode,
 *   and it is should be zero in the inferencing mode.
 * @param[in] q_size, k_size, v_size
 *   Input. The embedding length of queries, keys, and values.
 * @param[in] q_proj_size, k_proj_size, v_proj_size, o_proj_size
 *   Input. The embedding length of queries, keys, values and outputs after linear projection.
 * @param[in] qo_max_seq_length
 *   Input. The largest sequence length of queries and outputs.
 * @param[in] kv_max_seq_length
 *   Input. The largest sequence length of keys and values.
 * @param[in] max_batch_size
 *   Input. The maximum batch size of queries, keys, values and outputs.
 * @param[in] max_beam_size
 *   Input. The maximum beam size of queries and outputs.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_INITIALIZED
 *
 * @par API Dependency
 * - You need to call the ::cnnlCreateMultiHeadAttnDescriptor function to create the descriptor
 *   before calling this function.
 * - You need to call the ::cnnlDestroyMultiHeadAttnDescriptor function to destroy the descriptor
 *   after calling this function.
 *
 * @note
 * - The coefficient \p sm_scaler in range of (0.0, 1.0) for smoothing the softmax, and
 * \p sm_scaler > 1.0 for sharpening the softmax, \p sm_scaler = 1.0 for an ordinary softmax.
 *
 * @par Requirements
 * - \p sm_scaler > 0
 * - \p q_size >= 1, k_size >= 1, v_size >= 1
 * - \p q_proj_size >= 1, k_proj_size >= 1, v_proj_size >= 1, o_proj_size >= 1
 * - \p q_proj_size = k_proj_size
 * - \p qo_max_seq_length >= 1, kv_max_seq_length >= 1
 * - \p max_batch_size >= 1, max_beam_size >= 1

 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlSetMultiHeadAttnDescriptor(cnnlMultiHeadAttnDescriptor_t desc,
                                                         cnnlAttnMode attn_mode,
                                                         int nheads,
                                                         double sm_scaler,
                                                         cnnlDataType_t dtype,
                                                         cnnlDataType_t compute_type,
                                                         float attn_dropout,
                                                         float post_dropout,
                                                         int q_size,
                                                         int k_size,
                                                         int v_size,
                                                         int q_proj_size,
                                                         int k_proj_size,
                                                         int v_proj_size,
                                                         int o_proj_size,
                                                         int qo_max_seq_length,
                                                         int kv_max_seq_length,
                                                         int max_batch_size,
                                                         int max_beam_size);
// Group:MultiHeadAttnForward
/*!
 * @brief Destroys a multi-head attention descriptor \p desc that was previously created with the
 * ::cnnlCreateMultiHeadAttnDescriptor function.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] desc
 *   Input. The multi-head attention descriptor to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @par API Dependency
 *  - This function need to be called after ::cnnlCreateMultiHeadAttnDescriptor and
 *    ::cnnlSetMultiHeadAttnDescriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlDestroyMultiHeadAttnDescriptor(cnnlMultiHeadAttnDescriptor_t desc);

// Group:MultiHeadAttnForward
/*!
 * @brief Assigns the quantization parameters of the filter used in multi-head attention
 * operation. Including \p wq_position, \p wk_position, \p wv_position, \p wo_position,
 * \p wq_scale, \p wk_scale, \p wv_scale, \p wo_scale. It is only used when the data type of
 * filter is fixed-data. If the \p attn_mode described in \p desc is set to \p ALL_TO_ONE_BIAS or
 * \p ONE_TO_ONE_BIAS, the data type of bias should be aligned to the \p dtype described in \p desc.
 * For more information about quantization, see "Cambricon CNNL User Guide".
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] desc
 *   Input. Descriptor of multi-head attention operation.
 * @param[in] wtype
 *   Input. The data type of queries, keys, values, outputs of filter,
 *   Set this parameter to CNNL_DTYPE_HALF or CNNL_DTYPE_FLOAT in training
 *   mode. Set this parameter to CNNL_DTYPE_HALF, CNNL_DTYPE_FLOAT or
 *   CNNL_DTYPE_INT16, CNNL_DTYPE_INT8 in inference mode.
 * @param[in] wq_position
 *   Input. The position of queries filter in fixed-point data type.
 * @param[in] wk_position
 *   Input. The position of keys filter in fixed-point data type.
 * @param[in] wv_position
 *   Input. The position of values filter in fixed-point data type.
 * @param[in] wo_position
 *   Input. The position of outputs filter in fixed-point data type.
 * @param[in] wq_scale
 *   Input. The scale of queries filter in fixed-point data type.
 * @param[in] wk_scale
 *   Input. The scale of keys filter in fixed-point data type.
 * @param[in] wv_scale
 *   Input. The scale of values filter in fixed-point data type.
 * @param[in] wo_scale
 *   Input. The scale of outputs filter in fixed-point data type.
 * @param[in] reserve_size_bytes
 *   Input. The size of reserved space in bytes, pass NULL in the inference mode.
 *
 * @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_NOT_INITIALIZED, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called after ::cnnlCreateMultiHeadAttnDescriptor to create the
 *   descriptor of multi-head attention operation, and ::cnnlSetMultiHeadAttnDescriptor to set
 *   descriptor.
 *
 * @note
 * - It can only be set in inference mode.
 * - The data type of filters are in fixed type and the data type of bias are in float or half
 *   type.
 * - The fixed-point data type is not supported in ::cnnlMultiHeadAttnForward, and this function
 *   is not recommended to use the fixed-point data type.
 * - \f$2^{position}*scale\f$ should not be larger than 127 when \p wtype is
 *   \p CNNL_DTYPE_INT8.
 * - \f$2^{position}*scale\f$ should not be larger than 32767 when \p wtype is
 *   \p CNNL_DTYPE_INT16.
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t cnnlSetMultiHeadAttnWeightsQuantifyInfo(cnnlMultiHeadAttnDescriptor_t desc,
                                                     cnnlDataType_t wtype,
                                                     int wq_position,
                                                     int wk_position,
                                                     int wv_position,
                                                     int wo_position,
                                                     float wq_scale,
                                                     float wk_scale,
                                                     float wv_scale,
                                                     float wo_scale,
                                                     size_t *reserve_size_bytes);

// Group:MultiHeadAttnForward
/*!
 * @brief Gets the size of MLU memory buffers for multi-head attention, including
 * \p filter_size_bytes, \p workspace_size_bytes and \p reserve_size_bytes.
 * The size of the extra workspace is based on the given information of the multi-head attention
 * descriptor.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the get multi-head attention buffers operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] desc
 *   Input. Descriptor of multi-head attention operation.
 * @param[out] filter_size_bytes
 *   Output. The minimum size in bytes required for multi-head attention trainable parameters.
 * @param[out] workspace_size_bytes
 *   Output. The minimum buffer size in bytes required to hold the ::cnnlMultiHeadAttnForward,
 *   ::cnnlMultiHeadAttnBackwardData (training mode) and
 *   ::cnnlMultiHeadAttnBackwardWeights (training mode).
 * @param[out] reserve_size_bytes
 *   Output. The minimum buffer size in bytes required to store the intermediate results of
 *   ::cnnlMultiHeadAttnForward, ::cnnlMultiHeadAttnBackwardData and
 *   ::cnnlMultiHeadAttnBackwardWeights.
 *   Set this parameter to NULL in the inference mode.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called after ::cnnlCreateMultiHeadAttnDescriptor and
 *   ::cnnlSetMultiHeadAttnDescriptor.
 *
 * @note
 * All filter and bias tensors are aggregated in a single filter buffer. The layout of filter
 * inside the filter buffer may be different from your inputs for performance optimization. If
 * \p reserve_size_bytes is set to NULL, the inference mode is used and only ::cnnlMultiHeadAttnForward
 * is invoked. Otherwise, ::cnnlMultiHeadAttnForward, ::cnnlMultiHeadAttnBackwardData and
 * ::cnnlMultiHeadAttnBackwardWeights are invoked.
 *
 * @par Example
 * - None.
 *
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlGetMultiHeadAttnBuffers(cnnlHandle_t handle,
                                                      const cnnlMultiHeadAttnDescriptor_t desc,
                                                      size_t *filter_size_bytes,
                                                      size_t *workspace_size_bytes,
                                                      size_t *reserve_size_bytes);

/*! @brief Enumeration variables describing the filter and biases of multi-head attention.
 *  The shapes of the filter of queries, keys, values are [1, nheads, proj_size, embedding_size] and
 *  the shape of outputs filter is [1, nheads, o_proj_size, v_proj_size].
 *  If extra biases are configured in the multi-head attention descriptor, the shape of bias
 *  of queries, keys and values is [1, 1, nheads, proj_size] and the shape of bias of the outputs is
 *  [1, 1, 1, o_proj_size].
 *  The enumeration is only used for ::cnnlGetMultiHeadAttnWeights.
 *
 *  It is deprecated and will be removed in future release.
 *
 */
typedef enum {
  CNNL_MH_ATTN_Q_WEIGHT, /*!< Multi-head attention filter for queries. */
  CNNL_MH_ATTN_K_WEIGHT, /*!< Multi-head attention filter for keys. */
  CNNL_MH_ATTN_V_WEIGHT, /*!< Multi-head attention filter for values. */
  CNNL_MH_ATTN_O_WEIGHT, /*!< Multi-head attention filter for outputs. */
  CNNL_MH_ATTN_Q_BIAS, /*!< Multi-head attention bias for queries. */
  CNNL_MH_ATTN_K_BIAS, /*!< Multi-head attention bias for keys. */
  CNNL_MH_ATTN_V_BIAS, /*!< Multi-head attention bias for values. */
  CNNL_MH_ATTN_O_BIAS, /*!< Multi-head attention bias for outputs. */
} cnnlMultiHeadAttnWeightKind_t;

// Group:MultiHeadAttnForward
/*!
 *  @brief Gets the multi-head attention filter and biases with the given filter or bias
 *  tensor. The function returns the start address of the filter/biases tensor stored
 *  in the filter buffer. See ::cnnlMultiHeadAttnWeightKind_t for details.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the get multi-head attention filter operation. For detailed information,
 *   see ::cnnlHandle_t.
 *  @param[in] desc
 *    Input. Descriptor of multi-head attention operation.
 *  @param[in] wkind
 *    Input. The requested filter/bias kind. See ::cnnlMultiHeadAttnWeightKind_t for the
 *    description of the enumeration type.
 *  @param[in] filter_size_bytes
 *    Input. The size of filter size in bytes, whose size can be derived through
 *    ::cnnlGetMultiHeadAttnBuffers.
 *  @param[in] filter
 *    Input. The start address of the filter buffer on host or device memory.
 *  @param[out] w_desc
 *    Output. The tensor descriptor of the requested filter or bias. For filter, the shape is
 *    [1, nheads, proj_size, original_size]. For biases, the shape of queries, keys, values biases
 *    are [1, 1, nheads, proj_size] and the shape of output biases is [1, 1, 1, o_proj_size].
 *  @param[out] w_addr
 *    Output. The start address of the requested filter or bias on host or device memory.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 *  - This function need to be called after ::cnnlCreateMultiHeadAttnDescriptor,
 *    ::cnnlSetMultiHeadAttnDescriptor, and ::cnnlGetMultiHeadAttnBuffers.
 *
 * @note
 * - When requested tensor is one of {\p CNNL_MH_ATTN_Q_BIAS, \p CNNL_MH_ATTN_K_BIAS,
 *  \p CNNL_MH_ATTN_V_BIAS, \p CNNL_MH_ATTN_O_BIAS}, and ::cnnlAttnMode is set to \p ALL_TO_ONE_NOBIAS or
 *  \p ONE_TO_ONE_NOBIAS, the element size of requested tensor is 0, the function will return
 * an defaulted descriptor and a NULL \p w_addr. Meanwhile, the function returns ::CNNL_STATUS_SUCCESS.
 *
 * @par Example
 * - None.
 *
 * @par Requirements
 * - None.
 *
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlGetMultiHeadAttnWeights(cnnlHandle_t handle,
                                                      const cnnlMultiHeadAttnDescriptor_t desc,
                                                      cnnlMultiHeadAttnWeightKind_t wkind,
                                                      size_t filter_size_bytes,
                                                      const void *filter,
                                                      const cnnlTensorDescriptor_t w_desc,
                                                      void **w_addr);

// Group:MultiHeadAttnForward
/*!
 * @brief Computes the forward process of multi-head attention operation in both inference and
 * training mode.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the multi-head attention forward operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] attn_desc
 *   Input. Descriptor of multi-head attention operation.
 * @param[in] curr_idx
 *   Input. Time-step in \p queries to process. When curr_idx greater than or equal to 0,
 *   this function will process the selected time-step only.
 *   When curr_idx is less than 0, all \p qo_max_seq_length time-steps will be processed automatically.
 * @param[in] padding_mask_desc
 *   Input. Descriptor of \p padding_mask.
 *   Pass NULL when padding mask is not requested.
 * @param[in] padding_mask
 *   Input. Pointer to \p padding_mask on the device memory.
 *   Pass NULL when padding mask is not requested.
 * @param[in] attn_mask_desc
 *   Input. Descriptor of \p attn_mask.
 *   Pass NULL when attention mask is not requested.
 * @param[in] attn_mask
 *   Input. Pointer to attention mask data on the device memory.
 *   Pass NULL when attention mask is not requested.
 * @param[in] q_desc
 *   Input. Descriptor for the queries and residuals. For detailed information,
 *   see ::cnnlSeqDataDescriptor_t.
 * @param[in] queries
 *   Input. Pointer to queries sequence data on the device memory.
 * @param[in] residuals
 *   Input. Pointer to residuals sequence data on the device memory.
 *   Pass NULL when residual connection is not requested.
 * @param[in] k_desc
 *   Input. Descriptor for the keys. For detailed information, see
 *   ::cnnlSeqDataDescriptor_t.
 * @param[in] keys
 *   Input. Pointer to the filter on the device memory, which contains attention bias
 *   and filter for queries, keys, values and outputs. Parameters of attention filter,
 *   i.e. filter types, positions and scales, can be set via
 *   ::cnnlSetMultiHeadAttnWeightsQuantifyInfo in the inference mode.
 * @param[in] v_desc
 *   Input. Descriptor for the values. For detailed information, see
 *   ::cnnlSeqDataDescriptor_t.
 * @param[in] values
 *   Input. Pointer to value sequence data on the device memory.
 * @param[in] o_desc
 *   Input. Descriptor for the outputs. For detailed information, see
 *   ::cnnlSeqDataDescriptor_t.
 * @param[out] outputs
 *   Output. Pointer to outputs data on the device memory.
 * @param[in] filter_size_bytes
 *   Input. Size of filter in bytes, which can be derived through
 *   ::cnnlGetMultiHeadAttnBuffers.
 * @param[in] filter
 *   Input. Pointer to the filter on the device memory.
 * @param[in] workspace_size_bytes
 *   Input. Size of workspace buffer in bytes used for temporary storage, which can be derived through
 *   ::cnnlGetMultiHeadAttnBuffers.
 * @param[in,out] workspace
 *   Input/Output. Pointer to the workspace buffer used for temporary storage on the device memory.
 * @param[in] reservespace_size_bytes
 *   Input. Size of reserved space in bytes. The parameter should be 0 in the inference mode.
 *   In the training mode, the size can be derived through ::cnnlGetMultiHeadAttnBuffers.
 * @param[in,out] reservespace
 *   Input/Output. Pointer to the reserved space buffer used for data exchange between
 *   forward function ::cnnlMultiHeadAttnForward, and backward function
 *   ::cnnlMultiHeadAttnBackwardData
 *   and ::cnnlMultiHeadAttnBackwardWeights.
 *   Pass NULL in the inference mode and non-NULL in the training mode.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @note
 * - The padding mask is a three-dimensional tensor adding before performing softmax operation.
 *   Typically, the values of padding mask should be near negative infinity in the masked part
 *   and 0 in the unmasked part.
 *   When the attention mode is set \p ALL_TO_ONE_BIAS or \p ALL_TO_ONE_NOBIAS, the dimensions of
 *   padding mask are required to be [max_batch_size, 1, kv_max_seq_length].
 *   When the attention mode is set \p ONE_TO_ONE_BIAS or \p ONE_TO_ONE_NOBIAS, the dimensions of
 *   padding mask are required to be [max_batch_size, max_beam_size, kv_max_seq_length].
 * - The attention mask is a two-dimensional tensor adding before softmax.
 *   The attention mask is used in the masked self attention on the transformer's
 *   decoder side. Typically, it is an upper triangular matrix with all upper triangular values
 *   negative infinity.
 *   The shape of attention mask is required to be [qo_max_seq_length, kv_max_seq_length].
 * - If \p reservespace is set to NULL and \p reservespace_size_bytes is 0, no intermediate data is
 *   stored and this function is implemented in the inference mode.
 *   Otherwise, this function is implemented in the training mode. Meanwhile,
 *   intermediate data are passed through ::cnnlMultiHeadAttnForward,
 *   ::cnnlMultiHeadAttnBackwardData
 *   and ::cnnlMultiHeadAttnBackwardWeights on the \p reservespace.
 * - In the inference mode, \p curr_idx is greater than or equal to 0, specifies the time-step of the embedding
 *   vectors to be processed. In this mode, you can perform adaptive masking through:
 *   1. Start from a single time-steps forward, \p curr_idx is set to 0.
 *   2. Update \p queries, \p keys, \p values and the \p attn_mask dynamically before the next
 *      time-step iteration.
 *   3. Repeat the two steps above until all time-steps are processed.
 * - When all \p qo_max_seq_length time-steps are available (for example, in the training mode or
 *   in the inference mode on the encoder side in self-attention), the user can pass a
 *   negative \p curr_idx(i.e. -1).
 *   ::cnnlMultiHeadAttnForward will automatically go through all time-steps and mask
 *   according to the \p padding_mask and \p attn_mask given.
 * - The descriptors of \p queries and \p outputs should match while the descriptors of \p keys and
 *   \p values should match. The \p queries and \p residuals share the same \p q_desc.
 *   Note that when \p residual is activated (\p residuals != NULL), the \p o_proj_size and
 *   \p q_size recorded in the \p attn_desc should be the same, so that the residual connection
 *   is feasible. Set \p residual to NULL when residual connection is not requested.
 * - Aside from \p residuals, \p queries, \p keys and \p values are not allowed to set NULL even
 *   if any two or three of them are the same. If it is the self-attention situation, pass them the
 *   same device address. And if it is the encoder-decoder attention situation, \p keys and \p values are
 *   the same device address.

 * @par API Dependency
 * - Before calling this function to implement multi-head attention, you need to create the
 * ::cnnlSeqDataDescriptor_t of \p queries, \p keys, \p values, and \p outputs by
 * ::cnnlCreateSeqDataDescriptor, and set the data type, layout, dimensions, sequence lengths,
 * padding fill for the descriptors using ::cnnlSetSeqDataDescriptor. Then, create a multi-head
 * attention descriptor using ::cnnlCreateMultiHeadAttnDescriptor and set the multi-head
 * attention descriptor using ::cnnlSetMultiHeadAttnDescriptor.
 * - Get MLU device filter size, workspace size and reserved space size using
 * ::cnnlGetMultiHeadAttnBuffers and allocate memory for them. Copy in the filter data to the
 * address given by ::cnnlGetMultiHeadAttnWeights.
 * - Allocate MLU device memory and copy in for all inputs or outputs.
 *
 * @par Performance Optimization
 * - The traditional multi-head attention with 8, 12, 16 heads is optimized with fused algorithm.
 * - To get better performance, set \p q_proj_size, \p k_proj_size, \p v_proj_size to 64, and
 *   \p set \p q_size, \p k_size, \p v_size and \p o_proj_size to nheads * 64.
 * - The addresses of \p queries, \p keys, and \p values are the same, which is the
 *   self-attention scenario.
 * - The address of \p keys and \p values are the same, \p padding_mask is NULL, which is the
 *   encoder-decoder attention scenario.
 *
 * @par Example
 * - The example of padding mask. It prevents attention to the padding part of sequences.
 *   Suppose two 5-word sequence arrays with the real sequence length of 3 and 4.
 *   The padding lengths of the first and second array are 1 and 2, separately.
 *   The padding mask are as follows:
     @verbatim
      padding_mask = [[[0, 0, 0, -inf, -inf]], [[0, 0, 0, 0, -inf]]]
      where inf represents infinity
     @endverbatim
     The padding mask usually occurs in self-attention on the encoder side and
     encoder-decoder attention on the decoder side.
 * - The example of a typical attention mask in the training scenario. The \p attn_mask is an attention
 *   mask prevents attention to certain positions. The attention mask for a 5-word sequence array is
 *   as follows:
     @verbatim
       attn_mask = [[   0, -inf, -inf, -inf, -inf],
                    [   0,    0, -inf, -inf, -inf],
                    [   0,    0,    0, -inf, -inf],
                    [   0,    0,    0,    0, -inf],
                    [   0,    0,    0,    0,    0]].
     @endverbatim
 *   The attention mask usually occurs in masked self-attention on the decoder side.
 * - Multi-head attention, layer norm and residual connections are highly coupled
 *   in the transformers networks. This function does not include layer norm. You
 *   can set any data as \p residuals to realize residual connection,
 *   as long as it has the same shape and data type with \p queries.
 *   Following are two typical cases indicating the residual connection.
     @verbatim
       1. Residual connection before layernorm.
       - layernorm_out = LayerNormForward(queries)
       - residuals = queries
       - out = cnnlMultiHeadAttnForward(layernorm_out, keys, values) call.
       2. Residual connection outside the layernorm.
       - residuals = queries
       - out = cnnlMultiHeadAttnForward(queries, keys, values) call.
       - layernorm_out = LayerNormForward(out)
     @endverbatim
 *
 * @par Data Layout
 * - \p Queries, \p keys, \p values, \p outputs, and \p residuals only support \p CNNL_SEQDATA_NBTC
 *  layout.
 *
 * @par Data Type
 * - Note that the data type of queries, keys, values, outputs, residuals and filter should be the same.
 *   The computing type is used for all matrix multiplications in the multi-head attention
 *   operation. The computing type can be the same as the data type on MLU300 series or above.
 * - The data types supported are as follows:
 *   - queries, keys, values, outputs, residuals: float, half.
 *   - filter: attention bias in filter should be float and half, attention filter for queries, keys, values
 *     and outputs can be int8, int16, float and half.
 *   - computing type: int8, int16, float (MLU300 series), half (MLU300 series).
 *
 * @par Scale Limitation
 * - Inference mode (curr_idx >= 0) is not supported.
 *
 * @par Reference
 * - Attention is All You Need, vaswani ashish, et, al., 2017.
 * - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Jacob
 * Devlin, et al., 2019.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlMultiHeadAttnForward(cnnlHandle_t handle,
                                                   const cnnlMultiHeadAttnDescriptor_t attn_desc,
                                                   int curr_idx,
                                                   const cnnlTensorDescriptor_t padding_mask_desc,
                                                   const void *padding_mask,
                                                   const cnnlTensorDescriptor_t attn_mask_desc,
                                                   const void *attn_mask,
                                                   const cnnlSeqDataDescriptor_t q_desc,
                                                   const void *queries,
                                                   const void *residuals,
                                                   const cnnlSeqDataDescriptor_t k_desc,
                                                   const void *keys,
                                                   const cnnlSeqDataDescriptor_t v_desc,
                                                   const void *values,
                                                   const cnnlSeqDataDescriptor_t o_desc,
                                                   void *outputs,
                                                   size_t filter_size_bytes,
                                                   const void *filter,
                                                   size_t workspace_size_bytes,
                                                   void *workspace,
                                                   size_t reservespace_size_bytes,
                                                   void *reservespace);

// Group:MultiHeadAttnBackwardData
/*!
 * @brief Computes the first-order derivatives of the multi-head attention \p dqueries, \p keys,
 *  \p dvalues, \p doutputs with respect to the queries, keys, values, and forward outputs.
 *  Meanwhile, this function computes intermediate data and saves to the reserved space for
 *  ::cnnlMultiHeadAttnBackwardWeights to derive the derivatives of filter.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the multi-head attention backward data operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] attn_desc
 *   Input. Descriptor of multi-head attention operation.
 * @param[in] padding_mask_desc
 *   Input. Descriptor of \p padding_mask.
 *   Pass NULL when padding mask is not requested.
 * @param[in] padding_mask
 *    Input. Pointer to \p padding_mask on the device memory.
 *    Pass NULL when padding mask is not requested.
 * @param[in] attn_mask_desc
 *    Input. Descriptor of \p attn_mask.
 *    Pass NULL when attention mask is not requested.
 * @param[in] attn_mask
 *    Input. Pointer to attention mask data on the device memory.
 *    Pass NULL when attention mask is not requested.
 * @param[in] do_desc
 *   Input. The descriptor of \p doutputs. For detailed information, see
 *   ::cnnlSeqDataDescriptor_t.
 * @param[in] doutputs
 *   Input. Pointer to \p doutputs on the device memory.
 * @param[in] dq_desc
 *   Input. The descriptor of output tensor \p dqueries. For detailed information,
 *   see ::cnnlSeqDataDescriptor_t.
 * @param[out] dqueries
 *   Output. Pointer to dqueries on the device memory.
 * @param[in] dk_desc
 *   Input. The descriptor of output tensor \p dkeys. For detailed information,
 *   see ::cnnlSeqDataDescriptor_t.
 * @param[out] dkeys
 *   Output. Pointer to \p dkeys on the device memory.
 * @param[in] dv_desc
 *   Input. The descriptor of output tensor \p dvalues. For detailed information,
 *   see ::cnnlSeqDataDescriptor_t.
 * @param[out] dvalues
 *   Output. Pointer to \p dvalues on the device memory.
 * @param[in] filter_size_bytes
 *   Input. The size of filter in bytes, which can be derived through
 *   ::cnnlGetMultiHeadAttnBuffers.
 * @param[in] filter
 *   Input. Pointer to filter on the device memory. Note that the filter should be the same as
 *   the filter in ::cnnlMultiHeadAttnForward.
 * @param[in] workspace_size_bytes
 *   Input. The size of workspace in bytes, which can be derived through
 *   ::cnnlGetMultiHeadAttnBuffers.
 * @param[in] workspace
 *   Input. Pointer to the workspace on the device memory.
 * @param[in] reservespace_size_bytes
 *   Input. The size of reserved space in bytes.
 *   In the training mode, the size can be derived through ::cnnlGetMultiHeadAttnBuffers.
 * @param[in,out] reservespace
 *   Input, Output. Pointer to the reserved space on the device memory. Note that the reserve space
 *   should be the same as the \p reservespace in the ::cnnlMultiHeadAttnForward.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par API Dependency
 *  - ::cnnlMultiHeadAttnBackwardData should be invoked after ::cnnlMultiHeadAttnForward, and
 *   before ::cnnlMultiHeadAttnBackwardWeights.
 *  - \p padding_mask, \p attn_mask, \p filter and \p reservespace are same as them in
 *   the ::cnnlMultiHeadAttnForward.
 *
 * @note
 * - The ::cnnlMultiHeadAttnBackwardData function does not compute the partial derivatives of
 *   the residual connections, since the result is simply \p doutputs. If there is a residual
 *   connection between the queries and outputs, you need to add \p doutputs to \p dqueries to get
 *   the correct result.
 *
 * @par Data Layout
 * - \p dqueries, \p dkeys, \p dvalues, \p doutputs, only support \p CNNL_SEQDATA_NBTC layout.
 *
 * @par Data Type
 * - Note that the data type of dqueries, dkeys, dvalues, doutputs and filters should be the same.
 *   The computing type is used for all matrix multiplications in the multi-head attention
 *   operation. The computing type can be the same as the data type on MLU300 series or above.
 * - The data types supported are as follows:
 *   - dqueries, dkeys, dvalues, doutputs: float, half.
 *   - filter: float, half.
 *   - computing type: int16, float (MLU300 series), half (MLU300 series).
 *
 * @par Performance Optimization
 * - To get better performance, set \p q_proj_size, \p k_proj_size, \p v_proj_size to 64, and
 *   \p set \p q_size, \p k_size, \p v_size and \p o_proj_size to nheads * 64.
 *
 * @par Scale Limitation
 * - Device of CE3226 is not supported.
 *
 * @par Reference
 * - Attention is All You Need, vaswani ashish, et, al., 2017.
 * - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Jacob
 * Devlin, et al., 2019.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t cnnlMultiHeadAttnBackwardData(cnnlHandle_t handle,
                                           const cnnlMultiHeadAttnDescriptor_t attn_desc,
                                           const cnnlTensorDescriptor_t padding_mask_desc,
                                           const void *padding_mask,
                                           const cnnlTensorDescriptor_t attn_mask_desc,
                                           const void *attn_mask,
                                           const cnnlSeqDataDescriptor_t do_desc,
                                           const void *doutputs,
                                           const cnnlSeqDataDescriptor_t dq_desc,
                                           void *dqueries,
                                           const cnnlSeqDataDescriptor_t dk_desc,
                                           void *dkeys,
                                           const cnnlSeqDataDescriptor_t dv_desc,
                                           void *dvalues,
                                           const size_t filter_size_bytes,
                                           const void *filter,
                                           const size_t workspace_size_bytes,
                                           void *workspace,
                                           const size_t reservespace_size_bytes,
                                           void *reservespace);
/*!
 * @brief Enumeration variables describing how buffers that hold gradients of the loss function
 *  with respect to trainable parameters, are updated. Currently, this type is
 *  used by ::cnnlMultiHeadAttnBackwardWeights function only.
 *
 */
typedef enum {
    CNNL_WGRAD_MODE_ADD = 0,
    /*!< Add filter gradients to the output buffers. Note that the dfilters
     * should be initialized to zero before using this mode. Alternatively, use CNNL_WGRAD_MOD_SET in
     * the first time ::cnnlMultiHeadAttnBackwardWeights or ::cnnlRNNBackwardWeights is called.*/
    CNNL_WGRAD_MODE_SET = 1, /*!< Write partial gradients to dfilters buffers directly.*/
} cnnlWgradMode_t;

// Group:MultiHeadAttnBackwardWeights
/*!
 * @brief Computes the gradient of the multi-head attention block with respect
 * to the filter.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] attn_desc
 *   Input. Descriptor of multi-head attention operation.
 * @param[in] add_grad
 *   Input. The requested filter/bias kind. Refer to ::cnnlMultiHeadAttnWeightKind_t for detail.
 * @param[in] q_desc
 *   Input. Descriptor of \p queries sequence data. For detailed information, see
 *   ::cnnlSeqDataDescriptor_t.
 * @param[in] queries
 *   Input. Pointer to the \p queries on the device memory.
 * @param[in] k_desc
 *   Input. Descriptor of \p keys sequence data. For detailed information, see
 *   ::cnnlSeqDataDescriptor_t.
 * @param[in] keys
 *   Input. Pointer to the \p keys on the device memory.
 * @param[in] v_desc
 *   Input. Descriptor of \p values sequence data. For detailed information, see
 *   ::cnnlSeqDataDescriptor_t.
 * @param[in] values
 *   Input. Pointer to the \p values on the device memory.
 * @param[in] dout_desc
 *   Input. Descriptor of delta outputs sequence data. For detailed information,
 *   see ::cnnlSeqDataDescriptor_t.
 * @param[in] doutputs
 *   Input. Pointer to the delta outputs on the device memory.
 * @param[in] filter_size_bytes
 *   Input. The size of filter size in bytes, which can be derived through
 *   ::cnnlGetMultiHeadAttnBuffers.
 * @param[in] filter
 *   Input. The start address of the filter/biases buffers.
 * @param[out] dfilter
 *   Output. The start address of the gradient of filter/biases buffers.
 * @param[in] workspace_size_bytes
 *   Input. The size of workspace in bytes, which can be derived through
 *   ::cnnlGetMultiHeadAttnBuffers.
 * @param[out] workspace
 *   Output. The start address of the workspace on the device memory.
 * @param[in] reservespace_size_bytes
 *   Input. The size of reserved space in bytes, which can be derived through
 *   ::cnnlGetMultiHeadAttnBuffers.
 * @param[in] reservespace
 *   Input. The start address of the reserved space on the device memory. Note that the reserve
 *   space should be the same as the \p reservespace in the ::cnnlMultiHeadAttnForward and
 *   ::cnnlMultiHeadAttnBackwardData.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par API Dependency
 *  - ::cnnlMultiHeadAttnBackwardWeights should be invoked after ::cnnlMultiHeadAttnForward and
 *    ::cnnlMultiHeadAttnBackwardData.
 *  - \p reservespace and \p filter should be the same as those in
 *    ::cnnlMultiHeadAttnForward and ::cnnlMultiHeadAttnBackwardData.
 *  - The \p queries, \p keys and \p values should be the same as those in
 *    ::cnnlMultiHeadAttnForward.
 *  - The \p doutputs should be the same as the one in ::cnnlMultiHeadAttnBackwardData.
 *
 * @note
 * - None.
 *
 * @par Data Layout
 * - \p dqueries, \p dkeys, \p dvalues, \p doutputs, only support \p CNNL_SEQDATA_NBTC layout.
 *
 * @par Data Type
 * - Note that the data type of queries, keys, values, doutputs, filter and dfilters should be the same.
 *   The computing type is used for all matrix multiplications in the multi-head attention
 *   operation. The computing type can be the same as the data type on MLU300 series.
 * - The data types supported are as follows:
 *   - queries, keys, values, doutputs: float, half.
 *   - filter, dfilter: float, half.
 *   - computing type: int16, float (MLU300 series), half (MLU300 series).
 *
 * @par Performance Optimization
 * - To get better performance, set \p q_proj_size, \p k_proj_size, \p v_proj_size to 64, and
 *   \p set \p q_size, \p k_size, \p v_size and \p o_proj_size to nheads * 64.
 *
 * @par Scale Limitation
 * - Device of CE3226 is not supported.
 *
 * @par Reference
 * - Attention is All You Need, vaswani ashish, et, al., 2017.
 * - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Jacob
 * Devlin, et al., 2019.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlMultiHeadAttnBackwardWeights(cnnlHandle_t handle,
                                 const cnnlMultiHeadAttnDescriptor_t attn_desc,
                                 cnnlWgradMode_t add_grad,
                                 const cnnlSeqDataDescriptor_t q_desc,
                                 const void *queries,
                                 const cnnlSeqDataDescriptor_t k_desc,
                                 const void *keys,
                                 const cnnlSeqDataDescriptor_t v_desc,
                                 const void *values,
                                 const cnnlSeqDataDescriptor_t dout_desc,
                                 const void *doutputs,
                                 size_t filter_size_bytes,
                                 const void *filter,
                                 void *dfilter,
                                 size_t workspace_size_bytes,
                                 void *workspace,
                                 size_t reservespace_size_bytes,
                                 void *reservespace);
/*****************************************************************************
 * Cambricon CNNL OP: Interp
 * ***************************************************************************/
/*!
 * @brief Enumeration variables describing the interpolation algorithms used to implement the
 *        interpolation operation.
 *
 */
typedef enum {
  CNNL_INTERP_NEAREST = 0,
  /*!< The interpolation mode is Nearest-Neighbor,
   * which means using nearest pixel for interpolation.*/
  CNNL_INTERP_BILINEAR = 1,
  /*!< The interpolation mode is Bilinear,
   * which means using four corner pixels with bilinear algorithm for interpolation.*/
  CNNL_INTERP_LINEAR = 2,
  /*!< The interpolation mode is Linear,
   * which means using two corner pixels with linear algorithm for interpolation.*/
  CNNL_INTERP_TRILINEAR = 3,
  /*!< The interpolation mode is Trilinear,
   * which means using eight corner pixels with trilinear algorithm for interpolation.*/
  CNNL_INTERP_BICUBIC = 4,
  /*!< The interpolation mode is Bicubic,
   * which means using sixteen corner pixels with bicubic algorithm for interpolation.*/
} cnnlInterpMode_t;

// Group:Interp
/*!
 * @brief Performs interpolation operation with nearest, linear, bilinear, bicubic and trilinear methods.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release. Use ::cnnlInterp_v3 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues. For detailed information, see ::cnnlHandle_t.
 * @param[in] align_corners
 *   Input. Boolean variable determines the method to align 4 corner pixels between output
 *          and original input images.
 *          Generally, pixels of input and output images are considered to be squares.
 *          If \p align_corners is set to true, the input and output images are aligned by the
 *          center points of 4 corner pixels.
 *          Otherwise, the input and output images are aligned by the upper-left corner points of
 *          the corresponding corner pixels.
 * @param[in] align_center
 *   Input. Boolean variable determines whether to use center or corner coordinates of each pixel.
 *          If this parameter is true, the center coordinates are used to represent each pixel.
 *          Otherwise, the upper-left corner coordinates of each pixel are used.
 *          For example, when align_center is set to true,
 *          the coordinate of the first pixel is (0, 0).
 *          When align_center is set to false, the coordinate of the first pixel is (0.5, 0.5).
 * @param[in] mode
 *   Input. The specific algorithm used to interpolate from original images to get output images.
 *          The algorithms are defined in ::cnnlInterpMode_t enum.
 * @param[in] input_desc
 *   Input. The descriptor of input tensor of input images.
 *          For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input images.
 * @param[in] output_desc
 *   Input. The descriptor of output tensor of output images.
 *          For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output images.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - input: float, half.
 * - output: float, half.
 *
 * @par Data Layout
 * - The supported of data layout of input tensor and output tensor are as follows:
 *   - input tensor: If \p mode is set to CNNL_INTERP_LINEAR, the layout of input tensor can only
 *     be \p CNNL_LAYOUT_NLC. If \p mode is set to CNNL_INTERP_BILINEAR or CNNL_INTERP_BICUBIC,
 *     the layout of input tensor can only be \p CNNL_LAYOUT_NHWC.
 *     If \p mode is set to CNNL_INTERP_NEAREST, the layout of
 *     input tensor can only be \p CNNL_LAYOUT_NLC or \p CNNL_LAYOUT_NHWC. If \p mode is set to
 *     CNNL_INTERP_TRILINEAR, the layout of input tensor can only be \p CNNL_LAYOUT_NDHWC.
 *   - output tensor: If \p mode is set to CNNL_INTERP_LINEAR, the layout of output tensor can only
 *     be \p CNNL_LAYOUT_NLC. If \p mode is set to CNNL_INTERP_BILINEAR or CNNL_INTERP_BICUBIC,
 *     the layout of output tensor can only be \p CNNL_LAYOUT_NHWC. If \p mode is set to
 *     CNNL_INTERP_NEAREST, the layout of output tensor can only be \p CNNL_LAYOUT_NLC or
 *     \p CNNL_LAYOUT_NHWC. If \p mode is set to CNNL_INTERP_TRILINEAR,
 *     the layout of output tensor can only be \p CNNL_LAYOUT_NDHWC.
 *
 * @note
 * - \p align_corners and \p align_center cannot be True at the same time.
 * - The input tensor and output tensor support 3D, 4D and 5D, and must have the same number of dimensions.
 * - If \p mode is set to CNNL_INTERP_NEAREST and the layout of input is \p CNNL_LAYOUT_NLC,
 *   \p align_corners and \p align_center can only be False.
 * - On MLU200 series, \p input containing NaN/infinity is not supported. On MLU300 series and CE3226,
 *   if \p input contains NaN/infinity, it may cause undefined behaviors.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlInterp_v3)
cnnlStatus_t CNNL_WIN_API cnnlInterp(cnnlHandle_t handle,
                                     bool align_corners,
                                     bool align_center,
                                     cnnlInterpMode_t mode,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const void *input,
                                     const cnnlTensorDescriptor_t output_desc,
                                     void *output);

// Group:Interp
/*!
 * @brief Performs interpolation operation with nearest, linear, bilinear, bicubic and trilinear methods.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release. Use ::cnnlInterp_v3 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues. For detailed information, see ::cnnlHandle_t.
 * @param[in] align_corners
 *   Input. Boolean variable determines the method to align 4 corner pixels between output
 *          and original input images.
 *          Generally, pixels of input and output images are considered to be squares.
 *          If \p align_corners is set to true, the input and output images are aligned by the
 *          center points of 4 corner pixels.
 *          Otherwise, the input and output images are aligned by the upper-left corner points of
 *          the corresponding corner pixels.
 * @param[in] align_center
 *   Input. Boolean variable determines whether to use center or corner coordinates of each pixel.
 *          If this parameter is true, the center coordinates are used to represent each pixel.
 *          Otherwise, the upper-left corner coordinates of each pixel are used.
 *          For example, when align_center is set to true,
 *          the coordinate of the first pixel is (0, 0).
 *          When align_center is set to false, the coordinate of the first pixel is (0.5, 0.5).
 * @param[in] mode
 *   Input. The specific algorithm used to interpolate from original images to get output images.
 *          The algorithms are defined in ::cnnlInterpMode_t enum.
 * @param[in] scale_factors
 *   Input. Pointer to the host memory that stores the scale factors. When mode is set to
 *          CNNL_INTERP_LINEAR, the length of \p scale_factors is one, scale_factors[0] is equal to
 *          the width of output images divided by the width of input images. When mode is set to
 *          CNNL_INTERP_BILINEAR, CNNL_INTERP_NEAREST or CNNL_INTERP_BICUBIC, the length of \p scale_factors is two,
 *          scale_factors[0] is equal to the height of output images divided by the height of input
 *          images, scale_factors[1] is equal to the width of output images divided by the width of
 *          input images. When mode is set to CNNL_INTERP_TRILINEAR, the length of \p scale_factors
 *          is three, scale_factors[0] is equal to the depth of output images divided by the depth
 *          of input images, scale_factors[1] is equal to the height of output images divided by
 *          the height of input images, scale_factors[2] is equal to the width of output images
 *          divided by the width of input images. The elements of scale factors should be greater
 *          than zero.
 * @param[in] recompute_scale_factor
 *   Input. Boolean variable determines whether to recompute the scale factors in the
 *          interpolation calculation. If \p recompute_scale_factor is false, the passed-in
 *          \p scale_factors will be used in the interpolation computation. Otherwise, a new
 *          scale factor will be computed based on the output and input sizes for use in the
 *          interpolation computation.
 * @param[in] input_desc
 *   Input. The descriptor of input tensor of input images.
 *          For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input images.
 * @param[in] output_desc
 *   Input. The descriptor of output tensor of output images.
 *          For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output images.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - input: float, half.
 * - output: float, half.
 *
 * @par Data Layout
 * - The supported of data layout of input tensor and output tensor are as follows:
 *   - input tensor: If \p mode is set to CNNL_INTERP_LINEAR, the layout of input tensor can only
 *     be \p CNNL_LAYOUT_NLC. If \p mode is set to CNNL_INTERP_BILINEAR or CNNL_INTERP_BICUBIC, the layout of
 *     input tensor can only be \p CNNL_LAYOUT_NHWC. If \p mode is set to CNNL_INTERP_NEAREST, the layout of
 *     input tensor can only be \p CNNL_LAYOUT_NLC or \p CNNL_LAYOUT_NHWC. If \p mode is set to
 *     CNNL_INTERP_TRILINEAR, the layout of input tensor can only be \p CNNL_LAYOUT_NDHWC.
 *   - output tensor: If \p mode is set to CNNL_INTERP_LINEAR, the layout of output tensor can only
 *     be \p CNNL_LAYOUT_NLC. If \p mode is set to CNNL_INTERP_BILINEAR or CNNL_INTERP_BICUBIC, the layout of
 *     output tensor can only be \p CNNL_LAYOUT_NHWC.If \p mode is set to CNNL_INTERP_NEAREST, the layout of
 *     output tensor can only be \p CNNL_LAYOUT_NLC or \p CNNL_LAYOUT_NHWC. If \p mode is set to
 *     CNNL_INTERP_TRILINEAR, the layout of output tensor can only be \p CNNL_LAYOUT_NDHWC.
 *
 * @note
 * - \p align_corners and \p align_center cannot be True at the same time.
 * - The input tensor and output tensor support 3D, 4D and 5D, and must have the same number of dimensions.
 * - When \p recompute_scale_factor is true, \p scale_factors is not necessary, so \p scale_factors
 *   must be NULL.
 * - If \p mode is set to CNNL_INTERP_NEAREST and the layout of input is \p CNNL_LAYOUT_NLC,
 *   \p align_corners and \p align_center can only be False.
 * - Given valid \p scale_factors, the shapes of \p input and \p output should satisfy the following
 *   requirements.
 *   - For 5D \p input and \p output, the shapes are [n, di, hi, wi, c] and [n, do, ho, wo, c],
 *     respectively. Written in C style, the relationship between \p di and \p do should follow equation:
 *     \p do = (int)((double) \p scale_factor_d * \p di), where \p scale_factor_d is the scale factor
 *     for d-dimension. The same equation applies to other dimensions except \p n and \p c.
 *   - For 3D and 4D \p input and \p output, the equation described above is also used to map the dimension
 *     value between \p input and \p output.
 * - On MLU200 series, \p input containing NaN/infinity is not supported. On MLU300 series and CE3226,
 *   if \p input contains NaN/infinity, it may cause undefined behaviors.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
       align_corners          : false
       align_center           : false
       mode                   : CNNL_INTERP_NEAREST
       scale_factors[]        : [2.3, 2.3]
       recompute_scale_factor : false

       Input tensor           : [[[[1.0], [2.0]],
                                  [[3.0], [4,0]]]]

       Output tensor          : [[[[1.0], [1.0], [1.0], [2.0]],
                                  [[1.0], [1.0], [1.0], [2.0]],
                                  [[1.0], [1.0], [1.0], [2.0]],
                                  [[3.0], [3.0], [3.0], [4.0]]]]

     @endverbatim
 */
CNNL_DEPRECATED_FOR(cnnlInterp_v3)
cnnlStatus_t CNNL_WIN_API cnnlInterp_v2(cnnlHandle_t handle,
                                     bool align_corners,
                                     bool align_center,
                                     cnnlInterpMode_t mode,
                                     const float scale_factors[],
                                     bool recompute_scale_factor,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const void *input,
                                     const cnnlTensorDescriptor_t output_desc,
                                     void *output);

/*!
 * @brief Enumeration variables describing the coordinate transformation algorithms used
 * to implement the interpolation operation.
 *
 */
typedef enum {
  CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO0 = 0,
  /*!< Algorithm for align_corners = false and align_center = true of TensorFlow 2.12 in nearest-like modes,
  and align_corners = false and align_center = false of PyTorch 1.13.1 in other modes.*/
  CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO1 = 1,
  /*!< Algorithm for align_corners = false and align_center = true of ONNX.*/
  CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO2 = 2,
  /*!< Algorithm for align_corners = true and align_center = false of TensorFlow 2.12
  and PyTorch 1.13.1.*/
  CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO3 = 3,
  /*!< Algorithm for align_corners = false and align_center = false of TensorFlow 2.12
  and PyTorch 1.13.1.*/
  CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO4 = 4,
  /*!< Deprecated.*/
  CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO5 = 5,
  /*!< Deprecated.*/
  CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO6 = 6,
  /*!< Deprecated.*/
  CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO7 = 7,
  /*!< Algorithm for align_corners = false and align_center = true of PyTorch 1.13.1
  in nearest-exact mode.*/
} cnnlInterpCoordinateTransformationMode_t;

/*!
 * @brief Enumeration variables describing the rounding modes used in the ::cnnlInterp operation.
 *
 */
typedef enum {
  CNNL_INTERP_ROUND_PERFER_CEIL = 0,
  /*!< Rounds half up. */
  CNNL_INTERP_ROUND_PERFER_FLOOR = 1,
  /*!< Rounds half down. */
  CNNL_INTERP_CEIL = 2,
  /*!< Rounds up.*/
  CNNL_INTERP_FLOOR = 3,
  /*!< Rounds down.*/
} cnnlInterpRoundMode_t;

/*!
 * @brief Enumeration variables describing the rounding modes used in the ::cnnlInterp operation.
 *
 */
typedef enum {
  CNNL_INTERP_ALGO_0 = 0,
  /*!< Algorithm for align_corners = false and align_center = false of PyTorch 1.13.1. */
  CNNL_INTERP_ALGO_1 = 1,
  /*!< Algorithm for align_corners = false and align_center = true of PyTorch 1.13.1. */
  CNNL_INTERP_ALGO_2 = 2,
  /*!< Algorithm for align_corners = true and align_center = false of PyTorch 1.13.1. */
  CNNL_INTERP_ALGO_3 = 3,
  /*!< Algorithm for align_corners = true and align_center = true of PyTorch 1.13.1. */
  CNNL_INTERP_ALGO_4 = 4,
  /*!< Algorithm for align_corners = false and align_center = false of TensorFlow 2.12. */
  CNNL_INTERP_ALGO_5 = 5,
  /*!< Algorithm for align_corners = false and align_center = true of TensorFlow 2.12. */
  CNNL_INTERP_ALGO_6 = 6,
  /*!< Algorithm for align_corners = true and align_center = false of TensorFlow 2.12. */
  CNNL_INTERP_ALGO_7 = 7,
  /*!< Algorithm for align_corners = true and align_center = true of TensorFlow 2.12. */
  CNNL_INTERP_ALGO_8 = 8,
  /*!< Algorithm for align_corners = false and align_center = false of ONNX. */
  CNNL_INTERP_ALGO_9 = 9,
  /*!< Algorithm for align_corners = false and align_center = true of ONNX. */
} cnnlInterpAlgo_t;


// Group:Interp
/*!
 * @brief Creates an interpolation descriptor that holds cnnlInterpMode_t,
 * cnnlInterpCoordinateTransformationMode_t, cnnlInterpRoundMode_t, scale_factors,
 * pad, cubic_coeff_a and exclude_outside.
 *
 * @param[out] interp_desc
 *   Output. Pointer to the interpolation descriptor that holds information about interp.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - None
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateInterpDescriptor(cnnlInterpDescriptor_t *interp_desc);

// Group:Interp
/*!
 * @brief Initializes the interpolation descriptor pointed by \p interp_desc that was previously
 * created with the ::cnnlCreateInterpDescriptor function.
 *
 * @param[in,out] interp_desc
 *   Input/output. The descriptor of the interpolation operation. For detailed information,
 *   see ::cnnlInterpDescriptor_t.
 * @param[in] mode
 *   Input. The interpolation mode. For detailed information, see ::cnnlInterpMode_t.
 * @param[in] coordinate_trans_mode
 *   Input. The coordinate transformation algorithms of interp.
 *   For detailed information, see ::cnnlInterpCoordinateTransformationMode_t.
 *   - ::CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO0 means (align_corners, align_center) is (false, true)
 *   in nearest-like modes and (false, false) in other modes.
 *   - ::CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO1 means (align_corners, align_center) is (false, true).
 *   - ::CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO2 means (align_corners, align_center) is (true, false).
 *   - ::CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO3 means (align_corners, align_center) is (false, false).
 *   - ::CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO7 means (align_corners, align_center) is (false, true),
 *   which is used in nearest-exact mode.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - \p coordinate_trans_mode only supports ::CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO0,
 *   ::CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO1, ::CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO2,
 *   ::CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO3 and ::CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO7.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlSetInterpDescriptor_v2)
cnnlStatus_t  CNNL_WIN_API cnnlSetInterpDescriptor(cnnlInterpDescriptor_t interp_desc,
                                                   const cnnlInterpMode_t mode,
                                                   const cnnlInterpCoordinateTransformationMode_t
                                                   coordinate_trans_mode);

// Group:Interp
/*!
 * @brief Initializes the interpolation descriptor pointed by \p interp_desc that was previously
 * created with the ::cnnlCreateInterpDescriptor function.
 * Compared with ::cnnlSetInterpDescriptor, this function supports more features with \p algo.
 *
 * @param[in,out] interp_desc
 *   Input/output. The descriptor of the interpolation operation. For detailed information,
 *   see ::cnnlInterpDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] mode
 *   Input. The interpolation mode. For detailed information, see ::cnnlInterpMode_t.
 * @param[in] algo
 *   Input. The algorithms of the interpolation operation.
 *   For detailed information, see ::cnnlInterpAlgo_t.
 * @param[in] scale_factors
 *   Input. Pointer to the host memory that stores the scaling factors. If \p scale_factors
 *   is not NULL, the values should be greater than 0.0.
 *   If \p scale_factors is not NULL, scale_factors[0] is equal to the depth of output images
 *   divided by the depth of input images, scale_factors[1] is equal to the height of output
 *   images divided by the height of input images, scale_factors[2] is equal to the width of
 *   output images divided by the width of input images.
 *   When mode is set to ::CNNL_INTERP_NEAREST, the length of \p scale_factors is two or three.
 *   When mode is set to ::CNNL_INTERP_LINEAR, the length of \p scale_factors is one.
 *   When mode is set to ::CNNL_INTERP_BILINEAR, the length of \p scale_factors is two.
 *   When mode is set to ::CNNL_INTERP_BICUBIC, the length of \p scale_factors is two.
 *   When mode is set to ::CNNL_INTERP_TRILINEAR, the length of \p scale_factors is three.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t  CNNL_WIN_API cnnlSetInterpDescriptor_v2(cnnlInterpDescriptor_t interp_desc,
                                                      const cnnlTensorDescriptor_t input_desc,
                                                      const cnnlInterpMode_t mode,
                                                      const cnnlInterpAlgo_t algo,
                                                      const double scale_factors[]);


// Group:Interp
/*!
 * @brief Set the extra parameters to the interpolation descriptor pointed by \p interp_desc
 * that was previously created with the ::cnnlCreateInterpDescriptor function.
 *
 * @param[in] interp_desc
 *   Input. The descriptor of the interpolation operation. For detailed information,
 *   see ::cnnlInterpDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] nearest_round_mode
 *   Input. The rounding mode of nearest. For detailed information,
 *   see ::cnnlInterpRoundMode_t.
 * @param[in] scale_factors
 *   Input. Must be same as \p scale_factors in previously created \p interp_desc.
 *   For detailed information, see ::cnnlCreateInterpDescriptor.
 * @param[in] pad
 *   Input. Pointer to the host memory that stores the pads. It contains four pad parameters.
 *   pad[0] is the top pad of height, pad[1] is the bottom pad of height, pad[2] is the left
 *   pad of width, pad[3] is the right pad of width.
 * @param[in] cubic_coeff_a
 *   Input. The coefficient 'a' used in cubic interpolation. Two common choices are
 *   -0.5 (in some cases of TensorFlow) and -0.75 (in PyTorch). Check out Equation (4) in
 *   https://ieeexplore.ieee.org/document/1163711 for the details.
 *   This attribute is valid only if interpolation mode is cubic.
 * @param[in] exclude_outside
 *   Input. It is only used in cubic mode. If set to true, the filter of sampling locations outside
 *   the input will be set to 0 and the filter will be renormalized so that their sum is 1.0.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - When \p pad is not NULL, interpolation mode only supports ::CNNL_INTERP_BILINEAR, the layout of
 *   input and output should be ::CNNL_LAYOUT_NHWC.
 * - \p nearest_round_mode is only used when interpolation mode is set to nearest or nearest-exact.
 * - When \p scale_factors is not used, it must be set to NULL, when \p scale_factors is used,
 *   it cannot be NULL.
 * - \p cubic_coeff_a only supports -0.75.
 * - \p exclude_outside only supports false.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t  CNNL_WIN_API cnnlSetInterpDescriptorEx(cnnlInterpDescriptor_t interp_desc,
                                                     const cnnlTensorDescriptor_t input_desc,
                                                     const cnnlInterpRoundMode_t nearest_round_mode,
                                                     const float scale_factors[],
                                                     const int pad[],
                                                     float cubic_coeff_a,
                                                     bool exclude_outside);

// Group:Interp
/*!
 * @brief Destroys an interpolation descriptor that was created by ::cnnlCreateInterpDescriptor.
 *
 * @param[in] interp_desc
 *   Input. The interpolation descriptor created by ::cnnlCreateInterpDescriptor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyInterpDescriptor(cnnlInterpDescriptor_t interp_desc);

// Group:Interp
/*!
 * @brief Performs interpolation operation with nearest, linear, bilinear, trilinear, and bicubic methods.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues. For detailed information, see ::cnnlHandle_t.
 * @param[in] interp_desc
 *   Input. The descriptor of interp.
 *   For detailed information, see ::cnnlInterpDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of input tensor of input images.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input images.
 * @param[in] output_desc
 *   Input. The descriptor of output tensor of output images.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output images.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED.
 *
 * @par Data Type
 * - input: uint8, float, half, bfloat16.
 * - output: uint8, float, half, bfloat16.
 * - The uint8 data type is only supported when \p mode is set to ::CNNL_INTERP_NEAREST.
 * - If \p mode is set to ::CNNL_INTERP_BILINEAR and pad is not 0, the bfloat16 data type is not supported.
 * - If \p mode is set to ::CNNL_INTERP_BILINEAR and pad is not 0, the bfloat16 data type is not supported.
 * - If \p mode is set to ::CNNL_INTERP_TRILINEAR, the bfloat16 data type is not supported.
 * - If \p mode is set to ::CNNL_INTERP_BICUBIC, the bfloat16 data type is not supported.
 *
 * @par Data Layout
 * - The supported of data layout of input tensor and output tensor are as follows:
 *   - input tensor: If \p mode is set to CNNL_INTERP_LINEAR, the layout of input tensor can only
 *     be \p CNNL_LAYOUT_NLC. If \p mode is set to CNNL_INTERP_BILINEAR or CNNL_INTERP_BICUBIC,
 *     the layout of input tensor can only be \p CNNL_LAYOUT_NHWC. If \p mode is set to
 *     CNNL_INTERP_NEAREST, the layout of input tensor can only be \p CNNL_LAYOUT_NLC, \p CNNL_LAYOUT_NHWC
 *     or \p CNNL_LAYOUT_NDHWC.
 *     If \p mode is set to CNNL_INTERP_TRILINEAR, the layout of input tensor can only be \p CNNL_LAYOUT_NDHWC.
 *   - output tensor: If \p mode is set to CNNL_INTERP_LINEAR, the layout of output tensor can only
 *     be \p CNNL_LAYOUT_NLC. If \p mode is set to CNNL_INTERP_BILINEAR or CNNL_INTERP_BICUBIC, the layout of
 *     output tensor can only be \p CNNL_LAYOUT_NHWC. If \p mode is set to CNNL_INTERP_NEAREST, the layout of
 *     output tensor can only be \p CNNL_LAYOUT_NLC or \p CNNL_LAYOUT_NHWC. If \p mode is set to
 *     CNNL_INTERP_TRILINEAR, the layout of output tensor can only be \p CNNL_LAYOUT_NDHWC.
 *
 * @note
 * - The input tensor and output tensor support 3D, 4D and 5D, and must have the same number of dimensions.
 *   The element number of each dimension in input tensor and output tensor should be no more than \p INT_MAX.
 * - \p coordinate_trans_mode in \p interp_desc does not support ::CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO4.
 *   ::CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO0 and ::CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO1
 *   mean \p align_corners is false and \p align_center is true.
 *   ::CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO2 means \p align_corners is true and \p align_center is false.
 *   ::CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO3 means \p align_corners is false and \p align_center is false.
 * - If \p coordinate_trans_mode is ::CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO1, \p mode only supports ::CNNL_INTERP_BILINEAR now.
 * - If \p mode is set to ::CNNL_INTERP_NEAREST, when coordinate_trans_mode is
 *   ::CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO0, the rounding mode in \p interp_desc must be
 *   ::CNNL_INTERP_CEIL. When coordinate_trans_mode in \p interp_desc is
 *   ::CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO2, the rounding mode in \p interp_desc must be
 *   ::CNNL_INTERP_ROUND_PERFER_CEIL. When coordinate_trans_mode is
 *   ::CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO3, the rounding mode in \p interp_desc must be
 *   ::CNNL_INTERP_FLOOR.
 * - Given valid \p scale_factors, the shapes of \p input and \p output should satisfy the following
 *   requirements.
 *   For 5D \p input and \p output, the shapes are [n, di, hi, wi, c] and [n, do, ho, wo, c],
 *   respectively. Written in C style, the relationship between \p di and \p do should follow the equation:
 *   \p do = (int)((double) \p scale_factor_d * \p di), where \p scale_factor_d is the scale factor
 *   for d-dimension. The same equation applies to other dimensions except \p n and \p c.
 *   For 3D and 4D \p input and \p output, the equation described above is also used to map the dimension
 *   value between \p input and \p output.
 * - On MLU300 series, if \p input contains NaN/infinity, it may cause undefined behavior.
 * - Due to the representation of integers and floating-point numbers, errors may occur in the result when the
 *   dimension (\p di, \p hi, \p wi, \p do, \p ho, \p wo) exceeds 2^23.
 * - If \p mode is set to ::CNNL_INTERP_BILINEAR and pad is not 0, a fixed space is required to malloc,
 *   which can be calculated by 4 * \p wo * sizeof(int) + 4 * \p ho * sizeof(int) + 7 * sizeof(Data Type).
 *   \p hi, \p wi, \p ho, \p wo cannot exceeds 2^23.
 *   This space size cannot be larger than 753664 on MLU300 Series or 491520 on MLU500 Series.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlInterp_v3(cnnlHandle_t handle,
                                       const cnnlInterpDescriptor_t interp_desc,
                                       const cnnlTensorDescriptor_t input_desc,
                                       const void* input,
                                       const cnnlTensorDescriptor_t output_desc,
                                       void* output);

// Group:Lerp
/*!
 * @brief Implements a linear interpolation of two tensors \p a and \p b based on
 *        a scalar or tensor \p w and returns the results in \p d tensor.
 *
 * The shapes of a and b must be broadcastable. If \p w is a tensor, then the shapes
 * of \p w, \p a, and \p b must be broadcastable.
 *
 * This function supports partial in-place operation, which means that the first input
 * tensor \p a and the output tensor \p d can be the same one. This function also
 * supports tensor broadcasting as long as \p a, \p b, \p w, and \p d satisfy the
 * broadcasting conditions.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the ::cnnlLerp operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] a_desc
 *   Input. The descriptor of the first input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. Pointer to the MLU memory that stores the first input tensor.
 * @param[in] b_desc
 *   Input. The descriptor of the second input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. Pointer to the MLU memory that stores the second input tensor.
 * @param[in] w_desc
 *   Input. The descriptor of the input filter tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] w
 *   Input. Pointer to the MLU memory that stores the input filter scalar or tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlLerp operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the ::cnnlLerp operation. You can get the size of the workspace with
 *   the ::cnnlGetLerpWorkspaceSize function.
 * @param[in] d_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] d
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "cnnlLerp" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input and output.
 *   - \p a: half, float, bfloat16
 *   - \p b: half, float, bfloat16
 *   - \p w \p scalar: half, float, bfloat16
 *   - \p w \p tensor: half, float, bfloat16
 *   - \p d: half, float, bfloat16
 *
 *   The bfloat16 data type is supported only on MLU500 series.
 *
 *   Note that the data type of the input and output must be the same.
 *
 * @par Limitations
 * - None.
 *
 * @par API Dependency
 * - Before calling this function to perform the ::cnnlLerp operation, you need to get the
 *   size of workspace by the ::cnnlGetLerpWorkspaceSize function.
 *
 * @par Performance Optimization
 * - None.
 *
 * @note
 * - You can specify the stride of all dimensions for a_desc, b_desc, w_desc and
 *   d_desc with ::cnnlSetTensorDescriptorEx.
 * - This operation is not supported on the 1V platforms.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlLerp(cnnlHandle_t handle,
                                   const cnnlTensorDescriptor_t a_desc,
                                   const void *a,
                                   const cnnlTensorDescriptor_t b_desc,
                                   const void *b,
                                   const cnnlTensorDescriptor_t w_desc,
                                   const void *w,
                                   void *workspace,
                                   size_t workspace_size,
                                   const cnnlTensorDescriptor_t d_desc,
                                   void * d);

// Group:Lerp
/*!
 * @brief Returns in \p size the size of the MLU memory in bytes that is used as
 *        an extra workspace to optimize the ::cnnlLerp operation.
 *
 * The size of the extra workspace is based on the given information of the input
 * and output tensor descriptors, including \p a_desc, \p b_desc, \p w_desc,
 * and \p d_desc.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the ::cnnlLerp operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] a_desc
 *   Input. The descriptor of the first input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the second input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] w_desc
 *   Input. The descriptor of the input filter tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] d_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is
 *   used in the ::cnnlLerp operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlLerp function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetLerpWorkspaceSize(cnnlHandle_t handle,
                                                   const cnnlTensorDescriptor_t a_desc,
                                                   const cnnlTensorDescriptor_t b_desc,
                                                   const cnnlTensorDescriptor_t w_desc,
                                                   const cnnlTensorDescriptor_t d_desc,
                                                   size_t *size);

/*****************************************************************************
 * Cambricon CNNL OP: CropAndResize
 * ***************************************************************************/
/*!
 * @brief Enumeration variables describing the algorithms used to implement
 *        the CropAndResize operation.
 */
typedef enum {
  CNNL_CROP_AND_RESIZE_NEAREST = 0,
  /*!< The crop and resize mode is Nearest-Neighbor,
   * which means using the nearest pixel for interpolation.
   * The value of each pixel in crops is determined by its nearest neighbor in the input images.*/
  CNNL_CROP_AND_RESIZE_BILINEAR = 1,
  /*!< The crop and resize mode is Bilinear,
   * which means using the four corner pixels with bilinear algorithm for interpolation.
   * The value of each pixel in crops is determined by its 4 nearest corner pixels
   * in the input images with scaling factors calculated with bilinear algorithm.*/
} cnnlCropAndResizeMode_t;

// Group:CropAndResize
/*!
 * @brief Performs crop and resize operation with nearest and bilinear interpolation methods.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input data.
 * @param[in] boxes_desc
 *   Input. The descriptor of the boxes tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 *          The shape of bounding boxes tensor should be [boxes_num, 4],
 *          in which \p boxes[i] is composed of 4 normalized coordinates [y1, x1, y2, x2],
 *          specifying 4 corner coordinates of the corresponding box in the i-th image.
 * @param[in] boxes
 *   Input. Pointer to the MLU memory that stores the boxes data. The boxes data contains the coordinates
 *          of box which are [y1,x1,y2,x2]. The coordinate of [y1,x1] is beginning position,
 *          the coordinate of [y2,x2] is ending position.
 * @param[in] box_index_desc
 *   Input. The descriptor of the boxes tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] box_index
 *   Input. Pointer to the MLU memory that stores box index data.The box index data contains the
 *          index of input images, and the index should belong to the scope of [0, batch). Batch is
 *          the dimension N of input. The data type of box_index should be ::CNNL_DTYPE_INT32.
 * @param[in] mode
 *   Input. The specific algorithm used to resize the crops extracted from the images.
 *          The algorithms are defined in ::cnnlCropAndResizeMode_t enum.
 * @param[in] extrapolation_value
 *   Input. Scalar. When the coordinate of boxes is outside the scope of [0, 1], it will use
 *          extrapolation_value to extrapolate the input image values.
 * @param[out] output_desc
 *   Output. The descriptor of output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores \p output data.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - input: float, half.
 * - output: float, half.
 *
 * @note
 * - The data layout of input and output should be ::CNNL_LAYOUT_NHWC.
 * - The input tensors and output tensor only support 4D.
 * - The supported combinations of data types are shown below with the following order:
 *   \p input - \p boxes- \p output.
 *   - float - float - float
 *   - half - half -half
 *   - half - float - float
 * - The first dimension of boxes equals the first dimension of box_index and the first
 *   dimension of output.
 * - When \p boxes contains NaN or infinity:
 *   - On MLU200 series:
 *    - If \p boxes contains NaN or infinity, then \p output is \p extrapolation_value.
 *   - On MLU300 series and CE3226:
 *    - If \p boxes contains NaN or infinity, it may cause undefined behavior such as core
 *      dump.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlCropAndResize(cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const void *input,
                                     const cnnlTensorDescriptor_t boxes_desc,
                                     const void *boxes,
                                     const cnnlTensorDescriptor_t box_index_desc,
                                     const void *box_index,
                                     cnnlCropAndResizeMode_t mode,
                                     float extrapolation_value,
                                     const cnnlTensorDescriptor_t output_desc,
                                     void *output);

// Group:CropAndResizeBackwardImage
/*!
 * @brief Computes the gradients of images \p grads_image based on the gradients of crops \p grads,
 * bounding boxes \p boxes and index of each box referring to the corresponding images \p box_idx
 * to perform the backpropagation of ::cnnlCropAndResize operation.
 *
 * This function supports algorithms defined in ::cnnlCropAndResizeMode_t.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in ::cnnlCropAndResizeBackwardImage operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] grads_desc
 *   Input. The descriptor of the gradient tensor of the crops in the backpropagation process.
 *          For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] grads
 *   Input. Pointer to the MLU memory that stores the gradient tensor of the crops.
 * @param[in] boxes_desc
 *   Input. The descriptor of the bounding boxes tensor.
 *          For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] boxes
 *   Input. Pointer to the MLU memory that stores the bounding boxes tensor.
 * @param[in] box_idx_desc
 *   Input. The descriptor of box indices tensor.
 *          For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] box_idx
 *   Input. Pointer to the MLU memory that stores the box indices tensor.
 *          \p box_idx[i] indicates the specific batch of image which the i-th box refers to.
 * @param[in] mode
 *   Input. The specific algorithm used to resize the crops extracted from the images.
 *          The algorithms are defined in ::cnnlCropAndResizeMode_t enum.
 * @param[out] grads_image_desc
 *   Output. The descriptor of the gradients tensor of the original images.
 * @param[out] grads_image
 *   Output. Pointer to the MLU memory that stores the gradients tensor of the original images.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "CropAndResizeBackwardImage Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 *    This function supports the combinations of data types with the following order:
 *    \p grads - \p boxes - \p box_idx - \p grads_image.
 *    - float - float - int32 - float
 *    - half - half - int32 - half
 *    - float - float - int32 - half
 *
 * @par Data Layout
 * - The supported data layouts of \p grads, \p boxes, \p box_idx, \p grads_images tensor
 *   are as follows:
 *   - grads tensor: \p CNNL_LAYOUT_NHWC.
 *   - boxes tensor: \p CNNL_LAYOUT_ARRAY, only supports 2D tensor.
 *   - boxes_idx tensor: \p CNNL_LAYOUT_ARRAY, only supports 1D tensor.
 *   - grads_image tensor: \p CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - The shape of \p grads should be [boxes_num, crop_height, crop_width, channels].
 * - The shape of \p boxes should be [boxes_num, 4].
 * - The shape of \p box_idx should be [boxes_num].
 * - The shape of \p grads_image should be [batch_num, image_height, image_width, channels].
 * - The value of \p box_idx should be in range of [0, batch_num).
 * - Half data type is not recommended due to low precision
 *   and high possibility of numerical overflow, especially in any of the following cases:
 *   - \p boxes_num is big while \p batch_num is small.
 *   - |y2 - y1| is small.
 *   - |x2 - x1| is small.
 * - For half data type, the data value should be in range of [-65504.0, 65504.0].
 *
 * @par API Dependency
 * - None.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance,
 *   \p channels is recommended to be aligned to 128 Bytes.
 *
 * @note
 * - \p boxes[i] is composed of 4 normalized coordinates [y1, x1, y2, x2],
 *   specifying 4 corner coordinates of the corresponding box in the i-th image.
 * - The [0, 1] interval of boxes coordinates y and x is mapped to [0, image_height -1]
 *   and [0, image_width - 1] in image coordinates respectively.
 * - y1 > y2 or x1 > x2 is allowed, indicating the crop is an up-down or
 *   left-right flipped version of the original image.
 * - Normalized coordinates outside [0, 1] interval are also allowed.
 * - Coordinates in \p box should not be NaN or infinity, otherwise the results of \p output are
 *   unpredictable.
 * - When \p input contains infinity:
 *   - On MLU300 series and CE3226:
 *     - \p output will contain infinity.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/crop-and-resize-grad-image
 * - https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/crop-and-resize
 */
cnnlStatus_t CNNL_WIN_API cnnlCropAndResizeBackwardImage(cnnlHandle_t handle,
                                                         const cnnlTensorDescriptor_t grads_desc,
                                                         const void *grads,
                                                         const cnnlTensorDescriptor_t boxes_desc,
                                                         const void *boxes,
                                                         const cnnlTensorDescriptor_t box_idx_desc,
                                                         const void *box_idx,
                                                         cnnlCropAndResizeMode_t mode,
                                                         const cnnlTensorDescriptor_t
                                                               grads_image_desc,
                                                         void *grads_image);

/*****************************************************************************
 * Cambricon CNNL OP: CropAndResizeBackwardBoxes
 * ***************************************************************************/

// Group:CropAndResizeBackwardBoxes
/*!
 * @brief This function is one of the backpropagation operations of CropAndResize operation,
 * with which the grads_boxes is computed. The other backpropagation operation is
 * cnnlCropAndResizeBackwardImage() which calculates grads_image.
 *
 * @param[in]  handle
 *  Input. A handle struct with information that all you need in this operation.
 * @param[in]  input_desc
 *  Input. A descriptor of 4D input tensor,
 *         containing dimension, data type and layout information.
 *         The data type of input tensor should be half or float.
 * @param[in]  input
 *  Input. A pointer to the 4D input tensor.
 * @param[in]  image_desc
 *  Input. A descriptor of 4D image tensor,
 *         containing dimension, data type and layout information.
 *         The data type of image tensor should be half or float.
 * @param[in]  image
 *  Input. A pointer to the 4D image tensor.
 * @param[in]  box_desc
 *  Input. A descriptor of 2D box tensor,
 *         containing dimension, data type and layout information.
 *         The data type of box tensor should be the same as input's data type.
 *         in which box[i] is composed of 4 normalized coordinates [y1, x1, y2, x2],
 *         specifying 4 corner coordinates of the corresponding box in the i-th image.
 *         Both height and width of images are normalized to [0, 1].
 *         Notice that y1 > y2 and x1 > x2 are allowed, indicating the resized crop is a flipped
 *         version of the original image.
 *         Besides, normalized coordinates outside [0, 1] interval are allowed.
 * @param[in]  box
 *  Input. A pointer to the 2D box tensor.
 * @param[in]  index_desc
 *  Input. A descriptor of 1D index tensor,
 *         containing dimension, data type and layout information.
 *         The data type of index should be int32.
 *         The values of index should be in range of [0, num_boxes),
 *         For instance, index[i] indicates the specific batch of image
 *         which the i-th box refers to.
 * @param[in]  index
 *   Input. A pointer to the 1D index tensor.
 * @param[in]  algorithm
 *   Input. The specific algorithm used in resize stage.
 *          This is defined in ::cnnlCropAndResizeMode_t enum.
 *          Only "Bilinear" is supported:
 *          CNNL_CROP_AND_RESIZE_BILINEAR stands for the Bilinear algorithm,
 *          where each pixel value of crops is determined by the 4 nearest corner points
 *          in original images with scaling factors calculated with bilinear algorithm.
 * @param[out]  output_desc
 *  Output. A descriptor of 4D output tensor,
 *          containing dimension, data type and layout information.
 *          The data type of output tensor should be half or float.
 * @param[out]  output
 *  Output. A pointer to the 4D output tensor.
 *
 * @par Data Type
 * - By the order of input tensor - image tensor - boxes tensor - index tensor - output tensor,
 *   the supported data type combinations are as follows:
 *   - float - float - float - int32 - float
 *   - half - half - half - int32 - half
 *   - float - half - float - int32 - float
 * @par Data Layout
 *  - The support data layout of the input tensor, image tensor, box tensor, index tensor and
 *    output tensor are as follows:
 *    - input  tensor: \p CNNL_LAYOUT_NHWC
 *    - image  tensor: \p CNNL_LAYOUT_NHWC
 *    - box    tensor: \p CNNL_LAYOUT_ARRAY
 *    - index  tensor: \p CNNL_LAYOUT_ARRAY
 *    - output tensor: \p CNNL_LAYOUT_ARRAY
 *
 * @par Scale Limitation
 * - Data type of half is not recommended due to low precision
 *   and high possibility of numerial overflow.
 * - For half data type, the data value should be in range of [-65504.0, 65504.0].
 *
 * @par Performance Optimization
 * - The size of C dimension of grads is suggested to be aligned to 128 bytes
 *   for better performance.
 *
 * @note
 *  - The support data shape of the input tensor, image tensor, box tensor, index tensor and
 *    output tensor are as follows:
 *    - input tensor: \p [num_boxes, crop_height, crop_width, channels]
 *    - image tensor: \p [batch_size, image_height, image_width, channels]
 *    - box tensor: \p [num_boxes, 4]
 *    - index tensor: \p [num_boxes]
 *    - output tensor: \p [num_boxes, 4]
 * @retval  CNNL_STATUS_SUCCESS
 *  The function ends normally.
 * @retval  CNNL_STATUS_BAD_PARAM
 *  Any one of the following conditions is satisfied:
 *   - Handle is NULL.
 *   - Input is NULL.
 *   - Image is NULL.
 *   - Box is NULL.
 *   - Index is NULL.
 *   - Output is NULL.
 *   - Dimension of input, image or output is not 4.
 *   - Dimension of box is not 2.
 *   - Dimension of index is not 1.
 *   - Shape of output is different with box's.
 *   - N dimension of input, box, index and output is not the same as each other.
 *   - Data type of each input and output tensors does not satisfy the above data type combinations.
 *   - Layout of input, image and output is not NHWC.
 *   - Layout of box and box_idx is not ARRAY.
 *   - Mode is not CNNL_CROP_AND_RESIZE_BILINEAR.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCropAndResizeBackwardBoxes(cnnlHandle_t handle,
                               const cnnlTensorDescriptor_t input_desc,
                               const void *input,
                               const cnnlTensorDescriptor_t image_desc,
                               const void *image,
                               const cnnlTensorDescriptor_t box_desc,
                               const void *box,
                               const cnnlTensorDescriptor_t index_desc,
                               const void *index,
                               const cnnlTensorDescriptor_t output_desc,
                               void *output,
                               cnnlCropAndResizeMode_t algorithm);

// Group:LSTM
/*!
 *  @brief This function is used to get extra space size needed in LSTM operation.
 *
 *  **Scale Limitation**
 *
 *    ci and co should be greater than 0.
 *
 *  @param[in]  handle
 *    Input. A struct with information that all you need in this operation.
 *  @param[in]  ci
 *    Input. The number of columns of the input.
 *  @param[in]  co
 *    Input. The number of columns of the state input.
 *  @param[out]  size
 *    Output. Extra space size needed in the LSTM operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ended normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - handle is empty.
 *    - size is empty.
 *    - ci or co less than or equal to 0.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetLstmWorkspaceSize(cnnlHandle_t handle,
                                                   const int ci,
                                                   const int co,
                                                   size_t *size);

// Group:Cumsum
/*!
 * @brief Gets an extra workspace size needed in ::cnnlCumsum_v2.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the cumsum
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] axis
 *   Input. An int number that specifies the dimension to compute over.
 * @param[out] size
 *   Input. The size of the extra workspace in bytes that needs to be used in ::cnnlCumsum_v2.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 */
cnnlStatus_t CNNL_WIN_API cnnlGetCumsumWorkspaceSize(cnnlHandle_t handle,
                                                     const cnnlTensorDescriptor_t input_desc,
                                                     const int axis,
                                                     size_t *size);

// Group:Cumsum
/*!
 * @brief Gets cumulative sum of input tensor \p input according to \p axis.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlCumsum_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the cumsum
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] axis
 *   Input. An int number that specifies the dimension to compute over.
 * @param[in] exclusive
 *   Input. A Boolean value that determines whether the first row of \p axis of \p output is set
 *   to zero. If true, the first row of \p axis of \p output is set to zero, and the second row of
 *   \p axis of \p output is equal to the first row of \p input. Otherwise, the first row of \p axis
 *   of \p output is equal to the first row of \p input.
 * @param[in] reverse
 *   Input. A Boolean value that determines whether to compute from the last row of \p axis of
 *   \p input to the first row.
 * @param[in] nan_propagation
 *   Input. Enumeration variable which describes whether to propagate NaN elements. For detailed
 *   information, see ::cnnlNanPropagation_t. Currently, this parameter is not used in the operation.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_EXECUTION_FAILED, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Cumsum Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \p input and output tensor
 *   \p output. Data type of input tensor and output tensor should be the same.
 *   - input tensor: int8, int16, int32, int64, half, float.
 *   - output tensor: int8, int16, int32, int64, half, float.
 * - On MLU500 series, when both \p exclusive and \p reverse are false, this function additionally
 *   supports the bfloat16 data type.
 *   - input tensor: bfloat16.
 *   - output tensor: bfloat16.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor must meet the following requirements:
 *   - input tensor shape must be the same as output tensor shape.
 *   - large tensor is only supported on MLU500 series when both \p exclusive and \p reverse are false,
 *     which means that the input tensor and output tensor can exceed 2G.
 *   - The value of \p axis should be in range of [-input_dims, input_dims-1].
 *
 *
 * @note
 * - When both \p exclusive and \p reverse are false, this function supports in-place operation, which means
 *   the \p input and \p output can be the same one.
 * - When \p input contains NaN, the cumsum action is the same as that in PyTorch.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The examples of the cumsum operation are as follows:
     @verbatim
     input array by 2 * 3 --> input: [[1,2,3],[4,5,6]]

     1.param:
       axis: 1, exclusive: false, reverse: false

     output array by 2 * 3 --> output: [[1,2,3],[5,7,9]]

     2.param:
       axis: 1, exclusive: true, reverse: false

     output array by 2 * 3 --> output: [[0,0,0],[1,2,3]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.cumsum.html
 *
 */
CNNL_DEPRECATED_FOR(cnnlCumsum_v2)
cnnlStatus_t CNNL_WIN_API cnnlCumsum(cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const void *input,
                                     int axis,
                                     bool exclusive,
                                     bool reverse,
                                     cnnlNanPropagation_t nan_propagation,
                                     const cnnlTensorDescriptor_t output_desc,
                                     void *output);


// Group:Cumsum
/*!
 * @brief Gets cumulative sum of input tensor \p input according to \p axis.
 * Compared with ::cnnlCumsum, this function requires you to allocate some extra workspace
 * as an input parameter.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the cumsum
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] axis
 *   Input. An int number that specifies the dimension to compute over.
 * @param[in] exclusive
 *   Input. A Boolean value that determines whether the first row of \p axis of \p output is set
 *   to zero. If true, the first row of \p axis of \p output is set to zero, and the second row of
 *   \p axis of \p output is equal to the first row of \p input. Otherwise, the first row of \p axis
 *   of \p output is equal to the first row of \p input.
 * @param[in] reverse
 *   Input. A Boolean value that determines whether to compute from the last row of \p axis of
 *   \p input to the first row.
 * @param[in] nan_propagation
 *   Input. Enumeration variable which describes whether to propagate NaN elements. For detailed
 *   information, see ::cnnlNanPropagation_t. Currently, this parameter is not used in the operation.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for
 *   ::cnnlCumsum_v2.
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the ::cnnlCumsum_v2. You can get the size of the workspace with
 *   the ::cnnlGetCumsumWorkspaceSize function.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_EXECUTION_FAILED, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Cumsum Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \p input and output tensor
 *   \p output. Data type of input tensor and output tensor should be the same.
 *   - input tensor: int8, int16, int32, int64, half, float.
 *   - output tensor: int8, int16, int32, int64, half, float.
 * - On MLU500 series, when both \p exclusive and \p reverse are false, this function additionally
 *   supports the bfloat16 data type.
 *   - input tensor: bfloat16.
 *   - output tensor: bfloat16.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor must meet the following requirements:
 *   - input tensor shape must be the same as output tensor shape.
 *   - large tensor is only supported on MLU500 series when both \p exclusive and \p reverse are false,
 *     which means that the input tensor and output tensor can exceed 2G.
 *   - The value of \p axis should be in range of [-input_dims, input_dims-1].
 *
 *
 * @note
 * - When both \p exclusive and \p reverse are false, this function supports in-place operation, which means
 *   the \p input and \p output can be the same one.
 * - When \p input contains NaN, the cumsum action is the same as that in PyTorch.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The examples of the cumsum operation are as follows:
     @verbatim
     input array by 2 * 3 --> input: [[1,2,3],[4,5,6]]

     1.param:
       axis: 1, exclusive: false, reverse: false

     output array by 2 * 3 --> output: [[1,2,3],[5,7,9]]

     2.param:
       axis: 1, exclusive: true, reverse: false

     output array by 2 * 3 --> output: [[0,0,0],[1,2,3]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.cumsum.html
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlCumsum_v2(cnnlHandle_t handle,
                                        const cnnlTensorDescriptor_t input_desc,
                                        const void *input,
                                        int axis,
                                        bool exclusive,
                                        bool reverse,
                                        cnnlNanPropagation_t nan_propagation,
                                        const cnnlTensorDescriptor_t output_desc,
                                        void *output,
                                        void *workspace,
                                        size_t workspace_size);

// Group:Cumprod
/*!
 * @brief Gets an extra workspace size needed in ::cnnlCumprod_v2.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the cumsum
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] axis
 *   Input. An int number that specifies the dimension to compute over.
 * @param[out] size
 *   Input. The size of the extra workspace in bytes that needs to be used in ::cnnlCumprod_v2.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 */
cnnlStatus_t CNNL_WIN_API cnnlGetCumprodWorkspaceSize(cnnlHandle_t handle,
                                                     const cnnlTensorDescriptor_t input_desc,
                                                     const int axis,
                                                     size_t *size);

// Group:Cumprod
/*!
 * @brief Gets the cumulative product of input tensor \p input along the given \p axis.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlCumprod_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the cumprod
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] axis
 *   Input. An integer that specifies the dimension to compute over.
 * @param[in] exclusive
 *   Input. A Boolean value that determines whether the first row of \p axis of \p output is set
 *   to one. If true, the first row of \p axis of \p output is set to one, and the second row of
 *   \p axis of \p output equals the first row of \p input. Otherwise, the first row of \p axis
 *   of \p output equals the first row of \p input.
 * @param[in] reverse
 *   Input. A Boolean value that specifies whether to compute from the last row of \p axis of
 *   \p input to the first row.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_EXECUTION_FAILED, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Cumprod Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor and output tensor.
 *   Data type of input tensor and output tensor should be the same.
 *   - input tensor: int8, int16, int32, half, float.
 *   - output tensor: int8, int16, int32, half, float.
 * - When both \p exclusive and \p reverse are false, this function supports the bool data type.
 *   - input tensor: bool.
 *   - output tensor: bool.
 * - On MLU500 series, when both \p exclusive and \p reverse are false, this function additionally
 *   supports the bfloat16 data type.
 *   - input tensor: bfloat16.
 *   - output tensor: bfloat16.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor must meet the following requirements:
 *   - input tensor shape must be the same as output tensor shape.
 *   - large tensor is only supported on MLU500 series when both \p exclusive and \p reverse are false,
 *     which means that the input tensor and output tensor can exceed 2G.
 *   - The value of \p axis should be in range of [-input_dims, input_dims-1].
 *
 * @note
 * - When both \p exclusive and \p reverse are false, this function supports in-place operation, which means
 *   the \p input and \p output can be the same one.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The examples of the cumprod operation are as follows:
     @verbatim
     input array by 2 * 3 --> input: [[1,2,3],[4,5,6]]

     1.param:
       axis: 1

     output array by 2 * 3 --> output: [[1,2,3],[4,20,120]]

     2.param:
       axis: 0

     output array by 2 * 3 --> output: [[1,2,6],[4,10,18]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.cumprod.html
 *
 */
CNNL_DEPRECATED_FOR(cnnlCumprod_v2)
cnnlStatus_t CNNL_WIN_API cnnlCumprod(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t input_desc,
                                      const void *input,
                                      int axis,
                                      bool exclusive,
                                      bool reverse,
                                      const cnnlTensorDescriptor_t output_desc,
                                      void *output);

// Group:Cumprod
/*!
 * @brief Gets the cumulative product of input tensor \p input along the given \p axis.
 * Compared with ::cnnlCumprod, this function requires you to allocate some extra workspace
 * as an input parameter.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the cumprod
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] axis
 *   Input. An integer that specifies the dimension to compute over.
 * @param[in] exclusive
 *   Input. A Boolean value that determines whether the first row of \p axis of \p output is set
 *   to one. If true, the first row of \p axis of \p output is set to one, and the second row of
 *   \p axis of \p output equals the first row of \p input. Otherwise, the first row of \p axis
 *   of \p output equals the first row of \p input.
 * @param[in] reverse
 *   Input. A Boolean value that specifies whether to compute from the last row of \p axis of
 *   \p input to the first row.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlCumprod_v2.
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   ::cnnlCumprod_v2. You can get the size of the workspace with
 *   the ::cnnlGetCumprodWorkspaceSize function.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_EXECUTION_FAILED, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Cumprod Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor and output tensor.
 *   Data type of input tensor and output tensor should be the same.
 *   - input tensor: int8, int16, int32, half, float.
 *   - output tensor: int8, int16, int32, half, float.
 * - When both \p exclusive and \p reverse are false, this function supports the bool data type.
 *   - input tensor: bool.
 *   - output tensor: bool.
 * - On MLU500 series, when both \p exclusive and \p reverse are false, this function additionally
 *   supports the bfloat16 data type.
 *   - input tensor: bfloat16.
 *   - output tensor: bfloat16.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor must meet the following requirements:
 *   - input tensor shape must be the same as output tensor shape.
 *   - large tensor is only supported on MLU500 series when both \p exclusive and \p reverse are false,
 *     which means that the input tensor and output tensor can exceed 2G.
 *   - The value of \p axis should be in range of [-input_dims, input_dims-1].
 *
 * @note
 * - When both \p exclusive and \p reverse are false, this function supports in-place operation, which means
 *   the \p input and \p output can be the same one.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The examples of the cumprod operation are as follows:
     @verbatim
     input array by 2 * 3 --> input: [[1,2,3],[4,5,6]]

     1.param:
       axis: 1

     output array by 2 * 3 --> output: [[1,2,3],[4,20,120]]

     2.param:
       axis: 0

     output array by 2 * 3 --> output: [[1,2,6],[4,10,18]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.cumprod.html
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlCumprod_v2(cnnlHandle_t handle,
                                         const cnnlTensorDescriptor_t input_desc,
                                         const void *input,
                                         int axis,
                                         bool exclusive,
                                         bool reverse,
                                         const cnnlTensorDescriptor_t output_desc,
                                         void *output,
                                         void *workspace,
                                         size_t workspace_size);

// Group:Cummin
/*!
 * @brief Gets the cumulative minimum of input tensor \p input along the given \p axis.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the cummin
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] axis
 *   Input. An integer that specifies the dimension to compute over.
 * @param[in] output_values_desc
 *   Input. The descriptor of the \p output_values tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output_values
 *   Output. Pointer to the MLU memory that stores \p output_values tensor that
 *   is the cumulative minimum of elements of input tensor along the \p axis dimension.
 * @param[in] output_indices_desc
 *   Input. The descriptor of the \p output_indices tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output_indices
 *   Output. Pointer to the MLU memory that stores \p output_indices tensor that
 *   is the index of minimum in input tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH,
 *   ::CNNL_STATUS_NOT_SUPPORTED, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @par Formula
 * - See "Cummin Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor and output tensors.
 *   Data type of input tensor and values output tensor should be the same.
 *   - input tensor: uint8, int8, int16, int32, half, float, bool, bfloat16.
 *   - output_values tensor: uint8, int8, int16, int32, half, float, bool, bfloat16.
 *   - output_indices tensor: int32, int64.
 *
 *   bfloat16 is only supported on MLU500 series.
 *
 * @par Scale Limitation
 *   - If the data type is int32, each element in \p input should be in the range
 *     of (\f$-2^{24}\f$, \f$2^{24}\f$). If the data type is int8, int16 or int32, the intermediate results
 *     cannot exceed the value range of the corresponding data type.
 *   - Large tensor is only supported on MLU500 series, which means that the input tensor number can be equal to
 *     or larger than 2G.
 *   - The value of \p axis should be in range of [-input_dims, input_dims -1].
 *
 * @note
 *   - During the index calculation, the index of NaN value and next values can only be zero.
 *     For example, given an array [1, nan, 2, 3], index [0, 0, 0, 0] will be returned.
 *   - If the data type of \p input is bool, the input data that is not 0 is treated as 1.
 *     For example, given an array [2, 3, 0, 3], value [1, 1, 0, 0] and index [0, 1, 2, 2] will be returned.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The examples of the cummin operation are as follows:
     @verbatim
     input array by 2 * 3 --> input: [[1,2,3],[4,5,6]]

     1.param:
       axis: 1

     output array by 2 * 3 --> output: values=tensor([[1, 1, 1],     indices=tensor([[0, 0, 0],
                                                      [4, 4, 4]])                    [0, 0, 0]])]

     2.param:
       axis: 0

     output array by 2 * 3 --> output: values=tensor([[1, 2, 3],     indices=tensor([[0, 0, 0],
                                                      [1, 2, 3]]),                   [0, 0, 0]])
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.cummin.html
 *
 */
cnnlStatus_t  cnnlCummin(cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const void *input,
                                     int axis,
                                     const cnnlTensorDescriptor_t output_values_desc,
                                     void *output_values,
                                     const cnnlTensorDescriptor_t output_indices_desc,
                                     void *output_indices);

// Group:GroupNormForward
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the GroupNormForward operation.
 *
 * The size of the extra workspace is based on the given information \p group_num of the GroupNormForward
 * operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   GroupNormForward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] group_num
 *   Input. An integer value that specifies the number of groups to separate the channels (C) into.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   GroupNormForward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - This API is only used along with ::cnnlGroupNormForward_v2.
 *   ::cnnlGroupNormForward does not require this API.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetGroupNormForwardWorkspaceSize(cnnlHandle_t handle,
                                     int group_num,
                                     const cnnlTensorDescriptor_t x_desc,
                                     size_t *workspace_size);

// Group:GroupNormForward
/*!
 * @brief Applies group normalization over a mini-batch of inputs. Group normalization divides the channels
 *        into groups and computes the mean and variance for normalization within each group. Computation
 *        of group normalization is independent of batch sizes.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGroupNormForward_v3 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the group normalization
 *   forward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] eps
 *   Input. A float value added to the denominator for numerical stability.
 * @param[in] group_num
 *   Input. An integer value that specifies the number of groups to separate the channels (C) into.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] scale_bias_desc
 *   Input. The descriptor of the input \p scale and \p bias tensors. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] scale
 *   Input. Pointer to the MLU memory that stores the \p scale tensor.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the \p bias tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "GroupNorm Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below
 *   - half(input_tensor) - half(scale_tensor) - half(bias_tensor) - half(output_tensor).
 *   - float(input_tensor) - float(scale_tensor) - float(bias_tensor) - float(output_tensor).
 *   - bfloat16_t(input_tensor) - bfloat16_t(scale_tensor) - bfloat16_t(bias_tensor) - bfloat16_t(output_tensor).
 *
 *   bfloat16 is only supported on MLU500 series.
 *
 * @par Data Layout
 * - The supported data layout of the input tensors and output tensors must be
 *   \p CNNL_LAYOUT_NHWC or \p CNNL_LAYOUT_NCHW.
 *
 * @par Scale Limitation
 * - C dimension of tensor \p x is dividable by \p group_num.
 *
 * @note
 * - On MLU500 series, you can use only ::cnnlGroupNormForward_v2
 *   to perform the forward propagation of the group normalization computation.
 * - The element number of \p x should be no more than \p INT_MAX.
 * - The element number of \p scale should be no more than \p INT_MAX.
 * - The element number of \p bias should be no more than \p INT_MAX.
 * - The element number of \p y should be no more than \p INT_MAX.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the group normalization forward operation is as follows:
     @verbatim
      eps: 0.00001
      group_num: 3
      x: NHWC [2, 3, 4, 9]
      scale and bias: array [9]
      y: NHWC [2, 3, 4, 9] same as input
     @endverbatim
 *
 * @par Reference
 * - http://github.com/pytorch/pytorch/blob/master/torch/nn/modules/normalization.py
 */
CNNL_DEPRECATED_FOR(cnnlGroupNormForward_v3)
cnnlStatus_t CNNL_WIN_API cnnlGroupNormForward(cnnlHandle_t handle,
                                               float eps,
                                               int group_num,
                                               const cnnlTensorDescriptor_t x_desc,
                                               const void *x,
                                               const cnnlTensorDescriptor_t scale_bias_desc,
                                               const void *scale,
                                               const void *bias,
                                               const cnnlTensorDescriptor_t y_desc,
                                               void *y);

// Group:GroupNormForward
/*!
 * @brief Applies group normalization over a mini-batch of inputs. Group normalization divides the channels
 *        into groups and computes the mean and variance for normalization within each group. Computation
 *        of group normalization is independent of batch sizes.
 *
 * Compared with ::cnnlGroupNormForward, this function requires you to allocate some extra workspace
 * as an input parameter.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGroupNormForward_v3 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the group normalization
 *   forward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] eps
 *   Input. A float value added to the denominator for numerical stability.
 * @param[in] group_num
 *   Input. An integer value that specifies the number of groups to separate the channels (C) into.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] scale_bias_desc
 *   Input. The descriptor of the input \p scale and \p bias tensors. When both \p scale
 *   and \p bias are null pointer, The value of this pointer can be NULL. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] scale
 *   Input. Pointer to the MLU memory that stores the \p scale tensor.
 *   The value of this pointer can be NULL.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the \p bias tensor.
 *   The value of this pointer can be NULL.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlGroupNormForward_v2.
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the ::cnnlGroupNormForward_v2. You can get the size of the workspace with
 *   the ::cnnlGetGroupNormForwardWorkspaceSize function.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "GroupNorm Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below:
 *   - half(input_tensor) - half(scale_tensor) - half(bias_tensor) - half(output_tensor).
 *   - float(input_tensor) - float(scale_tensor) - float(bias_tensor) - float(output_tensor).
 *   - bfloat16_t(input_tensor) - bfloat16_t(scale_tensor) - bfloat16_t(bias_tensor) - bfloat16_t(output_tensor).
 *
 *   bfloat16 is only supported on MLU500 series.
 *
 * @par Data Layout
 * - The supported data layout of \p x tensors and \p y tensors must be one of:
 *   \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NLC,
 *   \p CNNL_LAYOUT_NCL, \p CNNL_LAYOUT_NCDHW, \p CNNL_LAYOUT_NCHW.
 * - The data layout of \p x tensors and \p y tensors must be the same.
 *
 * @par Scale Limitation
 * - C dimension of tensor \p x is dividable by \p group_num.
 *
 * @note
 * - The element number of \p x should be no more than \p INT_MAX.
 * - The element number of \p scale should be no more than \p INT_MAX.
 * - The element number of \p bias should be no more than \p INT_MAX.
 * - The element number of \p y should be no more than \p INT_MAX.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the group normalization forward operation is as follows:
     @verbatim
      eps: 0.00001
      group_num: 3
      x: NHWC [2, 3, 4, 9]
      scale and bias: array [9]
      y: NHWC [2, 3, 4, 9] same as input
     @endverbatim
 *
 * @par Reference
 * - http://github.com/pytorch/pytorch/blob/master/torch/nn/modules/normalization.py
 */
CNNL_DEPRECATED_FOR(cnnlGroupNormForward_v3)
cnnlStatus_t CNNL_WIN_API cnnlGroupNormForward_v2(cnnlHandle_t handle,
                                                  float eps,
                                                  int group_num,
                                                  const cnnlTensorDescriptor_t x_desc,
                                                  const void *x,
                                                  const cnnlTensorDescriptor_t scale_bias_desc,
                                                  const void *scale,
                                                  const void *bias,
                                                  void *workspace,
                                                  size_t workspace_size,
                                                  const cnnlTensorDescriptor_t y_desc,
                                                  void *y);

// Group:GroupNormForward
/*!
 * @brief Applies group normalization over a mini-batch of inputs. Group normalization divides the channels
 *        into groups and computes the mean and variance for normalization within each group. Computation
 *        of group normalization is independent of batch sizes.
 *
 * Compared with ::cnnlGroupNormForward_v2, this function can provide the output of \p saved_mean and \p saved_rstd.
 * In practical use, if you do not need these two outputs, you can also set them to null pointers.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the group normalization
 *   forward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] eps
 *   Input. A float value added to the denominator for numerical stability.
 * @param[in] group_num
 *   Input. An integer value that specifies the number of groups to separate the channels (C) into.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] scale_bias_desc
 *   Input. The descriptor of the input \p scale and \p bias tensors. When both \p scale
 *   and \p bias are null pointer, The value of this pointer can be NULL. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] scale
 *   Input. Pointer to the MLU memory that stores the \p scale tensor.
 *   The value of this pointer can be NULL.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the \p bias tensor.
 *   The value of this pointer can be NULL.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlGroupNormForward_v3.
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the ::cnnlGroupNormForward_v3. You can get the size of the workspace with
 *   the ::cnnlGetGroupNormForwardWorkspaceSize function.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] mean_rstd_desc
 *   Input. The descriptor of \p saved_mean and \p saved_rstd tensors. The value of this
 *   pointer can be NULL when \p saved_mean and \p saved_rstd are both null pointer. For
 *   detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] saved_mean
 *   Output. Pointer to the MLU memory that stores the output mean tensor.
 * @param[out] saved_rstd
 *   Output. Pointer to the MLU memory that stores the inverse of the variance tensor.
 *   Both the value of this pointer and \p saved_mean are null or are not null.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "GroupNorm Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below:
 *   - half(input_tensor) - half(scale_tensor) - half(bias_tensor) - half(output_tensor) - half(saved_mean) - half(saved_rstd).
 *   - float(input_tensor) - float(scale_tensor) - float(bias_tensor) - float(output_tensor) -float(saved_mean) - float(saved_rstd).
 *   - bfloat16_t(input_tensor) - bfloat16_t(scale_tensor) - bfloat16_t(bias_tensor) - bfloat16_t(output_tensor).
 *
 *   bfloat16 is only supported on MLU500 series.
 *
 * @par Data Layout
 * - The supported data layout of \p x tensors and \p y tensors must be one of:
 *   \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NLC,
 *   \p CNNL_LAYOUT_NCL, \p CNNL_LAYOUT_NCDHW, \p CNNL_LAYOUT_NCHW.
 * - The data layout of \p x tensors and \p y tensors must be the same.
 *
 * @par Scale Limitation
 * - C dimension of tensor \p x is dividable by \p group_num.
 *
 * @note
 * - \p saved_mean and \p saved_rstd should be NULL or not NULL at the same time.
 * - The element number of \p x should be no more than \p INT_MAX.
 * - The element number of \p scale should be no more than \p INT_MAX.
 * - The element number of \p bias should be no more than \p INT_MAX.
 * - The element number of \p y should be no more than \p INT_MAX.
 * - The element number of \p saved_mean should be no more than \p INT_MAX.
 * - The element number of \p saved_rstd should be no more than \p INT_MAX.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the group normalization forward operation is as follows:
     @verbatim
      eps: 0.00001
      group_num: 3
      x: NHWC [2, 3, 4, 27]
      scale and bias: array [27]
      y: NHWC [2, 3, 4, 27] same as input
      saved_mean: array [2, 3]
      saved_rstd: array [2, 3]
     @endverbatim
 *
 * @par Reference
 * - http://github.com/pytorch/pytorch/blob/master/torch/nn/modules/normalization.py
 */
cnnlStatus_t CNNL_WIN_API cnnlGroupNormForward_v3(cnnlHandle_t handle,
                                                  float eps,
                                                  int group_num,
                                                  const cnnlTensorDescriptor_t x_desc,
                                                  const void *x,
                                                  const cnnlTensorDescriptor_t scale_bias_desc,
                                                  const void *scale,
                                                  const void *bias,
                                                  void *workspace,
                                                  size_t workspace_size,
                                                  const cnnlTensorDescriptor_t y_desc,
                                                  void *y,
                                                  const cnnlTensorDescriptor_t mean_rstd_desc,
                                                  void *saved_mean,
                                                  void *saved_rstd);

// Group:GroupNormBackward
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used
 *        to store the temporary result of the ::cnnlGroupNormBackward operation.
 *        The size of this extra workspace is based on the given information of the ::cnnlGroupNormBackward
 *        operation. For more information about the workspace, see "Cambricon CNNL User Guide".
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGetGroupNormBackwardWorkspaceSizeV2 instead.
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   ::cnnlGroupNormBackward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] NC
 *   Input. The product of batch and channel. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in the
 *   ::cnnlGroupNormBackward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - None.
 *
 * @par API Dependency
 * - This function must be called before the ::cnnlGroupNormBackward function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */

CNNL_DEPRECATED_FOR(cnnlGetGroupNormBackwardWorkspaceSize_v2)
cnnlStatus_t CNNL_WIN_API
cnnlGetGroupNormBackwardWorkspaceSize(cnnlHandle_t handle,
                                      const int32_t NC,
                                      size_t *workspace_size);

// Group:GroupNormBackward
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used
 *        to store the temporary result of the ::cnnlGroupNormBackward operation.
 *        The size of this extra workspace is based on the given information of the ::cnnlGroupNormBackward
 *        operation. For more information about the workspace, see "Cambricon CNNL User Guide".
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGetGroupNormBackwardWorkspaceSize_v2 instead.
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   ::cnnlGroupNormBackward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   The descriptor of the \p x tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] G
 *   Input. An integer value that specifies the number of groups to separate the channels (C) into.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in the
 *   ::cnnlGroupNormBackward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - None.
 *
 * @par API Dependency
 * - This function must be called before the ::cnnlGroupNormBackward function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */

CNNL_DEPRECATED_FOR(cnnlGetGroupNormBackwardWorkspaceSize_v2)
cnnlStatus_t CNNL_WIN_API
cnnlGetGroupNormBackwardWorkspaceSizeV2(cnnlHandle_t handle,
                                        const cnnlTensorDescriptor_t x_desc,
                                        const int64_t G,
                                        size_t *workspace_size);

// Group:GroupNormBackward
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used
 *        to store the temporary result of the ::cnnlGroupNormBackward operation.
 *        The size of this extra workspace is based on the given information of the ::cnnlGroupNormBackward
 *        operation. For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * Compared with ::cnnlGetGroupNormBackwardWorkspaceSize, this function supports the larger tensor input
 * by using tensor descriptor \p x_desc instead of NC and extra parameter \p G.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   ::cnnlGroupNormBackward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   The descriptor of the \p x tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] G
 *   Input. An integer value that specifies the number of groups to separate the channels (C) into.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in the
 *   ::cnnlGroupNormBackward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - None.
 *
 * @par API Dependency
 * - This function must be called before the ::cnnlGroupNormBackward function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */

cnnlStatus_t CNNL_WIN_API
cnnlGetGroupNormBackwardWorkspaceSize_v2(cnnlHandle_t handle,
                                        const cnnlTensorDescriptor_t x_desc,
                                        const int64_t G,
                                        size_t *workspace_size);

// Group:GroupNormBackward
/*!
 * @brief Performs the backward group normalization computation. Group normalization divides the channels
 *   into groups and computes the mean and variance for normalization in every group, and the backward group
 *   normalization normalizes over the dimension of the input tensor \p x within each group.
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the group normalization
 *   backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   The descriptor of the \p x tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \p x. The shape of \p x
 *   is [batch, channel, height, width].
 * @param[in] diff_z_desc
 *   The descriptor of the input tensor \p diff_z. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] diff_z
 *   Input. Pointer to the MLU memory that stores the input tensor \p diff_z. The shape of \p diff_z
 *   is [batch, channel, height, width].
 * @param[in] gamma_desc
 *   The descriptor of the input tensor \p gamma. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] gamma
 *   Input. Pointer to the MLU memory that stores the input tensor \p gamma.
 * @param[in] mean_desc
 *   The descriptor of the \p mean tensor. For detailed information,
     see ::cnnlTensorDescriptor_t.
 * @param[in] mean
 *   Input. Pointer to the MLU memory that stores the input tensor \p mean, which is computed during
 *   the forward phase from the ::cnnlGroupNormForward call. The shape of \p mean
 *   is [batch, G].
 * @param[in] rstd_desc
 *   The descriptor of the result differential tensor \p rstd. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] rstd
 *   Input. Pointer to the MLU memory that stores the input tensor \p rstd, which is computed during
 *   the forward phase from the ::cnnlGroupNormForward call. The shape of \p rstd is [batch, G].
 * @param[in] G
 *   Input. An integer value that specifies the number of groups to separate the channels (C) into.
 * @param[in] diff_x_desc
 *   Input. Pointer to the MLU memory that stores the input tensor \p diff_x. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the output tensor \p diff_x.
 * @param[in] diff_scale_desc
 *   The descriptor of the \p diff_scale tensor.
 *   The value of this pointer can be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] diff_scale
 *   Output. Pointer to the MLU memory that stores the output tensor \p diff_scale.
 *   The value of this pointer can be NULL.
 * @param[in] diff_bias_desc
 *   Output. Pointer to the MLU memory that stores the output tensor \p diff_bias.
 *   The value of this pointer can be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] diff_bias
 *   Output. Pointer to the MLU memory that stores the output tensor \p diff_bias.
 *   The value of this pointer can be NULL.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlGroupNormBackward.
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the ::cnnlGroupNormBackward. You can get the size of the workspace with
 *   the ::cnnlGetGroupNormForwardWorkspaceSize function.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *   ::CNNL_STATUS_EXECUTION_FAILED, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par API Dependency
 * - Before calling this function to implement group normalization backward, you need to prepare
 *   all the parameters passed to this function. See each parameter description for details.
 *
 * @par Formula
 * - See "GroupNormBackward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensors and output tensors should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensors: half, float, bfloat16.
 *   - output tensors: half, float, bfloat16.
 *
 *   bfloat16 is only supported on MLU500 series.
 *
 * @par Data Layout
 * - The supported data layouts of \p x, \p diff_z, \p gamma, \p mean,
 *   \p rstd, \p diff_x, \p diff_scale and \p diff_bias are
 *   as follows:
 *   - x tensor: \p CNNL_LAYOUT_NCHW or \p CNNL_LAYOUT_NCL.
 *   - diff_z tensor: \p CNNL_LAYOUT_NCHW or \p CNNL_LAYOUT_NCL.
 *     The layout of the diff_z tensor should be the same as \p x, \p diff_z.
 *   - gamma tensor: \p CNNL_LAYOUT_ARRAY, and the dimension must be 1D.
 *   - mean tensor: \p CNNL_LAYOUT_ARRAY, and the dimension must be 2D.
 *   - rstd tensor: \p CNNL_LAYOUT_ARRAY, and the dimension must be 2D.
 *   - diff_x tensor: \p CNNL_LAYOUT_NCHW or \p CNNL_LAYOUT_NCL.
 *     The layout of the \p diff_x tensor should be the same as \p x tensor.
 *   - diff_scale tensor  tensor: \p CNNL_LAYOUT_ARRAY, and the dimension must be 1D.
 *   - diff_bias tensor: \p CNNL_LAYOUT_ARRAY, and the dimension must be 1D.
 *
 * @note
 *  - Batch size should be smaller than 766 on MLU200 series.
 *
 * @par Requirement
 * - None.
 *
 * @par Example
 * - The example of the groupnorm backward operation is as follows:
     @verbatim
      input :
      --> x = [[[[1.0, 2.0]],[[3.0, 4.0]],[[5.0, 6.0]],[[7.0, 8.0]]]]]

      --> diff_dz = [[[[1000.0, 2000.0]],[[5000.0, 6000.0]],
                      [[1100.0, 1200.0]], [1500.0, 1600.0]]]]

      --> scale= [1, 2, 4, 8]

      --> mean= [2.5,6.5]

      --> saved_invstd: [0.8944236755372, 0.8944236755372]

      param:
        G: 2

      output :
      --> diff_x = [[[[  804.9370, -1967.7463]],
                     [[ 1520.5354,  -357.7249]],
                     [[  679.7275, -1860.4131]],
                     [[ 1681.5283,  -500.8418]]]]

      --> diff_scale = [-4472.1177, 20571.7422, -4024.9060,  5634.8682]

      --> diff_bias = [ 6000., 22000.,  4600.,  6200.]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.com/pytorch/pytorch/blob/v1.6.0/torch/nn/functional.py#L2052
 *
 */

cnnlStatus_t CNNL_WIN_API
cnnlGroupNormBackward(cnnlHandle_t handle,
                      const cnnlTensorDescriptor_t x_desc,
                      const void *x,
                      const cnnlTensorDescriptor_t diff_z_desc,
                      const void *diff_z,
                      const cnnlTensorDescriptor_t gamma_desc,
                      const void *gamma,
                      const cnnlTensorDescriptor_t mean_desc,
                      const void *mean,
                      const cnnlTensorDescriptor_t rstd_desc,
                      const void *rstd,
                      const int64_t G,
                      const cnnlTensorDescriptor_t diff_x_desc,
                      void *diff_x,
                      const cnnlTensorDescriptor_t diff_scale_desc,
                      void *diff_scale,
                      const cnnlTensorDescriptor_t  diff_bias_desc,
                      void *diff_bias,
                      void *workspace,
                      size_t workspace_size);
// Group:Softplus
/*!
 * @brief Computes softplus on input tensor \p x, and returns the results in the output
 *        tensor \p y.
 *
 * Softplus Forward is used in activation operation to obtain the nonlinearity for artificial intelligence.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   softplus_forward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] beta
 *   Input. The  value used in the softplus_forward operation. For detailed information, see
 *   "SoftplusForward Operation" section in "Cambricon CNNL User Guide". The default value of
 *   beta is 1.
 * @param[in] threshold
 *   Input. The threshold value used in the softplus_forward operation. For detailed information, see
 *   "SoftplusForward Operation" section in "Cambricon CNNL User Guide". The default value of
 *   threshold is 20.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "SoftplusForward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensor and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float, bfloat16.
 *   - output tensor: half, float, bfloat16.
 *
 *   bfloat16 is only supported on MLU500 series.
 *
 * @par Scale Limitation
 * - The value of \p beta cannot be equal to 0.
 *
 * @note
 * - The result is low precision when the product of \p x and \p beta is less than or equal to around -7.75 on MLU300 and MLU500 series.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the softplus forward operation is as follows:
     @verbatim
      input one array by 2 * 3
      --> input: [[0.0, 3.4, 30],[-4.3, -1.2, -0.5]]

      param:
        beta: 1
        threshold: 20

      output array by 2 * 3
      --> output: [[0.69194, 3.43291, 30],[0.01348, 0.26330, 0.47297]]
     @endverbatim
 */
CNNL_DEPRECATED_FOR(cnnlSoftplusForward_v2)
cnnlStatus_t CNNL_WIN_API cnnlSoftplusForward(cnnlHandle_t handle,
                                              const cnnlTensorDescriptor_t x_desc,
                                              const void *x,
                                              const cnnlTensorDescriptor_t y_desc,
                                              void *y,
                                              const int beta,
                                              const int threshold);

// Group:Prelu
/*!
 * @brief Computes PReLu on input tensor \p x with alpha tensor \p alpha,
 *        and returns the results in the output tensor \p y.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the prelu operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] desc_x
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] desc_alpha
 *   Input. The descriptor of the alpha tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] alpha
 *   Input. Pointer to the MLU memory that stores the learned coefficient alpha tensor.
 * @param[in] desc_y
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH,
 *   ::CNNL_STATUS_NOT_SUPPORTED, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @par Formula
 * - See "Prelu Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensor, alpha tensor and output tensor must be the same.
 * - The supported data types of input tensor, alpha tensor and output tensor are as follows:
 *   - input tensor: half, float, bfloat16.
 *   - alpha tensor: half, float, bfloat16.
 *   - output tensor: half, float, bfloat16.
 *
 *   bfloat16 is only supported on MLU500 series.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - Shape of input tensor and output tensor must be the same.
 * - Dim of input tensor and output tensor must be the same.
 * - The supported shape of input tensor and alpha tensor are as follows:
 *   - input shape: [a, ..., b, ..., c], alpha shape: [1, ..., 1, ..., 1]
 *   - input shape: [a, ..., b, ..., c], alpha shape: [1, ..., 1, ..., c]
 *   - input shape: [a, ..., b, ..., c], alpha shape: [1, ..., b, ..., 1]
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      input two arrays by 2 * 3 and 1 * 3
      --> input: [[-0.3, 0.8, -0.5], [0.6, -0.4, -0.7]]

      --> alpha: [0.1, 0.2, 0.3]

      output array by 2 * 3
      --> output: [[-0.03, 0.8, -0.15], [0.6, -0.08, -0.21]]

     @endverbatim
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/keras/layers/PReLu
 */
cnnlStatus_t CNNL_WIN_API cnnlPrelu(cnnlHandle_t handle,
                                    const cnnlTensorDescriptor_t desc_x,
                                    const void *x,
                                    const cnnlTensorDescriptor_t desc_alpha,
                                    const void *alpha,
                                    const cnnlTensorDescriptor_t desc_y,
                                    void *y);

// Group:PreluBackward
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used
 *   to store the temporary result of the ::cnnlPreluBackward operation.
 *
 * The size of the extra workspace is based on the given information of the ::cnnlPreluBackward
 * operation. For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGetPreluBackwardV2WorkspaceSize instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *          ::cnnlPreluBackward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] alpha_grad_desc
 *   Input. The descriptor of alpha_grad tensor. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in the
 *           ::cnnlPreluBackwardV2 operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Scale Limitation
 * - Parameters must meet the following requirements:
 *   - The input \p alpha_grad_desc should not be null.
 *
 * @par API Dependency
 * - This function must be called before the ::cnnlPreluBackward function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlGetPreluBackwardV2WorkspaceSize)
cnnlStatus_t CNNL_WIN_API
cnnlGetPreluBackwardWorkspaceSize(cnnlHandle_t handle,
                                  const cnnlTensorDescriptor_t alpha_grad_desc,
                                  size_t *workspace_size);

// Group:PreluBackward
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used
 *   to store the temporary result of the ::cnnlPreluBackwardV2 operation.
 *
 * The size of extra workspace is based on the given information of the ::cnnlPreluBackwardV2
 * operation. For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *          ::cnnlPreluBackwardV2 operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of x tensor. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[in] grad_output_desc
 *   Input. The descriptor of grad_output tensor. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[in] alpha_desc
 *   Input. The descriptor of alpha tensor. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[in] x_grad_desc
 *   Input. The descriptor of x_grad tensor. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[in] alpha_grad_desc
 *   Input. The descriptor of alpha_grad tensor. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in the
 *           ::cnnlPreluBackward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Scale Limitation
 * - None.
 *
 * @par API Dependency
 * - This function must be called before the ::cnnlPreluBackwardV2 function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - Parameters must meet the following requirements:
 *   - The input \p x_desc, \p grad_output_desc and \p alpha_desc must meet the conditions for broadcasting.
 *   - The shape of output \p x_grad_desc and \p alpha_grad_desc is the same as
 *     the shape of the broadcast result.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetPreluBackwardV2WorkspaceSize(cnnlHandle_t handle,
                                    const cnnlTensorDescriptor_t x_desc,
                                    const cnnlTensorDescriptor_t grad_output_desc,
                                    const cnnlTensorDescriptor_t alpha_desc,
                                    const cnnlTensorDescriptor_t x_grad_desc,
                                    const cnnlTensorDescriptor_t alpha_grad_desc,
                                    size_t *workspace_size);

// Group:PreluBackward
/*!
 * @brief Implements the backward propagation for the prelu function.
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlPreluBackwardV2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *          ::cnnlPreluBackward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of \p x tensors. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] grad_output_desc
 *   Input. The descriptor of \p grad_output tensor. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[in] grad_output
 *   Input. Pointer to the MLU memory that stores the \p grad_output tensor,
 *          \p grad_output  is the input gradient of ReplicationPad.
 * @param[in] alpha_desc
 *   Input. The descriptor of \p alpha tensor. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[in] alpha
 *   Input. Pointer to the MLU memory that stores the learned coefficient alpha tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the ::cnnlPreluBackwardV2
 *          operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the
 *          ::cnnlPreluBackwardV2 operation. You can get the size of the workspace with the
 *          ::cnnlGetPreluBackwardV2WorkspaceSize function.
 * @param[in] x_grad_desc
 *   Input. The descriptor of \p x_grad tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] x_grad
 *   Output. Pointer to the MLU memory that stores the output tensor,
 *           the gradient calculation result of \p x.
 * @param[in] alpha_grad_desc
 *   Input. The descriptor of \p alpha_grad tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] alpha_grad
 *   Output. Pointer to the MLU memory that stores the \p alpha_grad tensor.
 *           \p alpha_grad is the calculated gradient result of \p alpha.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par Formula
 * - See "Prelu Backward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of \p x, \p grad_output, \p alpha, \p x_grad and \p alpha_grad
 *     should be the same.
 * - The supported data types of \p x, \p grad_output, \p alpha, \p x_grad and
 *     \p alpha_grad are as follows:
 *   - x: half, float, bfloat16.
 *   - grad_output: half, float, bfloat16.
 *   - alpha: half, float, bfloat16.
 *   - x_grad: half, float, bfloat16.
 *   - alpha_grad: half, float, bfloat16.
 *
 *   The data type bfloat16_t is only supported on MLU500 series or higher.
 *
 * @par Scale Limitation
 *   - None.
 *
 * @par API Dependency
 * - Before calling this function you need to call ::cnnlGetPreluBackwardWorkspaceSize
 *   to get the extra space size needed in ::cnnlPreluBackward operation.
 *
 * @note
 * - The shape of \p x, \p grad_output and \p x_grad must be the same.
 * - The shape of \p alpha and \p alpha_grad must be the same.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - http://github.com/pytorch/pytorch/blob/release/1.9/aten/src/ATen/native/cuda/Activation.cu
 */
CNNL_DEPRECATED_FOR(cnnlPreluBackwardV2)
cnnlStatus_t CNNL_WIN_API
cnnlPreluBackward(cnnlHandle_t handle,
                  const cnnlTensorDescriptor_t x_desc,
                  const void *x,
                  const cnnlTensorDescriptor_t grad_output_desc,
                  const void *grad_output,
                  const cnnlTensorDescriptor_t alpha_desc,
                  const void *alpha,
                  void *workspace,
                  size_t workspace_size,
                  const cnnlTensorDescriptor_t x_grad_desc,
                  void *x_grad,
                  const cnnlTensorDescriptor_t alpha_grad_desc,
                  void *alpha_grad);

// Group:PreluBackward
/*!
 * @brief Implements the backward propagation for the prelu function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *          ::cnnlPreluBackwardV2 operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of \p x tensors. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] grad_output_desc
 *   Input. The descriptor of \p grad_output tensor. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[in] grad_output
 *   Input. Pointer to the MLU memory that stores the \p grad_output tensor,
 *          \p grad_output  is the input gradient of ReplicationPad.
 * @param[in] alpha_desc
 *   Input. The descriptor of \p alpha tensor. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[in] alpha
 *   Input. Pointer to the MLU memory that stores the learned coefficient alpha tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the ::cnnlPreluBackward
 *          operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the
 *          ::cnnlPreluBackward operation. You can get the size of the workspace with the
 *          ::cnnlGetPreluBackwardWorkspaceSize function.
 * @param[in] x_grad_desc
 *   Input. The descriptor of \p x_grad tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] x_grad
 *   Output. Pointer to the MLU memory that stores the output tensor,
 *           the gradient calculation result of \p x.
 * @param[in] alpha_grad_desc
 *   Input. The descriptor of \p alpha_grad tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] alpha_grad
 *   Output. Pointer to the MLU memory that stores the \p alpha_grad tensor.
 *           \p alpha_grad is the calculated gradient result of \p alpha.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par Formula
 * - See "Prelu Backward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of \p x, \p grad_output, \p alpha, \p x_grad and \p alpha_grad
 *     should be the same.
 * - The supported data types of \p x, \p grad_output, \p alpha, \p x_grad and
 *     \p alpha_grad are as follows:
 *   - x: half, float, bfloat16.
 *   - grad_output: half, float, bfloat16.
 *   - alpha: half, float, bfloat16.
 *   - x_grad: half, float, bfloat16.
 *   - alpha_grad: half, float, bfloat16.
 *
 *   The data type bfloat16_t is only supported on MLU500 series or higher.
 *
 * @par Scale Limitation
 * - None.
 *
 * @par API Dependency
 * - Before calling this function you need to call ::cnnlGetPreluBackwardV2WorkspaceSize
 *   to get the extra space size needed in ::cnnlPreluBackwardV2 operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - The shape of the \p x, \p grad_output and \p alpha must meet the conditions for broadcasting.
 * - The shape of output \p x_grad and \p alpha_grad is the same as
 *   that of the broadcast result.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - http://github.com/pytorch/pytorch/blob/release/1.9/aten/src/ATen/native/cuda/Activation.cu
 */
cnnlStatus_t CNNL_WIN_API
cnnlPreluBackwardV2(cnnlHandle_t handle,
                    const cnnlTensorDescriptor_t x_desc,
                    const void *x,
                    const cnnlTensorDescriptor_t grad_output_desc,
                    const void *grad_output,
                    const cnnlTensorDescriptor_t alpha_desc,
                    const void *alpha,
                    void *workspace,
                    size_t workspace_size,
                    const cnnlTensorDescriptor_t x_grad_desc,
                    void *x_grad,
                    const cnnlTensorDescriptor_t alpha_grad_desc,
                    void *alpha_grad);
// Group:Trigon
/*!
 * @brief Computes trigonometric functions on input tensor \p x, and returns the results in the
 *        output tensor \p y.
 *
 * Trigon Forward is used in activation operation to obtain the nonlinearity for artificial intelligence.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   trigon operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  trigon_desc
 *   Input. The descriptor of the trigon operation. It contains trigon function mode
 *   which is defined in ::cnnlTrigonFunctionMode_t and computation function mode
 *   which is defined in ::cnnlComputationPreference_t.
 * @param[in]  x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in]  y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out]  y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "Trigon Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensor and output tensor must be the same.
 * - When \p prefer is ::CNNL_COMPUTATION_FAST or ::CNNL_COMPUTATION_ULTRAHIGH_PRECISION,
 *   the supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 * - When \p prefer is ::CNNL_COMPUTATION_HIGH_PRECISION,
 *   the supported data types of input and output tensors are as follows:
 *   - input tensor: half, float, bfloat16.
 *   - output tensor: half, float, bfloat16.
 *   bfloat16 is only supported on MLU500 series.
 *
 * @par Scale Limitation
 * - None
 *
 * @par API Dependency
 * - Before calling this function, you need to call the ::cnnlCreateTrigonDescriptor
 *   function to create the \p trigon_desc and to call the ::cnnlSetTrigonDescriptor
 *   function to set the information.
 * - You need to call the ::cnnlDestroyTrigonDescriptor function to destroy the descriptor.
 *
 * @note
 * - Now only asin, acos, tan and atan support ultrahigh precision mode.
 * - On MLU300 series, MLU500 series and CE3226:
 *   - Value range limit of input data:
 *     - CNNL_COMPUTATION_FAST
 *       - Sin and Cos: [-2, 2]
 *       - Tan: [-1.0, 1.0]
 *       - Asin and Acos:
 *         - half: [-0.9, 0.9]
 *         - float: [-1.0. 1.0]
 *       - Atan:
 *         - half: [-65504, 65504]
 *         - float: [-3.4e38, 3.4e38]
 *       - Sinh:
 *         - half: [-4.1815, 4.1815]
 *         - float: [-42.7536, 42.7536]
 *       - Cosh:
 *         - half: [-4.8749, 4.8749]
 *         - float: [-42.7460, 42.7460]
 *       - Tanh:
 *         - half: [-65504, 65504]
 *         - float: [-3.4e38, 3.4e38]
 *       - Asinh: [-100.0, 100.0]
 *       - Acosh: (1.0, 100.0]
 *       - Atanh: [-0.8, 0.8]
 *     - CNNL_COMPUTATION_HIGH_PRECISION
 *       - Sinh:
 *         - float: [-, -89.41599] and [-88.72283, 88.72283] and [89.41599, +]
 *       - Cosh:
 *         - float: [-, -89.41599] and [-88.72283, 88.72283] and [89.41599, +]
 *       - Asinh
 *         - float: [-18446742974197923840, 18446742974197923840]
 *       - Acosh:
 *         - float: [-, 18446742974197923840]
 *   - When the input data or parameter contains NaN or infinity:
 *     - SIN: If \p x is NaN, then \p y is NaN.
 *            If \p x is infinity, then \p y is NaN.
 *     - COS: If \p x is NaN, then \p y is NaN.
 *            If \p x is infinity, then \p y is NaN.
 *     - TAN: If \p x is NaN, then \p y is NaN.
 *            If \p x is infinity, then \p y is NaN.
 *     - ASIN: If \p x is NaN, then \p y is NaN.
 *             If \p x is infinity, then \p y is NaN.
 *     - ACOS: If \p x is NaN, then \p y is NaN.
 *             If \p x is infinity, then \p y is NaN.
 *     - ATAN: If \p x is NaN, then \p y is NaN.
 *             If \p x is infinity, then \p y is /2.
 *             If \p x is negative infinity, then \p y is -/2.
 *     - SINH: If \p x is NaN, then \p y is NaN.
 *             If \p x is positive infinity, then \p y is positive infinity.
 *             If \p x is negative infinity, then \p y is negative infinity.
 *     - COSH: If \p x is NaN, then \p y is NaN.
 *             If \p x is positive infinity, then \p y is positive infinity.
 *             If \p x is negative infinity, then \p y is positive infinity.
 *     - TANH: If \p x is NaN, then \p y is NaN.
 *             If \p x is positive infinity, then \p y is 1.
 *             If \p x is negative infinity, then \p y is -1.
 *     - ASINH: If \p x is NaN, then \p y is NaN.
 *              If \p x is positive infinity, then \p y is positive infinity.
 *              If \p x is negative infinity, then \p y is negative infinity.
 *     - ACOSH: If \p x is NaN, then \p y is NaN.
 *              If \p x is positive infinity, then \p y is positive infinity.
 *              If \p x is negative infinity, then \p y is NaN.
 *     - ATANH: If \p x is NaN, then \p y is NaN.
 *              If \p x is positive infinity, then \p y is NaN.
 *              If \p x is negative infinity, then \p y is NaN.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the trigon forward operation is as follows:
     @verbatim
      input one array by 2 * 3
      --> input: [[1.0, 1.2, 1.4],[-0.8, -1.2, -1.1]]

      param:
        mode: TRIGON_SIN
        prefer: CNNL_COMPUTATION_HIGH_PRECISION

      output array by 2 * 3
      --> output: [[0.84147, 0.93204, 0.98545],[-0.71736, -0.93204, -0.89121]]
     @endverbatim
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/sin
 * - https://www.tensorflow.org/api_docs/python/tf/cos
 * - https://www.tensorflow.org/api_docs/python/tf/tan
 * - https://www.tensorflow.org/api_docs/python/tf/asin
 * - https://www.tensorflow.org/api_docs/python/tf/acos
 * - https://www.tensorflow.org/api_docs/python/tf/atan
 * - https://www.tensorflow.org/api_docs/python/tf/sinh
 * - https://www.tensorflow.org/api_docs/python/tf/cosh
 * - https://www.tensorflow.org/api_docs/python/tf/tanh
 * - https://www.tensorflow.org/api_docs/python/tf/asinh
 * - https://www.tensorflow.org/api_docs/python/tf/acosh
 * - https://www.tensorflow.org/api_docs/python/tf/atanh
 */
cnnlStatus_t CNNL_WIN_API cnnlTrigonForward(cnnlHandle_t handle,
                                            const cnnlTrigonDescriptor_t trigon_desc,
                                            const cnnlTensorDescriptor_t x_desc,
                                            const void *x,
                                            const cnnlTensorDescriptor_t y_desc,
                                            void *y);
// Group:Trigon
/*!
 * @brief Creates a descriptor pointed by \p trigon_desc for a trigon forward operator.
 *
 * @param[out] trigon_desc
 *   Output. A host pointer to the trigon forward descriptor that holds the trigonometric
 *   function mode which is defined in ::cnnlTrigonFunctionMode_t and computation
 *   preference mode which is defined in ::cnnlComputationPreference_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetTrigonDescriptor function to initialize
 *   and set the information to trigon descriptor.
 * - You need to call the ::cnnlDestroyTrigonDescriptor function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateTrigonDescriptor(cnnlTrigonDescriptor_t *trigon_desc);

// Group:Trigon
/*!
 * @brief Initializes the trigon descriptor \p trigon_desc that was previously created with
 *        the ::cnnlCreateTrigonDescriptor function, and sets the trigonometric function mode
 *        \p mode which is defined in ::cnnlTrigonFunctionMode_t.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlSetTrigonDescriptor_v2 instead, which supports the \p prefer parameter to
 *   set computation preference.
 *
 * @param[in,out] trigon_desc
 *   Input/output. The descriptor of the trigon operation. It contains trigonometric function mode which
 *   is defined in ::cnnlTrigonFunctionMode_t.
 * @param[in] mode
 *   Input. A trigonometric function mode which is defined in ::cnnlTrigonFunctionMode_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, you need to call the ::cnnlCreateTrigonDescriptor
 *   function to create the \p trigon_desc.
 * - You need to call the ::cnnlDestroyTrigonDescriptor function to destroy the descriptor.
 *
 * @note
 * - The default value of prefer is set to ::CNNL_COMPUTATION_HIGH_PRECISION.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlSetTrigonDescriptor_v2)
cnnlStatus_t CNNL_WIN_API cnnlSetTrigonDescriptor(cnnlTrigonDescriptor_t trigon_desc,
                                                  cnnlTrigonFunctionMode_t mode);
// Group:Trigon
/*!
 * @brief Initializes the trigon descriptor \p trigon_desc that was previously created with
 *        the ::cnnlCreateTrigonDescriptor function, and sets the trigonometric function mode
 *        \p mode which is defined in ::cnnlTrigonFunctionMode_t and computation preference mode
 *        \p prefer which is defined in ::cnnlComputationPreference_t.
 *
 * @param[in,out] trigon_desc
 *   Input/output. The descriptor of the trigon operation. It contains trigonometric function mode which
 *   is defined in ::cnnlTrigonFunctionMode_t and computation preference mode which is defined
 *   in ::cnnlComputationPreference_t.
 * @param[in] mode
 *   Input. A trigonometric function mode which is defined in ::cnnlTrigonFunctionMode_t.
 * @param[in] prefer
 *   Input. A computation preference mode which is defined in ::cnnlComputationPreference_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, you need to call the ::cnnlCreateTrigonDescriptor
 *   function to create the \p trigon_desc.
 * - You need to call the ::cnnlDestroyTrigonDescriptor function to destroy the descriptor.
 *
 * @note
 * - Only asin, acos, tan and atan support ultrahigh precision computation mode.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetTrigonDescriptor_v2(cnnlTrigonDescriptor_t trigon_desc,
                                                     cnnlTrigonFunctionMode_t mode,
                                                     cnnlComputationPreference_t prefer);

// Group:Trigon
/*!
 * @brief Destroys a trigon descriptor \p trigon_desc that was previously created with
 *        the ::cnnlCreateTrigonDescriptor function.
 *
 * The trigon descriptor is defined in ::cnnlTrigonDescriptor_t and holds the information
 * about trigonometric function mode and computation preference mode.
 *
 * @param[in] trigon_desc
 *   Input. The trigon descriptor to be destroyed.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - You need to call this function after calling the ::cnnlTrigonForward function.
 *   Otherwise, ::CNNL_STATUS_BAD_PARAM is returned.
 * - This function should be called to destroy the trigon descriptor.
 *   Otherwise, the memory leak may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyTrigonDescriptor(cnnlTrigonDescriptor_t trigon_desc);

// Group:TriIndices
/*!
 * @brief Returns the indices of the lower/upper triangular part of a
 * row-by-col matrix.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the TriIndices operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] row
 *   Input. A scalar represents the number of rows in the 2-D matrix.
 * @param[in] col
 *   Input. A scalar represents the number of columns in the 2-D matrix.
 * @param[in] offset
 *   Input. A scalar represents the diagonal offset from the main diagonal.
 * @param[in] tri_up_mode
 *   Input. A Boolean value determines to apply tril mode or triu mode.
 *   True means triu mode and false means tril mode.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the \p output tensor, where
 *   the first row contains row coordinates of all indices and the second row
 *   contains column coordinates.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH,
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Data Type
 *  - The supported data types for \p output tensor are as follows:
 *    - output tensor: int32.
 *
 * @par Data Layout
 *  - The data layout of the output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The \p output must be two-dimensional and the size of the first
 *   dimension must be 2.
 * - On the platforms MLU300 series, MLU500 series, CE3226 and 1V,
 *      row and col must be greater than or equal to 0,
 *      offset should be in range of [INT_MIN, INT_MAX].
 * - On MLU500 series, row * col should be in range of [0, \f$2^{47}-1\f$],
 *      row and col should be in range of [0, INT_MAX].
 * - On 1V, row * col should be in range of [0, \f$2^{47}-1\f$],
 *      row and col should be in range of [0, \f$2^{24}\f$].
 * - On MLU300 series and CE3226, row * col should be in range of [0, INT_MAX],
 *      row and col should be in range of [0, INT_MAX].
 *
 * @note
 * - This function is only supported on MLU300 series, MLU500 series, CE3226 and 1V.
 * - The argument offset controls which diagonal to consider. A positive value
 *   includes just as many diagonals above the main diagonal, and similarly a
 *   negative value excludes just as many diagonals below the main diagonal.
 * - Output Tensor shape is determined by row, col, and offset.
 * - Indices are ordered based on rows and then columns.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this function is as follows:
     @verbatim
     int row = 4;
     int col = 3;
     int offset = 1;
     bool tri_up_mode = false;
     shape of output = [2, 11]
     cnnlTriIndices(handle, row, col, offset, tri_up_mode, output_desc, output);

     Then we will get the output:
     output: --> [[0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3],
                  [0, 1, 0, 1, 2, 0, 1, 2, 0, 1, 2]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.tril_indices.html
 * - https://pytorch.org/docs/stable/generated/torch.triu_indices.html
 */
cnnlStatus_t CNNL_WIN_API cnnlTriIndices(cnnlHandle_t handle,
                                         const int64_t row,
                                         const int64_t col,
                                         const int64_t offset,
                                         const bool tri_up_mode,
                                         const cnnlTensorDescriptor_t output_desc,
                                         void* output);

// Group:Nms
/*!
 *  @brief Creates a descriptor pointed by \p desc for NMS operation, and allocates
 *         memory for holding the information about the NMS operation. The information
 *         is defined in ::cnnlNmsDescriptor_t. For more information about descriptor,
 *         see "Cambricon CNNL User Guide".
 *
 *  @param[out] desc
 *   Output. A host pointer to the NMS descriptor that holds information about the NMS operation.
 *
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 *  @par API Dependency
 *  - After calling this function, you can call the ::cnnlSetNmsDescAttr function to initialize
 *    and set the information to the NMS descriptor.
 *  - You need to call the ::cnnlDestroyNmsDescriptor function to destroy the descriptor.
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateNmsDescriptor(cnnlNmsDescriptor_t *desc);

// Group:Nms
/*!
 *  @brief Destroys a NMS descriptor \p desc that was previously created with the
 *         ::cnnlCreateNmsDescriptor function.
 *
 *  The NMS descriptor is defined in ::cnnlNmsDescriptor_t and holds the information
 *  about the NMS operation.
 *
 *  @param[in] desc
 *    Input. The NMS descriptor to be destroyed.
 *
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - You need to call this function after calling the ::cnnlNms_v2 function.
 *    Otherwise, ::CNNL_STATUS_BAD_PARAM is returned.
 *  - This function should be called to destroy the NMS descriptor. Otherwise, the
 *    memory leak may occur.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyNmsDescriptor(cnnlNmsDescriptor_t desc);

// Group:Nms
/*!
 * @brief Initializes the NMS descriptor \p desc that was previously created with the
 * ::cnnlCreateNmsDescriptor function, and sets the information about the NMS operation
 * to the NMS descriptor \p desc. The information includes the output mode \p mode, the
 * intersection over union threshold \p iou_threshold, the maximum output size \p max_output_size,
 * the score threshold \p confidence_threshold, the data layout of input \p input_layout,
 * the running mode of MLU \p run_mode.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlSetNmsDescAttr instead. Compared with
 *   ::cnnlSetNmsDescriptor, ::cnnlSetNmsDescAttr supports setting parameters separately.
 *
 * @param[in,out] desc
 *   Input/output. The descriptor of the NMS operation. For detailed information, see ::cnnlNmsDescriptor_t.
 * @param[in] output_mode
 *   Input. The output mode of NMS descriptor to be set. For detailed information,
 *   see ::cnnlNmsOutputMode_t.
 * @param[in] iou_threshold
 *   Input. The intersection over union (iou) threshold used in NMS computation.
 *   Boxes would be filtered out if the intersection over union is greater than \p iou_threshold.
 * @param[in] max_output_size
 *   Input. The maximum number of output boxes.
 * @param[in] confidence_threshold
 *   Input. The confidence threshold used in NMS computation.
 *   Boxes would be filtered out directly if the confidence of boxes are less than this
 *   threshold.
 * @param[in] input_layout
 *   Input. The input boxes layout. Supported values are 0 and 1. 0 represents
 *   [boxes_num, 4], [boxes_num, 7] or [batches_num, boxes_num, 4]. 1 represents
 *   [4, boxes_num], [7, boxes_num] or [batches_num, 4, boxes_num].
 * @param[in] run_mode
 *   Input. The supported core version kernel. 0 reepresents block and 1 represents union1.
 *
 * @par Returns
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - For the \p max_output_size, it should not be less than 0.
 *
 * @note
 * - When the input boxes are in NMS3D format([boxes_num, 7] or [7, boxes_num]),
 *   only parameters iou_threshold and layout are valid and other parameters
 *   can be arbitrary. Besides, the mode is set as 0 during the computation.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlSetNmsDescAttr)
cnnlStatus_t CNNL_WIN_API cnnlSetNmsDescriptor(cnnlNmsDescriptor_t desc,
                                               const cnnlNmsOutputMode_t output_mode,
                                               const float iou_threshold,
                                               const int max_output_size,
                                               const float confidence_threshold,
                                               const int input_layout,
                                               const int run_mode);

// Group:Nms
/*!
 * @brief Initializes the NMS descriptor \p desc that was previously created with the
 * ::cnnlCreateNmsDescriptor function, and sets the information about the NMS operation
 * to the NMS descriptor \p desc. The information includes the output mode \p mode, the
 * intersection over union threshold \p iou_threshold, the maximum output size \p max_output_size,
 * the score threshold \p confidence_threshold, the data layout of input \p input_layout. The
 * difference between this function and ::cnnlSetNmsDescriptor is that this function removes
 * the \p run_mode.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *  Use ::cnnlSetNmsDescAttr instead. Compared with
 *  ::cnnlSetNmsDescriptor_v2, ::cnnlSetNmsDescAttr supports setting parameters separately.
 *
 * @param[in,out] desc
 *   Input/output. The descriptor of the NMS operation. For detailed information, see ::cnnlNmsDescriptor_t.
 * @param[in] output_mode
 *   Input. The output mode of NMS descriptor to be set. For detailed information,
 *   see ::cnnlNmsOutputMode_t.
 * @param[in] iou_threshold
 *   Input. The intersection over union (iou) threshold used in NMS computation.
 *   Boxes would be filtered out if the intersection over union is greater than \p iou_threshold.
 * @param[in] max_output_size
 *   Input. The maximum number of output boxes.
 * @param[in] confidence_threshold
 *   Input. The confidence threshold used in NMS computation.
 *   Boxes would be filtered out directly if the confidence of boxes are no more than this
 *   threshold.
 * @param[in] input_layout
 *   Input. The input data layout. Supported values are 0 and 1. 0 represents
 *   [boxes_num, 4], [boxes_num, 7] or [batches_num, boxes_num, 4]. 1 represents
 *   [4, boxes_num], [7, boxes_num] or [batches_num, 4, boxes_num].
 *
 * @par Returns
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - For the \p max_output_size, it should not be less than 0.
 *
 * @note
 * - When the input boxes are in NMS3D format([boxes_num, 7] or [7, boxes_num]),
 *   only parameters iou_threshold and layout are valid and other parameters
 *   can be arbitrary. Besides, the mode is set as 0 during the computation.
 * - when the dimension of input box is 3 ([batches_num, boxes_num, 4] or
 *   [batches_num, 4, boxes_num]) rather than 2 ([boxes_num, 4] or [4, boxes_num]),
 *   the \p algo should be set as CNNL_NMS_ALGO_INCLUDE_BOUNDARY in general. If you set
 *   \p algo to others, ::cnnlNms_v2 will set it to CNNL_NMS_ALGO_INCLUDE_BOUNDARY automatically.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlSetNmsDescAttr)
cnnlStatus_t CNNL_WIN_API cnnlSetNmsDescriptor_v2(cnnlNmsDescriptor_t desc,
                                                  const cnnlNmsOutputMode_t output_mode,
                                                  const float iou_threshold,
                                                  const int max_output_size,
                                                  const float confidence_threshold,
                                                  const int input_layout);

// Group:Nms
/*!
 * @brief Initializes the NMS descriptor \p desc that was previously created with the
 * ::cnnlCreateNmsDescriptor function, and sets the information about the NMS operation
 * to the NMS descriptor \p desc. The information includes the output mode \p mode, the
 * intersection over union threshold \p iou_threshold, the computation algorithm \p algo,
 * the maximum output size \p max_output_size, the score threshold \p confidence_threshold,
 * the offset of boundary \p offset, the data layout of input \p input_layout. The main
 * difference between this function and ::cnnlSetNmsDescriptor_v2 is that this function
 * supports setting computation algorithm and the offset of boundary.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlSetNmsDescAttr instead. Compared with
 *   ::cnnlSetNmsDescriptor_v3, ::cnnlSetNmsDescAttr supports setting parameters separately.
 *
 * @param[in,out] desc
 *   Input/output. The descriptor of the NMS operation. For detailed information, see ::cnnlNmsDescriptor_t.
 * @param[in] output_mode
 *   Input. The output mode of NMS descriptor to be set. For detailed information,
 *   see ::cnnlNmsOutputMode_t.
 * @param[in] algo
 *   Input. The computation algorithm of NMS operation. For detailed information,
 *   see ::cnnlNmsAlgo_t.
 * @param[in] iou_threshold
 *   Input. The intersection over union (iou) threshold used in NMS computation.
 *   Boxes would be filtered out if the intersection over union is greater than or equal to \p iou_threshold.
 * @param[in] max_output_size
 *   Input. The maximum number of output boxes.
 * @param[in] confidence_threshold
 *   Input. The confidence threshold used in NMS computation.
 *   Boxes would be filtered out directly if the confidence of boxes are no more than this
 *   threshold.
 * @param[in] offset
 *   Input. The offset size of boundary used in NMS computation.
 *   This value would be used when \p algo is ::CNNL_NMS_ALGO_INCLUDE_BOUNDARY.
 * @param[in] input_layout
 *   Input. The input data layout. Supported values are 0 and 1. 0 represents
 *   [boxes_num, 4], [boxes_num, 7] or [batches_num, boxes_num, 4]. 1 represents
 *   [4, boxes_num], [7, boxes_num] or [batches_num, 4, boxes_num].
 *
 * @par Returns
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - For the \p offset, it should be 0.0 or 1.0.
 * - For the \p max_output_size, it should not be less than 0.
 *
 * @note
 * - When the input boxes are in NMS3D format([boxes_num, 7] or [7, boxes_num]),
 *   only parameters iou_threshold and layout are valid and other parameters
 *   can be arbitrary. Besides, the mode is set as 0 during the computation.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlSetNmsDescAttr)
cnnlStatus_t CNNL_WIN_API cnnlSetNmsDescriptor_v3(cnnlNmsDescriptor_t desc,
                                                  const cnnlNmsOutputMode_t output_mode,
                                                  const cnnlNmsAlgo_t algo,
                                                  const float iou_threshold,
                                                  const int max_output_size,
                                                  const float confidence_threshold,
                                                  const float offset,
                                                  const int input_layout);

// Group:Nms
/*!
 * @brief Initializes the NMS descriptor \p desc that was previously created with the
 * ::cnnlCreateNmsDescriptor function, and sets the information about the NMS operation
 * to the NMS descriptor \p desc. The information includes the box data structure mode
 * \p box_mode, the output mode \p mode, the confidence update method mode \p method_mode,
 * the intersection over union threshold \p iou_threshold, the parameter sigma for soft NMS
 * with Gaussian method \p soft_nms_sigma, the maximum output size \p max_output_size, the
 * score threshold \p confidence_threshold, the offset of boundary \p offset, and the data
 * layout of input \p input_layout. The main difference between this function and the
 * ::cnnlSetNmsDescriptor_v3 is that this function supports box data structure mode, the
 * confidence update method mode, and the soft NMS parameter sigma. Besides, \p algo is
 * deprecated in this function. One important thing is that when the dimension of input box
 * is 2 ([boxes_num, 4] or [4, boxes_num]) rather than 3 ([batches_num, boxes_num, 4] or
 * [batches_num, 4, boxes_num]), the new features above will not work, and it is recommended
 * to use ::cnnlSetNmsDescriptor_v3. In addition, because of the different implementation,
 * when the dimension of input box is 2, it has better performance.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlSetNmsDescAttr instead. Compared with
 *   ::cnnlSetNmsDescriptor_v4, ::cnnlSetNmsDescAttr supports setting parameters separately.
 *
 * @param[in,out] desc
 *   Input/output. The descriptor of the NMS operation. For detailed information, see ::cnnlNmsDescriptor_t.
 * @param[in] box_mode
 *   Input. The box data structure mode of NMS descriptor to be set. For detailed information,
 *   see ::cnnlNmsBoxPointMode_t.
 * @param[in] output_mode
 *   Input. The output mode of NMS descriptor to be set. For detailed information,
 *   see ::cnnlNmsOutputMode_t.
 * @param[in] method_mode
 *   Input. The confidence update method mode. For detailed information,
 *   see ::cnnlNmsMethodMode_t.
 *   Note that method mode 1 and 2 are not supported in current version,
 *   and it will be supported in the future.
 * @param[in] iou_threshold
 *   Input. The intersection over union (iou) threshold used in NMS computation.
 *   Boxes would be filtered out if the intersection over union is greater than or equal to \p iou_threshold.
 * @param[in] soft_nms_sigma
 *   Input. The parameter used in soft NMS with Gaussian method.
 *   This value would be used when method_mode is ::CNNL_NMS_SOFT_NMS_GAUSSIAN.
 * @param[in] max_output_size
 *   Input. The maximum number of output boxes. If the dimension of input box is 3, such as
 *   [batches_num, boxes_num, 4] or [batches_num, 4, boxes_num], this parameter indicates
 *   the maximum number of output boxes per class.
 * @param[in] confidence_threshold
 *   Input. The confidence threshold used in NMS computation.
 *   Boxes would be filtered out directly if the confidence of boxes are no more than this
 *   threshold.
 * @param[in] offset
 *   Input. The offset size of boundary used in NMS computation.
 *   This value would be used when \p algo is ::CNNL_NMS_ALGO_INCLUDE_BOUNDARY.
 * @param[in] input_layout
 *   Input. The input data layout. Supported values are 0 and 1. 0 represents
 *   [boxes_num, 4], [boxes_num, 7] or [batches_num, boxes_num, 4]. 1 represents
 *   [4, boxes_num], [7, boxes_num] or [batches_num, 4, boxes_num].
 *
 * @par Returns
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - \p soft_nms_sigma should not be less than 0.0.
 * - \p offset should be 0.0 or 1.0.
 * - \p input_layout should be 0 or 1.
 * - \p max_output_size should not be less than 0.
 *
 * @note
 * - If the dimension of input box is 3 ([batches_num, boxes_num, 4] or
 *   [batches_num, 4, boxes_num]), the dimension of input confidence is
 *   3 ([batches_num, classes_num, boxes_num]). If the dimension of input box
 *   is 2 ([boxes_num, 4], [4, boxes_num], [boxes_num, 7] or [7, boxes_num]),
 *   the dimension of input confidence is 1 ([boxes_num]).
 * - When the dimension of input confidence is 3, the box mode 1 is supported.
 * - When the dimension of input confidence is 3, the output mode of 0, 1 and 2
 *   are supported only in the case of batches_num = 1 and classes_num = 1.
 * - Method mode 1 and 2 are not supported in current version,
 *   and they will be supported in the future.
 * - When the input boxes are in NMS3D format([boxes_num, 7] or [7, boxes_num]),
 *   only parameters iou_threshold and layout are valid and other parameters
 *   can be arbitrary. Besides, the mode is set as 0 during the computation.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlSetNmsDescAttr)
cnnlStatus_t CNNL_WIN_API cnnlSetNmsDescriptor_v4(cnnlNmsDescriptor_t desc,
                                                  const cnnlNmsBoxPointMode_t box_mode,
                                                  const cnnlNmsOutputMode_t output_mode,
                                                  const cnnlNmsMethodMode_t method_mode,
                                                  const float iou_threshold,
                                                  const float soft_nms_sigma,
                                                  const int max_output_size,
                                                  const float confidence_threshold,
                                                  const float offset,
                                                  const int input_layout);

// Group:Nms
/*!
 * @brief Initializes the NMS descriptor \p desc that was previously created with the
 * ::cnnlCreateNmsDescriptor function, and sets the information about the NMS operation
 * to the NMS descriptor \p desc. Compared with ::cnnlSetNmsDescriptor_v4, this function
 * supports \p algo and the \p output padded with zeros.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *  Use ::cnnlSetNmsDescAttr instead. Compared with
 *  ::cnnlSetNmsDescriptor_v5, ::cnnlSetNmsDescAttr supports setting parameters separately.
 *
 * @param[in,out] desc
 *   Input/output. The descriptor of the NMS operation. For detailed information, see ::cnnlNmsDescriptor_t.
 * @param[in] box_mode
 *   Input. The box data structure mode of NMS descriptor to be set. For detailed information,
 *   see ::cnnlNmsBoxPointMode_t.
 * @param[in] output_mode
 *   Input. The output mode of NMS descriptor to be set. For detailed information,
 *   see ::cnnlNmsOutputMode_t.
 * @param[in] algo
 *   Input. The computation algorithm of NMS operation. For detailed information,
 *   see ::cnnlNmsAlgo_t.
 * @param[in] method_mode
 *   Input. The confidence update method mode. For detailed information,
 *   see ::cnnlNmsMethodMode_t.
 *   Note that method mode 1 and 2 are not supported in current version,
 *   and it will be supported in the future.
 * @param[in] iou_threshold
 *   Input. The intersection over union (iou) threshold used in NMS computation.
 *   Boxes would be filtered out if the intersection over union is greater than or equal to \p iou_threshold.
 * @param[in] soft_nms_sigma
 *   Input. The parameter used in soft NMS with Gaussian method.
 *   This value would be used when method_mode is ::CNNL_NMS_SOFT_NMS_GAUSSIAN.
 * @param[in] max_output_size
 *   Input. The maximum number of output boxes. If the dimension of input box is 3, i.e.
 *   [batches_num, boxes_num, 4] or [batches_num, 4, boxes_num], this parameter indicates
 *   the maximum number of output boxes per class.
 * @param[in] confidence_threshold
 *   Input. The confidence threshold used in NMS computation.
 *   Boxes would be filtered out directly if the confidence of boxes are no more than this
 *   threshold.
 * @param[in] offset
 *   Input. The offset size of boundary used in NMS computation.
 *   This value would be used when \p algo is ::CNNL_NMS_ALGO_INCLUDE_BOUNDARY.
 * @param[in] input_layout
 *   Input. The input data layout. Supported values are 0 and 1. 0 represents
 *   [boxes_num, 4], [boxes_num, 7] or [batches_num, boxes_num, 4]. 1 represents
 *   [4, boxes_num], [7, boxes_num] or [batches_num, 4, boxes_num].
 * @param[in] pad_to_max_output_size
 *   Input. When the \p pad_to_max_output_size set is true, the \p output will be padded to max_output_size
 *   with zero. Default is false.
 * @par Returns
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - \p soft_nms_sigma should not be less than 0.0.
 * - \p offset should be 0.0 or 1.0.
 * - \p input_layout should be 0 or 1.
 * - \p max_output_size should not be less than 0.
 * - \p pad_to_max_output_size should be true or false.
 *
 * @note
 * - If the dimension of input box is 3 ([batches_num, boxes_num, 4] or
 *   [batches_num, 4, boxes_num]), the dimension of input confidence is
 *   3 ([batches_num, classes_num, boxes_num]). If the dimension of input box
 *   is 2 ([boxes_num, 4], [4, boxes_num], [boxes_num, 7] or [7, boxes_num]),
 *   the dimension of input confidence is 1 ([boxes_num]).
 * - When the dimension of input confidence is 3, the box mode 1 is supported.
 * - When the dimension of input confidence is 3, the output mode of 0, 1 and 2
 *   are supported only in the case of batches_num = 1 and classes_num = 1.
 * - Method mode 1 and 2 are not supported in current version,
 *   and they will be supported in the future.
 * - When the input boxes are in NMS3D format([boxes_num, 7] or [7, boxes_num]),
 *   only parameters iou_threshold and layout are valid and other parameters
 *   can be arbitrary. Besides, the mode is set as 0 during the computation.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlSetNmsDescAttr)
cnnlStatus_t CNNL_WIN_API cnnlSetNmsDescriptor_v5(cnnlNmsDescriptor_t desc,
                                                  const cnnlNmsBoxPointMode_t box_mode,
                                                  const cnnlNmsOutputMode_t output_mode,
                                                  const cnnlNmsAlgo_t algo,
                                                  const cnnlNmsMethodMode_t method_mode,
                                                  const float iou_threshold,
                                                  const float soft_nms_sigma,
                                                  const int max_output_size,
                                                  const float confidence_threshold,
                                                  const float offset,
                                                  const int input_layout,
                                                  const bool pad_to_max_output_size);

// Group:Nms
/*!
 * @brief Sets the matrix multiplication extension descriptor \p desc that is previously created with
 *        the ::cnnlCreateNmsDescriptor function. The information includes the attribute defined in
 *        ::cnnlNmsDescAttribute_t, the host pointer to the attribute
 *        value \p buf, and the size of buffer for verification.
 *
 * @param[in,out] desc
 *  Input/Output. The descriptor of the nms. For detailed information, see ::cnnlNmsDescriptor_t.
 * @param[in] attr
 *   Input. Attribute of the nms descriptor to be set. For detailed information, see ::cnnlNmsDescAttribute_t.
 * @param[in] buf
 *   Input. A host pointer to the attribute value set by this function.
 * @param[in] size_in_bytes
 *   Input. Buffer in bytes for verification.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetNmsDescAttr(cnnlNmsDescriptor_t desc,
                                             cnnlNmsDescAttribute_t attr,
                                             const void *buf,
                                             size_t size_in_bytes);

// Group:Nms
/*!
 * @brief Computes the subset of input tensor \p boxes with the scores of \p confidence, and returns
 *        the results in the output tensor \p output.
 *
 * NMS(Non-Maximum Suppression) operation is a necessary procedure in detection networks. And this
 * operation selects no more than \p max_output_size targets with high confidence, based on their
 * intersection over union. This function needs extra MLU memory, and you can get the size of workspace
 * \p workspace_size with the ::cnnlGetNmsWorkspaceSize function.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release, and there are memory copy operation
 *   from device to host, which may result in problems. Use ::cnnlNms_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the NMS operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] desc
 *   Input. The descriptor of the NMS operation. For detailed information, see ::cnnlNmsDescriptor_t.
 * @param[in]  boxes_desc
 *   Input. The descriptor of the input boxes tensor, including the information of dimension, data type and
 *   layout of input boxes. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  boxes
 *   Input. Pointer to the MLU memory that stores the input boxes tensor.
 * @param[in]  confidence_desc
 *   Input. The descriptor of input confidence tensor, including the information of dimension, data type and
 *   layout of input confidence. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  confidence
 *   Input. Pointer to the MLU memory that stores the input confidence tensor.
 * @param[in]  workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the NMS operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in]  workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the NMS operation. You can
 *   get the size of the workspace with the ::cnnlGetNmsWorkspaceSize function.
 * @param[in]  output_desc
 *   Input. The descriptor of the output tensor, including the information of dimension, data type and layout
 *   of output. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Returns
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "NMS Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This operation supports combinations of the following data types for input boxes tensor \p boxes,
 *   input confidence tensor \p confidence, and output tensor \p output.
 *   - input boxes tensor: half, float.
 *   - input confidence tensor: half, float
 *   - output tensor: half, float, int32, uint32.
 * - If the output is the indices of boxes, the output data type should be uint32, otherwise
 *   the output data type should the same as input data type.
 *    Note that the combinations of input boxes tensor and input confidence tensor must be half-half
 *   or float-float.
 *
 * @par Data Layout
 * - The combination of the shape of input boxes with confidence can be [boxes_num, 4] with [boxes_num], [4, boxes_num] with
 *   [boxes_num], [boxes_num, 7] with [boxes_num], [7, boxes_num] with [boxes_num], [batches_num, boxes_num, 4] with
 *   [batches_num, classes_num, boxes_num] and [batches_num, 4, boxes_num] with [batches_num, classes_num, boxes_num].
 * - The output tensor is a 1D tensor if the output result is the indices of boxes, otherwise it is a 2D
 *   tensor, which containing the coordinates and confidence of output boxes.
 *
 * @par Scale Limitation
 *  - For the input boxes tensor, if the shape is [boxes_num, 4], the order of coordinates is x_01, y_01, x_02, y_02,
 *    x_11, y_11, x_12, y_12, ...  x_n1, y_n1, x_n2, y_n2. And x_i1 must be less than x_i2, y_i1 must be less than y_i2.
 *    The (x_i1, y_i1) and (x_i2, y_i2) represent the top left corner and bottom right corner coordinates, respectively.
 *  - For the input boxes tensor, if the shape is  [4, boxes_num], the order of coordinates is x_01, x_11, ... x_n1,
 *    y_01, y_11, ... y_n1, x_02, x_12, ... x_n2, x_01, x_11, ... x_n1. And x_i1 must be less than x_i2, y_i1 must be less than y_i2.
 *    The (x_i1, y_i1) and (x_i2, y_i2) represent the top left corner and bottom right corner coordinates, respectively.
 *
 * @par API Dependency
 *  - Before calling this function to implement NMS, you need to prepare all the parameters passed to this function.
 *    See each parameter description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the shape of input boxes tensor to [4, num_boxes].
 *   The num_boxes represent the number of input boxes.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.google.cn/api_docs/python/tf/
 */
CNNL_DEPRECATED_FOR(cnnlNms_v2)
cnnlStatus_t CNNL_WIN_API cnnlNms(cnnlHandle_t handle,
                                  const cnnlNmsDescriptor_t desc,
                                  const cnnlTensorDescriptor_t boxes_desc,
                                  const void *boxes,
                                  const cnnlTensorDescriptor_t confidence_desc,
                                  const void *confidence,
                                  void *workspace,
                                  size_t workspace_size,
                                  cnnlTensorDescriptor_t output_desc,
                                  void *output);

// Group:Nms
/*!
 * @brief Computes the subset of input tensor \p boxes with the scores of \p confidence, and returns
 *        the results in the output tensor \p output and \p output_size.
 *
 * NMS(Non-Maximum Suppression) operation is a necessary procedure in detection networks. And this
 * operation selects no more than \p max_output_size targets with high confidence, based on their
 * intersection over union. This function needs extra MLU memory, and you can get the size of workspace
 * \p workspace_size with the ::cnnlGetNmsWorkspaceSize_v3 function. The difference between this
 * function and ::cnnlNms is that this function returns the valid output size in \p output_size.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the NMS operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] desc
 *   Input. The descriptor of the NMS operation. For detailed information, see ::cnnlNmsDescriptor_t.
 * @param[in]  boxes_desc
 *   Input. The descriptor of the input boxes tensor, including the information of dimension, data type and
 *   layout of input boxes. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  boxes
 *   Input. Pointer to the MLU memory that stores the input boxes tensor.
 * @param[in]  confidence_desc
 *   Input. The descriptor of input confidence tensor, including the information of dimension, data type and
 *   layout of input confidence. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  confidence
 *   Input. Pointer to the MLU memory that stores the input confidence tensor.
 * @param[in]  workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the NMS operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in]  workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the NMS operation. You can
 *   get the size of the workspace with the ::cnnlGetNmsWorkspaceSize_v3 function.
 * @param[in]  output_desc
 *   Input. The descriptor of the output tensor, including the information of dimension, data type and layout
 *   of output. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[out] output_size
 *   Output. Pointer to the MLU memory that stores the number of output boxes.
 *
 * @par Returns
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "NMS Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This operation supports combinations of the following data types for input boxes tensor \p boxes,
 *   input confidence tensor \p confidence, and output tensor \p output.
 *   - input boxes tensor: half, float.
 *   - input confidence tensor: half, float
 *   - output tensor: half, float, int32, uint32, int64.
 *   - output size: int32, uint32.
 * - If the output is the indices of boxes, the output data type should be int32, uint32 or int64, otherwise
 *   the output data type should be the same as that of input boxes. The data type of output size is int32 or uint32.
 *    Note that when the shape of \p boxes is [boxes_num, 4] or [4, boxes_num]
 *   , the combinations of input boxes tensor and input confidence tensor can be float-half, otherwise the data
 *   type of input boxes and input confidence tensor must be the same.
 *    Note that output tensor can be int64 only when the shape of \p boxes is [boxes_num, 4] or [4, boxes_num].
 *
 * @par Data Layout
 * - The input boxes tensor should be a 2D tensor, and the input confidence tensor should be a 1D tensor.
 * - The output tensor is a 1D tensor if the output result is the indices of boxes, otherwise it is a 2D
 *   tensor, which containing the coordinates and confidence of output boxes.
 *
 * @par Scale Limitation
 *  - For the input boxes tensor, if the shape is [boxes_num, 4], the order of coordinates is x_01, y_01, x_02, y_02,
 *    x_11, y_11, x_12, y_12, ...  x_n1, y_n1, x_n2, y_n2. And x_i1 must be less than x_i2, y_i1 must be less than y_i2.
 *    The (x_i1, y_i1) and (x_i2, y_i2) represent the top left corner and bottom right corner coordinates, respectively.
 * -  For the input boxes tensor, if the shape is  [4, boxes_num], the order of coordinates is x_01, x_11, ... x_n1,
 *    y_01, y_11, ... y_n1, x_02, x_12, ... x_n2, x_01, x_11, ... x_n1. And x_i1 must be less than x_i2, y_i1 must be less than y_i2.
 *    The (x_i1, y_i1) and (x_i2, y_i2) represent the top left corner and bottom right corner coordinates, respectively.
 *
 * @par API Dependency
 *  - Before calling this function to implement NMS, you need to prepare all the parameters passed to this function.
 *    See each parameter description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the shape of input boxes tensor to [4, num_boxes].
 *   The num_boxes represent the number of input boxes.
 * - When the dimension of input box is 2, it has better performance.
 *
 * @note
 * - When the input boxes are in NMS3D format([boxes_num, 7] or [7, boxes_num]),
 *   both of confidence_desc and confidence should be provided as null pointer.
 * - In NMS3D mode, ::cnnlNms_v2 will get low precision on MLU200 platform.
 * - In NMS3D mode, when finding the point with minimum y and minimum x in convex-hull-graham,
 *   it performs min-pooling operation. If the input data of pooling contains NaN:
 *   - On MLU200 series:
 *    - The \p output value is the NaN.
 *   - On MLU300 series and CE3226:
 *    - If the last value in the kernel of the pooling is NaN, the \p output value is NaN.
 *      Otherwise, the \p output value is the minimum value after the last NaN.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.google.cn/api_docs/python/tf/
 */
cnnlStatus_t CNNL_WIN_API cnnlNms_v2(cnnlHandle_t handle,
                                  const cnnlNmsDescriptor_t desc,
                                  const cnnlTensorDescriptor_t boxes_desc,
                                  const void *boxes,
                                  const cnnlTensorDescriptor_t confidence_desc,
                                  const void *confidence,
                                  void *workspace,
                                  size_t workspace_size,
                                  const cnnlTensorDescriptor_t output_desc,
                                  void *output,
                                  void *output_size);

// Group:Nms
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace
 * needed in NMS operation.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGetNmsWorkspaceSize_v4 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the NMS operation. For detailed information, see ::cnnlHandle_t.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used
 *   in the NMS operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlNms function to perform the NMS operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlGetNmsWorkspaceSize_v4)
cnnlStatus_t CNNL_WIN_API cnnlGetNmsWorkspaceSize(cnnlHandle_t handle, size_t *size);

// Group:Nms
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace
 * needed in NMS operation.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGetNmsWorkspaceSize_v4 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the NMS operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  confidence_desc
 *   Input. The descriptor of input confidence tensor, including the information of dimension, data type and
 *   layout of input confidence. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used
 *   in the NMS operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlNms_v2 function to perform the NMS operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlGetNmsWorkspaceSize_v4)
cnnlStatus_t CNNL_WIN_API cnnlGetNmsWorkspaceSize_v2(cnnlHandle_t handle,
                                                     const cnnlTensorDescriptor_t confidence_desc,
                                                     size_t *size);

// Group:Nms
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace
 * needed in NMS operation. Compared with ::cnnlGetNmsWorkspaceSize_v2, this function adds an
 * additional parameter \p boxes_desc and supports the \p confidence_desc to be null pointer
 * in the case of NMS3D where \p confidence is unnecessary.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGetNmsWorkspaceSize_v4 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the NMS operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] boxes_desc
 *   Input. The descriptor of input boxes tensor, including the information of dimension, data type and
 *   layout of input boxes. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] confidence_desc
 *   Input. The descriptor of input confidence tensor, including the information of dimension, data type and
 *   layout of input confidence. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used
 *   in the NMS operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlNms_v2 function to perform the NMS operation.
 *
 * @note
 * - When the input boxes are in NMS3D format([boxes_num, 7] or [7, boxes_num]),
 *   the confidence_desc must be provided with null pointer.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlGetNmsWorkspaceSize_v4)
cnnlStatus_t CNNL_WIN_API cnnlGetNmsWorkspaceSize_v3(cnnlHandle_t handle,
                                                     const cnnlTensorDescriptor_t boxes_desc,
                                                     const cnnlTensorDescriptor_t confidence_desc,
                                                     size_t *size);

// Group:Nms
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace
 * needed in the NMS operation. Compared with ::cnnlGetNmsWorkspaceSize_v3, this function adds an
 * additional parameter \p desc.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the NMS operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] desc
 *   Input. The descriptor of nms tensor. For detailed information, see ::cnnlNmsDescriptor_t.
 * @param[in] boxes_desc
 *   Input. The descriptor of input boxes tensor, including the information of dimension, data type and
 *   layout of input boxes. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] confidence_desc
 *   Input. The descriptor of input confidence tensor, including the information of dimension, data type and
 *   layout of input confidence. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used
 *   in the NMS operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlNms_v2 function to perform the NMS operation.
 *
 * @note
 * - When the input boxes are in NMS3D format([boxes_num, 7] or [7, boxes_num]),
 *   the confidence_desc must be provided with null pointer.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetNmsWorkspaceSize_v4(cnnlHandle_t handle,
                                                     const cnnlNmsDescriptor_t desc,
                                                     const cnnlTensorDescriptor_t boxes_desc,
                                                     const cnnlTensorDescriptor_t confidence_desc,
                                                     size_t *size);

/******************************************************************************
 * Cambricon CNNL OP: ScaledTanh
 ******************************************************************************/

// Group:ScaledTanh
/*!
 * @brief Performs a scaled hyperbolic tangent activation fuction with the alpha
 *        \p alpha and \p beta scale factors on every element of input tensor
 *        \p input by the dimension axis \p axis, and places the result into
 *        the corresponding element of output tensor \p output.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the scaledtanh operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] axis
 *   Input. An int number that specifies which dimension of input tensor is used in
 *   the operation. Users can use the specified axis value or vector to perform
 *   scaledtanh calculations across the whole input tensor.
 *   The axis value is in range of [0, total_number_of_input_dimensions -1]
 * @param[in] desc_input
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] desc_alpha
 *   Input. The descriptor of the alpha tensor that is used to scale the input tensor.
 * @param[in] alpha
 *   Input. Pointer to the MLU memory that stores the alpha tensor.
 * @param[in] desc_beta
 *   Input. The descriptor of the beta tensor that is used to scale the result of
 *   tanh(alpha * input).
 * @param[in] beta
 *   Input. Pointer to the MLU memory that stores the beta tensor.
 * @param[in] desc_output
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "ScaledTanh Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensor \p input, alpha tensor \p alpha, beta tensor \p
 *   beta and output tensor \p output must be the same.
 * - The supported data types of input, alpha, beta and output tensors are as follows:
 *   - input tensor: half, float.
 *   - alpha tensor: half, float.
 *   - beta tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - This function supports multi-dimension.
 * - The supported data layout of input, alpha, beta, output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor must have the same shape.
 *
 * @note
 * - If the values of alpha tensor and beta tensor are set to 1.0, the Tanh function is performed.
 * - This operation is not supported on the 1V platforms.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the scaledtanh operation is as follows:
     @verbatim
      input: an array[dim0, dim1, dim2, dim3, ...]
      axis: 2 (assume axis is 2, which marks operating 3th dimension)
      alpha: [1, 1, 1, 1, ...] or [1, 1, dim2, 1, ...], dim2 is marked dimension.
      beta: [1, 1, 1, 1, ...] or [1, 1, dim2, 1, ...], dim2 is marked dimension.
      output: an array[dim0, dim1, dim2, dim3, ...] same as input and
              output[dim0, dim1, dim2, dim3, ...] = tanh(alpha[1, 1, dim2, 1, ...] *
              input[dim0, dim1, dim2, dim3, ...]) * beta[1, 1, dim2, 1, ...]
     @endverbatim
 *
 * @par Reference
 * - https://lasagne.readthedocs.io/en/latest/modules/nonlinearities.html#lasagne.nonlinearites.ScaledTanH
 * - https://docs.microsoft.com/en-us/windows/win32/api/directml/ns-directml-dml_active_scaled_tanh_operator_desc
 *
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlScaledTanh(cnnlHandle_t handle,
                                         const int axis,
                                         const cnnlTensorDescriptor_t desc_input,
                                         const void *input,
                                         const cnnlTensorDescriptor_t desc_alpha,
                                         const void *alpha,
                                         const cnnlTensorDescriptor_t desc_beta,
                                         const void *beta,
                                         const cnnlTensorDescriptor_t desc_output,
                                         void *output);
// Group:UnarySelect
/*!
 *  @brief Selects elements in one tensor according to the corresponding values in another tensor. The
 *   elements in \p input will be selected if the corresponding elements in \p index are not equal to zero.
 *   The result composed of two parts. The first part is the number of selected elements which stored
 *   in \p number, and the second part is the selected elements which stored in \p output.
 *
 *  @deprecated
 *   This function is deprecated and will be removed in future release.
 *    Use ::cnnlMasked_v4 instead. The masked_mode should be set to \p CNNL_MASKED_SELECT.
 *    ::cnnlMasked_v4 supports the masked_mode parameter that determines how to compute.
 *
 *  @param[in]  handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *    queues in the index select operation. For detailed information, see ::cnnlHandle_t.
 *  @param[in]  input_desc
 *    Input. The descriptors of the \p index tensors. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[in]  input
 *    Input. Pointer to the MLU memory that stores the input tensor.
 *  @param[in]  index_desc
 *    Input. The descriptors of the \p index tensors. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[in]  index
 *    Input. Pointer to the MLU memory that stores the index data.
 *  @param[in]  output_desc
 *    Input. The descriptors of the \p output tensors. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[in]  output
 *    Output. Pointer to the MLU memory that stores the output tensor.
 *  @param[in]  number
 *    Input. Pointer to the MLU memory that stores the length of output data.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH.
 *
 *  @par Data Type
 *  - \p input: float, half, int8, int16, int32.
 *  - \p index: bool, float, half.
 *  - \p output: float, half, int8, int16, int32.
 *  - \p number: int32.
 *  - When the data type of index tensor is not bool, the data type of input tensor must
 *    be the same as the data type of the index tensor.
 *
 *  @par Scale Limitation
 *  - None.
 *
 *  @par API Dependency
 *  - None.
 *
 *  @note
 *  - The output memory size should keep same as input data size. The selected output data
 *    keep the same sequence with the input data.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - The example of the index select operation is as follows:
      @verbatim
      input array
        input = [[0, 1, 2, 3],
                 [4, 5, 6, 7],
                 [8, 9, 10, 11]]
      index array
        index = [[0, 1, 0, 0],
                 [0, 0, 0, 1],
                 [1, 0, 0, 0]]
      output array
        output = [1, 7, 8]
      number value
        number = 3
      @endverbatim
 *
 */

CNNL_DEPRECATED_FOR(cnnlMasked_v4)
cnnlStatus_t CNNL_WIN_API cnnlUnarySelect(cnnlHandle_t handle,
                                          const cnnlTensorDescriptor_t input_desc,
                                          const void *input,
                                          const cnnlTensorDescriptor_t index_desc,
                                          const void *index,
                                          const cnnlTensorDescriptor_t output_desc,
                                          void *output,
                                          uint32_t *number);

// Group:IsFinite
/*!
 *  @brief Judges every element in \p input tensor is finite number or not, the element
 *    in \p output tensor is Boolean value.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *    queues in the index select operation. For detailed information, see ::cnnlHandle_t.
 *  @param[in] x_desc
 *    Input. The descriptors of the \p input tensors. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[in] x
 *    Input. Pointer to the MLU memory that stores the input tensor.
 *  @param[in] y_desc
 *    Input. The descriptors of the \p output tensors. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[out] y
 *    Output. Pointer to the MLU memory that stores the output tensor. The element
 *    in \p output tensor is Boolean value.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED

 *  @par Data Type
 *  - \p input: float, half, bfloat16 (bfloat16 is only supported on MLU500 series).
 *  - \p output: bool.
 *
 *  @par Scale Limitation
 *  - None.
 *
 *  @par Reference
 *  - https://pytorch.org/docs/master/generated/torch.isfinite.html
 *
 *  @par API Dependency
 *  - None.
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - The example of the index select operation is as follows:
      @verbatim
      input array
        input = [0, 1, 2, 100000000000],
      output array
        output = [true, true, true, false],
      @endverbatim
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlIsFinite(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t x_desc,
                                       const void *x,
                                       const cnnlTensorDescriptor_t y_desc,
                                       void *y);

// Group:IsInf
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 *   workspace to optimize the ::cnnlIsInf operation.
 *   The size of the extra workspace is based on the given information of the Isinf operation.
 *   For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the IsInf
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] mode
 *   Input. The detection and output mode of isinf.
 *   See ::cnnlIsInfMode_t for details.
 * @param[in] reduce
 *   Input. A Boolean value indicating whether to reduce the dimension of output.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in the
 *   IsInf operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetIsinfWorkspaceSize(cnnlHandle_t handle,
                                                    const cnnlIsInfMode_t mode,
                                                    const bool reduce,
                                                    size_t *workspace_size);

// Group:IsInf
/*!
 *  @brief Judges every element in \p input tensor is negative/positive infinite number or not,
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *    queues in the isinf operation. For detailed information, see ::cnnlHandle_t.
 *  @param[in] input_desc
 *    Input. The descriptor of the \p input tensors. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[in] input
 *    Input. Pointer to the MLU memory that stores the input tensor.
 *  @param[in] mode
 *    Input. The detection and output mode of isinf.
 *    See ::cnnlIsInfMode_t for details.
 *  @param[in] reduce
 *    Input. A Boolean value indicating whether to reduce the dimension of output.
 *    If \p reduce is false, the shape of \p output is the same as input; If \p reduce is
 *    true, reduce the dimensions of \p output to 1D with one element.
 *      - When \p mode=CNNL_POS_INF and \p reduce= true: outputs 1D tensor containing only
 *        one "true" element if all elements in \p input are positive infinity. Otherwise,
 *        return 1D tensor containing one "false" element.
 *      - When \p mode=CNNL_NEG_INF and \p reduce= true: outputs 1D tensor containing only
 *        one "true" element if all elements in \p input are negative infinity. Otherwise,
 *        return 1D tensor containing one "false" element.
 *      - When \p mode = NNL_INF_INDICATOR and \p reduce= true:
 *        Output "1" if all elements in \p input are negative infinity;
 *        Output "2" if all elements in \p input are positive infinity;
 *        Output "3" if the \p input contains both negative and positive infinity;
 *        Output "0" if the \p input contains neither negative infinity nor positive infinity.
 *  @param[in]  workspace
 *    Input. Pointer to the MLU memory that is used as an extra workspace for the IsInf
 *    operation.
 *    only when \p mode is CNNL_INF_INDICATOR and \p reduce is true, workspace is required.
 *  @param[in] workspace_size
 *    Input. The size of the extra workspace in bytes that needs to be used in the IsInf
 *    operation when \p reduce is true. You can get the size of the workspace with the
 *    ::cnnlGetIsinfWorkspaceSize function.
 *  @param[in] output_desc
 *    Input. The descriptor of the \p output tensors. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[out] output
 *    Output. Pointer to the MLU memory that stores the \p output tensor. The element
 *    in \p output tensor is Boolean or indicator value.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *  @par Data Type
 *   - This function supports any combinations of the following data types for the \p input, \p reduce and \p output tensors:
 *     - \p input: float, half, bfloat16.
 *     - \p reduce: bool.
 *     - \p output: bool, int8.
 *
 *     The bfloat16 data type is supported only on MLU500 series.
 *
 *  @par Scale Limitation
 *  - None.
 *
 *  @par API Dependency
 *  - Before calling this function, you need to get the size of workspace by the ::cnnlGetIsinfWorkspaceSize function
 *  and pass the allocated extra workspace to the function.
 *
 *  @note
 *  - The in-place operation is not supported.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - The examples of the isinf operation are as follows:
      @verbatim
        input{9, -inf, 2, nan, +inf} mode = CNNL_NEG_INF, reduce = false; output{false, true, false, false, false}
        input{9, -inf, 2, nan, +inf} mode = CNNL_POS_INF, reduce = false; output{false, false, false, false, true}
        input{9, -inf, 2, nan, +inf} mode = CNNL_INF, reduce = false; output{false, true, false, false, true}
        input{9, -inf, 2, nan, +inf} mode = CNNL_NEG_INF, reduce = true; outputtrue
        input{9, -inf, 2, nan, +inf} mode = CNNL_POS_INF, reduce = true; outputtrue
        input{9, -inf, 2, nan, +inf} mode = CNNL_INF, reduce = true;  outputtrue
        input{9, -inf, 2, nan, +inf} mode = CNNL_INF_INDICATOR, reduce = false; output{0, 1, 0, 0, 2}
        input{9, -inf, 2, nan, +inf} mode = CNNL_INF_INDICATOR, reduce = true;  output3
      @endverbatim
 *
 *  @par Reference
 *  - https://pytorch.org/docs/1.9.0/generated/torch.isinf.html?highlight=isinf#torch.isinf
 *  - https://tensorflow.google.cn/api_docs/python/tf/math/is_inf
 */
cnnlStatus_t CNNL_WIN_API cnnlIsInf(cnnlHandle_t handle,
                                    const cnnlTensorDescriptor_t input_desc,
                                    const void *input,
                                    const cnnlIsInfMode_t mode,
                                    const bool reduce,
                                    void *workspace,
                                    size_t workspace_size,
                                    const cnnlTensorDescriptor_t output_desc,
                                    void *output);

// Group:IndexFill
/*!
 * @brief Fills the elements of the input with value by selecting the indices in the order
 * given by index. This function supports in-place operation when the pointer to the input
 * and output tensors are the same tensor.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlIndexFill_v3 instead, which supports the parameter \p value_desc that sets \p value
 *   to host pointer or device pointer.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] dim
 *   Input. The data of int type to indicate which dimension to be selected to fill.
 * @param[in] value
 *   Input. The data of float type to indicate the value to fill with.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. A device pointer to the MLU memory that stores the input tensor.
 * @param[in] index_desc
 *   Input. The descriptor of the index tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] index
 *   Input. A device pointer to the MLU memory that stores the index tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. A device pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "IndexFill Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input and output tensors.
 *   - input tensor: uint8, int8, uint16, int16, uint32, int32, bool, half, float, bfloat16, int64.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int32, bool, half, float, bfloat16, int64.
 *   Note that the data type of output should be the same as input.
 *
 * @par Data Layout
 * - Data layouts of input tensor and output tensor must be the same.
 *
 * @par Scale Limitation
 * - Each dimension of the input tensor must be the same as that of the output tensor.
 * - The shape of input tensor and output tensor must be the same.
 * - The index tensor must be 1D tensor.
 * - The data type of \p index must be CNNL_DTYPE_INT32 or CNNL_DTYPE_INT64.
 * - The value of \p dim should be in range of [-input_dim_size, input_dim_size -1]
 *   where \p input_dim_size is the size of input dimension.
 * - The value of each element in \p index should be in range of the value of the
 *   dimension indicated by the \p dim. Negative dim is not supported.
 *   Because the \p index is a device pointer to the MLU memory, this operation does not
 *   do the check function on CPU. Users need to guarantee that the value of each element
 *   in \p index in the correct range.
 * - When the data type of input tensor and output tensor is \p CNNL_DTYPE_HALF, there may
 *   be a precision error. This error is caused by data type conversion from float32 to
 *   float16 on param \p value.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set this operation to be in-place.
 *
 * @note
 * - You can specify the stride of all dimensions for input_desc, index_desc and
 *   output_desc with ::cnnlSetTensorDescriptorEx.
 * - The value of \p index_desc element of in64_t type should be in range of [-2^47, 2^47-1].
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
       Input tensor  :   [[1, 2, 3],
                          [4, 5, 6],
                          [7, 8, 9]]

       Dim           :   1

       Value         :   -1

       Index tensor  :   [0,2]

       Output tensor :   [[-1, 2, -1],
                          [-1, 5, -1],
                          [-1, 8, -1]]
     @endverbatim
 *
 */
CNNL_DEPRECATED_FOR(cnnlIndexFill_v2)
cnnlStatus_t CNNL_WIN_API cnnlIndexFill(cnnlHandle_t handle,
                                        const int dim,
                                        const float value,
                                        const cnnlTensorDescriptor_t input_desc,
                                        const void *input,
                                        const cnnlTensorDescriptor_t index_desc,
                                        const void *index,
                                        const cnnlTensorDescriptor_t output_desc,
                                        void *output);

// Group:IndexFill
/*!
 * @brief Fills the elements of the input with value by selecting the indices in the order
 * given by index. This function supports in-place operation when the pointer to the input
 * and output tensors are the same tensor.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlIndexFill_v3 instead, which supports the parameter \p value_desc that sets \p value
 *   to host pointer or device pointer.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] dim
 *   Input. The data of int type to indicate which dimension to be selected to fill.
 * @param[in] pointer_mode
 *   Input.  An enum value that indicates that the scalar value \p value is
 *   passed by reference on the host or device. The information is defined in ::cnnlPointerMode_t.
 * @param[in] value
 *   Input.  A pointer to the value to fill with.
 *   If the \p pointer_mode is \p CNNL_POINTER_MODE_DEVICE, the \p value should be a device pointer.
 *   If the \p pointer_mode is \p CNNL_POINTER_MODE_HOST, the \p value should be a host pointer.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. A device pointer to the MLU memory that stores the input tensor.
 * @param[in] index_desc
 *   Input. The descriptor of the index tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] index
 *   Input. A device pointer to the MLU memory that stores the index tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. A device pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "IndexFill Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input and output tensors.
 *   - input tensor: uint8, int8, uint16, int16, uint32, int32, bool, half, float, bfloat16, int64.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int32, bool, half, float, bfloat16, int64.
 *
 * @par Data Layout
 * - Data layouts of input tensor and output tensor must be the same.
 *
 * @par Scale Limitation
 * - Each dimension of the input tensor must be the same as that of the output tensor.
 * - The shape of input tensor and output tensor must be the same.
 * - The index tensor must be 1D tensor.
 * - The data type of \p index must be CNNL_DTYPE_INT32 or CNNL_DTYPE_INT64.
 * - The value of \p dim should be in range of [-input_dim_size, input_dim_size -1]
 *   where \p input_dim_size is the size of input dimension.
 * - Using dimension_size to indicate the value of the dimension indicated by the \p dim,
 *   the value of each element in \p index should be in range of [-dimension_size, dimension_size - 1].
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set this operation to be in-place.
 *
 * @note
 * - Data type of \p value, input tensor \p input and output tensor \p output should be the same.
 * - The number of elements of \p value only supports one.
 * - You can specify the stride of all dimensions for input_desc, index_desc and
 *   output_desc with ::cnnlSetTensorDescriptorEx.
 * - The value of \p index_desc element of in64_t type should be in range of [-2^47, 2^47-1].
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
       Input tensor  :   [[1, 2, 3],
                          [4, 5, 6],
                          [7, 8, 9]]

       Dim           :   1

       Value         :   -1

       Index tensor  :   [-3,2]

       Output tensor :   [[-1, 2, -1],
                          [-1, 5, -1],
                          [-1, 8, -1]]
     @endverbatim
 *
 */
CNNL_DEPRECATED_FOR(cnnlIndexFill_v3)
cnnlStatus_t CNNL_WIN_API cnnlIndexFill_v2(cnnlHandle_t handle,
                                           const int dim,
                                           const cnnlPointerMode_t pointer_mode,
                                           const void *value,
                                           const cnnlTensorDescriptor_t input_desc,
                                           const void *input,
                                           const cnnlTensorDescriptor_t index_desc,
                                           const void *index,
                                           const cnnlTensorDescriptor_t output_desc,
                                           void *output);

// Group:IndexFill
/*!
 * @brief Fills the elements of the input with value by selecting the indices in the order
 * given by index. This function supports in-place operation when the pointer to the input and output tensors is the same tensor.
 *
 * Compared with ::cnnlIndexFill, this function supports the parameter \p value_desc
 * that sets \p value to host pointer or device pointer, passes the parameter \p value
 * with void* pointer and supports negative index. Compared with ::cnnlIndexFill_v2,
 * this function supports the parameter \p value_desc that sets \p value to host pointer
 * or device pointer instead of \p pointer_mode.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] dim
 *   Input. The dimension to be selected to fill.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. A device pointer to the MLU memory that stores the input tensor.
 * @param[in] index_desc
 *   Input. The descriptor of the index tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] index
 *   Input. A device pointer to the MLU memory that stores the index tensor.
 * @param[in] value_desc
 *   Input. The descriptor of the value tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] value
 *   Input. A pointer to the MLU memory or host memory that stores the value to fill with.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. A device pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "IndexFill Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input and output tensors.
 *   - input tensor: uint8, int8, uint16, int16, uint32, int32, bool, half, float, bfloat16, int64.
 *   - value tensor: uint8, int8, uint16, int16, uint32, int32, bool, half, float, bfloat16, int64.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int32, bool, half, float, bfloat16, int64.
 *   - Data type of \p value, input tensor \p input and output tensor \p output should be the same.
 *
 * @par Data Layout
 * - Data layouts of input tensor and output tensor must be the same.
 *
 * @par Scale Limitation
 * - Each dimension of the input tensor must be the same as that of the output tensor.
 * - The shape of input tensor and output tensor must be the same.
 * - The index tensor must be 1D tensor.
 * - The data type of \p index must be CNNL_DTYPE_INT32 or CNNL_DTYPE_INT64.
 * - The value of \p dim should be in range of [-input_dim_size, input_dim_size -1]
 *   where \p input_dim_size is the size of input dimension.
 * - Using dimension_size to indicate the value of the dimension indicated by the \p dim,
 *   the value of each element in \p index should be in range of [-dimension_size, dimension_size - 1].
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set this operation to be in-place.
 *
 * @note
 * - The number of elements of \p value only supports one.
 * - You can specify the stride of all dimensions for input_desc, index_desc and
 *   output_desc with ::cnnlSetTensorDescriptorEx.
 * - The value of \p index_desc element of in64_t type should be in range of [-2^47, 2^47-1].
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
       Input tensor  :   [[1, 2, 3],
                          [4, 5, 6],
                          [7, 8, 9]]

       Dim           :   1

       Value         :   -1

       Index tensor  :   [-3,2]

       Output tensor :   [[-1, 2, -1],
                          [-1, 5, -1],
                          [-1, 8, -1]]
     @endverbatim
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlIndexFill_v3(cnnlHandle_t handle,
                                           const int dim,
                                           const cnnlTensorDescriptor_t input_desc,
                                           const void *input,
                                           const cnnlTensorDescriptor_t index_desc,
                                           const void *index,
                                           const cnnlTensorDescriptor_t value_desc,
                                           const void *value,
                                           const cnnlTensorDescriptor_t output_desc,
                                           void *output);

// Group:Hardtanh
/*!
 * @brief Computes hardtanh on input tensor \p x by the maximum value \p max_val and minimum value
 *        \p min_val of the linear range, and returns the result in the output tensor \p y.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the hardtanh
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] max_val
 *   Input. A float value used as the maximum value of the linear range.
 * @param[in] min_val
 *   Input. A float value used as the minimum value of the linear range.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Handtanh Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor \p x and output tensor \p y must be the same. The supported data types of
 *   input tensor \p x and output tensor \p y are as follows:
 *   - input tensor: half, float, bfloat16.
 *   - output tensor: half, float, bfloat16.
 *
 *   bfloat16 is only supported on MLU500 series.
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - http://pytroch.org/docs/master/nn.functional.html#torch.nn.functional.hardtanh
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlHardtanh(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t x_desc,
                                       const void *x,
                                       const float max_val,
                                       const float min_val,
                                       const cnnlTensorDescriptor_t y_desc,
                                       void *y);

// Group:HardtanhBackward
/*!
 * @brief Computes gradient of hardtanh on input tensor \p x and \p diff_y by the maximum value
 *        \p max_val and minimum value \p min_val of the linear range, and returns the result
 *        in the output tensor \p diff_x.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   hardtanh_backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] diff_y_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] diff_y
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] max_val
 *   Input. A float value used as the maximum value of the linear range.
 * @param[in] min_val
 *   Input. A float value used as the minimum value of the linear range.
 * @param[in] diff_x_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "HandtanhBackward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensors \p x and \p diff_y must be the same as the data type output tensor
 *   \p diff_x. The supported data types of input tensors and output tensor are as follows:
 *   - input tensors: half, float, bfloat16.
 *   - output tensor: half, float, bfloat16.
 *
 *   bfloat16 is only supported on MLU500 series.
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - http://pytroch.org/docs/master/n.functional.html#torch.nn.functional.hardtanh_backward
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlHardtanhBackward(cnnlHandle_t handle,
                                               const cnnlTensorDescriptor_t x_desc,
                                               const void *x,
                                               const cnnlTensorDescriptor_t diff_y_desc,
                                               const void *diff_y,
                                               const float max_val,
                                               const float min_val,
                                               const cnnlTensorDescriptor_t diff_x_desc,
                                               void *diff_x);

// Group:WeightNorm
/*!
 *  @brief Performs the filter normalization operator computation.
 *
 *  Filter normalization is a reparameterization that decouples the magnitude of
 *  the filter tensor from its direction. The parameter \p filter in network layer is
 *  replaced by magnitude (g) and direction (v).
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] axis
 *   Input. Dimension of filter over which to compute normalization in network layer.
 * @param[in] v_desc
 *   Input. Descriptor of \p v that is direction of filter in network layer.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] v
 *   Input. Pointer to the MLU memory that stores the \p v tensor.
 * @param[in] g_desc
 *   Input. Descriptor of \p g that is magnitude of filter in network layer.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] g
 *   Input. Pointer to the MLU memory that stores the \p g tensor.
 * @param[in] w_desc
 *   Input. Descriptor of \p w that is filter in network layer.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] w
 *   Output. Pointer to the MLU memory that stores the \p w tensor.
 * @param[in] norm_recip_desc
 *   Input. Descriptor of \p norm_recip that is intermediate data for
 *   weightnorm backward. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] norm_recip
 *   Output. Pointer to the MLU memory that stores the \p norm_recip tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - Data types of input tensors and output tensors must be the same.
 * - The supported combinations of data types are as follows in the order of
 *   \p v - \p g - \p w - \p norm_recip:
 *   - half - half - half - half
 *   - half - half - half - float
 *   - float - float - float - float
 *   - bfloat16 - bfloat16 - bfloat16 - bfloat16
 *   - bfloat16 - bfloat16 - bfloat16 - float
 *
 *   The bfloat16 data type is supported only on MLU500 series.
 *
 * @note
 * - Assume the dimension of \p v is dim, \p axis can only be set to 0 or dim -1 currently.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - http://arxiv.org/abs/1602.07868v3
 */

cnnlStatus_t CNNL_WIN_API cnnlWeightNorm(cnnlHandle_t handle,
                                         const int axis,
                                         const cnnlTensorDescriptor_t v_desc,
                                         const void *v,
                                         const cnnlTensorDescriptor_t g_desc,
                                         const void *g,
                                         const cnnlTensorDescriptor_t w_desc,
                                         void *w,
                                         const cnnlTensorDescriptor_t norm_recip_desc,
                                         void *norm_recip);

// Group:SparseSoftmaxCrossEntropyWithLogits
/*!
 * @brief Computes sparse softmax cross-entropy and the gradients with inputs
 * and index of sparse labels.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in ::cnnlSparseSoftmaxCrossEntropyWithLogits operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] mode
 *   Input. The specific algorithm used to determine which dimension the operation would be
 *   performed.
 *   The algorithms are defined in ::cnnlSoftmaxMode_t enum.
 * @param[in] x_desc
 *   Input. The descriptor of logits tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the logits tensor.
 *   The value of \p x is per-label activation, indicating the probabilities of the
 *   corresponding output from the previous layer in a artificial intelligence.
 * @param[in] idx_desc
 *   Input. The descriptor of index tensor that determines the sparse label.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] idx
 *   Input. Pointer to the MLU memory that stores the index tensor.
 *   For the i-th batch, the probability of the idx[i]-th class is 1.0, while the probabilities of
 *   other classes are 0.0.
 * @param[in] y_desc
 *   Input. The descriptor of softmax cross-entropy tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the softmax cross-entropy tensor.
 * @param[in] diff_y_desc
 *   Input. The descriptor of the gradients tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] diff_y
 *   Output. Pointer to the MLU memory that stores the gradients tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED.
 *
 * @par Formula
 * - See "SparseSoftmaxCrossEntropyWithLogits Operator" section in "Cambricon CNNL User Guide"
 *   for details.
 *
 * @par Data Type
 *  - This function supports the following data types for \p x, \p idx, \p y, and \p diff_y:
 *    - \p x: half, float.
 *    - \p idx: int32.
 *    - \p y: half, float.
 *    - \p diff_y: half, float.
 *  - Data type of \p x, \p y, and \p diff_y should be the same.
 *
 * @par Data Layout
 * - The supported data layouts of \p x, \p idx, \p y, \p diff_y tensor are as follows:
 *   - x tensor: \p CNNL_LAYOUT_ARRAY.
 *   - idx tensor: \p CNNL_LAYOUT_ARRAY.
 *   - y tensor: \p CNNL_LAYOUT_ARRAY.
 *   - diff_y tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - Only \p CNNL_SOFTMAX_MODE_LOW_DIMENSION of \p mode is supported.
 * - The shape of \p x should be [high_dim, mid_dim, low_dim] or [high_dim, low_dim].
 * - If \p mode is set to \p CNNL_SOFTMAX_MODE_LOW_DIMENSION:
 *   - If the shape of \p x is [high_dim, mid_dim, low_dim],
 *     the shape of \p idx should be [high_dim, mid_dim].
 *   - If the shape of \p x is [high_dim, low_dim], the shape of \p idx should be [high_dim].
 * - The shape of \p y and \p idx should be the same.
 * - The shape of \p diff_y and \p x should be the same.
 * - |x - x_max| of \p x is recommended to be in range of [-15.5, 0] for higher precision,
 *   where x represents each value of \p x and x_max represents the maximum value of \p x along the
 *   specified dimension in \p mode.
 * - Half data type is not recommended as the precision cannot be ensured for large amount of data.
 * - For half data type, the value of \p x is recommended to be in range of [-1, 1], and the size
 *   of \p low_dim of \p x is recommended to be less than 128 bytes for higher precision.
 * - The sum of \p exp(x - x_max) along the specified dimension in \p mode should be in the value
 *   range of the corresponding data type.
 * - When \p low_dim = 1, the value of \p idx should be in range of [\f$-2^{23}\f$, \f$2^{23}\f$)] on MLU200 series.
 *
 * @par API Dependency
 * - None.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance,
 *   the \p mode is recommended to be \p CNNL_SOFTMAX_MODE_LOW_DIMENSION.
 *
 * @note
 * - When \p x contains NaN or infinity:
 *   - On MLU200 series:
 *     - NaN in \p x is considered as positive saturation value.
 *     - Positive infinity in \p x is considered as positive saturation value.
 *     - Negative infinity in \p x is considered as negative saturation value.
 *     - If the lowest dimension of \p x is 1, then \p y and \p diff_y are 0.
 *   - On MLU300 series and CE3226:
 *     - If the lowest dimension of \p x is 1, then \p y and \p diff_y are 0.
 *
 * @par Requirements
 * - The value of \p idx should be in range of [0, low_dim), otherwise the corresponding \p y and
 *   \p diff_y will be NaN.
 *
 * @par Example
 * - The example of the SparseSoftmaxCrossEntropyWithLogits operation is as follows:
     @verbatim
      input two arrays by 3 * 4 and 3 -->
          x: [[1, 2, 3, 4],
              [4, 3, 2, 1],
              [1, 4, 3, 2]]

      --> idx: [0, 1, 2]

      param:
        mode: CNNL_SOFTMAX_MODE_LOW_DIMENSION

      output two arrays by 3 and 3 * 4 -->
          y: [3.44019, 1.44019, 2.44019]

      --> diff_y: [[-0.96794, 0.08714, 0.23688, 0.64391],
                   [0.64391, -0.76312, 0.08714, 0.03206],
                   [0.03206, 0.64391, -0.76312, 0.08714]]
      @endverbatim
 *
 * @par Reference
 * - http://www.tensorflow.org/api_doc/python/tf/nn/sparse_softmax_cross_entropy_with_logits
 */

cnnlStatus_t CNNL_WIN_API
cnnlSparseSoftmaxCrossEntropyWithLogits(cnnlHandle_t handle,
                                        cnnlSoftmaxMode_t mode,
                                        const cnnlTensorDescriptor_t x_desc,
                                        const void *x,
                                        const cnnlTensorDescriptor_t idx_desc,
                                        const void *idx,
                                        const cnnlTensorDescriptor_t y_desc,
                                        void *y,
                                        const cnnlTensorDescriptor_t diff_y_desc,
                                        void *diff_y);

// Group:SparseSoftmaxCrossEntropyWithLogits
/*!
 * @brief Computes sparse softmax cross-entropy and the gradients with inputs
 * and index of sparse labels.
 *
 * When \p prefer is set to ::CNNL_COMPUTATION_HIGH_PRECISION, this function has the same function
 * with ::cnnlSparseSoftmaxCrossEntropyWithLogits.
 *
 * @deprecated
 * This function is deprecated and will be removed in future
 *   release. Use ::cnnlSparseSoftmaxCrossEntropyWithLogits instead, which does
 *   not support the \p prefer parameter.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in ::cnnlSparseSoftmaxCrossEntropyWithLogits_v2 operation.
 * @param[in] prefer
 *   Input. The algorithm used to compute the output. For detailed information,
 *   see ::cnnlComputationPreference_t.
 * @param[in] mode
 *   Input. The specific algorithm used to determine which dimension the operation would be
 *   performed.
 *   The algorithms are defined in ::cnnlSoftmaxMode_t enum.
 * @param[in] x_desc
 *   Input. The descriptor of logits tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the logits tensor.
 *   The value of \p x is per-label activation, indicating the probabilities of the
 *   corresponding output from the previous layer in a artificial intelligence.
 * @param[in] idx_desc
 *   Input. The descriptor of index tensor that determines the sparse label.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] idx
 *   Input. Pointer to the MLU memory that stores the index tensor.
 *   For the i-th batch, the probability of the idx[i]-th class is 1.0, while the probabilities of
 *   other classes are 0.0.
 * @param[in] y_desc
 *   Input. The descriptor of softmax cross-entropy tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the softmax cross-entropy tensor.
 * @param[in] diff_y_desc
 *   Input. The descriptor of the gradients tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] diff_y
 *   Output. Pointer to the MLU memory that stores the gradients tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED.
 *
 * @par Formula
 * - See "SparseSoftmaxCrossEntropyWithLogits Operator" section in "Cambricon CNNL User Guide"
 *   for details.
 *
 * @par Data Type
 *  - This function supports the following data types for \p x, \p idx, \p y, and \p diff_y:
 *    - \p x: half, float.
 *    - \p idx: int32.
 *    - \p y: half, float.
 *    - \p diff_y: half, float.
 *  - Data type of \p x, \p y, and \p diff_y should be the same.
 *
 * @par Data Layout
 * - The supported data layouts of \p x, \p idx, \p y, \p diff_y tensor are as follows:
 *   - x tensor: \p CNNL_LAYOUT_ARRAY.
 *   - idx tensor: \p CNNL_LAYOUT_ARRAY.
 *   - y tensor: \p CNNL_LAYOUT_ARRAY.
 *   - diff_y tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - Only \p CNNL_SOFTMAX_MODE_LOW_DIMENSION of \p mode is supported.
 * - The shape of \p x should be [high_dim, mid_dim, low_dim] or [high_dim, low_dim].
 * - If \p mode is set to \p CNNL_SOFTMAX_MODE_LOW_DIMENSION:
 *   - If the shape of \p x is [high_dim, mid_dim, low_dim],
 *     the shape of \p idx should be [high_dim, mid_dim].
 *   - If the shape of \p x is [high_dim, low_dim], the shape of \p idx should be [high_dim].
 * - The shape of \p y and \p idx should be the same.
 * - The shape of \p diff_y and \p x should be the same.
 * - |x - x_max| of \p x is recommended to be in range of [-15.5, 0] for higher precision,
 *   where x represents each value of \p x and x_max represents the maximum value of \p x along the
 *   specified dimension in \p mode.
 * - Half data type is not recommended as the precision cannot be ensured for large amount of data.
 * - For half data type, the value of \p x is recommended to be in range of [-1, 1], and the size
 *   of \p low_dim of \p x is recommended to be less than 128 bytes for higher precision.
 * - The sum of \p exp(x - x_max) along the specified dimension in \p mode should be in the value
 *   range of the corresponding data type.
 * - When \p low_dim = 1, the value of \p idx should be in range of [\f$-2^{23}\f$, \f$2^{23}\f$] on MLU200 series.
 *
 * @par API Dependency
 * - None.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance,
 *   the \p mode is recommended to be \p CNNL_SOFTMAX_MODE_LOW_DIMENSION.
 *
 * @note
 * - When \p x contains NaN or infinity:
 *   - On MLU200 series:
 *     - NaN in \p x is considered as positive saturation value.
 *     - Positive infinity in \p x is considered as positive saturation value.
 *     - Negative infinity in \p x is considered as negative saturation value.
 *     - If the lowest dimension of \p x is 1, then \p y and \p diff_y are 0.
 *   - On MLU300 series and CE3226:
 *     - If the lowest dimension of \p x is 1, then \p y and \p diff_y are 0.
 *
 * @par Requirements
 * - The value of \p idx should be in range of [0, low_dim), otherwise the corresponding \p y and
 *   \p diff_y will be NaN.
 *
 * @par Example
 * - The example of the ::cnnlSparseSoftmaxCrossEntropyWithLogits_v2 operation is as follows:
     @verbatim
      input two arrays by 3 * 4 and 3 -->
          x: [[1, 2, 3, 4],
              [4, 3, 2, 1],
              [1, 4, 3, 2]]

      --> idx: [0, 1, 2]

      param:
        mode: CNNL_SOFTMAX_MODE_LOW_DIMENSION

      output two arrays by 3 and 3 * 4 -->
          y: [3.44019, 1.44019, 2.44019]

      --> diff_y: [[-0.96794, 0.08714, 0.23688, 0.64391],
                   [0.64391, -0.76312, 0.08714, 0.03206],
                   [0.03206, 0.64391, -0.76312, 0.08714]]
      @endverbatim
 *
 * @par Reference
 * - http://www.tensorflow.org/api_doc/python/tf/nn/sparse_softmax_cross_entropy_with_logits
 */

CNNL_DEPRECATED_FOR(cnnlSparseSoftmaxCrossEntropyWithLogits)
cnnlStatus_t CNNL_WIN_API
cnnlSparseSoftmaxCrossEntropyWithLogits_v2(cnnlHandle_t handle,
                                           const cnnlComputationPreference_t prefer,
                                           const cnnlSoftmaxMode_t mode,
                                           const cnnlTensorDescriptor_t x_desc,
                                           const void *x,
                                           const cnnlTensorDescriptor_t idx_desc,
                                           const void *idx,
                                           const cnnlTensorDescriptor_t y_desc,
                                           void *y,
                                           const cnnlTensorDescriptor_t diff_y_desc,
                                           void *diff_y);

// Group:Diag
/*!
 * @brief Returns in \p output the square matrix where the input tensor \p input is the diagonal
 *        \p k, or the k-th diagonal of a given matrix.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the diag
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] k
 *   Input. The diagonal used in this operation.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Diag Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \p input and output tensor
 *   \p output. Data type of input tensor and output tensor should be the same.
 *   - input tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, bfloat16, float.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, bfloat16, float.
 *
 * @par Scale Limitation
 * - None.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the diag operation is as follows:
     @verbatim
     input array by 3 * 3 --> input: [[1,2,3],[4,5,6],[7,8,9]]

     param:
       k: 1

     output array by 1 * 2 --> output: [[2,6]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.diag.html
 */
cnnlStatus_t CNNL_WIN_API cnnlDiag(cnnlHandle_t handle,
                                   const int k,
                                   const cnnlTensorDescriptor_t input_desc,
                                   const void* input,
                                   const cnnlTensorDescriptor_t output_desc,
                                   void *output);

// Group:Trace
/*!
 * @brief Returns the sum of the elements of the diagonal of the input 2-D matrix.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the Trace
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   Trace operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the Trace operation. You can get the size of the workspace with
 *   the ::cnnlGetTraceWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Trace Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \p input and output tensor
 *   \p output. Data type of input tensor and output tensor should be the same.
 *  - input tensor: int32, half, float, bfloat16.
 *  - output tensor: int32, half, float, bfloat16.
 *
 *   bfloat16 is only supported on MLU500 series.
 *
 * @par Scale Limitation
 * - On MLU300 series and MLU500 series:
 *  - The element number of input tensor should be positive and within int range (0, 2147483647].
 * - On CE3226 and 1V:
 *  - The size of the input tensor should be less than 600,000,000 bytes.
 *
 * @note
 * - The input tensor should be a 2D tensor, and the output tensor should be a 1D tensor.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the Trace operation is as follows:
     @verbatim
     input array by 3 * 2 --> input: [[1,2],[4,5],[7,8]]

     output array by 1 --> output: [6]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.trace.html
 */
cnnlStatus_t CNNL_WIN_API cnnlTrace(cnnlHandle_t handle,
                                    const cnnlTensorDescriptor_t input_desc,
                                    const void *input,
                                    void *workspace,
                                    const size_t workspace_size,
                                    const cnnlTensorDescriptor_t output_desc,
                                    void *output);

// Group:Trace
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace to
 * optimize the Trace operation.
 *
 * The size of the extra workspace is based on the given information of the Trace operation,
 * including the input tensor descriptor \p a_desc. For more information about the workspace,
 * see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   Trace operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. Descriptor of input data \p a, including dimension, data type (int32, half and float),
 *   and data layout.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *   the Trace operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetTraceWorkspaceSize(cnnlHandle_t handle,
                                                    const cnnlTensorDescriptor_t input_desc,
                                                    size_t *size);

// Group:Median
/*!
 * @brief Computes the median of the input tensor. If \p is_dim_None is true, \p dim is disabled,
 * and this function returns the median value of the input tensor flattened to one dimension.
 * Otherwise, \p dim is enabled, and this function returns the median values and the corresponding
 * indices of the input tensor in dimension \p dim.
 *
 * Generally the median is not unique for input tensor with an even number of elements. But in this
 * function, only the smaller of the two medians is returned when the input tensor has an even
 * number of elements.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlMedian_v2 instead, which supports \p ignore_nan.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the median
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_values_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output_values
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] output_indices_desc
 *   Input. The descriptor of the index tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output_indices
 *   Output. Pointer to the MLU memory that stores the index tensor.
 * @param[in] dim
 *   Input. An integer value that determines the dimension to get median.
 *   When \p is_dim_Node is true, \p dim will be ignored. otherwise, \p dim should in range [-dim, dim-1].
 * @param[in] is_dim_None
 *   Input. A Boolean value that determines whether \p dim is enabled.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH,
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Median Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \p input, output tensor
 *   \p output_values, and index tensor \p output_indices.
 *   Data type of input tensor and output tensor should be the same.
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *   - index tensor: int32.
 * - On MLU500 series or above, this function also supports bfloat16 data type for \p input and \p output.
 *
 * @par Scale Limitation
 * - The input tensor, output tensor, and index tensor must meet the following requirement:
 *   - When \p is_dim_None is false, the shape of \p output_values and \p output_indices should be
 *   the same.
 *   - When \p is_dim_None is false, the total size of \p input in dimension \p dim should be less
 *   than 167936 bytes.
 *   - When \p is_dim_None is true, the total size of \p input should be less than 671744 bytes.
 *
 * @note
 * - This operation is not supported on the 1V platforms.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.median.html
 */
CNNL_DEPRECATED_FOR(cnnlMedian_v2)
cnnlStatus_t CNNL_WIN_API
cnnlMedian(cnnlHandle_t handle,
           const cnnlTensorDescriptor_t input_desc,
           const void *input,
           const cnnlTensorDescriptor_t output_values_desc,
           void *output_values,
           const cnnlTensorDescriptor_t output_indices_desc,
           void *output_indices,
           const int dim,
           const bool is_dim_None);

// Group:Median
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace
 * in ::cnnlMedian_v2 operation.
 *
 * The size of the extra workspace is based on the given information of ::cnnlMedian_v2,
 * including the \p input tensor descriptor \p input_desc, the \p output_values tensor descriptor \p output_values_desc,
 * the \p output_indices tensor descriptor \p output_indices_desc,
 * the parameter \p dim, the parameter \p is_ignore_nan and the parameter \p is_dim_None.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in ::cnnlGetMedianWorkspaceSize. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] dim
 *   Input. An integer value that determines the dimension of input tensor to get the median element.
 * @param[in] is_dim_None
 *   Input. A Boolean value that determines whether \p dim is enabled.
 * @param[in] is_ignore_nan
 *   Input. A Boolean value that determines whether to ignore the NaN value in input. If true, find the median of all the non-NaN values,
 *   otherwise, return the first NaN value.
 * @param[in] input_desc
 *   Input. The descriptor of \p input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] output_values_desc
 *   Input. The descriptor of \p output_values tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] output_indices_desc
 *   Input. The descriptor of \p output_indices tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in
 *   ::cnnlMedian_v2 operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - None.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetMedianWorkspaceSize(cnnlHandle_t handle,
                           const int32_t dim,
                           const bool is_dim_None,
                           const bool is_ignore_nan,
                           const cnnlTensorDescriptor_t input_desc,
                           const cnnlTensorDescriptor_t output_values_desc,
                           const cnnlTensorDescriptor_t output_indices_desc,
                           size_t *size);

// Group:Median
/*!
 * @brief Computes the median of the input tensor. If \p is_dim_None is true, \p dim is disabled,
 * and this function returns the median value of the input tensor flattened to one dimension.
 * Otherwise, \p dim is enabled, and this function returns the median values and the corresponding
 * indices of the input tensor in dimension \p dim.
 * Compared with ::cnnlMedian function, ::cnnlMedian_v2 has parameter \p is_ignore_nan. If true, it finds the median of all the non-NaN values,
 * otherwise, it returns the first NaN value.
 *
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the median
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] dim
 *   Input. An integer value that determines the dimension of input tensor to get median element.
 * @param[in] is_dim_None
 *   Input. A Boolean value that determines whether \p dim is enabled.
 * @param[in] is_ignore_nan
 *   Input. A Boolean value that determines whether to ignore the NaN value in input. If true, find the median of all the non-NaN values.
 *   otherwise, return the first NaN value.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for
 *   ::cnnlMedian_v2. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   ::cnnlMedian_v2. You can get the size of the workspace with
 *   ::cnnlGetMedianWorkspaceSize function.
 * @param[in] output_values_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output_values
 *   Output. Pointer to the MLU memory that stores tensor for the median elements in the given dimension \p dim.
 * @param[in] output_indices_desc
 *   Input. The descriptor of the index tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output_indices
 *   Output. Pointer to the MLU memory that stores the index tensor for the median elements.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH,
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "median Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \p input, output tensor
 *   \p output_values, and index tensor \p output_indices.
 *   Data type of input tensor and output tensor should be the same.
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *   - index tensor: int32.
 * - On MLU500 series or above, this function also supports bfloat16 data type for \p input and \p output_values, and int64 data type
 *   for \p output_indices.
 *
 * @par Scale Limitation
 * - The input tensor, output tensor, and index tensor must meet the following requirement:
 *   - When \p is_dim_None is false, the shape of \p output_values and \p output_indices should be
 *   the same.
 *
 * @par API Dependency
 *  - Before calling this function, you need to get the size of workspace by calling ::cnnlGetMedianWorkspaceSize
 *  and pass the allocated extra workspace to the function.
 *
 * @note
 * - NaN/Inf is only supported on MLU500 series.
 *
 * @par Example
 * - None.
 */

cnnlStatus_t CNNL_WIN_API
cnnlMedian_v2(cnnlHandle_t handle,
              const int dim,
              const bool is_dim_None,
              const bool is_ignore_nan,
              const cnnlTensorDescriptor_t input_desc,
              const void *input,
              void *workspace,
              size_t workspace_size,
              const cnnlTensorDescriptor_t output_values_desc,
              void *output_values,
              const cnnlTensorDescriptor_t output_indices_desc,
              void *output_indices);

// Group:AsStrided
/*!
 * @brief Creates a view of the input tensor \p input with specified \p strides and
 *        the offset of the first element \p storage_offset.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   as_strided operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] stride
 *   Input. The stride for every dimension of output.
 * @param[in] storage_offset
 *   Input. The offset of the first element of the output tensor \p output in the input tensor
 *   \p input.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "AsStrided Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \p input and output tensor
 *   \p output. Data type of input tensor and output tensor should be the same.
 *   - input tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float, bfloat16.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float, bfloat16.
 *
 * @par Scale Limitation
 *  - The number of dimensions is no more than \p CNNL_DIM_MAX.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the as_strided operation is as follows:
     @verbatim
     input array by 3 * 3 --> input: [[1,2,3],[4,5,6],[7,8,9]]

     param:
       stride: [1,2], offset: 1

     output array by 2 * 2 --> output: [[2,4],[3,5]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.as_strided.html
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlAsStrided(cnnlHandle_t handle,
                                        const cnnlTensorDescriptor_t input_desc,
                                        const void *input,
                                        const uint32_t stride[],
                                        uint32_t storage_offset,
                                        const cnnlTensorDescriptor_t output_desc,
                                        void *output);

// Group:SoftplusBackward
/*!
 * @brief Computes the gradient of an operation of softplus backward.
 *
 * The softplus operation is used in activation layer to obtain the nonlinearity for artificial intelligence.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the softplus backward operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] diff_y_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] diff_y
 *   Input. Pointer to the MLU memory that stores the input tensor which is a gradient.
 * @param[in] diff_x_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] beta
 *   Input. Scale value compatible with PyTorch and TensorFlow interfaces. The default is 1.
 * @param[in] threshold
 *   Input. Scale value compatible with PyTorch and TensorFlow interfaces. The default is 20.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "SoftplusBackward operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Example
 * - The example of the softplus backward operation is as follows:
     @verbatim
      x: [4, 1, 3]
      diff_y: [4, 1, 3]
      diff_x: [4, 1, 3]
     @endverbatim
 *
 * @par Data Type
 * - Data type of input tensors and output tensor should be the same.
 *   The supported data types of input and output tensors are as follows:
 *   - input tensors: float, half, bfloat16.
 *   - output tensors: float, half, bfloat16.
 *
 *   bfloat16 is only supported on MLU500 series.
 * - Softplus backward operation is an element-wise operation. The dimensions of \p x, \p diff_y
 *   and \p diff_x must be the same.
 * - The number of dimensions is no more than \p CNNL_DIM_MAX.
 *
 * @note
 * - The input tensor \p x should be in the following range to guarantee the accuracy of output:
 *   - [-7.75, 7.75].
 *
 * @par Requirements
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/master/generated/torch.nn.Softplus.html#torch.nn.Softplus.
 *
 */
CNNL_DEPRECATED_FOR(cnnlSoftplusBackward_v2)
cnnlStatus_t CNNL_WIN_API cnnlSoftplusBackward(cnnlHandle_t handle,
                                               const cnnlTensorDescriptor_t x_desc,
                                               const void *x,
                                               const cnnlTensorDescriptor_t diff_y_desc,
                                               const void *diff_y,
                                               const cnnlTensorDescriptor_t diff_x_desc,
                                               void *diff_x,
                                               int beta,
                                               int threshold);

// Group:BceWithLogitsBackward
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace
 * in the ::cnnlBceWithLogitsBackward operation.
 *
 * The size of the extra workspace is based on the given information of the ::cnnlBceWithLogitsBackward,
 * including the \p target tensor descriptor \p target_desc, \p filter tensor descriptor \p filter_desc,
 * and \p pos_filter tensor descriptor \p pos_filter_desc. For more information about the workspace,
 * see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the ::cnnlGetBceWithLogitsBackwardWorkspaceSize. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] target_desc
 *   Input. The descriptor of \p target tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] filter_desc
 *   Input. The descriptor of \p filter tensor. Set it to NULL if not required.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] pos_filter_desc
 *   Input. The descriptor of \p pos_filter tensor. Set it to NULL if not required.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in
 *   the ::cnnlBceWithLogitsBackward operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetBceWithLogitsBackwardWorkspaceSize(
                                                    cnnlHandle_t handle,
                                                    const cnnlTensorDescriptor_t target_desc,
                                                    const cnnlTensorDescriptor_t filter_desc,
                                                    const cnnlTensorDescriptor_t pos_filter_desc,
                                                    size_t *size);

// Group:BceWithLogitsBackward
/*!
 *  @brief Computes gradients of ::cnnlBceWithLogits with \p grad tensor, \p input tensor,
 *  \p target tensor, \p filter tensor and \p pos_filter tensor, and returns the results
 *  in the \p diff_input tensor.
 *
 *  If \p filter tensor and \p pos_filter tensor need to be broadcasted, this function needs
 *  extra host memory as the workspace to broadcast \p filter tensor and \p pos_filter tensor.
 *  You can get the workspace size with the ::cnnlGetBceWithLogitsBackwardWorkspaceSize
 *  function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in ::cnnlBceWithLogitsBackward. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] grad_desc
 *   Input. The descriptor of grad tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] grad
 *    Input. Pointer to the MLU memory that stores the grad tensor. The \p grad
 *    tensor is gradient.
 * @param[in] input_desc
 *   Input. The descriptor of input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] target_desc
 *   Input. The descriptor of target tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] target
 *   Input. Pointer to the MLU memory that stores the target tensor. The \p target tensor
 *   has the same shape as \p input tensor.
 * @param[in] filter_desc
 *   Input. The descriptor of filter tensor. Set it to NULL if not required.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the filter tensor. The \p filter tensor is
 *   a filter of the result of binary cross entropy, and must match \p target tensor shape.
 *   Set it to NULL if not required.
 * @param[in] pos_filter_desc
 *   Input. The descriptor of pos_filter tensor. Set it to NULL if not required.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] pos_filter
 *   Input. Pointer to the MLU memory that stores the pos_filter tensor. The \p pos_filter tensor
 *   is a filter for dealing with data imbalance. It must be a vector and match \p target tensor shape.
 *   Set it to NULL if not required.
 * @param[in] reduction
 *   Input. An enum value that describing the reduction dimension, including:
 *   CNNL_BCE_WITH_LOGITS_REDUCTION_NONE, CNNL_BCE_WITH_LOGITS_REDUCTION_MEAN,
 *   CNNL_BCE_WITH_LOGITS_REDUCTION_SUM. See the ::cnnlBceWithLogitsReduction_t
 *   enum definition.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for
 *   ::cnnlBceWithLogitsBackward. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   ::cnnlBceWithLogitsBackward. You can get the size of the workspace with
 *   the ::cnnlGetBceWithLogitsBackwardWorkspaceSize function.
 * @param[out] diff_input_desc
 *   Output. The descriptor of diff_input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] diff_input
 *   Output. Pointer to the MLU memory that stores the diff_input tensor. The \p diff_input
 *   tensor is the gradients of ::cnnlBceWithLogits.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for
 *   \p grad tensor, \p input tensor, \p target tensor, \p filter tensor,
 *   \p pos_filter tensor and \p diff_input tensor.
 *   Note that the tensor type of all input and output tensors must be the same.
 *   bfloat16 supported only on MLU500 series.
 *   - grad tensor: half, float, bfloat16
 *   - input tensor: half, float, bfloat16
 *   - target tensor: half, float, bfloat16
 *   - filter tensor: half, float, bfloat16
 *   - pos_filter tensor: half, float, bfloat16
 *   - diff_input tensor: half, float, bfloat16
 *
 * @par Scale Limitation
 * - Half type is not recommended as the precision cannot be ensured for large amount of data.
 *   Because the max number of half is 65504.
 * - The range of \p grad tensor, \p input tensor,\p target tensor, \p filter tensor and
 *   \p pos_filter tensor should be small enough to prevent the result from data overflow in half data type.
 *
 * @par API Dependency
 * - You need get the extra space size by ::cnnlGetBceWithLogitsBackwardWorkspaceSize which need to get
 *   with ::cnnlBceWithLogitsBackward operation.
 *
 * @note
 * - The dimension of \p input tensor, \p target tensor, \p diff_input tensor should be the same.
 * - If reduction is CNNL_BCE_WITH_LOGITS_MEAN or CNNL_BCE_WITH_LOGITS_SUM,
 *   the \p grad tensor should be a number. Otherwise, the dimension of \p grad tensor and
 *   \p input tensor should be the same.
 * - The dimension values of the \p filter tensor and the \p target tensor need to meet the requirements of broadcast.
 * - The dimension values of the \p pos_filter tensor and the \p target tensor need to meet the requirements of broadcast.
 * - \p filter tensor and \p pos_filter tensor are optional. When you do not need them, set them to NULL.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the ::cnnlBceWithLogitsBackward function is as follows:
     @verbatim
       grad:  [2.],

       input:  [1.,2.,3.,4.]

       target: [1.,2.,3.,4.]

       filter: NULL,

       pos_filter:  NULL,

       reduction: CNNL_BCE_WITH_LOGITS_MEAN.

       Then we will get the diff_input.

       diff_input  -->: [-0.13424, -0.5594, -1.0236, -1.5090].
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/nn.functional.html?highlight=binary_cross_entropy_with_logits#torch.nn.functional.binary_cross_entropy_with_logits
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlBceWithLogitsBackward(cnnlHandle_t handle,
                          const cnnlTensorDescriptor_t grad_desc,
                          const void *grad,
                          const cnnlTensorDescriptor_t input_desc,
                          const void *input,
                          const cnnlTensorDescriptor_t target_desc,
                          const void *target,
                          const cnnlTensorDescriptor_t filter_desc,
                          const void *filter,
                          const cnnlTensorDescriptor_t pos_filter_desc,
                          const void *pos_filter,
                          cnnlBceWithLogitsReduction_t reduction,
                          void *workspace,
                          size_t workspace_size,
                          const cnnlTensorDescriptor_t diff_input_desc,
                          void *diff_input);

// Group:AsStridedBackward
/*!
 * @brief Calculates the gradient of input \p grad_x based on the gradient of response \p grad_y to
 * perform the backpropagation of ::cnnlAsStrided.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in]  grad_y_desc
 *   Input. The descriptor of the input gradient tensor in the backpropagation.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  grad_y
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in]  grad_x_desc
 *   Input. The descriptor of the output gradient tensor in the backpropagation.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out]  grad_x
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in]  strides_y
 *   Input. Pointer to the host memory that stores the stride of every dimension.
 *   The length of \p strides_y should be equal to the length of \p grad_y.
 * @param[in]  storage_offset_y
 *   Input. The offset of the first element.
 * @param[in]  workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the AsStrided Backward
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the AsStrided Backward
 *   operation. You can get the size of the workspace with the ::cnnlGetAsStridedBackwardWorkspaceSize
 *   function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Data Type
 * - This function only supports float and half data types for both \p grad_y and \p grad_x.
 * - On MLU500 series or above:
 *   - This function also supports bfloat16 data type for both \p grad_y and \p grad_x.
 *
 * @par Scale Limitation
 * - The value of \p strides_y cannot be less than 0 for any dimension.
 * - The number of input dimensions is no more than 8.
 *
 * @par API Dependency
 * - Before calling this function, you need to call the ::cnnlGetAsStridedBackwardWorkspaceSize
 *   function to get the \p workspace_size.
 *
 * @note
 * - Data type of input should be the same as output.
 * - The \p grad_x_desc of ::cnnlAsStridedBackward should be the same as the \p input_desc of ::cnnlAsStrided.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the AsStrided Backward is as follows:
   @verbatim
   grad_y: input an array by 4*4 --> [[11, 12, 13, 14],
                                      [21, 22, 23, 24],
                                      [31, 32, 33, 34],
                                      [41, 42, 43, 44]]

   shape of grad_x: [4,8]

   strides_y: an array by 2  --> [3,2]

   storage_offset_y: 3

   Then we will get the output:

   output: an array by 4*8 --> [[11,  0, 12, 21, 13, 22, 45, 23],
                                [32, 65, 33, 42, 34, 43,  0, 44],
                                [0,   0,  0,  0,  0,  0,  0,  0],
                                [0,   0,  0,  0,  0,  0,  0,  0]]
   @endverbatim
 *
 * @par Reference
 * - https://www.pytorch.org/docs/master/tensors.html#torch.Tensor.expand
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlAsStridedBackward(cnnlHandle_t handle,
                      const cnnlTensorDescriptor_t grad_y_desc,
                      const void *grad_y,
                      const cnnlTensorDescriptor_t grad_x_desc,
                      void *grad_x,
                      uint32_t strides_y[],
                      uint32_t storage_offset_y,
                      void *workspace,
                      uint32_t workspace_size);

// Group:AsStridedBackward
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the ::cnnlAsStridedBackward operation.
 *
 * The size of the extra workspace is based on the given information of the AsStrided Backward
 * operation, including the input tensor descriptors \p grad_y_desc. For more information about
 * the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the AsStrided
 *   Backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  grad_y_desc
 *   Input. The descriptor of the output gradient tensor in the backpropagation.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in the
 *   AsStrided Backward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetAsStridedBackwardWorkspaceSize(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t grad_y_desc,
                                      uint32_t *workspace_size);

/*!
 * @brief Enumeration variables describing the interpolation algorithms used to implement the
 *        InterpBackward operation.
 *
 */
typedef enum {
  CNNL_INTERP_BACKWARD_NEAREST = 0,
  /*!< The interp_backward mode is Nearest-Neighbor,
   * which means using the nearest pixel for interpolation.*/
  CNNL_INTERP_BACKWARD_BILINEAR = 1,
  /*!< The interp_backward mode is Bilinear,
   * which means using the four corner pixels with bilinear algorithm for interpolation.*/
  CNNL_INTERP_BACKWARD_LINEAR = 2,
  /*!< Linear interpolation for 3D tensor only.*/
  CNNL_INTERP_BACKWARD_TRILINEAR = 3,
  /*!< Trilinear interpolation for 5D tensor only.*/
  CNNL_INTERP_BACKWARD_BICUBIC = 4,
  /*!< Bicubic convolution interpolation for 4D tensor only.*/
} cnnlInterpBackwardMode_t;

// Group:InterpBackward
/*!
 * @brief Computes the gradients of the original images \p output based on the gradients of
 *        images after interpolation \p input for the backpropagation of ::cnnlInterp operation.
 *
 * This function supports algorithms defined in ::cnnlInterpBackwardMode_t.
 *
 * Additional parameters are \p align_corners and \p align_center, which affect the calculation of
 * the scaling factor of H dimension (scale_w) and the scaling factor of W dimension (scale_h).
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlInterpBackward_v3 instead, which supports parameters of
 *   \p interp_desc for more features.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in ::cnnlInterpBackward operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] align_corners
 *   Input. Boolean variable that determines the method to align 4 corner pixels between
 *          the original images and the images after interpolation.
 *          Generally, pixels of images are considered to be squares.
 *          If \p align_corners is set to true, images after interpolation and the original images
 *          are all aligned by the center points of 4 corner pixels.
 *          Otherwise, images after interpolation and the original images are aligned by the
 *          upper-left corner points of the corresponding corner pixels.
 * @param[in] align_center
 *   Input. Boolean variable that determines whether to shift the coordinates of each pixel from
 *          the upper-left corner coordinates by an offset of 0.5.
 *          If \p align_center is set to true, the center coordinates are used to represent each
 *          pixel.
 *          Otherwise, the upper-left corner coordinates of each pixel are used.
 *          For example, when \p align_center is set to true, the coordinates of the first pixel
 *          are (0, 0).
 *          When \p align_center is set to false, the coordinates of the first pixel are (0.5, 0.5).
 * @param[in] mode
 *   Input. The specific algorithm used to interpolate from original images to get output images.
 *          The algorithms are defined in ::cnnlInterpBackwardMode_t enum.
 * @param[in] input_desc
 *   Input. The descriptor of the gradients tensor of images after interpolation.
 *          For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the gradients tensor of images
 *          after interpolation.
 * @param[in] output_desc
 *   Input. The descriptor of the gradients tensor of the original images.
 *          For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the gradients tensor of the original images.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "InterpBackward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for \p align_corners, \p align_center,
 *   \p input, and \p output:
 *   - \p align_corners: bool.
 *   - \p align_center: bool.
 *   - \p input: half, float.
 *   - \p output: half, float.
 *
 * @par Data Layout
 * - The supported data layouts of \p input and \p output are as follows:
 *   - input tensor: \p CNNL_LAYOUT_NLC, \p CNNL_LAYOUT_NHWC.
 *   - output tensor: \p CNNL_LAYOUT_NLC, \p CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - The number of elements in \p input and \p output should be smaller than 2G.
 * - \p align_corners and \p align_center cannot be set to true at the same time.
 * - If \p mode is set to CNNL_INTERP_BACKWARD_NEAREST and the layout of \p input and \p output is
 *   \p CNNL_LAYOUT_NLC, \p align_corners and \p align_center can only be set to false.
 * - If the layout of \p input and \p output is \p CNNL_LAYOUT_NLC, the shape of \p input and
 *   \p output should be [n, li, c] and [n, lo, c] respectively.
 * - If the layout of \p input and \p output is \p CNNL_LAYOUT_NHWC, the shape of \p input and
 *   \p output should be [n, hi, wi, c] and [n, ho, wo, c] respectively.
 * - \p n and \p c represent the batches and channels of both \p input and \p output tensor
 *   respectively.
 *   \p hi and \p ho represent the height of \p input and \p output tensor respectively.
 *   \p wi and \p wo represent the width of \p input and \p output tensor respectively.
 *   \p li and \p lo represent the length of \p input and \p output tensor respectively.
 * - Data type of \p input and \p output should be the same.
 *
 * @par API Dependency
 * - None.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance,
 *   channels \p c of both \p input and \p output are recommended to be aligned to 128 Bytes.
 *
 * @note
 * - Only the size of \p input and \p output can be set in this operation.
 *   To use scaling factors rather than the size of \p input and \p output,
 *   the formula which converts scaling factors to the size of \p input and \p output is:
 *   - If align_corners is set to true, scale_h = (hi - 1) / (ho - 1),
 *      scale_w = (wi - 1) / (wo - 1).
 *   - If align_corners is set to false, scale_h = hi / ho, scale_w = wi / wo.
 *   The scale_h and scale_w represent the scaling factors along the corresponding dimensions.
 *   You can use this formula to obtain the requisite size of \p input and \p output with
 *   scaling factors.
 * - When \p input contains infinity:
 *   - On MLU200 series:
 *     - NaN in \p input is considered as positive saturation value.
 *     - Positive infinity in \p input is considered as positive saturation value.
 *     - Negative infinity in \p input is considered as negative saturation value.
 *   - On MLU300 series and CE3226:
 *     - If \p mode is ::CNNL_INTERP_BACKWARD_BILINEAR, \p output will contain infinity.
 *
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.pytorch.org/docs/stable/generated/torch.nn.UpsamplingBilinear2d
 * - https://www.pytorch.org/docs/stable/generated/torch.nn.UpsamplingNearest2d
 * - https://pytorch.org/docs/1.6.0/nn.functional.html?highlight=interp#torch.nn.functional.inter
 *   polate
 * - https://www.tensorflow.org/api_docs/python/tf/raw_ops/ResizeBilinearGrad
 * - https://www.tensorflow.org/api_docs/python/tf/raw_ops/ResizeNearestNeighborGrad
 */
CNNL_DEPRECATED_FOR(cnnlInterpBackward_v3)
cnnlStatus_t CNNL_WIN_API cnnlInterpBackward(cnnlHandle_t handle,
                                             bool align_corners,
                                             bool align_center,
                                             cnnlInterpBackwardMode_t mode,
                                             const cnnlTensorDescriptor_t input_desc,
                                             const void *input,
                                             const cnnlTensorDescriptor_t output_desc,
                                             void *output);

// Group:InterpBackward
/*!
 * @brief Computes the gradients of the original images \p output based on the gradients of
 *        images after interpolation \p input for the backpropagation of ::cnnlInterp or
 *        ::cnnlInterp_v2 operation.
 *
 * Parameters of \p align_corners and \p align_center affect the calculation of
 * the scaling factor of H dimension (scale_w) and the scaling factor of W dimension (scale_h).
 *
 * Compared with ::cnnlInterpBackward, this function provides the flexibility of whether to
 * recompute the scaling factors or to use the scaling factors with additional parameters
 * \p recompute_scale_factor and \p scale_factors. This function also supports modes of linear
 * and trilinear interpolation defined in ::cnnlInterpBackwardMode_t.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlInterpBackward_v3 instead, which supports parameters of
 *   \p interp_desc for more features.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in ::cnnlInterpBackward_v2 operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] align_corners
 *   Input. Boolean variable that determines the method to align corner pixels between
 *          the original images and the images after interpolation.
 *          Generally, pixels of images are considered to be squares.
 *          If \p align_corners is set to true, images after interpolation and the original images
 *          are all aligned by the center points of corner pixels.
 *          Otherwise, images after interpolation and the original images are aligned by the
 *          upper-left corner points of the corresponding corner pixels.
 * @param[in] align_center
 *   Input. Boolean variable that determines whether to shift the coordinates of each pixel from
 *          the upper-left corner coordinates by an offset of 0.5.
 *          If \p align_center is set to true, the center coordinates are used to represent each
 *          pixel.
 *          Otherwise, the upper-left corner coordinates of each pixel are used.
 *          For example, when \p align_center is set to true, the coordinates of the first pixel
 *          are (0, 0).
 *          When \p align_center is set to false, the coordinates of the first pixel are (0.5, 0.5).
 * @param[in] mode
 *   Input. The specific algorithm used to interpolate from original images to get output images.
 *          The algorithms are defined in ::cnnlInterpBackwardMode_t enum.
 * @param[in] scale_factors
 *   Input. Pointer to the host memory that stores the scale factors, which are the same as the ones
 *          passed to ::cnnlInterp_v2.
 * @param[in] recompute_scale_factor
 *   Input. Boolean variable that determines whether or not to recompute the scale factors. When \p
 *   align_corners is set to false and \p recompute_scale_factor is set to false or not specified,
 *   the valid values set in \p scale_factors will be used in the interpolation.
 *   Otherwise, \p scale_factors will be recomputed based on the sizes of \p input and \p output.
 * @param[in] input_desc
 *   Input. The descriptor of the gradients tensor of images after interpolation.
 *          For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the gradients tensor of images
 *          after interpolation.
 * @param[in] output_desc
 *   Input. The descriptor of the gradients tensor of the original images.
 *          For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the gradients tensor of the original images.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "InterpBackward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for \p align_corners, \p align_center,
 *   \p scale_factors, \p recompute_scale_factor, \p input, and \p output:
 *   - \p align_corners: bool.
 *   - \p align_center: bool.
 *   - \p scale_factors: float.
 *   - \p recompute_scale_factor: bool.
 *   - \p input: half, float.
 *   - \p output: half, float.
 *   - Data type of \p input and \p output should be the same.
 *
 * @par Data Layout
 * - The supported data layouts of \p input and \p output are as follows:
 *   - \p input: \p CNNL_LAYOUT_NLC, \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC.
 *   - \p output: \p CNNL_LAYOUT_NLC, CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC.
 *   - Layout of \p input and \p output should be the same.
 *
 * @par Scale Limitation
 * - The number of elements in \p input and \p output should be smaller than 2G.
 * - For all modes defined in ::cnnlInterpBackwardMode_t, \p align_corners and \p align_center
 *   cannot be set to true at the same time.
 * - If \p mode is set to CNNL_INTERP_BACKWARD_NEAREST and the layout of \p input and \p output is
 *   \p CNNL_LAYOUT_NLC, \p align_corners and \p align_center can only be set to false.
 * - When mode is \p CNNL_INTERP_BACKWARD_LINEAR, \p CNNL_INTERP_BACKWARD_TRILINEAR or \p CNNL_INTERP_BACKWARD_BICUBIC,
 *   \p align_corners and \p align_center cannot be set to false at the same time.
 * - When mode is \p CNNL_INTERP_BACKWARD_NEAREST or \p CNNL_INTERP_BACKWARD_BILINEAR,
 *   \p recompute_scale_factor should be set to true and \p scale_factors should be set to NULL.
 * - Except \p CNNL_INTERP_BACKWARD_BICUBIC, all the other modes are not recommended to use half data type due to low precision.
 *
 * @par API Dependency
 * - None.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance,
 *   channels \p c of both \p input and \p output are recommended to be aligned to 128 Bytes.
 *
 * @note
 * - \p scale_factors should be greater than 0.
 * - If \p recompute_scale_factor is true, then \p scale_factors must be set to NULL.
 * - When mode is \p CNNL_INTERP_BACKWARD_LINEAR, the shape and layout of \p input should be
 *   [n, li, c] and \p CNNL_LAYOUT_NLC respectively. The shape of \p output should be [n, lo, c].
 * - When mode is \p CNNL_INTERP_BACKWARD_NEAREST, the layout of \p input and \p output should be
 *   \p CNNL_LAYOUT_NLC or \p CNNL_LAYOUT_NHWC. If the layout of \p input and \p output is
 *   \p CNNL_LAYOUT_NLC, the shape of \p input and \p output should be [n, li, c] and [n, lo, c]
 *   respectively. If the layout of \p input and \p output is \p CNNL_LAYOUT_NHWC, the shape of
 *   \p input and \p output should be [n, hi, wi, c] and [n, ho, wo, c] respectively.
 * - When mode is \p CNNL_INTERP_BACKWARD_BILINEAR, the shape and layout of \p input should be
 *   [n, hi, wi, c] and \p CNNL_LAYOUT_NHWC respectively. The shape of \p output should
 *   be [n, ho, wo, c].
 * - When mode is \p CNNL_INTERP_BACKWARD_TRILINEAR, the shape and layout of \p input should be
 *   [n, di, hi, wi, c] and \p CNNL_LAYOUT_NDHWC respectively. The shape of \p output should be
 *   [n, do, ho, wo, c].
 * - When mode is \p CNNL_INTERP_BACKWARD_BICUBIC, the shape and layout of \p input should be
 *   [n, hi, wi, c] and \p CNNL_LAYOUT_NHWC respectively. The shape of \p output should be
 *   [n, ho, wo, c].
 * - \p n and \p c represent the batches and channels of both \p input and \p output tensor
 *   respectively.
 *   \p di and \p do represent the depth of \p input and \p output tensor respectively.
 *   \p hi and \p ho represent the height of \p input and \p output tensor respectively.
 *   \p wi and \p wo represent the width of \p input and \p output tensor respectively.
 *   \p li and \p lo represent the length of \p input and \p output tensor respectively.
 * - Given valid \p scale_factors, \p di and \p do should satisfy the following equation (C-style):
 *   \p di = (int)((double)\p scale_factor_d * \p do), where \p scale_factor_d is the scale factor
 *   for d-dimension. The same relationship applies to other dimensions except \p n and \p c.
 * - When \p input contains infinity:
 *   - On MLU300 series and CE3226:
 *     - If \p mode is ::CNNL_INTERP_BACKWARD_BILINEAR, \p output will contain infinity.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.pytorch.org/docs/stable/generated/torch.nn.UpsamplingBilinear2d
 * - https://www.pytorch.org/docs/stable/generated/torch.nn.UpsamplingNearest2d
 * - https://pytorch.org/docs/1.6.0/nn.functional.html?highlight=interp#torch.nn.functional.inter
 *   polate
 * - https://www.tensorflow.org/api_docs/python/tf/raw_ops/ResizeBilinearGrad
 * - https://www.tensorflow.org/api_docs/python/tf/raw_ops/ResizeNearestNeighborGrad
 */
CNNL_DEPRECATED_FOR(cnnlInterpBackward_v3)
cnnlStatus_t CNNL_WIN_API cnnlInterpBackward_v2(cnnlHandle_t handle,
                                                bool align_corners,
                                                bool align_center,
                                                cnnlInterpBackwardMode_t mode,
                                                const float scale_factors[],
                                                bool recompute_scale_factor,
                                                const cnnlTensorDescriptor_t input_desc,
                                                const void *input,
                                                const cnnlTensorDescriptor_t output_desc,
                                                void *output);

// Group:InterpBackward
/*!
 * @brief Computes the gradients of the original images \p output based on the gradients of
 *        images after interpolation \p input for the backpropagation of ::cnnlInterp,
 *        ::cnnlInterp_v2 and ::cnnlInterp_v3 operations.
 *
 * Compared with ::cnnlInterpBackward_v2, this function confirms the compatibility of different
 * open-source frameworks such as TensorFlow and PyTorch with the parameter \p interp_desc
 * set by ::cnnlSetInterpDescriptor_v2.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in ::cnnlInterpBackward_v3 operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] interp_desc
 *   Input. The descriptor of the interpolation operation set by ::cnnlSetInterpDescriptor_v2.
 *   For detailed information, see ::cnnlInterpDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the gradients tensor of images after interpolation.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the gradients tensor of images
 *   after interpolation.
 * @param[in] output_desc
 *   Input. The descriptor of the gradients tensor of the original images.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the gradients tensor of the original images.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "InterpBackward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following combination of data types for \p input and \p output:
 *   - \p input: half, float, bfloat16.
 *   - \p output: half, float, bfloat16.
 *   - Data type of \p input and \p output should be the same.
 *
 *   bfloat16 is only supported on MLU500 series.
 *
 *   bfloat16 only supports \p CNNL_INTERP_ALGO_0, \p CNNL_INTERP_ALGO_1 and \p CNNL_INTERP_ALGO_2. For more
 *   information, see ::cnnlInterpAlgo_t.
 *
 * @par Data Layout
 * - The supported data layouts of \p input and \p output are as follows:
 *   - If \p mode of \p interp_desc is set to \p CNNL_INTERP_LINEAR, the layout of \p input and
 *     \p output should be \p CNNL_LAYOUT_NLC.
 *   - If \p mode of \p interp_desc is set to \p CNNL_INTERP_NEAREST, the layout of \p input and
 *     \p output should be \p CNNL_LAYOUT_NLC, \p CNNL_LAYOUT_NHWC or \p CNNL_LAYOUT_NDHWC.
 *   - If \p mode of \p interp_desc is set to \p CNNL_INTERP_BILINEAR or \p CNNL_INTERP_BICUBIC,
 *     the layout of \p input and \p output should be \p CNNL_LAYOUT_NHWC.
 *   - If \p mode of \p interp_desc is set to \p CNNL_INTERP_TRILINEAR, the layout of \p input and
 *     \p output should be \p CNNL_LAYOUT_NDHWC.
 *   - Layout of \p input and \p output should be the same.
 *
 * @par Scale Limitation
 * - The number of elements in \p input and \p output should be smaller than 2G.
 *
 *   The exception to this limitation is the following:
 *   - When \p mode of \p interp_desc is set to \p CNNL_INTERP_BICUBIC, the number of elements in
 *     \p input and \p output can be larger than 2G.
 *   - However, when the dimensions of \p input or \p output is described as [n, h, w, c],
 *     a single dimension cannot exceed 2G, and the value of n*h*w or h*w cannot exceed 2G due to
 *     performance issues.
 * - The half data type is not recommended due to low precision.
 *
 * @par API Dependency
 * - Before calling this function, you need to prepare all the parameters passed to this function.
 *   Call ::cnnlSetInterpDescriptor_v2 to set information about the parameter \p interp_desc.
 * - After calling this function, call ::cnnlDestroyInterpDescriptor to destroy
 *   the parameter \p interp_desc.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance,
 *   channels \p c of both \p input and \p output are recommended to be aligned to 128 Bytes.
 *
 * @note
 * - Suppose the shape of \p input is (n, di, hi, wi, c) and \p output is (n, do, ho, wo, c),
 *   given valid \p scale_factors of \p interp_desc, \p di and \p do should satisfy the following
 *   equation (C-style) when \p algo is \p CNNL_INTERP_ALGO_0 or \p CNNL_INTERP_ALGO_1:
 *   \p di = (int)((double)\p scale_factor_d * \p do), where \p scale_factor_d is the scale factor
 *   for d-dimension. The same relationship applies to other dimensions except \p n and \p c.
 * - The n-dim and c-dim of \p input and \p output should be the same.
 * - When \p scale_factors of \p interp_desc is not NULL, use \p scale_factors for interpolation.
 *   Otherwise, calculate scale factors inside the operation.
 *   \p scale_factors of \p interp_desc should be greater than 0.0 if it is not NULL.
 * - When \p mode is \p CNNL_INTERP_NEAREST, \p algo can be \p CNNL_INTERP_ALGO_0,
 *   \p CNNL_INTERP_ALGO_1, \p CNNL_INTERP_ALGO_4, \p CNNL_INTERP_ALGO_5 or \p CNNL_INTERP_ALGO_6.
 *   However, \p algo can only be \p CNNL_INTERP_ALGO_0 or \p CNNL_INTERP_ALGO_1 if the layout of
 *   \p input is \p CNNL_LAYOUT_NDHWC.
 *   The \p CNNL_INTERP_ALGO_1 corresponds to the nearest-exact mode of PyTorch.
 * - When \p mode is \p CNNL_INTERP_BILINEAR, \p algo can be \p CNNL_INTERP_ALGO_1,
 *   \p CNNL_INTERP_ALGO_2, \p CNNL_INTERP_ALGO_4, \p CNNL_INTERP_ALGO_5 or \p CNNL_INTERP_ALGO_6.
 * - When \p mode is \p CNNL_INTERP_LINEAR, \p CNNL_INTERP_TRILINEAR or \p CNNL_INTERP_BICUBIC,
 *   \p algo can be \p CNNL_INTERP_ALGO_1, or \p CNNL_INTERP_ALGO_2.
 * - When \p input contains infinity:
 *   - On MLU300 series and the above:
 *     - If \p mode of \p interp_desc is ::CNNL_INTERP_BILINEAR, ::CNNL_INTERP_LINEAR,
 *     ::CNNL_INTERP_TRILINEAR or ::CNNL_INTERP_BICUBIC, \p output will contain infinity.
 * - This operation is not supported on the 1V platforms.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/1.13/generated/torch.nn.functional.interpolate.html?highlight=interpolate
 * - https://www.tensorflow.org/api_docs/python/tf/raw_ops/ResizeBilinearGrad
 * - https://www.tensorflow.org/api_docs/python/tf/raw_ops/ResizeNearestNeighborGrad
 */

cnnlStatus_t CNNL_WIN_API cnnlInterpBackward_v3(cnnlHandle_t handle,
                                                const cnnlInterpDescriptor_t interp_desc,
                                                const cnnlTensorDescriptor_t input_desc,
                                                const void *input,
                                                const cnnlTensorDescriptor_t output_desc,
                                                void *output);

// Group:ApplyAdaGrad
/*!
 * @brief Updates \p var tensor by using adagrad method. If the \p update_slots is
 * true, this operation also updates the \p accum tensor.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in ::cnnlApplyAdaGrad. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  grad_desc
 *   Input. The descriptor of the \p grad tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  grad
 *   Input. Pointer to the MLU memory that stores the \p grad tensor. It is the gradient of \p var.
 *   With the \p grad value, ::cnnlApplyAdaGrad can be used to calculate \p accum and \p var.
 * @param[in] accum_desc
 *   Input. The descriptor of the \p accum tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out] accum
 *   Input/Output. Pointer to the MLU memory that stores the \p accum tensor.
 *   The \p accum is the accumulation of gradient. It will be updated when \p update_slots is true.
 * @param[in] var_desc
 *   Input. The descriptor of the \p var tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out] var
 *   Input/Output. Pointer to the MLU memory that stores the \p var tensor.
 *   The \p var value is the optimization goal of whole algorithm.
 * @param[in] lr
 *   Input. Pointer to the MLU memory that stores the \p lr parameter.
 *   It is the learning rate of this optimizer.
 * @param[in] update_slots
 *   Input. A Boolean value that specifies whether to update \p accum. If the value of
 *   this parameter is true, \p accum is updated. If the value of this parameter is false,
 *   \p accum will not be updated.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for
 *   \p grad tensor, \p accum tensor, \p var tensor.
 *   Note that the combinations of these tensor must be half-half or float-float.
 *   bfloat16 supported only on MLU500 series.
 *   - grad tensor: half, float, bfloat16
 *   - accum tensor: half, float, bfloat16
 *   - var tensor: half, float, bfloat16
 *
 * @par Scale Limitation
 * - The dimensions of \p grad, \p accum and \p var should be the same.
 * - The number of dimensions is no more than 8.
 *
 * @note
 * - The \p accum should be greater than zero.
 * - The precision may lose if the data type of \p accum is half, especially when \p accum is close to
 *   zero or (var - lr * grad / sqrt(accum)) is close to zero. If the data type of \p accum is half,
 *   the \p accum value should be greater than 5e-5 to avoid precision loss.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
     input three arrays by 4, 4, 4
     --> grad:  [0, 1, 2, 3]
     --> accum: [0, 1, 2, 3]
     --> var:   [0, 1, 2, 3]

     param:
        lr: 0.01, update_slots:true,

     output array by 4, 4
      --> accum: [0,2,6,12]
      --> var: [0, 0.992929,1.99181,2.99134]
     @endverbatim
 *
 * @par Reference
 * - https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/core/kernels/training_ops.cc
 *
 */

cnnlStatus_t CNNL_WIN_API
cnnlApplyAdaGrad(cnnlHandle_t handle,
                 const cnnlTensorDescriptor_t grad_desc,
                 const void *grad,
                 const cnnlTensorDescriptor_t accum_desc,
                 void *accum,
                 const cnnlTensorDescriptor_t var_desc,
                 void *var,
                 const void *lr,
                 bool update_slots);
// Group:ApplyAdaGradV2
/*!
 * @brief Updates \p var tensor by using adagrad method. If the \p update_slots is
 * true, this operation also updates the \p accum tensor. Compared with ::cnnlApplyAdaGrad,
 * this function uses \p epsilon to avoid division by 0.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in ::cnnlApplyAdaGradV2. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  grad_desc
 *   Input. The descriptor of the \p grad tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  grad
 *   Input. Pointer to the MLU memory that stores the \p grad tensor. It is the gradient of \p var.
 *   With the \p grad value, ::cnnlApplyAdaGradV2 function can be used to calculate \p accum and \p var.
 * @param[in] accum_desc
 *   Input. The descriptor of the \p accum tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out] accum
 *   Input/Output. Pointer to the MLU memory that stores the \p accum tensor.
 *   The \p accum is the accumulation of gradient. It will be updated when \p update_slots is true.
 * @param[in] var_desc
 *   Input. The descriptor of the \p var tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out] var
 *   Input/Output. Pointer to the MLU memory that stores the \p var tensor.
 *   The \p var value is the optimization goal of whole algorithm.
 * @param[in] lr
 *   Input. Pointer to the MLU memory that stores the \p lr parameter.
 *   It is the learning rate of this optimizer.
 * @param[in] epsilon
 *   Input. Pointer to the MLU memory that stores the \p epsilon parameter.
 *   It is a small positive number just as \f$10^{-8}\f$, to avoid division by 0.
 * @param[in] update_slots
 *   Input. A Boolean value that specifies whether to update \p accum. If the value of
 *   this parameter is true, \p accum is updated. If the value of this parameter is false,
 *   \p accum will not be updated.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - The supported data types are as follows:
 *   - grad tensors: half, float.
 *   - accum tensors: half, float.
 *   - var tensors: half, float.
 *   - lr parameter: half, float
 *   - epsilon parameter: half, float
 *
 * @par Scale Limitation
 * - The dimensions of \p grad, \p accum and \p var should be the same.
 *
 * @note
 * - The data type of \p grad tensor, \p accum tensor and \p var tensor must be the same.
 * - The data type of \p lr parameter and \p epsilon parameter must be the same as \p var tensor.
 * - Use data type of float for \p grad, \p accum and \p var for higher
 * precision.
 * - On MLU200 series, if the data type is half and the data value is not in range of
 * [-65504, 65504], the value will be treated as a saturation value.
 * - On MLU200 series, if the value of (\p accum + \p grad * \p grad) is less than 0, the output
 * \p var will be unpredictable.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
     input three arrays by 4, 4, 4
     --> grad:  [0, 1, 2, 3]
     --> accum: [0, 1, 2, 3]
     --> var:   [0, 1, 2, 3]

     param:
        lr: 0.01, epsilon: 0.0001, update_slots: true,

     output array by 4, 4
      --> accum: [0, 2, 6, 12]
      --> var: [0, 0.992929, 1.991835, 2.99134]
     @endverbatim
 *
 * @par Reference
 * - https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/core/kernels/training_ops.cc
 *
 */

cnnlStatus_t CNNL_WIN_API
cnnlApplyAdaGradV2(cnnlHandle_t handle,
                   const cnnlTensorDescriptor_t grad_desc,
                   const void *grad,
                   const cnnlTensorDescriptor_t accum_desc,
                   void *accum,
                   const cnnlTensorDescriptor_t var_desc,
                   void *var,
                   const void *lr,
                   const void *epsilon,
                   bool update_slots);

// Group:BceWithLogits
/*!
 * @brief Computes binary cross entropy with \p input tensor, \p target tensor,
 * \p filter tensor and \p pos_filter tensor, and returns the results
 * in the \p output tensor.
 *
 * If \p filter tensor and \p pos_filter tensor need to be broadcasted, this function needs
 * extra host memory as the workspace to broadcast \p filter tensor and \p pos_filter tensor.
 * You can get the workspace size with the ::cnnlGetBceWithLogitsWorkspaceSize
 * function.
 *
 * Compared with ::cnnlBceWithLogits_v2, removes the \p prefer parameter.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in ::cnnlBceWithLogits. For detailed information, see ::cnnlHandle_t.
 * @param[in] desc_input
 *   Input. The descriptor of input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] desc_target
 *   Input. The descriptor of target tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] target
 *   Input. Pointer to the MLU memory that stores the target tensor which is the target value of input.
 * @param[in] desc_filter
 *   Input. The descriptor of filter tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the filter tensor. The \p filter tensor is
 *   a filter of the binary cross entropy. Set it to NULL if not required.
 * @param[in] desc_pos_filter
 *   Input. The descriptor of pos_filter tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] pos_filter
 *   Input. Pointer to the MLU memory that stores the pos_filter tensor. The \p pos_filter
 *   tensor is a filter for dealing with imbalance. Set it to NULL if not required.
 * @param[in] reduction
 *   Input. An enum value that describes the reduction dimension. For detailed information,
 *   see ::cnnlBceWithLogitsReduction_t.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlBceWithLogits. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the ::cnnlBceWithLogits. You can get the size of the workspace with
 *   the ::cnnlGetBceWithLogitsWorkspaceSize function.
 * @param[in] desc_output
 *   Output. The descriptor of output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor. The \p output
 *   tensor is the result of ::cnnlBceWithLogits.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "BceWithLogits Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for \p input tensor,
 *   \p target tensor, \p filter tensor, \p pos_filter tensor and \p output tensor.
 *   Note that the tensor type of all input and output tensors must be the same.
 *   bfloat16 supported only on MLU500 series.
 *   - input: half, float, bfloat16.
 *   - target: half, float, bfloat16.
 *   - filter: half, float, bfloat16.
 *   - pos_filter: half, float, bfloat16.
 *   - output: half, float, bfloat16.
 *
 * @par Scale Limitation
 * - None.
 *
 * @par API Dependency
 * - Before calling this function, you need to call ::cnnlGetBceWithLogitsWorkspaceSize function to
 *   get extra workspace size for ::cnnlBceWithLogits_v2 function.
 *
 * @note
 * - The number of dimensions of \p input tensor and that of \p target tensor, should be the same.
 * - If \p reduction is CNNL_BCE_WITH_LOGITS_MEAN or CNNL_BCE_WITH_LOGITS_SUM,
 *   then the \p output tensor should be a number. Otherwise, the dimension of \p output tensor and
 *   \p input tensor should be the same.
 * - For each dimension, the dimension length of the \p filter tensor and the \p target tensor need to meet the requirements of broadcasting.
 * - For each dimension, the dimension length of the \p pos_filter tensor and the \p target tensor need to meet the requirements of broadcasting.
 * - The precision cannot be ensured for large amounts of data if the data type of input, target, filter, pos_filter is half, because the maximum number of half is 65504.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the ::cnnlBceWithLogits function is as follows:
     @verbatim
       input:  shape=(4) -> [1.5, 1.5, 1.5, 1.5],

       target:  shape=(4) -> [1, 1, 1, 1],

       filter: NULL,

       pos_filter:  NULL,

       reduction: CNNL_BCE_WITH_LOGITS_MEAN.

       Then we will get the output.

       output  -->: [0.2014].

     @endverbatim
 *
 *  @par Reference
 *  - https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss.
 */
cnnlStatus_t CNNL_WIN_API
cnnlBceWithLogits(cnnlHandle_t handle,
                  const cnnlTensorDescriptor_t desc_input,
                  const void *input,
                  const cnnlTensorDescriptor_t desc_target,
                  const void *target,
                  const cnnlTensorDescriptor_t desc_filter,
                  const void *filter,
                  const cnnlTensorDescriptor_t desc_pos_filter,
                  const void *pos_filter,
                  const cnnlBceWithLogitsReduction_t reduction,
                  void *workspace,
                  size_t workspace_size,
                  const cnnlTensorDescriptor_t desc_output,
                  void *output);

// Group:BceWithLogits
/*!
 * @brief Computes binary cross entropy with \p input tensor, \p target tensor,
 * \p filter tensor and \p pos_filter tensor, and returns the results
 * in the \p output tensor.
 *
 * If \p filter tensor and \p pos_filter tensor need to be broadcasted, this function needs
 * extra host memory as the workspace to broadcast \p filter tensor and \p pos_filter tensor.
 * You can get the workspace size with the ::cnnlGetBceWithLogitsWorkspaceSize
 * function.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlBceWithLogits instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the ::cnnlBceWithLogits_v2. For detailed information, see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. The algorithm used to compute the output. For detailed information,
 *   see ::cnnlComputationPreference_t. Only supports ::CNNL_COMPUTATION_HIGH_PRECISION and
 *   ::CNNL_COMPUTATION_FAST currently.
 * @param[in] desc_input
 *   Input. The descriptor of input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] desc_target
 *   Input. The descriptor of target tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] target
 *   Input. Pointer to the MLU memory that stores the target tensor which is the target value of input.
 * @param[in] desc_filter
 *   Input. The descriptor of filter tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the filter tensor. The \p filter tensor is
 *   a filter of the binary cross entropy. Set it to NULL if not required.
 * @param[in] desc_pos_filter
 *   Input. The descriptor of pos_filter tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] pos_filter
 *   Input. Pointer to the MLU memory that stores the pos_filter tensor. The \p pos_filter
 *   tensor is a filter for dealing with imbalance. Set it to NULL if not required.
 * @param[in] reduction
 *   Input. An enum value that describes the reduction dimension. For detailed information,
 *   see ::cnnlBceWithLogitsReduction_t.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlBceWithLogits_v2. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the ::cnnlBceWithLogits_v2. You can get the size of the workspace with
 *   the ::cnnlGetBceWithLogitsWorkspaceSize function.
 * @param[in] desc_output
 *   Output. The descriptor of output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor. The \p output
 *   tensor is the result of ::cnnlBceWithLogits_v2.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "BceWithLogits Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for \p input tensor,
 *   \p target tensor, \p filter tensor, \p pos_filter tensor and \p output tensor.
 *   Note that the tensor type of all input and output tensors must be the same.
 *   bfloat16 supported only on MLU500 series.
 *   - input: half, float, bfloat16.
 *   - target: half, float, bfloat16.
 *   - filter: half, float, bfloat16.
 *   - pos_filter: half, float, bfloat16.
 *   - output: half, float, bfloat16.
 *
 * @par Scale Limitation
 * - None.
 *
 * @par API Dependency
 * - Before calling this function, you need to call ::cnnlGetBceWithLogitsWorkspaceSize function to
 *   get extra workspace size for ::cnnlBceWithLogits_v2 function.
 *
 * @note
 * - The number of dimensions of \p input tensor and that of\p target tensor, should be the same.
 * - If \p reduction is CNNL_BCE_WITH_LOGITS_MEAN or CNNL_BCE_WITH_LOGITS_SUM,
 *   then the \p output tensor should be a number. Otherwise, the dimension of \p output tensor and
 *   \p input tensor should be the same.
 * - For each dimension, the dimension length of the \p filter tensor and the \p target tensor need
 *   to meet the requirements of broadcasting.
 * - For each dimension, the dimension length of the \p pos_filter tensor and the \p target tensor
 *   need to meet the requirements of broadcasting.
 * - The precision cannot be ensured for large amount of data if the data type of input, target,
 *   filter, pos_filter is half, because the maximum number of half is 65504.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the ::cnnlBceWithLogits_v2 function is as follows:
     @verbatim
       input:  shape=(4) -> [1.5, 1.5, 1.5, 1.5],

       target:  shape=(4) -> [1, 1, 1, 1],

       filter: NULL,

       pos_filter:  NULL,

       reduction: CNNL_BCE_WITH_LOGITS_MEAN.

       Then we will get the output.

       output  -->: [0.2014].

     @endverbatim
 *
 *  @par Reference
 *  - https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss.
 */
CNNL_DEPRECATED_FOR(cnnlBceWithLogits)
cnnlStatus_t CNNL_WIN_API
cnnlBceWithLogits_v2(cnnlHandle_t handle,
                     const cnnlComputationPreference_t prefer,
                     const cnnlTensorDescriptor_t desc_input,
                     const void *input,
                     const cnnlTensorDescriptor_t desc_target,
                     const void *target,
                     const cnnlTensorDescriptor_t desc_filter,
                     const void *filter,
                     const cnnlTensorDescriptor_t desc_pos_filter,
                     const void *pos_filter,
                     const cnnlBceWithLogitsReduction_t reduction,
                     void *workspace,
                     const size_t workspace_size,
                     const cnnlTensorDescriptor_t desc_output,
                     void *output);

// Group:BceWithLogits
/*!
 * @brief Returns in \p size the size of the host memory that is used as an extra workspace
 * in ::cnnlBceWithLogits or ::cnnlBceWithLogits_v2 operation.
 *
 * The size of the extra workspace is based on the given information of the ::cnnlBceWithLogits or
 * ::cnnlBceWithLogits_v2, including the \p target tensor descriptor \p target_desc, \p filter
 * tensor descriptor \p filter_desc, and \p pos_filter tensor descriptor \p pos_filter_desc.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in ::cnnlGetBceWithLogitsWorkspaceSize. For detailed information, see ::cnnlHandle_t.
 * @param[in] desc_input
 *   Input. The descriptor of the \p input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] desc_filter
 *   Input. The descriptor of \p filter tensor. The \p filter tensor is a filter
 *   of the binary cross entropy.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] desc_pos_filter
 *   Input. The descriptor of \p pos_filter tensor. The \p pos_filter tensor is
 *   a filter for dealing with imbalance.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra MLU workspace in bytes that is used in
 *   ::cnnlBceWithLogits or ::cnnlBceWithLogits_v2 function.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetBceWithLogitsWorkspaceSize(cnnlHandle_t handle,
                                  const cnnlTensorDescriptor_t desc_input,
                                  const cnnlTensorDescriptor_t desc_filter,
                                  const cnnlTensorDescriptor_t desc_pos_filter,
                                  size_t *size);

// Group:Flip
/*!
 * @brief Reverses the order of an N-dimensional tensor along the axis in dims \p dimension.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   flip operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] dimension
 *   Input. An array that stores the axis to flip on. For each dimension.
 * @param[in] dimension_len
 *   Input. The length of \p dimension which is an array that stores the axis to flip on.
 * @param[in] desc_input
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores input tensor.
 * @param[in] desc_output
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to MLU memory that stores output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - The data type of \p desc_input and \p desc_output must be the same.
 *   And this (I/O)function supports the following data widths:
 *   - input tensor: 1-byte, 2-byte, 4-byte, 8-byte.
 *   - output tensor: 1-byte, 2-byte, 4-byte, 8-byte.
 *
 * @par Data Layout
 * - The dimension of input tensor should be less than or equal to 8-dimension.
 *
 * @par Scale Limitation
 * - The \p dimension_len and \p dimension must meet the following requirements:
 *   - \p 0 < dimension_len <= n, where n is the rank of \p input.
 *   - \p -n <= dimension[i] < n, where n is the rank of \p input.
 *   - \p dimension[i] is not equal to dimension[j], where i is not equal to j.
 *   If dimension[i] < 0, it corresponds to dimension[i] + n.
 * - The \p input and \p output have same shape, and \p input or \p output dimension i should be
 *   greater than 0.
 * - The size of \p input should be less than or equal to INT_MAX on MLU300 series and CE3226.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the flip operation is as follows:
     @verbatim
     input array by 2 * 2 * 2-->
         input: [[[0, 1], [2, 3]],
                 [[4, 5], [6, 7]]]
     param:
       dimension_len: 3, dimension: (0, 1, 2),

     output array by 2 * 2 * 2 -->
         output: [[[7, 6], [5, 4]],
                  [[3, 2], [1, 0]]]
   @endverbatim
 */
cnnlStatus_t CNNL_WIN_API cnnlFlip(cnnlHandle_t handle,
                                   const int dimension[],
                                   const int dimension_len,
                                   const cnnlTensorDescriptor_t desc_input,
                                   const void * input,
                                   const cnnlTensorDescriptor_t desc_output,
                                   void * output);

// Group:Polar
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra workspace
 * in ::cnnlPolar operation.
 *
 * The size of the extra workspace is based on the given information of the ::cnnlPolar operation
 * including the \p abs tensor descriptor \p abs_desc, \p angle
 * tensor descriptor \p angle_desc, and \p output tensor descriptor \p output_desc.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in ::cnnlGetPolarWorkspaceSize. For detailed information, see ::cnnlHandle_t.
 * @param[in] abs_desc
 *   Input. The descriptor of \p abs tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] angle_desc
 *   Input. The descriptor of \p angle tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of \p output tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra MLU workspace in bytes that is used in
 *   ::cnnlPolar function.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetPolarWorkspaceSize(cnnlHandle_t handle,
                          const cnnlTensorDescriptor_t abs_desc,
                          const cnnlTensorDescriptor_t angle_desc,
                          const cnnlTensorDescriptor_t output_desc,
                          size_t *workspace_size);

// Group:Polar
/*!
 * @brief Returns a complex tensor based on the given tensors in polar coordinates.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   Polar operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] abs_desc
 *   Input. The descriptor of the input tensor \p abs, which is the
 *   absolute value of input polar coordinates. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] abs
 *   Input. Pointer to the MLU memory that stores input tensor \p abs.
 * @param[in] angle_desc
 *   Input. The descriptor of the input tensor \p angle, which is the angle of
 *   input polar coordinates. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] angle
 *   Input. Pointer to the MLU memory that stores input tensor \p angle.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor \p output. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores output tensor \p output.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for
 *   ::cnnlPolar. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of an extra workspace in bytes that is used in ::cnnlPolar.
 *   You can get the size of the workspace with ::cnnlGetPolarWorkspaceSize.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH.
 *
 * @par Formula
 * - See "Polar" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \p abs,
 *   \p angle and output tensor \p output.
 *   Data type of input tensors \p abs and \p angle must be the same.
 *   - abs: float.
 *   - angle: float.
 *   - output: complex_float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensors and output tensor must be \p CNNL_LAYOUT_ARRAY.
 * - The dimension of input tensor and output should be less than or equal to CNNL_DIM_MAX.
 *
 * @par Scale Limitation
 * - The two input tensors \p abs and \p angle must have the same shape or satisfy
 *   the broadcasting rules.
 *
 * @par API Dependency
 * - The size of the workespace must be calculated by ::cnnlGetPolarWorkspaceSize and allocated to this function.
 *
 * @note
 * - This function is only supported on MLU300 series, MLU500 series, CE3226 and 1V.
 * - The behavior of this function is undefined, if abs is negative or NaN, or if
 *   angle is infinite.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this function is as follows:
     @verbatim
     abs array by 2 -->
         input: [1, 2]
     angle array by 2 -->
         input: [M_PI / 2, 5 * M_PI / 4]

     output array by 2 -->
         output: [0.0 + 1.0j, -1.4142-1.4142j]
     @endverbatim
 * @par Reference
 * - https://pytorch.org/docs/1.9.0/generated/torch.polar.html
 */
cnnlStatus_t CNNL_WIN_API
cnnlPolar(cnnlHandle_t handle,
          const cnnlTensorDescriptor_t abs_desc,
          const void *abs,
          const cnnlTensorDescriptor_t angle_desc,
          const void *angle,
          const cnnlTensorDescriptor_t output_desc,
          void *output,
          void *workspace,
          size_t workspace_size);

// Group:PoolingWithIndex
/*!
 * @brief Computes the maximum pooling forward and pooling index with the ::cnnlPoolingDescriptor_t
 * pooling_desc, returns the maximum pooling in the \p output tensor, and the index in the \p index tensor.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in ::cnnlPoolingForwardWithIndex. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] pooling_desc
 *   Input. The descriptor of pooling operation. For detailed information,
 *   see ::cnnlPoolingMode_t.
 * @param[in] alpha, beta
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] x_desc
 *   Input. The descriptor of \p x tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the \p y tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor. The
 *   \p y tensor is the result of maximum pooling.
 * @param[in] index_desc
 *   Input. The descriptor of the \p index tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] index
 *   Output. Pointer to the MLU memory that stores the index of the \p y tensor.
 * @param[in]  workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlPoolingForwardWithIndex. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in]  workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the ::cnnlPoolingForwardWithIndex. You can get the size of the workspace with
 *   ::cnnlGetPoolingWithIndexWorkspaceSize or ::cnnlGetPoolingWithIndexWorkspaceSize_v2.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @par Formula
 * - See "Pooling Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *   input tensor \p input - output tensor \p output - output tensor \p index.
 *   - half - half - int16
 *   - half - half - int64
 *   - float - float - int32
 *   - float - float - int64
 * - On MLU500 series, this function additionally supports the following data type combinations:
 *   - bfloat16 - bfloat16 - int16
 *   - bfloat16 - bfloat16 - int64
 *
 * @par Data Layout
 *   Note that the layout of the \p x tensor, \p y tensor, and \p index tensor must be the same.
 * - In the 2D pooling operation, the supported data layouts of the \p x tensor, \p y tensor, \p index tensor are as follows:
 *   - x tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NCHW.
 *   - y tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NCHW.
 *   - index tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NCHW.
 * - In the 3D pooling operation, the supported data layouts of the \p x tensor, \p y tensor, \p index tensor are as follows:
 *   - x tensor: \p CNNL_LAYOUT_NDHWC.
 *   - y tensor: \p CNNL_LAYOUT_NDHWC.
 *   - index tensor: \p CNNL_LAYOUT_NDHWC.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the layout of the \p x
 *   tensor, \p y tensor, \p index tensor to NHWC.
 *
 * @note
 * - Currently, 2D and 3D poolings have been supported.
 * - Currently supports dilation greater than 1 in any mode.
 * - In the 2D maximum pooling, when the kernel of the pooling contains NaN:
 *   - On MLU200 series:
 *    - The \p output value is the NaN, and the \p index value is the index of the last NaN.
 *   - On MLU300 series and CE3226:
 *    - If the last value in the kernel of the pooling is NaN, the \p output value is NaN and
 *      the \p index is the index of the last value. Otherwise, the \p output value is the maximum
 *      value after the last NaN and the \p index value is the index of the output value.
 * - In the 3D maximum pooling, the \p output is unpredictable since a NaN value is noncomparable.
 *
 * @par Scale Limitation
 * - According to the definition of pooling, parameters in \p pooling_desc should satisfy the following conditions:
 *   padding >= 0, stride >= 1.
 * - Parameters in \p x_desc should satisfy the following conditions: in_batch > 0, in_height > 0, in_width > 0, in_channel > 0.
 *   - in_batch is the batch of the input tensor, and in_height is the height of the input tensor.
 *   - in_width is the width of the input tensor, and in_channel is the channel of the input tensor.
 * - Parameters in \p y_desc should satisfy the following conditions: out_batch > 0, out_height > 0, out_width > 0, out_channel > 0.
 *   - out_batch is the batch of the output tensor, and out_height is the height of the output tensor.
 *   - out_width is the width of the output tensor, and out_channel is the channel of the output tensor.
 * - Parameters in \p pooling_desc should satisfy the following conditions:
 * - In both 2D and 3D pooling forward operation, the pooling window size is limited to the capacity of NRAM. The maximum pooling window size varies by the MLU platform.
 *   When the pooling window size exceeds the supported maximum value, the function will return ::CNNL_STATUS_NOT_SUPPORTED.
 * - For [N,H,W,C] inputs, only MLU500 series support large tensors in which any single dimension cannot exceed INT32_MAX and H*W cannot exceed INT32_MAX.
 * - For [N,D,H,W,C] inputs, only MLU500 series support large tensors in which any single dimension cannot exceed INT32_MAX and D*H*W*C cannot exceed INT32_MAX.
 * - When dilation is equal to 2, the input size that can be processed is limited to the capacity of NRAM.
 *   If the input size exceeds the supported maximum input size, the function will return ::CNNL_STATUS_NOT_SUPPORTED.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 *  - The example of this operation is as follows:
     @verbatim
      input layout is NHWC, and input shape is (1,4,4,1).
        input: [[[1,2,3,4],
                 [5,6,7,8],
                 [9,10,11,12],
                 [13,14,15,16]]]
      param:
        pad:(0,0,0,0), stride:(2,2), kernel:(2,2), mode: CNNL_POOLING_MAX

      output: [[[6],[8],[14],[16]]]

      index:  [[[3],[3],[3],[3]]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/nn.functional.html#avg-pool2d
 * - https://pytorch.org/docs/stable/nn.functional.html#max-pool2d
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlPoolingForwardWithIndex(cnnlHandle_t handle,
                                                      const cnnlPoolingDescriptor_t pooling_desc,
                                                      const void *alpha,
                                                      const cnnlTensorDescriptor_t x_desc,
                                                      const void *x,
                                                      const void *beta,
                                                      const cnnlTensorDescriptor_t y_desc,
                                                      void *y,
                                                      const cnnlTensorDescriptor_t index_desc,
                                                      void *index,
                                                      void *workspace,
                                                      size_t workspace_size);

// Group:PoolingWithIndex
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra workspace
 * in the ::cnnlPoolingForwardWithIndex operation.
 *
 * The size of the extra workspace is based on the given information of ::cnnlPoolingForwardWithIndex,
 * including the \p x tensor descriptor \p x_desc, \p y tensor descriptor \p y_desc. For more information
 * about the workspace, see "Cambricon CNNL User Guide".
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGetPoolingWithIndexWorkspaceSize_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in ::cnnlGetPoolingWithIndexWorkspaceSize. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of \p x tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] y_desc
 *   Input. The descriptor of \p y tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out]  workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in
 *   the ::cnnlPoolingForwardWithIndex operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called before the ::cnnlPoolingForwardWithIndex function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
CNNL_DEPRECATED_FOR(cnnlGetPoolingWithIndexWorkspaceSize_v2)
cnnlStatus_t CNNL_WIN_API cnnlGetPoolingWithIndexWorkspaceSize(cnnlHandle_t handle,
                                                               const cnnlTensorDescriptor_t x_desc,
                                                               const cnnlTensorDescriptor_t y_desc,
                                                               size_t *workspace_size);

// Group:PoolingWithIndex
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra workspace
 * in the ::cnnlPoolingForwardWithIndex operation.
 *
 * The size of the extra workspace is based on the given information of ::cnnlPoolingForwardWithIndex,
 * including the \p x tensor descriptor \p x_desc, \p y tensor descriptor \p y_desc and \p index tensor descriptor \p index_desc.
 * Compare with ::cnnlGetPoolingWithIndexWorkspaceSize, when data type of \p index is int64, users need to use the
 * ::cnnlGetPoolingWithIndexWorkspaceSize_v2 function to obtain the correct \p workspace_size.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in ::cnnlGetPoolingWithIndexWorkspaceSize_v2. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of \p x tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] y_desc
 *   Input. The descriptor of \p y tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] index_desc
 *   Input. The descriptor of \p index tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out]  workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in
 *   the ::cnnlPoolingForwardWithIndex operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called before the ::cnnlPoolingForwardWithIndex function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetPoolingWithIndexWorkspaceSize_v2(cnnlHandle_t handle,
                                        const cnnlTensorDescriptor_t x_desc,
                                        const cnnlTensorDescriptor_t y_desc,
                                        const cnnlTensorDescriptor_t index_desc,
                                        size_t *workspace_size);

// Group:NanInf
/*!
 * @brief Checks if the exception values NaN (not a number) or infinity is contained in the
 *        input tensor \p input.
 *
 * You can call this function to check whether the input tensor includes NaN or infinity. Returns true when
 * the following conditions are met:
 *
 * - The data type of the input tensor is half and the value of input tensor includes NaN, infinity,
 *   0x7bff, or 0xfbff.
 * - The data type of the input tensor is float and the value of input tensor includes NaN, infinity,
 *   0x7f7fffff, or 0xff7fffff.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the nan_inf
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output value.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @par Formula
 * - See "NanInf Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for input tensor \p input
 *   and output \p output:
 *   - input tensor: half, float.
 *   - output: bool.
 *
 * @note
 * - This operation is not supported on the 1V platforms.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlNanInf(cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const void * input,
                                     bool * output);

// Group:Normalize
/*!
 * @brief Creates a descriptor pointed by \p desc for normalization operation, and allocates
 * memory for holding the information about the normalization operation. The information is
 * defined in ::cnnlNormalizeDescriptor_t. For more information about descriptor,
 * see "Cambricon CNNL User Guide".
 *
 * @param[out] desc
 *   Output. Pointer to the normalization descriptor that holds information about ::cnnlNormalize.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetNormalizeDescriptor function to initialize
 *   and set the information to the normalization descriptor.
 * - You need to call the ::cnnlDestroyNormalizeDescriptor function to destroy the descriptor.
 *   Otherwise, the memory leak may occur.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateNormalizeDescriptor(cnnlNormalizeDescriptor_t *desc);

// Group:Normalize
/*!
 * @brief Destroys a normalization descriptor \p desc that was previously created with the
 *        ::cnnlCreateNormalizeDescriptor function.
 *
 * The normalization descriptor is defined in ::cnnlNormalizeDescriptor_t and holds
 * the information about the ::cnnlNormalize operation.
 *
 * @param[in] desc
 *   Input. The normalization descriptor to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - You need to call this function after calling the ::cnnlNormalize operation.
 *   Otherwise, the memory leak may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyNormalizeDescriptor(cnnlNormalizeDescriptor_t desc);

// Group:Normalize
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace
 * to optimize the normalize operation.
 *
 * The size of the extra workspace is based on the given information of the normalize operation,
 * including the normalize descriptor \p normalize_desc, input tensor descriptor \p input_desc,
 * output tensor descriptor \p output_desc and norm tensor descriptor \p norm_desc.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the normalize operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  normalize_desc
 *   Input. The descriptor of the normalize operation. For detailed information,
 *   see ::cnnlNormalizeDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] norm_desc
 *   Input. The descriptor of the norm tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *   the normalize operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called to calculate the workspace size \p workspace_size.
 * - The allocated extra workspace should be passed to the ::cnnlNormalize_v3 function
 *   to perform the normalize operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetNormalizeWorkspaceSize(cnnlHandle_t handle,
                                                        const cnnlNormalizeDescriptor_t
                                                        normalize_desc,
                                                        const cnnlTensorDescriptor_t input_desc,
                                                        const cnnlTensorDescriptor_t output_desc,
                                                        const cnnlTensorDescriptor_t norm_desc,
                                                        size_t *workspace_size);
// Group:Normalize
/*!
 * @brief Initializes the normalization descriptor \p normalize_desc that was previously created with the
 * ::cnnlCreateNormalizeDescriptor function, and sets the information about the ::cnnlNormalize or ::cnnlNormalize_v2
 * to the normalization descriptor \p normalize_desc. Different from ::cnnlSetNormalizeDescriptor, ::cnnlSetNormalizeDescriptor_v2
 * supports arbitrary p-norm and scaling the \p output tensor by setting the parameters \p pnorm, \p channel_shared and \p across_spatial,
 * and then launchs the function ::cnnlNormalize_v2 or ::cnnlNormalize. Note that if the function ::cnnlNormalize is launched, the
 * \p channel_shared and \p across_spatial will be regarded as 0 in this function. The information includes the normalization \p axis,
 * the number of axises \p axis_num, the NaN propagation mode \p nan_propagation, the lower bound of normalization \p eps,
 * the exponent value \p pnorm, control parameters \p channel_shared and \p across_spatial.
 *
 * @param[in,out] normalize_desc
 *   Input/output. The descriptor of normalization operation. For detailed information,
 *   see ::cnnlNormalizeDescriptor_t.
 * @param[in] axis[]
 *   Input. The dimension vector of the ::cnnlNormalize operation. The size of axis vector cannot be greater than the dimension
 *   size of the input tensor \p input that is used in ::cnnlNormalize.
 * @param[in] axis_num
 *   Input. The number of axises. The \p axis_num should equal to the size of \p axis.
 * @param[in] nan_propagation
 *   Input. The NaN propagation mode defined in ::cnnlNormalizeMode_t.
 *   Only supports CNNL_NOT_PROPAGATE_NAN now.
 * @param[in] eps
 *   Input. A lower bound value for \p norm that is used in ::cnnlNormalize or ::cnnlNormalize_v2. If the value of \p norm is less than
 *   the value of this parameter, this parameter is used as the divisor.
 * @param[in] pnorm
 *   Input. The exponent value in the norm formulation.
 * @param[in] channel_shared
 *   Input. If it is set to 0, the \p output tensor will not be scaled; If it is set to 1, the \p output tensor will be scaled by the only one
 *   element in \p scale tensor; If it is set to 2, the \p output tensor will be scaled by the different element in \p scale tensor.
 * @param[in] across_spatial
 *   Input. If it is set to 0, the input \p axis[] will be used; If it is set to 1, the input \p axis[] will be discarded and replaced by [n-1];
 *   If it is set to 2, the input \p axis[] will be discarded and replaced by [1,...,n-1], where n is equal to the dimension of input \p input tensor.
 *
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS
 *
 * @par API Dependency
 * - Some functions need to be called before and after this function. The dependencies are as follows:
 * - ::cnnlCreateNormalizeDescriptor needs to be called before this function.
 * - ::cnnlDestroyNormalizeDescriptor needs to be called after this function.
 *
 * @note
 * - None.
 *
 * @par requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetNormalizeDescriptor_v2(cnnlNormalizeDescriptor_t normalize_desc,
                           int axis[],
                           int axis_num,
                           cnnlNanPropagation_t nan_propagation,
                           float eps,
                           float pnorm,
                           int channel_shared,
                           int across_spatial);

// Group:Normalize
/*!
 * @brief Initializes the normalization descriptor \p normalize_desc that was previously created with the
 * ::cnnlCreateNormalizeDescriptor function, and sets the information about the ::cnnlNormalize or ::cnnlNormalize_v2
 * to the normalization descriptor \p normalize_desc. cnnlSetNormalizeDescriptor only supports 2-norm,
 * if there is a need to use arbitrary p-norm or scale the \p output tensor, refer to ::cnnlSetNormalizeDescriptor_v2.
 * The information includes the normalization \p axis, the number of axises \p axis_num,
 * the NaN propagation mode \p nan_propagation, and the lower bound of normalization \p eps.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlSetNormalizeDescriptor_v2 instead, which supports the parameters \p pnorm, \p channel_shared
 *   and \p across_spatial.
 *
 * @param[in,out] normalize_desc
 *   Input/output. The descriptor of normalization operation. For detailed information,
 *   see ::cnnlNormalizeDescriptor_t.
 * @param[in] mode
 *   Input. The \p mode describes the algorithm that is used in the implementation of the ::cnnlNormalize operation.
 *   This parameter only supports CNNL_NORMALIZE_EUCLIDEAN now.
 * @param[in] axis[]
 *   Input. The dimension vector of the ::cnnlNormalize operation. The size of axis vector cannot be greater than the dimension
 *   size of the input tensor \p input that is used in ::cnnlNormalize.
 * @param[in] axis_num
 *   Input. The number of axises. The \p axis_num should equal to the size of \p axis.
 * @param[in] nan_propagation
 *   Input. The NaN propagation mode defined in ::cnnlNormalizeMode_t.
 *   Only supports CNNL_NOT_PROPAGATE_NAN now.
 * @param[in] eps
 *   Input. A lower bound value for \p norm that is used in ::cnnlNormalize. If the value of \p norm is less than the value of this parameter,
 *   this parameter is used as the divisor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS
 *
 * @par API Dependency
 * - Some functions need to be called before and after this function. The dependencies are as follows:
 * - ::cnnlCreateNormalizeDescriptor needs to be called before this function.
 * - ::cnnlDestroyNormalizeDescriptor needs to be called after this function.
 *
 * @note
 * - None.
 *
 * @par requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlSetNormalizeDescriptor_v2)
cnnlStatus_t CNNL_WIN_API
cnnlSetNormalizeDescriptor(cnnlNormalizeDescriptor_t normalize_desc,
                           int axis[],
                           int axis_num,
                           cnnlNormalizeMode_t mode,
                           cnnlNanPropagation_t nan_propagation,
                           float eps);

// Group:Normalize
/*!
 * @brief Normalizes the input tensor \p input with the normalization descriptor \p normalize_desc,
 * and returns the p-norm in \p norm tensor, and the normalization results in the \p output tensor.
 * If \p channel_shared or \p across_spatial has been set in cnnlSetNormalizeDescriptor_v2, \p channel_shared and
 * \p across_spatial will be regarded as 0 in this function.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlNormalize_v3 instead, which supports the \p scale tensor that determines
 *   whether to scale the \p output tensor.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in ::cnnlNormalize. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] normalize_desc
 *   Input. The descriptor of normalization operation. For detailed information,
 *   see ::cnnlNormalizeDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the \p input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the \p input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the \p output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the \p output tensor. The
 *   \p output tensor is the results of the normalization operation.
 * @param[in] norm_desc
 *   Input. The descriptor of the \p norm tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] norm
 *   Output. Pointer to the MLU memory that stores the \p norm tensor. The \p
 *   norm tensor is the results of p-norm.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for
 *   \p input tensor, \p output tensor, \p norm tensor.
 *   Note that the combinations of these tensor must be half-half or float-float.
 *   - input tensor: half, float
 *   - output tensor: half, float
 *   - norm tensor: half, float
 *
 * @note
 * - The ::cnnlNormalize function only supports normalization if the dimension of the input param \p axis[] is continuous when the \p across_spatial is set to 0.
 * - The half type only can represent up to 65504, thus when the results of p-norm exceeds 65504 in half type, there will be an accuracy problem.
 * - This function involves using the activation table for high precision in MLU270, such as exphp (exp in high precision mode), loghp (log in high precision
 *   mode) and sqrthp (sqrt in high precision mode). When the absolute value of \p input tensor is less than 2e-5 in half type, accuracy problem may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/master/nn.functional.html?highlight=normalize#toch.nn.functional.normalize
 *
 */

CNNL_DEPRECATED_FOR(cnnlNormalize_v3)
cnnlStatus_t CNNL_WIN_API cnnlNormalize(cnnlHandle_t handle,
                                        const cnnlNormalizeDescriptor_t normalize_desc,
                                        const cnnlTensorDescriptor_t input_desc,
                                        const void * input,
                                        const cnnlTensorDescriptor_t output_desc,
                                        void * output,
                                        const cnnlTensorDescriptor_t norm_desc,
                                        void * norm);

// Group:Normalize
/*!
 * @brief Normalizes the input tensor \p input with the normalization descriptor \p normalize_desc,
 * returns the p-norm in \p norm tensor, and the normalization results in the \p output tensor.
 * If \p channel_shared is set to 1 or 2 in ::cnnlSetNormalizeDescriptor_v2, the \p scale tensor will scale
 * the \p output tensor. And if \p channel_shared is set to 0, the \p scale tensor will not work.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlNormalize_v3 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in ::cnnlNormalize_v2. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] normalize_desc
 *   Input. The descriptor of normalization operation. For detailed information,
 *   see ::cnnlNormalizeDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the \p input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the \p input tensor.
 * @param[in] scale_desc
 *   Input. The descriptor of the \p scale tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] scale
 *   Input. Pointer to the MLU memory that stores the \p scale tensor. The \p
 *   scale is used to scale the \p output tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the \p output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the \p output tensor. The
 *   \p output tensor is the results of the normalization operation.
 * @param[in] norm_desc
 *   Input. The descriptor of the \p norm tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] norm
 *   Output. Pointer to the MLU memory that stores the \p norm tensor. The \p
 *   norm tensor is the results of p-norm.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for
 *   \p input tensor, \p scale tensor, \p output tensor and \p norm tensor.
 *   Note that the combinations of these tensor must be half-half or float-float.
 *   - input tensor: half, float
 *   - scale tensor: half, float
 *   - output tensor: half, float
 *   - norm tensor: half, float
 *
 * @note
 * - The ::cnnlNormalize function only supports normalization if the dimension of the input param \p axis[] is continuous when the across_spatial is set to 0.
 * - The half type only can represent up to 65504, thus when the results of p-norm exceeds 65504 in half type, there will be an accuracy problem.
 * - This function involves using the activation table for high precision in MLU270, such as exphp (exp in high precision mode), loghp (log in high precision
 *   mode) and sqrthp (sqrt in high precision mode). When the absolute value of \p input tensor is less than 2e-5 in half type, accuracy problem may occur.
 *
 *
 * @par Requirements
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/master/nn.functional.html?highlight=normalize#toch.nn.functional.normalize
 */
CNNL_DEPRECATED_FOR(cnnlNormalize_v3)
cnnlStatus_t CNNL_WIN_API cnnlNormalize_v2(cnnlHandle_t handle,
                                          const cnnlNormalizeDescriptor_t normalize_desc,
                                          const cnnlTensorDescriptor_t input_desc,
                                          const void * input,
                                          const cnnlTensorDescriptor_t scale_desc,
                                          const void * scale,
                                          const cnnlTensorDescriptor_t output_desc,
                                          void * output,
                                          const cnnlTensorDescriptor_t norm_desc,
                                          void * norm);

// Group:Normalize
/*!
 * @brief Normalizes the input tensor \p input with the normalization descriptor \p normalize_desc,
 * returns the p-norm in \p norm tensor, and returns the normalization results in the \p output tensor.
 * If \p channel_shared is set to 1 or 2 in ::cnnlSetNormalizeDescriptor_v2, the \p scale tensor will scale
 * the \p output tensor. And if \p channel_shared is set to 0, the \p scale tensor will not work. Compared with
 * ::cnnlNormalize_v2, ::cnnlNormalize_v3 supports p-norm and normalization for discontinuous axes with extra
 * input space.
 *
 * @deprecated
 * - ::cnnlNormalize and ::cnnlNormalize_v2 are deprecated and will be removed in future release.
 *   Use ::cnnlNormalize_v3 instead, which supports the parameters \p pnorm, \p channel_shared
 *   and \p across_spatial.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in ::cnnlNormalize_v3. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] normalize_desc
 *   Input. The descriptor of normalization operation. For detailed information,
 *   see ::cnnlNormalizeDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the \p input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the \p input tensor.
 * @param[in] scale_desc
 *   Input. The descriptor of the \p scale tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] scale
 *   Input. Pointer to the MLU memory that stores the \p scale tensor. The \p
 *   scale is used to scale the \p output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   normalize operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the normalize operation. You can get the size of the workspace with
 *   the ::cnnlGetNormalizeWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the \p output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the \p output tensor. The
 *   \p output tensor is the results of the normalization operation.
 * @param[in] norm_desc
 *   Input. The descriptor of the \p norm tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] norm
 *   Output. Pointer to the MLU memory that stores the \p norm tensor. The \p
 *   norm tensor is the results of p-norm.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for
 *   \p input tensor, \p scale tensor, \p output tensor and \p norm tensor.
 *   Note that the combinations of these tensor must be half-half or float-float.
 *   - input tensor: half, float
 *   - scale tensor: half, float
 *   - output tensor: half, float
 *   - norm tensor: half, float
 *
 * @note
 * - The ::cnnlNormalize_v3 function supports normalization if the dimension of the input parameter \p axis[] is discontinuous when the across_spatial is set to 0.
 * - The half type only can represent up to 65504, thus when the results of p-norm exceeds 65504 in half type, there will be an accuracy problem.
 * - This function involves using the activation table for high precision in MLU270, such as exphp (exp in high precision mode), loghp (log in high precision
 *   mode) and sqrthp (sqrt in high precision mode). When the absolute value of \p input tensor is less than 2e-5 in half type, accuracy problem may occur.
 *
 *
 * @par Requirements
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/master/nn.functional.html?highlight=normalize#toch.nn.functional.normalize
 */

cnnlStatus_t CNNL_WIN_API cnnlNormalize_v3(cnnlHandle_t handle,
                                          const cnnlNormalizeDescriptor_t normalize_desc,
                                          const cnnlTensorDescriptor_t input_desc,
                                          const void * input,
                                          const cnnlTensorDescriptor_t scale_desc,
                                          const void * scale,
                                          void *workspace,
                                          const size_t workspace_size,
                                          const cnnlTensorDescriptor_t output_desc,
                                          void * output,
                                          const cnnlTensorDescriptor_t norm_desc,
                                          void * norm);

// Group:AdaptivePoolingBackward
/*!
 * @brief Computes gradients of 2D or 3D adaptive average and maximum pooling. For detailed information,
 *        see ::cnnlAdaptivePoolingForward.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the adaptive
 *   pooling backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] y_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] index_desc
 *   Input. The descriptor of the index tensor used in maximum mode. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] index
 *   Input. Pointer to the MLU memory that stores the tensor used to store index of each element of
 *   \p dx in corresponding pooling kernel.
 * @param[in] mode
 *   Input. The pooling mode, which is defined in the ::cnnlPoolingMode_t enum.
 * @param[in] dx_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] dx
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH,
 *   ::CNNL_STATUS_NOT_SUPPORTED, ::CNNL_STATUS_INTERNAL_ERROR, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @par Formula
 * - See "AdaptivePoolingBackward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *   \p y - \p index - \p output.
 *   - half-int16-half.
 *   - float-int32-float.
 * - On MLU500 series, this function additionally supports the following data type combinations:
 *   - bfloat16-int16-bfloat16.
 *
 * @par Data Layout
 * - The supported data layouts of the input tensor, index tensor, and output tensor are
 *   as follows:
 *   - input tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC.
 *   - index tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC.
 *   - output tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC.
 *
 * @par Scale Limitation
 * - \p mode: \p mode == \p CNNL_POOLING_AVERAGE_COUNT_INCLUDE_PADDING or \p mode ==
 *   \p CNNL_POOLING_MAX
 * - In the 2D adaptive pooling backward, pooling acts on the HW dimension. The input tensor, output tensor, index tensor, and mode must meet the following requirements:
 *   - The input tensor and output tensor must have four dimensions.
 *   - Size of the first and fourth dimension of input tensor and output tensor should be the same.
 *   - input tensor: \p batch > 0, \p height > 0, \p width > 0, \p channel > 0
 *   - output tensor: each dimension should fall within range of int32. When \p mode == \p
 *     CNNL_POOLING_MAX, the size of each kernel should meet the following requirement, where the maximum
 *     kernel size \p max_kernel_size is computed as \p kh_max * \p kw_max, where \p kh_max is the maximum value of
 *     ((int)ceil((float)(\p i + 1) * \p output_h / \p input_h) - (int)floor((float) \p i * \p output_h / \p input_h)) with \p i running from 0 to (\p input_h - 1),
 *     and \p kw_max is the maximum value of
 *     ((int)ceil((float)(\p i + 1) * \p output_w / \p input_w) - (int)floor((float) \p i * \p output_w / \p input_w)) with \p i running from 0 to (\p input_w - 1).
 *      - If the data type of \p output is float32, (\p max_kernel_size - 1) should be no greater than (\f$2^{31}-1\f$),
 *        which is the upper limit of int32. Within this range, values can be exactly represented by float.
 *      - If the data type of \p output is float16 or bfloat16, (\p max_kernel_size - 1) should be no greater than (\f$2^{15}-1\f$), which is
 *        the upper limit of int16.
 *   - If \p mode == \p CNNL_POOLING_MAX, the index tensor should have four dimensions and the same
 *     shape as the input tensor.
 * - In the 3D adaptive pooling backward, pooling acts on the DHW dimension. The input tensor, output tensor, index tensor, and mode must meet the following requirements:
 *   - The input tensor and output tensor must have five dimensions.
 *   - The first and fifth dimension of input tensor and output tensor should be the same.
 *   - input tensor: \p batch > 0, \p depth > 0, \p height > 0, \p width > 0, \p channel > 0
 *   - output tensor: kernel size cannot be too big, and CNNL_POOLING_MAX mode should satisfy
 *     (\p output_d / \p input_d + 2) * (\p output_h / \p input_h + 2) * (\p output_w / \p input_w + 2) <= 3582
 *     CNNL_POOLING_AVERAGE_COUNT_INCLUDE_PADDING mode should satisfy
 *     (\p output_h / \p input_h + 2) * (\p output_w / \p input_w + 2) <= 3582
 *   - If \p mode == \p CNNL_POOLING_MAX, the index tensor should have five dimensions and the same
 *     shape as the input tensor.
 * - On MLU500 series, under all modes of adaptive pooling backward, input, output, and index now support large tensor, which means that the element number of the tensor can exceed \f$2^{31}\f$.
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveMaxPool2d.html
 * - https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html
 * - https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveMaxPool3d.html
 * - https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool3d.html
 */
cnnlStatus_t CNNL_WIN_API cnnlAdaptivePoolingBackward(cnnlHandle_t handle,
                                                      const cnnlTensorDescriptor_t y_desc,
                                                      const void * y,
                                                      const cnnlTensorDescriptor_t index_desc,
                                                      const void * index,
                                                      cnnlPoolingMode_t mode,
                                                      const cnnlTensorDescriptor_t dx_desc,
                                                      void * dx);

/******************************************************************************
 * Cambricon CNNL FusedOp
 ******************************************************************************/

/*! A pointer to cnnlFusedOpsPlan struct that holds the descriptors of the fusion operation
 *  including the constant parameters and variant parameters.
 *
 *  You need to call the ::cnnlCreateFusedOpsPlan function to create a plan, and set the fusion
 *  information to this plan. Also, you need to destroy the plan at the end with the
 *  ::cnnlDestroyFusedOpsPlan function.
 */
typedef struct cnnlFusedOpsPlan* cnnlFusedOpsPlan_t;

/*! A pointer to cnnlFusedOpsConstParamPack struct that holds the information of the constant
 *  parameters in the fusion operation such as the operator descriptors and the tensor descriptors.
 *  You need to call the ::cnnlCreateFusedOpsConstParamPack function to
 *  create a pack and call the ::cnnlSetFusedOpsConstParamPackAttribute
 *  function to add a new constant parameter to the pack.
 *  You can call the ::cnnlGetFusedOpsConstParamPackAttribute function
 *  to get a specific parameter from the pack.
 *  Also, you need to destroy the pack at the end with the
 *  ::cnnlDestroyFusedOpsConstParamPack function.
 */
typedef struct cnnlFusedOpsConstParamPack* cnnlFusedOpsConstParamPack_t;

/*! A pointer to cnnlFusedOpsVariantParamPack struct that holds the information of the variant
 *  parameters in the fusion operation such as the input data addresses and the workspace address.
 *
 *  You need to call the ::cnnlCreateFusedOpsVariantParamPack function to
 *  create a pack and call the ::cnnlSetFusedOpsVariantParamPackAttribute function
 *  to add a new variant parameter to the pack. You can call
 *  the ::cnnlGetFusedOpsVariantParamPackAttribute function to get a specific
 *  parameter from the pack.
 *  Also, you need to destroy the pack at the end with the
 *  ::cnnlDestroyFusedOpsVariantParamPack function.
 */
typedef struct cnnlFusedOpsVariantParamPack* cnnlFusedOpsVariantParamPack_t;

/*!
 * @brief Enumeration variables describing the supported fusion mode in CNNL. The fusion mode
 * specifies the order of the operations to be implemented.
 */
typedef enum {
  CNNL_CONV_SCALE_BN_ACTIVATION,
  /*!< Per-channel basis, performs these operations in the following order: ::cnnlConvolutionForward,
   * ::cnnlScale, ::cnnlBatchNormForwardInference and ::cnnlActivationForward. Apart from
   * ::cnnlConvolutionForward, the other operations are optional. */
  CNNL_CONV_SCALE_BN_PRELU,
  /*!< Per-channel basis, performs these operations in the following order: ::cnnlConvolutionForward,
   * ::cnnlScale, ::cnnlBatchNormForwardInference and ::cnnlPrelu. Apart from
   * ::cnnlConvolutionForward, the other operations are optional. Not support now.*/
  CNNL_DECONV_SCALE,
  /*!< Per-channel basis, performs these operations in the following order: ::cnnlDeconvolution,
   * ::cnnlScale. Apart from ::cnnlDeconvolution, the other operation is optional. */
  CNNL_ACTIVATION_CONV_BN_ACTIVATION_MASKZERO CNNL_DEPRECATED_ENUM_FOR(),
  /*!< Performs these operations in the following order: ::cnnlActivationForward, ::cnnlConvolutionForward,
   * ::cnnlBatchNormForwardInference, ::cnnlActivationForward and ::cnnlMaskZero. Apart from
   * ::cnnlConvolutionForward, the other operations are optional.
   *
   * After CNNL 2.0, ::CNNL_ACTIVATION_CONV_BN_ACTIVATION_MASKZERO will be deprecated
   *   for no longer supporting the ::cnnlMaskZero operation.*/
  CNNL_ACTIVATION_CONV_BN_MASKZERO_ACTIVATION CNNL_DEPRECATED_ENUM_FOR(),
  /*!< Performs these operations in the following order: ::cnnlActivationForward, ::cnnlConvolutionForward,
   * ::cnnlBatchNormForwardInference, ::cnnlMaskZero and ::cnnlActivationForward. Apart from
   * ::cnnlConvolutionForward, the other operations are optional.
   *
   * After CNNL 2.0, ::CNNL_ACTIVATION_CONV_BN_MASKZERO_ACTIVATION will be deprecated
   *   for no longer supporting the ::cnnlMaskZero operation.*/
  CNNL_DECONV_SCALE_BN_ACTIVATION,
  /*!< Per-channel basis, performs these operations in the following order: ::cnnlDeconvolution,
   * ::cnnlScale, ::cnnlBatchNormForwardInference and ::cnnlActivationForward. Apart from
   * ::cnnlDeconvolution, the other operations are optional. Only supports 3D transposed
   * convolution operator currently. */
  CNNL_CONV_SCALE_BN_ACTIVATION_ADD_ACTIVATION,
  /*!< Performs these operations in the following order: ::cnnlConvolutionForward, ::cnnlScale,
   * ::cnnlBatchNormForwardInference, ::cnnlActivationForward, ::cnnlOpTensor
   * and ::cnnlActivationForward. Apart from ::cnnlConvolutionForward and ::cnnlOpTensor,
   * the other operations are optional. */
} cnnlFusedOps_t;

/*!
 * @brief Enumeration variables describing the supported constant parameter type used in the fusion
 * operation. The constant parameter can be a tensor descriptor, an operation descriptor or a
 * scalar variable used in operations.
 *
 * - Setting:
 *
 *   You need to call ::cnnlSetFusedOpsConstParamPackAttribute to set specific constant
 *   parameter to a previously created ::cnnlFusedOpsConstParamPack_t pack that stores all
 *   constant parameters used in the fusion operation.
 * - Getting:
 *
 *   You need to call ::cnnlGetFusedOpsConstParamPackAttribute to get a specific constant
 *   parameter from the previously created and initialized ::cnnlFusedOpsConstParamPack_t pack.
 */
typedef enum {
  CNNL_XDESC = 0,
  /*!< The descriptor of the input tensor \p x. In the setting, the \p cparam should be a pointer
   * to a previously initialized ::cnnlTensorDescriptor_t. */
  CNNL_WDESC = 1,
  /*!< The descriptor of the filter tensor \p w. In the setting, the \p cparam should be a pointer
   * to a previously initialized ::cnnlTensorDescriptor_t. */
  CNNL_YDESC = 2,
  /*!< The descriptor of the output tensor \p y. In the setting, the \p cparam should be a pointer
   * to a previously initialized ::cnnlTensorDescriptor_t. */
  CNNL_BIAS_DESC = 3,
  /*!< The descriptor of the bias tensor. In the setting, the \p cparam should be a pointer to a
   * previously initialized ::cnnlTensorDescriptor_t. */
  CNNL_CONV_DESC = 4,
  /*!< The descriptor of the convolution operation. In the setting, the \p cparam should be a
   * pointer to a previously initialized ::cnnlConvolutionDescriptor_t. */
  CNNL_BN_WEIGHT_BIAS_MEAN_VAR_DESC = 5,
  /*!< The descriptor of the filter, bias, mean and variance tensor used in
   * ::cnnlBatchNormForwardInference operation. In the setting, the \p cparam should be a pointer
   * to a previously initialized ::cnnlTensorDescriptor_t.*/
  CNNL_SCALE_ALPHA_DESC = 6,
  /*!< The descriptor of the alpha tensor used in ::cnnlScale operation. In the setting,
   * the \p cparam should be a pointer to a previously initialized ::cnnlTensorDescriptor_t. */
  CNNL_SCALE_BETA_DESC = 7,
  /*!< The descriptor of the beta tensor used in ::cnnlScale operation. In the setting,
   * the \p cparam should be a pointer to a previously initialized ::cnnlTensorDescriptor_t. */
  CNNL_ACTIVATION_DESC = 8,
  /*!< The descriptor of the ::cnnlActivationForward operation. In the setting, the \p cparam
   * should be a pointer to a previously initialized ::cnnlActivationDescriptor_t. */
  CNNL_PRELU_ALPHA_DESC = 9,
  /*!< The descriptor of the alpha tensor used in ::cnnlPrelu operation. In the setting,
   * the \p cparam should be a pointer to a previously initialized ::cnnlTensorDescriptor_t. */
  CNNL_SCALAR_CONV_FWD_ALGO = 10,
  /*!< The scalar describing the convolution algorithms that are used to implement the
   * ::cnnlConvolutionForwardInference operation. In the setting, the \p cparam should be
   * a pointer to a previously initialized ::cnnlConvolutionForwardAlgo_t. */
  CNNL_SCALAR_CONV_FWD_CAST_MODE = 11,
  /*!< The scalar describing the cast mode used to convert the data type of
   * input and output tensors for ::cnnlConvolutionForwardInference operation.
   * In the setting, the \p cparam should be a pointer to a previously initialized
   * ::cnnlConvolutionCastMode_t. */
  CNNL_DECONV_DESC = 12,
  /*!< The descriptor of the deconvolution operation. In the setting, the \p cparam should be a
   * pointer to a previously initialized ::cnnlDeconvolutionDescriptor_t. */
  CNNL_SCALAR_DECONV_ALGO = 13,
  /*!< The scalar describing the deconvolution algorithms that is used to implement the
   * ::cnnlDeconvolution operation. In the setting, the \p cparam should be
   * a pointer to a previously initialized ::cnnlDeconvolutionAlgo_t. */
  CNNL_MASKZERO_LABEL_DESC CNNL_DEPRECATED_ENUM_FOR() = 14,
  /*!< The descriptor of the label used in
   * ::cnnlMaskZero operation. In the setting, the \p cparam should be a pointer
   * to a previously initialized ::cnnlTensorDescriptor_t.
   *
   * After CNNL 2.0, ::CNNL_MASKZERO_LABEL_DESC will be deprecated
   *   for no longer supporting the ::cnnlMaskZero operation.*/
  CNNL_PRE_ACTIVATION_DESC = 15,
  /*!< The descriptor of the ::cnnlActivationForward operation that is implemented before
   * the ::cnnlConvolutionForward operation. In the setting, the \p cparam
   * should be a pointer to a previously initialized ::cnnlActivationDescriptor_t. */
  CNNL_SCALAR_DECONV_CAST_MODE = 16,
  /*!< The scalar describing the cast mode used to convert the data type of
   * input and output tensors for ::cnnlDeconvolutionInference function.
   * In the setting, the \p cparam should be a pointer to a previously initialized
   * ::cnnlDeconvolutionCastMode_t. */
  CNNL_ADAPTIVE_QUANTIZE_PLACEHOLDER = 17,
  /*!< Adaptive quantization is used.
   * In the setting, the \p cparam should be a pointer to a previously
   * initialized ::cnnlFusedOpsPointerPlaceHolder_t.
   * If adaptive quantization is needed, set this enum to ::CNNL_PTR_VALID.
   * Otherwise, set it to ::CNNL_PTR_NULL or leave it unset.*/
  CNNL_ADD_INPUT1_DESC = 18,
  /*!< The descriptor of the first input tensor \p a used in ::cnnlOpTensor operation. In the setting,
   * \p cparam should be a pointer to a previously initialized ::cnnlTensorDescriptor_t. */
  CNNL_POST_ACTIVATION_DESC = 19,
  /*!< The descriptor of the ::cnnlActivationForward operation that is implemented after
   * the ::cnnlOpTensor operation. In the setting, \p cparam
   * should be a pointer to a previously initialized ::cnnlActivationDescriptor_t. */
  CNNL_FUSE_OPS_CONST_PARAM_MAX
} cnnlFusedOpsConstParamLabel_t;

/*!
 * @brief Enumeration variables describing the supported variant parameter type used
 * in the fusion operation. The variant parameter is the pointer to the host or device
 * memory, which can be changed in each iteration.
 *
 * - Setting:
 *
 *   You need to call ::cnnlSetFusedOpsVariantParamPackAttribute to set specific
 *   variant parameter to a previously created ::cnnlFusedOpsVariantParamPack_t pack that stores
 *   all variant parameters used in the fusion operation.
 * - Getting:
 *
 *   You need to call ::cnnlGetFusedOpsVariantParamPackAttribute to get a specific
 *   variant parameter from the previously created and initialized ::cnnlFusedOpsVariantParamPack_t pack.
 */
typedef enum {
  CNNL_PTR_X = 0,
  /*!< Pointer to the device memory that stores the input tensor \p x.
   * In the setting, \p vparam with the void * type is expected to be passed in. */
  CNNL_PTR_W = 1,
  /*!< Pointer to the device memory that stores the filter tensor \p w.
   * In the setting, \p vparam with the void * type is expected to be passed in. */
  CNNL_PTR_Y = 2,
  /*!< Pointer to the device memory that stores the output tensor \p y.
   * In the setting, \p vparam with the void * type is expected to be passed in. */
  CNNL_PTR_BIAS = 3,
  /*!< Pointer to the device memory that stores the bias tensor. In the setting, \p vparam
   * with the void * type is expected to be passed in. */
  CNNL_PTR_BN_MEAN = 4,
  /*!< Pointer to the device memory that stores the mean tensor used in
   * ::cnnlBatchNormForwardInference operation. In the setting, \p vparam with the void * type
   * is expected to be passed in. */
  CNNL_PTR_BN_VAR = 5,
  /*!< Pointer to the device memory that stores the variance tensor used in
   * ::cnnlBatchNormForwardInference operation. In the setting, \p vparam with the void * type
   * is expected to be passed in. */
  CNNL_PTR_BN_WEIGHT = 6,
  /*!< Pointer to the device memory that stores the filter tensor used in
   * ::cnnlBatchNormForwardInference operation. In the setting, \p vparam with the void * type
   * is expected to be passed in. */
  CNNL_PTR_BN_BIAS = 7,
  /*!< Pointer to the device memory that stores the bias tensor used in
   * ::cnnlBatchNormForwardInference operation. In the setting, \p vparam with the void * type
   * is expected to be passed in. */
  CNNL_PTR_SCALE_ALPHA = 8,
  /*!< Pointer to the device memory that stores the alpha tensor used in ::cnnlScale operation.
   * In the setting, \p vparam with the void * type is expected to be passed in. */
  CNNL_PTR_SCALE_BETA = 9,
  /*!< Pointer to the device memory that stores the beta tensor used in ::cnnlScale operation.
   * In the setting, \p vparam with the void * type is expected to be passed in. */
  CNNL_PTR_PRELU_ALPHA = 10,
  /*!< Pointer to the device memory that stores the alpha tensor used in ::cnnlPrelu operation.
   * In the setting, \p vparam with the void * type is expected to be passed in. */
  CNNL_PTR_WORKSPACE = 11,
  /*!< Pointer to the workspace memory on the device allocated by user. In the setting,
   * \p vparam with the void * type is expected to be passed in. */
  CNNL_SCALAR_WORKSPACE_SIZE = 12,
  /*!< Scalar in the host memory describing the workspace size allocated by users in bytes.
   * The size needs to be equal to or larger than that requested in ::cnnlMakeFusedOpsPlan.
   * In the setting, \p vparam with the void * type is expected to be passed in. */
  CNNL_SCALAR_BN_EPSILON = 13,
  /*!< Scalar value added to the denominator for numerical stability in
   * ::cnnlBatchNormForwardInference operation. In the setting, \p vparam with the void * type
   * is expected to be passed in. */
  CNNL_PTR_MASKZERO_LABEL CNNL_DEPRECATED_ENUM_FOR() = 14,
  /*!< Pointer to the device memory that stores the label tensor used in
   * ::cnnlMaskZero operation. In the setting, \p vparam with the void * type
   * is expected to be passed in.
   *
   * After CNNL 2.0, ::CNNL_PTR_MASKZERO_LABEL will be deprecated
   *   for no longer supporting the ::cnnlMaskZero operation.*/
  CNNL_SCALAR_MASKZERO_PAD_LABEL CNNL_DEPRECATED_ENUM_FOR() = 15,
  /*!< Scalar value used to compare with label in ::cnnlMaskZero operation.
   * In the setting, \p vparam with the void * type is expected to be passed in.
   *
   * After CNNL 2.0, ::CNNL_SCALAR_MASKZERO_PAD_LABEL will be deprecated
   *   for no longer supporting the ::cnnlMaskZero operation.*/
  CNNL_X_POSITION = 16,
  /*!< Scalar value used to compare with label in ::cnnlQuantizeConvolutionForward operation.
   * In the setting, \p vparam with the void * type is expected to be passed in. */
  CNNL_X_SCALE = 17,
  /*!< Scalar value used to compare with label in ::cnnlQuantizeConvolutionForward operation.
   * In the setting, \p vparam with the void * type is expected to be passed in. */
  CNNL_X_OFFSET = 18,
  /*!< Scalar value used to compare with label in ::cnnlQuantizeConvolutionForward operation.
   * In the setting, \p vparam with the void * type is expected to be passed in. */
  CNNL_W_POSITION = 19,
  /*!< Scalar value used to compare with label in ::cnnlQuantizeConvolutionForward operation.
   * In the setting, \p vparam with the void * type is expected to be passed in. */
  CNNL_W_SCALE = 20,
  /*!< Scalar value used to compare with label in ::cnnlQuantizeConvolutionForward operation.
   * In the setting, \p vparam with the void * type is expected to be passed in. */
  CNNL_W_OFFSET = 21,
  /*!< Scalar value used to compare with label in ::cnnlQuantizeConvolutionForward operation.
   * In the setting, \p vparam with the void * type is expected to be passed in. */
  CNNL_PTR_ADD_INPUT1 = 22,
  /*!< Pointer to the device memory that stores the first input tensor \p a used in
   * ::cnnlOpTensor operation. In the setting, \p vparam with the void * type
   * is expected to be passed in. */
  CNNL_PTR_ADD_ALPHA1 = 23,
  /*!< A host pointer to the scaling factor of tensor \p a used in ::cnnlOpTensor operation.
   * In the setting, \p vparam with the void * type is expected to be passed in. */
  CNNL_PTR_ADD_ALPHA2 = 24,
  /*!< A host pointer to the scaling factor of tensor \p b used in ::cnnlOpTensor operation.
   * In the setting, the \p vparam with the void * type is expected to be passed in. */
  CNNL_FUSE_OPS_VAR_PARAM_MAX
} cnnlFusedOpsVariantParamLabel_t;

// Group:FusedOp
/*!
 * @brief Creates a plan descriptor pointed by \p fusion_plan for the fusion operation,
 * and allocates memory for holding the information about the fusion operation.
 * The information is defined in ::cnnlFusedOpsPlan_t.
 *
 * @param[out] fusion_plan
 *  Output. Pointer to the instance of the fusion plan created by this function.
 * @param[in] fusion_type
 *  Input. The specific sequence of fusion operations for which this fusion plan
 *  should be created. For more information, see ::cnnlFusedOps_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_NOT_INITIALIZED,
 *   ::CNNL_STATUS_ALLOC_FAILED, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetFusedOpsConstParamPackAttribute
 *   and ::cnnlSetFusedOpsVariantParamPackAttribute function to initialize and set the
 *   information to the fusion plan.
 * - You need to call the ::cnnlDestroyFusedOpsPlan function to destroy the plan.
 *   Otherwise, the memory leak may occur.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateFusedOpsPlan(cnnlFusedOpsPlan_t *fusion_plan,
                                                 const cnnlFusedOps_t fusion_type);

// Group:FusedOp
/*!
 * @brief Creates a constant parameter pack pointed by \p cparam_pack for the fusion operation,
 * and allocates memory for holding the information about the constant parameter.
 *
 * @param[in] cparam_pack
 *  Input. Pointer to the constant parameter pack that holds information about
 *  fusion operation, such as the descriptors of tensor and operation.
 * @param[in] fusion_type
 *  Input. The specific sequence of fusion operations to perform as defined in the
 *  enumeration type ::cnnlFusedOps_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_NOT_INITIALIZED,
 *   ::CNNL_STATUS_ALLOC_FAILED, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par API Dependency
 * - After calling this function, you need to call the ::cnnlSetFusedOpsConstParamPackAttribute
 *   to add specific parameter to the constant parameter pack one by one.
 * - You need to call the ::cnnlDestroyFusedOpsConstParamPack function to
 *   destroy the pack. Otherwise, the memory leak may occur.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateFusedOpsConstParamPack(
    cnnlFusedOpsConstParamPack_t *cparam_pack,
    const cnnlFusedOps_t fusion_type);

// Group:FusedOp
/*!
 * @brief Adds the constant parameter pointed by the \p cparam to \p cparam_pack with
 * constant parameter type \p cparam_label. The \p cparam should be a pointer
 * pointed to a previously initialized constant parameter. The supported constant parameter
 * type is defined in ::cnnlFusedOpsConstParamLabel_t.
 *
 * @param[in] cparam_pack
 *  Input. Pointer to the constant parameter pack that holds information about
 *  fusion operation, such as the descriptors of tensor and operation.
 * @param[in] cparam_label
 *  Input. The \p cparam_label indicates the constant parameter type pointed to by the \p cparam.
 *  For detailed information, see ::cnnlFusedOpsConstParamLabel_t.
 * @param[in] cparam
 *  Input. Pointer to the specific constant parameter on the host memory. The type of
 *  the constant parameter depends on the value of \p cparam_label, and the values or
 *  opaque structure pointed by this pointer will be copied into the \p cparam_pack.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_NOT_INITIALIZED
 *   ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, you should call the
 *   ::cnnlCreateFusedOpsConstParamPack to initialize the constant parameter pack.
 * - After calling this function, you can call the ::cnnlGetFusedOpsConstParamPackAttribute
 *   to get the specific constant parameter from the constant parameter pack.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetFusedOpsConstParamPackAttribute(
    cnnlFusedOpsConstParamPack_t cparam_pack,
    const cnnlFusedOpsConstParamLabel_t cparam_label,
    const void *cparam);

// Group:FusedOp
/*!
 * @brief Retrieves the constant parameter with the type of \p cparam_label from
 * \p cparam_pack. The parameter type is defined in ::cnnlFusedOpsConstParamLabel_t and the
 * retrieved value or opaque structure will be copied to the host memory buffer pointed by
 * the \p cparam.
 *
 * @param[in] cparam_pack
 *  Input. Pointer to the constant parameter pack that holds information about
 *  fusion operation, such as the descriptors of tensor and operation.
 * @param[in] cparam_label
 *  Input. The \p cparam_label indicates the constant parameter type pointed to by the \p cparam.
 *  For detailed information, see ::cnnlFusedOpsConstParamLabel_t.
 * @param[in] cparam
 *  Input. Pointer to the specific constant parameter that should be retrieved on the host memory.
 *  The type of the constant parameter depends on the value of \p cparam_label. The value or the opaque
 *  structure in the \p cparam_pack will be copied to the host memory buffer pointed by the
 *  \p cparam. If no specific constant parameter type \p cparam_label can be found, the pointer \p cparam
 *  will be set to NULL.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_NOT_INITIALIZED
 *   ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, you should call the
 *   ::cnnlSetFusedOpsConstParamPackAttribute to add specific parameter to constant parameter pack.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetFusedOpsConstParamPackAttribute(
    const cnnlFusedOpsConstParamPack_t cparam_pack,
    const cnnlFusedOpsConstParamLabel_t cparam_label,
    void *cparam);

// Group:FusedOp
/*!
 * @brief Destroys a constant parameter pack \p cparam_pack in fusion operation that is
 * previously created with the ::cnnlCreateFusedOpsConstParamPack function.
 *
 * @param[in] cparam_pack
 *   Input. The constant parameter pack to be destroyed.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - Before calling this function, you should call the
 *   ::cnnlCreateFusedOpsConstParamPack and ::cnnlFusedOpsExecute function.
 *   Otherwise, ::CNNL_STATUS_BAD_PARAM is returned.
 * - This function should be called to destroy the constant parameter pack.
 *   Otherwise, the memory leak may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyFusedOpsConstParamPack(
     cnnlFusedOpsConstParamPack_t cparam_pack);

// Group:FusedOp
/*!
 * @brief Creates a variant parameter pack pointed by \p vparam_pack for the fusion operation,
 * and allocates memory for holding the information about the variant parameter.
 *
 * @param[in] vparam_pack
 *  Input. Pointer to the variant parameter pack that holds information about
 *  the variant parameters of fusion operation. For detailed information,
 *  see ::cnnlFusedOpsVariantParamPack_t.
 * @param[in] fusion_type
 *  Input. The specific sequence of fusion operations to perform as defined in the
 *  enumeration type ::cnnlFusedOps_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_NOT_INITIALIZED,
 *   ::CNNL_STATUS_ALLOC_FAILED, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par API Dependency
 * - After calling this function, you need to call the ::cnnlSetFusedOpsVariantParamPackAttribute
 *   to add specific parameter to the variant parameter pack one by one.
 * - You need to call the ::cnnlDestroyFusedOpsVariantParamPack function to
 *   destroy the pack. Otherwise, the memory leak may occur.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateFusedOpsVariantParamPack(
    cnnlFusedOpsVariantParamPack_t *vparam_pack,
    const cnnlFusedOps_t fusion_type);

// Group:FusedOp
/*!
 * @brief Adds a specific variant parameter \p vparam to \p vparam_pack with variant parameter type
 * \p vparam_label. The supported variant parameter type is defined in ::cnnlFusedOpsVariantParamLabel_t.
 *
 * @param[in] vparam_pack
 *  Input. Pointer to the variant parameter pack that holds information about the
 *  variant parameters of fusion operation.
 * @param[in] vparam_label
 *  Input. Enumeration variable describing the variant parameter type to be set in this function.
 *  For detailed information, see ::cnnlFusedOpsVariantParamLabel_t.
 * @param[in] vparam
 *  Input. Pointer to the host or device memory that stores the variant parameter.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_NOT_INITIALIZED
 *   ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, you should call the
 *   ::cnnlCreateFusedOpsVariantParamPack to initialize the variant parameter pack.
 * - After calling this function, you can call the
 *   ::cnnlGetFusedOpsVariantParamPackAttribute to get the specific variant parameter
 *   from the variant parameter pack.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetFusedOpsVariantParamPackAttribute(
    cnnlFusedOpsVariantParamPack_t vparam_pack,
    const cnnlFusedOpsVariantParamLabel_t vparam_label,
    const void *vparam);

// Group:FusedOp
/*!
 * @brief Retrieves a specific parameter \p vparam from \p vparam_pack by variant parameter type
 * \p vparam_label. The supported variant parameter type is defined in ::cnnlFusedOpsVariantParamLabel_t.
 *
 * @param[in] vparam_pack
 *  Input. Pointer to the variant parameter pack that holds information about
 *  the variant parameters of fusion operation.
 * @param[in] vparam_label
 *  Input. Enumeration variable describing the variant parameter type in the \p vparam_pack.
 *  The retrieved variant parameter values vary according to the value of this parameter.
 *  For detailed information, see ::cnnlFusedOpsVariantParamLabel_t.
 * @param[out] vparam
 *  Output. Pointer to the host or device memory where the retrieved value is written into. If no
 *  specific \p vparam_label can be found in \p vparam_pack, this pointer \p vparam will be set to NULL.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_NOT_INITIALIZED
 *   ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, you should call the
 *   ::cnnlSetFusedOpsVariantParamPackAttribute to add specific parameter to variant parameter
 *   pack.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetFusedOpsVariantParamPackAttribute(
    const cnnlFusedOpsVariantParamPack_t vparam_pack,
    const cnnlFusedOpsVariantParamLabel_t vparam_label,
    void *vparam);

// Group:FusedOp
/*!
 * @brief Destroys a variant parameter pack \p vparam_pack in fusion operation that is
 *        previously created with the ::cnnlCreateFusedOpsVariantParamPack function.
 *
 * @param[in] vparam_pack
 *   Input. The variant parameter pack to be destroyed.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - Before calling this function, you should call the
 *   ::cnnlCreateFusedOpsVariantParamPack and ::cnnlFusedOpsExecute function.
 *   Otherwise, ::CNNL_STATUS_BAD_PARAM is returned.
 * - This function should be called to destroy the variant parameter pack.
 *   Otherwise, the memory leak may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyFusedOpsVariantParamPack(
    cnnlFusedOpsVariantParamPack_t vparam_pack);

// Group:FusedOp
/*!
 * @brief Returns in \p size the byte size of the MLU memory that is used as an extra
 *        workspace to optimize the fusion operation. The size of the extra workspace is
 *        based on the constant parameters of the fusion operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the fusion operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] fusion_plan
 *   Input. Pointer to a previously created and initialized fusion plan. For detailed information,
 *   see ::cnnlFusedOpsPlan_t.
 * @param[in] cparam_pack
 *   Input. Pointer to the constant parameter pack in fusion operation.
 *   For detailed information, see ::cnnlFusedOpsConstParamPack_t.
 * @param[out] size
 *   Output. Pointer to the returned size of the extra workspace in bytes that
 *   is used in the fusion operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, you should call the ::cnnlCreateFusedOpsPlan,
 *   the ::cnnlCreateFusedOpsConstParamPack and the ::cnnlSetFusedOpsConstParamPackAttribute
 *   functions to create and set the constant parameter pack \p cparam_pack.
 * - The allocated extra workspace should be added to the variant parameter by
 *   ::cnnlSetFusedOpsVariantParamPackAttribute function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlMakeFusedOpsPlan(cnnlHandle_t handle,
    const cnnlFusedOpsPlan_t fusion_plan,
    const cnnlFusedOpsConstParamPack_t cparam_pack,
    size_t *size);

// Group:FusedOp
/*!
 * @brief Computes a series operations which initialized in \p fusion_plan.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the convolution operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] fusion_plan
 *   Input. Pointer to a previously created and initialized fusion plan.
 *   For detailed information, see ::cnnlFusedOpsPlan_t.
 * @param[in] vparam_pack
 *   Input. Pointer to the variant parameter pack in fusion operation.
 *   For detailed information, see ::cnnlFusedOpsVariantParamPack_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par API Dependency
 * - Before calling this function to implement fusion operation, you need to prepare
 *   all the parameters passed to this function. See each parameter description
 *   for details.
 *
 * - To perform fusion operations, you need to call the related APIs with the following order:
 *   1. Create a fusedop plan with ::cnnlCreateFusedOpsPlan.
 *   2. Create a pack that stores the constant parameters with ::cnnlCreateFusedOpsConstParamPack.
 *   3. Set a constant parameter to the pack with ::cnnlSetFusedOpsConstParamPackAttribute.
 *      This function can only set a parameter for each call. You need to call this function
 *      multiple times to set all constant parameters.
 *   4. Retrieve the constant parameter you set in the pack with ::cnnlGetFusedOpsConstParamPackAttribute.
 *      This function can only retrieve a parameter for each call. You need to call this function
 *      multiple times to retrieve all constant parameters.
 *   5. Create a pack to store variant parameters with ::cnnlCreateFusedOpsVariantParamPack.
 *   6. Set a variant parameter to the pack with ::cnnlSetFusedOpsVariantParamPackAttribute.
 *      This function can only set a parameter for each call. You need to call this function
 *      multiple times to set all variant parameters.
 *   7. Retrieve the variant parameter you set in the pack with ::cnnlGetFusedOpsVariantParamPackAttribute.
 *      This function can only retrieve a parameter for each call. You need to call this function
 *      multiple times to retrieve all variant parameters.
 *   8. Retrieve the extra workspace size to be used in the fusion operation with ::cnnlMakeFusedOpsPlan.
 *   9. Release all the resoureces with ::cnnlFusedOpsExecute, ::cnnlDestroyFusedOpsConstParamPack,
 *      ::cnnlDestroyFusedOpsVariantParamPack, ::cnnlDestroyFusedOpsPlan accrodingly.
 *
 * @note
 * - The parameter \p onchip_dtype in \p output_desc can be different from \p dtype when cnnlFusedOpsPlan_t
 *   is CNNL_ACTIVATION_CONV_BN_ACTIVATION_MASKZERO or CNNL_ACTIVATION_CONV_BN_MASKZERO_ACTIVATION.
 * - Except CNNL_DECONV_SCALE or CNNL_DECONV_SCALE_BN_ACTIVATION, others do not support CNNL_LAYOUT_NHWC
 *   and CNNL_LAYOUT_NDHWC about the parameter \p layout in output_desc.
 * - After CNNL 2.0, the combination of half input data type and int16 input onchip data type
 *   will be deprecated.
 * - After CNNL 2.0, the combination of half filter data type and int16 filter onchip data type
 *   will be deprecated.
 * - After CNNL 2.0, the combination of int16 output data type and half output onchip data type
 *   will be deprecated.
 * - After CNNL 2.0, ::CNNL_ACTIVATION_CONV_BN_ACTIVATION_MASKZERO will be deprecated
 *   for no longer supporting the ::cnnlMaskZero operation.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlFusedOpsExecute(cnnlHandle_t handle,
                                              const cnnlFusedOpsPlan_t fusion_plan,
                                              const cnnlFusedOpsVariantParamPack_t vparam_pack);

// Group:FusedOp
/*!
 * @brief Destroys a fusion plan \p fusion_plan that was previously created with the
 *        ::cnnlCreateFusedOpsPlan function.
 *
 * @param[in] fusion_plan
 *   Input. The fusion plan to be destroyed.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - Before calling this function, you should call the ::cnnlCreateFusedOpsPlan
 *   and ::cnnlFusedOpsExecute function. Otherwise, ::CNNL_STATUS_BAD_PARAM is returned.
 * - This function should be called to destroy the fusion plan. Otherwise,
 *   the memory leak may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyFusedOpsPlan(cnnlFusedOpsPlan_t fusion_plan);

// Group:AdaptivePoolingForward
/*!
 * @brief Computes average and maximum pooling of \p input tensor. Different from ::cnnlPoolingForward,
 * ::cnnlAdaptivePoolingForward calculates pooling based on the dimensions of \p input tensor and
 * \p output tensor. This operation can calculate the kernel and stride parameters of
 * ::cnnlPoolingForward automatically.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlAdaptivePoolingForward_v2 instead, which may improve the precision and performance of some
 *   cases in the \p CNNL_POOLING_AVERAGE_COUNT_INCLUDE_PADDING mode of this operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   adaptive_pooling_forward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the \p input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the \p input tensor.
 * @param[in] mode
 *   Input. The \p mode parameter is defined in ::cnnlPoolingMode_t.
 *   This operation only supports the \p CNNL_POOLING_MAX and the
 *   \p CNNL_POOLING_AVERAGE_COUNT_INCLUDE_PADDING modes.
 * @param[in] output_desc
 *   Input. The descriptor of the \p output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the \p output tensor, which is the results of
 *   this operation.
 * @param[in] index_desc
 *   Input. The descriptor of the \p index tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] index
 *   Output. Pointer to the MLU memory that stores the pooling index in the \p CNNL_POOLING_MAX mode.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_INTERNAL_ERROR, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *   input tensor \p input - output tensor \p output - output tensor \p index.
 *   - half - half - int16
 *   - float - float - int32
 * - On MLU500 series, this function additionally supports the following data type combinations:
 *   - bfloat16 - bfloat16 - int16
 *
 * @par Data Layout
 * - The supported data layouts are as follows:
 *   Note that the data layouts of the \p input tensor, \p output tensor, and \p index tensor should be the same.
 *   - input tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC.
 *   - output tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC.
 *   - index tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC.
 *
 * @par Scale Limitation
 * - The input tensor, output tensor, and mode must meet the following requirements:
 *   - input tensor: \p batch > 0, \p depth > 0, \p height > 0, \p width > 0, and \p channel > 0.
 *   - output tensor: In the maxpool with index, kernel size cannot be too big, and should satisfy
 *     (\p input_h / \p output_h + 2) * (\p input_w / \p output_w + 2) <= 3582.
 *   - \p mode: \p mode == \p CNNL_POOLING_AVERAGE_COUNT_INCLUDE_PADDING or \p mode ==
 *     \p CNNL_POOLING_MAX for 2D pooling, and \p mode == \p CNNL_POOLING_AVERAGE_COUNT_INCLUDE_PADDING
 *     for 3D pooling.
 *
 * @note
 * - This operation only supports 2D and 3D pooling. 3D pooling only supports average mode.
 * - In the average pooling, set the \p index_desc and \p index to NULL.
 * - In the maximum pooling, if you do not need pooling index, set the \p index_desc
 *   and \p index tensor to NULL.
 * - In the 2D maximum pooling, when the kernel of the pooling contains NaN:
 *   - On MLU200 series:
 *    - The \p output value is the NaN, and the \p index value is the index of the last NaN.
 *   - On MLU300 series and CE3226:
 *    - If the last value in the kernel of the pooling is NaN, the \p output value is NaN and
 *      the \p index is the index of the last value. Otherwise, the \p output value is the maximum
 *      value after the last NaN and the \p index value is the index of the output value.
 *   - On MLU500 series:
 *    - If the kernel of the pooling contains NaN, the \p output value is NaN and the \p index is
 *      the index of the first NaN value. Otherwise, the \p output value is the maximum value and
 *      the \p index value is the index of the first maximum value.
 * - Large tensor support: The operator now supports tensors with a total number of elements greater than 2 billion (2G) on MLU500 series.
*    Regardless of the \p mode, no single dimension can exceed 2 billion (2G).
 *   - 2D mode:
 *     - AVG mode: Input and output tensors can both exceed 2 billion (2G).
 *     - MAX mode: Only input tensor can exceed 2 billion (2G).
 *   - 3D mode:
 *     - AVG mode: Input and output tensors can both exceed 2 billion (2G).
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
     input tensor by 1 *1 * 14 * 1
     --> input:[[[[0,1,2,3,4,5,6,7,8,9,10,11,12,13]]]]
     --> index: NULL
     param:
       mode: CNNL_POOLING_MAX
     output array by 1 * 1 * 4 * 1 -->output [[[[3,6,10,13]]]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/master/generated/torch.nn.AdaptiveMaxPool2d.html
 * - https://pytorch.org/docs/master/generated/torch.nn.AdaptiveAvgPool2d.html
 * - https://pytorch.org/docs/master/generated/torch.nn.AdaptiveAvgPool3d.html
 */
CNNL_DEPRECATED_FOR(cnnlAdaptivePoolingForward_v2)
cnnlStatus_t CNNL_WIN_API cnnlAdaptivePoolingForward(cnnlHandle_t handle,
                                                     const cnnlTensorDescriptor_t input_desc,
                                                     const void * input,
                                                     const cnnlPoolingMode_t mode,
                                                     const cnnlTensorDescriptor_t output_desc,
                                                     void * output,
                                                     const cnnlTensorDescriptor_t index_desc,
                                                     void * index);

// Group:AdaptivePooling
/*!
 * @brief Computes average and maximum pooling of \p input tensor. Different from ::cnnlPoolingForward,
 * ::cnnlAdaptivePoolingForward calculates pooling based on the dimensions of \p input tensor and
 * \p output tensor. This operation can calculate the kernel and stride parameters of ::cnnlPoolingForward
 * automatically. Compared with ::cnnlAdaptivePoolingForward, this function adds another two parameters
 * \p workspace and \p workspace_size, which provide the extra space needed in computation. Specifically,
 * it may improve the precision and performance of some cases in the \p CNNL_POOLING_AVERAGE_COUNT_INCLUDE_PADDING
 * mode of this operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   adaptive_pooling_forward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the \p input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the \p input tensor.
 * @param[in] mode
 *   Input. The \p mode parameter is defined in ::cnnlPoolingMode_t.
 *   This operation only supports the \p CNNL_POOLING_MAX and the
 *   \p CNNL_POOLING_AVERAGE_COUNT_INCLUDE_PADDING modes.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the adaptive_pooling_forward operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the adaptive_pooling_forward operation.
 *   You can get the size of the workspace with the ::cnnlGetAdaptivePoolingForwardWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the \p output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the \p output tensor, which is the results of
 *   this operation.
 * @param[in] index_desc
 *   Input. The descriptor of the \p index tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] index
 *   Output. Pointer to the MLU memory that stores the pooling index in the \p CNNL_POOLING_MAX mode.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_INTERNAL_ERROR, ::CNNL_STATUS_EXECUTION_FAILED, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *   input tensor \p input - output tensor \p output - output tensor \p index.
 *   - half - half - int16
 *   - float - float - int32
 * - On MLU500 series, this function additionally supports the following data type combinations:
 *   - bfloat16 - bfloat16 - int16
 *
 * @par Data Layout
 * - The supported data layouts are as follows:
 *   Note that the data layouts of the \p input tensor, \p output tensor, and \p index tensor should be the same.
 *   - input tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC.
 *   - output tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC.
 *   - index tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC.
 *
 * @par Scale Limitation
 * - The input tensor, output tensor, and mode must meet the following requirements:
 *   - input tensor: \p batch > 0, \p depth > 0, \p height > 0, \p width > 0, and \p channel > 0.
 *   - output tensor: In the maxpool with index, kernel size cannot be too big, and should satisfy
 *     (\p input_h / \p output_h + 2) * (\p input_w / \p output_w + 2) <= 3582.
 *   - \p mode: \p mode == \p CNNL_POOLING_AVERAGE_COUNT_INCLUDE_PADDING or \p mode ==
 *     \p CNNL_POOLING_MAX for 2D pooling, and \p mode == \p CNNL_POOLING_AVERAGE_COUNT_INCLUDE_PADDING
 *     for 3D pooling.
 *
 * @note
 * - This operation only supports 2D and 3D pooling. 3D pooling only supports average mode.
 * - In the average pooling, set the \p index_desc and \p index to NULL.
 * - In the maximum pooling, if you do not need pooling index, set the \p index_desc
 *   and \p index tensor to NULL.
 * - In the 2D maximum pooling, when the kernel of the pooling contains NaN:
 *   - On MLU200 series:
 *    - The \p output value is the NaN, and the \p index value is the index of the last NaN.
 *   - On MLU300 series and CE3226:
 *    - If the last value in the kernel of the pooling is NaN, the \p output value is NaN and
 *      the \p index is the index of the last value. Otherwise, the \p output value is the maximum
 *      value after the last NaN and the \p index value is the index of the output value.
 *   - On MLU500 series:
 *    - If the kernel of the pooling contains NaN, the \p output value is NaN and the \p index is
 *      the index of the first NaN value. Otherwise, the \p output value is the maximum value and
 *      the \p index value is the index of the first maximum value.
 * - Large tensor support: The operator now supports tensors with a total number of elements greater than 2 billion (2G) on MLU500 series.
*    Regardless of the \p mode, no single dimension can exceed 2 billion (2G).
 *   - 2D mode:
 *     - AVG mode: Input and output tensors can both exceed 2 billion (2G).
 *     - MAX mode: Only input tensor can exceed 2 billion (2G).
 *   - 3D mode:
 *     - AVG mode: Input and output tensors can both exceed 2 billion (2G).
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
     input tensor by 1 *1 * 14 * 1
     --> input:[[[[0,1,2,3,4,5,6,7,8,9,10,11,12,13]]]]
     --> index: NULL
     param:
       mode: CNNL_POOLING_MAX
     output array by 1 * 1 * 4 * 1 -->output [[[[3,6,10,13]]]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/master/generated/torch.nn.AdaptiveMaxPool2d.html
 * - https://pytorch.org/docs/master/generated/torch.nn.AdaptiveAvgPool2d.html
 * - https://pytorch.org/docs/master/generated/torch.nn.AdaptiveAvgPool3d.html
 */
cnnlStatus_t CNNL_WIN_API cnnlAdaptivePoolingForward_v2(cnnlHandle_t handle,
                                                        const cnnlTensorDescriptor_t input_desc,
                                                        const void * input,
                                                        const cnnlPoolingMode_t mode,
                                                        void *workspace,
                                                        size_t workspace_size,
                                                        const cnnlTensorDescriptor_t output_desc,
                                                        void * output,
                                                        const cnnlTensorDescriptor_t index_desc,
                                                        void * index);
// Group:AdaptivePooling
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used to get extra space size
 *        in adaptive_pooling_forward operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   adaptive_pooling_forward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of input tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] mode
 *   Input. The \p mode parameter is defined in ::cnnlPoolingMode_t.
 *   This operation only supports the \p CNNL_POOLING_MAX and the
 *   \p CNNL_POOLING_AVERAGE_COUNT_INCLUDE_PADDING mode.
 * @param[in] output_desc
 *   Input. The descriptor of the \p output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the adaptive_pooling_forward
 *   operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This API is only used along with ::cnnlAdaptivePoolingForward_v2. ::cnnlAdaptivePoolingForward does not require this API.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetAdaptivePoolingForwardWorkspaceSize(cnnlHandle_t handle,
                                                          const cnnlTensorDescriptor_t input_desc,
                                                          const cnnlPoolingMode_t mode,
                                                          const cnnlTensorDescriptor_t output_desc,
                                                          size_t *workspace_size);


// Group:IndexAdd
/*!
 * @brief Adds vectors or scalars from \p input_b into \p input_a along \p dim according to the entries in \p index and
 * stores the results into \p output, then leaves other values in \p input_a as they are in \p output. Supports in-place: \p output = \p input_a.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlIndexAdd_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the index add operation. For more detailed information, see ::cnnlHandle_t.
 * @param[in] dim
 *   Input. The dimension of the \p input_a and \p input_b to be indexed.
 * @param[in] input_a_desc
 *   Input. The descriptor of the \p input_a. For more detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input_a
 *   Input. Pointer to the MLU memory that stores the input tensor \p input_a which to be indexed and added into.
 * @param[in] index_desc
 *   Input. The descriptor of the \p index tensors. For more detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] index
 *   Input. Pointer to the MLU memory that stores the input tensor \p index which maps vectors or scalars from \p input_a to \p input_b.
 * @param[in] input_b_desc
 *   Input. The descriptor of the \p input_b. For more detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input_b
 *   Input. Pointer to the MLU memory that stores the input tensor \p input_b which to be added.
 * @param[in] output_desc
 *   Input. The descriptor of the \p output tensor. For more detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Index Add Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *   input tensor \p input_a - input tensor \p input_b - output tensor \p output.
 *   - float - float - float
 *   - half - half - half
 *   - int32 - int32 - int32
 *   - int16 - int16 - int16
 *   - bfloat16 - bfloat16 - bfloat16
 * - \p dim: int64
 * - \p index: int32, int64
 *
 * @par Scale Limitation
 * - The \p input_a tensor, \p index tensor, \p input_b tensor and \p dim must meet the following
 *   requirements:
 *   - The dim-th dimension of \p input_b must have the same size as the length of
 *        \p index.
 *   - \p index must be a vector.
 *   - All dimensions apart from the dim-th dimension of \p input_b must match
 *        all other dimensions of \p input_a.
 *
 * @note
 * - The accuracy of computing result might decrease when the data type of \p input_a and \p input_b is half.
 * - Multiple accumulations at the same place may cause data overflow.
 * - The maximum of \p index must be smaller than the length of the dim-th dimension of \p input_a and every element
 *   of \p index must be greater than or equal to zero.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the index add operation is as follows:
     @verbatim
     dim = 0
     input_a array
       input_a = [[1, 1, 1],
                  [1, 1, 1],
                  [1, 1, 1],
                  [1, 1, 1],
                  [1, 1, 1]]
     index array
       index = [0, 4, 2]
     input_b array
       input_b = [[1, 2, 3],
                  [4, 5, 6],
                  [7, 8, 8]]
     output array
       output = [[2, 3, 4],
                 [1, 1, 1],
                 [8, 9, 10],
                 [1, 1, 1],
                 [5, 6, 7]]
     @endverbatim
 */
CNNL_DEPRECATED_FOR(cnnlIndexAdd_v2)
cnnlStatus_t CNNL_WIN_API cnnlIndexAdd(cnnlHandle_t handle,
                                       const int64_t dim,
                                       const cnnlTensorDescriptor_t input_a_desc,
                                       const void *input_a,
                                       const cnnlTensorDescriptor_t index_desc,
                                       const void *index,
                                       const cnnlTensorDescriptor_t input_b_desc,
                                       const void *input_b,
                                       const cnnlTensorDescriptor_t output_desc,
                                       void *output);

// Group:IndexAdd
/*!
 * @brief Adds vectors or scalars from \p input_b into \p input_a along \p dim according to the entries in \p index, and
 * stores the results into \p output, leaving other values in \p input_a as they are in \p output.
 * It supports in-place operation: \p output = \p input_a. Compared with ::cnnlIndexAdd, this function adds two parameters
 * \p workspace and \p workspace_size, which provide the extra space needed in computation in order to support the int64 input data type.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the index add operation. For more detailed information, see ::cnnlHandle_t.
 * @param[in] dim
 *   Input. The dimension of the \p input_a and \p input_b to be indexed.
 * @param[in] input_a_desc
 *   Input. The descriptor of the \p input_a. For more detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input_a
 *   Input. Pointer to the MLU memory that stores the input tensor \p input_a which to be indexed and added into.
 * @param[in] index_desc
 *   Input. The descriptor of the \p index tensor. For more detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] index
 *   Input. Pointer to the MLU memory that stores the input tensor \p index which maps vectors or scalars from \p input_a to \p input_b.
 * @param[in] input_b_desc
 *   Input. The descriptor of the \p input_b. For more detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input_b
 *   Input. Pointer to the MLU memory that stores the input tensor \p input_b which to be added.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the index_add operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the index_add operation.
 *   You can get the size of the workspace with the ::cnnlGetIndexAddWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the \p output tensor. For more detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Index Add Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *   input tensor \p input_a - input tensor \p input_b - output tensor \p output.
 *   - float - float - float
 *   - half - half - half
 *   - int64 - int64 - int64
 *   - int32 - int32 - int32
 *   - int16 - int16 - int16
 *   - bfloat16 - bfloat16 - bfloat16
 * - \p dim: int64
 * - \p index: int32, int64
 *
 * @par Scale Limitation
 * - The \p input_a tensor, \p index tensor, \p input_b tensor and \p dim must meet the following
 *   requirements:
 *   - The dim-th dimension of \p input_b must have the same size as the length of
 *        \p index.
 *   - \p index must be a vector.
 *   - All dimensions apart from the dim-th dimension of \p input_b must match
 *        all other dimensions of \p input_a.
 *
 * @note
 * - The accuracy of computing result might decrease when the data type of \p input_a and \p input_b is half.
 * - Multiple accumulations at the same place may cause data overflow.
 * - The maximum of \p index must be smaller than the length of the dim-th dimension of \p input_a and every element
 *   of \p index must be greater than or equal to zero.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the index add operation is as follows:
     @verbatim
     dim = 0
     input_a array
       input_a = [[1, 1, 1],
                  [1, 1, 1],
                  [1, 1, 1],
                  [1, 1, 1],
                  [1, 1, 1]]
     index array
       index = [0, 4, 2]
     input_b array
       input_b = [[1, 2, 3],
                  [4, 5, 6],
                  [7, 8, 8]]
     output array
       output = [[2, 3, 4],
                 [1, 1, 1],
                 [8, 9, 10],
                 [1, 1, 1],
                 [5, 6, 7]]
     @endverbatim
 */
cnnlStatus_t CNNL_WIN_API cnnlIndexAdd_v2(cnnlHandle_t handle,
                                          const int64_t dim,
                                          const cnnlTensorDescriptor_t input_a_desc,
                                          void *input_a,
                                          const cnnlTensorDescriptor_t index_desc,
                                          const void *index,
                                          const cnnlTensorDescriptor_t input_b_desc,
                                          const void *input_b,
                                          void *workspace,
                                          const size_t workspace_size,
                                          const cnnlTensorDescriptor_t output_desc,
                                          void *output);

// Group:IndexAdd
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used to get extra space size
 *        in index_add operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   index_add operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_a_desc
 *   Input. The descriptor of the \p input_a.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the index_add
 *   operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This API is only used along with ::cnnlIndexAdd_v2. ::cnnlIndexAdd does not require this API.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetIndexAddWorkspaceSize(cnnlHandle_t handle,
                                                       const cnnlTensorDescriptor_t input_a_desc,
                                                       size_t *workspace_size);

// Group:InTopK
/*!
 * @brief Computes whether the \p targets are in top k \p predictions. If both \p k_desc and \p k
 *        are not NULL, \p k is used in this operation, and \p k_int is not used. Otherwise,
 *        \p k_int is used.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the intopk
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] predictions_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] predictions
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] targets_desc
 *   Input. The descriptor of the target tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] targets
 *   Input. Pointer to the MLU memory that stores the target tensor.
 * @param[in] k_desc
 *   Input. The descriptor of the k tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] k
 *   Input. Pointer to the MLU memory that stores the k tensor, which has at most one element and
 *   determines the number of top elements in \p predictions to compare with.
 * @param[in] k_int
 *   Input. An integer value that determines the number of top elements in \p predictions to compare
 *   with.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "InTopK Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports only the combination of the following data types for \p predictions,
 *   \p targets, \p k and \p output:
 *   - predictions tensor: float.
 *   - targets tensor: int32.
 *   - k tensor: int32.
 *   - output tensor: bool.
 *
 * @par Scale Limitation
 * - The predictions tensor, targets tensor, and k tensor must meet the following requirement:
 *   - The first dimension size of \p predictions and \p targets should be equal.
 *   - k tensor: If valid, the element number should be 1.
 *
 * @note
 * - This operation is not supported on the 1V platforms.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/raw_ops/InTopKV2
 */
cnnlStatus_t CNNL_WIN_API cnnlInTopK(cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t predictions_desc,
                                     const void *predictions,
                                     const cnnlTensorDescriptor_t targets_desc,
                                     const void *targets,
                                     const cnnlTensorDescriptor_t k_desc,
                                     const void *k,
                                     const int k_int,
                                     const cnnlTensorDescriptor_t output_desc,
                                     void *output);

// Group:ConvolutionForward
/*!
 * @brief Gets the memory size that stores the output of reordering convolution filter data or
 *        convolution bias data on host.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] dev
 *   Input. The MLU device information. For detailed information,
 *   see ::cnnlDeviceType_t.
 * @param[in] conv_desc
 *   Input. The descriptor of the convolution operation. For detailed information,
 *   see ::cnnlConvolutionDescriptor_t.
 * @param[in] w_tensor_desc
 *   Input. The descriptor of the filter tensor in the convolution operation.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] bias_tensor_desc
 *   Input. The descriptor of the bias tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] cast_mode
 *   Input. The quantization mode used for the convolution quantization. For detailed information,
 *   see ::cnnlConvolutionCastMode_t.
 * @param[in] algo
 *   Input. The algorithm used to compute the convolution. The algorithms are
 *   defined in the ::cnnlConvolutionForwardAlgo_t enumeration. You can get the best
 *   suited algorithm with the ::cnnlGetConvolutionForwardAlgo function.
 * @param[in] x_dtype
 *   Input. The offchip data type of convolution forward input. For detailed information,
 *   See ::cnnlDataType_t.
 * @param[in] x_onchip_dtype
 *   Input. The onchip data type of convolution forward input. For detailed information,
 *   See ::cnnlDataType_t.
 * @param[in] y_dtype
 *   Input. The offchip data type of convolution forward output. For detailed information,
 *   See ::cnnlDataType_t.
 * @param[in] y_onchip_dtype
 *   Input. The onchip data type of convolution forward output. For detailed information,
 *   See ::cnnlDataType_t.
 * @param[in] fusion_plan
 *   Input. Pointer to a previously created and initialized fusion plan. For detailed information,
 *   see ::cnnlFusedOpsPlan_t.
 * @param[in] vparam_pack
 *   Input. Pointer to the variant parameter pack in fusion operation.
 *   For detailed information, see ::cnnlFusedOpsVariantParamPack_t.
 * @param[out] w_reorder_bytesize
 *   Output. The size of storing the output of reordering convolution filter.
 * @param[out] bias_reorder_bytesize
 *   Output. The size of the memory int bytes that stores the output of
 *   reordering convolution bias data.
 * @par API Dependency
 * - Before calling this function, you should call the ::cnnlSetConvolutionDescriptorReorderType
 *   to set filter and bias reorder type, and when running fusedOp, you should call
 *   the ::cnnlMakeFusedOpsPlan and ::cnnlSetFusedOpsVariantParamPackAttribute to get fusion_plan
 *   and vparam_pack.
 * - After calling this function, you can call the ::cnnlHostReorderConvData function to reorder
 *   filter data or bias data on the host.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - The parameter cast_mode must be set to CNNL_NO_QUANTIZE, when input onchip type is float or half.
 * - Due to the limitation of cnnlHostReorderConvData, only MLU Edge devices can use
 *   cnnlGetReorderConvDataSize when you use the cluster num optional function.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlGetReorderConvDataSize(
                                                cnnlDeviceType_t dev,
                                                const cnnlConvolutionDescriptor_t conv_desc,
                                                const cnnlTensorDescriptor_t w_tensor_desc,
                                                const cnnlTensorDescriptor_t bias_tensor_desc,
                                                const cnnlConvolutionCastMode_t cast_mode,
                                                const cnnlConvolutionForwardAlgo_t algo,
                                                const cnnlDataType_t x_dtype,
                                                const cnnlDataType_t x_onchip_dtype,
                                                const cnnlDataType_t y_dtype,
                                                const cnnlDataType_t y_onchip_dtype,
                                                const cnnlFusedOpsPlan_t fusion_plan,
                                                const cnnlFusedOpsVariantParamPack_t vparam_pack,
                                                size_t *w_reorder_bytesize,
                                                size_t *bias_reorder_bytesize);

// Group:ConvolutionForward
/*!
 * @brief Reorders filter data and bias data for convolution forward on the host.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] dev
 *   Input. The MLU device information. For detailed information,
 *   see ::cnnlDeviceType_t.
 * @param[in] conv_desc
 *   Input. The descriptor of the convolution operation. For detailed information,
 *   see ::cnnlConvolutionDescriptor_t.
 * @param[in] w_tensor_desc
 *   Input. The descriptor of the filter tensor in the convolution operation.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] bias_tensor_desc
 *   Input. The descriptor of the bias tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] cast_mode
 *   Input. The quantization mode used for the convolution quantization. For detailed information,
 *   see ::cnnlConvolutionCastMode_t.
 * @param[in] algo
 *   Input. The algorithm used to compute the convolution. The algorithms are
 *   defined in the ::cnnlConvolutionForwardAlgo_t enumeration. You can get the best
 *   suited algorithm with the ::cnnlGetConvolutionForwardAlgo function.
 * @param[in] x_dtype
 *   Input. The offchip data type of convolution forward input. For detailed information,
 *   See ::cnnlDataType_t.
 * @param[in] x_onchip_dtype
 *   Input. The onchip data type of convolution forward input. For detailed information,
 *   See ::cnnlDataType_t.
 * @param[in] y_dtype
 *   Input. The offchip data type of convolution forward output. For detailed information,
 *   See ::cnnlDataType_t.
 * @param[in] y_onchip_dtype
 *   Input. The onchip data type of convolution forward output. For detailed information,
 * @param[in] fusion_plan
 *   Input. Pointer to a previously created and initialized fusion plan. For detailed information,
 *   see ::cnnlFusedOpsPlan_t.
 * @param[in] vparam_pack
 *   Input. Pointer to the variant parameter pack in fusion operation.
 *   For detailed information, see ::cnnlFusedOpsVariantParamPack_t.
 * @param[in] w
 *   Input. A pointer to the address of filter data before reordering filter.
 * @param[out] reordered_w
 *   Output. A pointer to the address of filter data after reordering filter.
 * @param[in] w_reorder_bytesize
 *   Input. The size of storing the output of reordering convolution filter.
 * @param[in] bias
 *   Input. A pointer to the address of bias data before reordering bias data.
 * @param[out] reordered_bias
 *   Output. A pointer to the address of bias data after reordering bias data.
 * @param[in] bias_reorder_bytesize
 *   Input. The size of the memory in bytes that stores the output of
 *   reordering convolution bias data.
 * @par API Dependency
 * - Before calling this function, you should call the ::cnnlGetReorderConvDataSize
 *   to get filter_reorder_bytesize or get bias_reorder_bytesize.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - The parameter cast_mode must be set to CNNL_NO_QUANTIZE, when input onchip type is float or half.
 * - Due to the limitation of cnnlHostReorderConvData now, only MLU Edge devices can use
 *   the API when you use the cluster num optional function.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlHostReorderConvData(cnnlDeviceType_t dev,
                                     const cnnlConvolutionDescriptor_t conv_desc,
                                     const cnnlTensorDescriptor_t w_tensor_desc,
                                     const cnnlTensorDescriptor_t bias_tensor_desc,
                                     const cnnlConvolutionCastMode_t cast_mode,
                                     const cnnlConvolutionForwardAlgo_t algo,
                                     const cnnlDataType_t x_dtype,
                                     const cnnlDataType_t x_onchip_dtype,
                                     const cnnlDataType_t y_dtype,
                                     const cnnlDataType_t y_onchip_dtype,
                                     const cnnlFusedOpsPlan_t fusion_plan,
                                     const cnnlFusedOpsVariantParamPack_t vparam_pack,
                                     const void *w,
                                     void *reordered_w,
                                     const size_t w_reorder_bytesize,
                                     const void *bias,
                                     void *reordered_bias,
                                     const size_t bias_reorder_bytesize);
// Group:Deconvolution
/*!
 * @brief Returns the size in bytes of memory which stores the reordered deconvolution filter and
 *        bias on host.
 *
 * @deprecated
 * This function is deprecated and will be removed
 *   in future release.
 *
 * @param[in] dev
 *   Input. The enum indicating the MLU device. For detailed information,
 *   see ::cnnlDeviceType_t.
 * @param[in] filter_desc
 *   Input. The descriptor of the filter tensor in the deconvolution operation.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] bias_desc
 *   Input. The descriptor of the bias tensor in the deconvolution operation.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input_dtype
 *   Input. The offchip data type of input tensor in the deconvolution operation.
 *   For detailed information, see ::cnnlDataType_t.
 * @param[in] input_onchip_dtype
 *   Input. The onchip data type of input tensor in the deconvolution operation.
 *   For detailed information, see ::cnnlDataType_t.
 * @param[in] output_dtype
 *   Input. The offchip data type of output tensor in the deconvolution operation.
 *   For detailed information, see ::cnnlDataType_t.
 * @param[in] output_onchip_dtype
 *   Input. The onchip data type of output tensor in the deconvolution operation.
 *   For detailed information, see ::cnnlDataType_t.
 * @param[in] deconv_desc
 *   Input. The descriptor of the deconvolution operation. For detailed information,
 *   see ::cnnlDeconvolutionDescriptor_t.
 * @param[in] cast_mode
 *   Input. The quantization mode used for the deconvolution operation. For detailed information,
 *   see ::cnnlDeconvolutionCastMode_t.
 * @param[in] algo
 *   Input. The algorithm used to perform the deconvolution operation. The algorithms are
 *   defined in the ::cnnlDeconvolutionAlgo_t enumeration. You can get the best
 *   suited algorithm with the ::cnnlGetDeconvolutionAlgo function.
 * @param[in] fusion_plan
 *   Input. Pointer to a previously created and initialized fusion plan used in the fusion
 *   operation. For detailed information, see ::cnnlFusedOpsPlan_t.
 *   This pointer should be empty if the deconvolution operation is not fused.
 * @param[in] vparam_pack
 *   Input. Pointer to the variant parameter pack in the fusion operation.
 *   For detailed information, see ::cnnlFusedOpsVariantParamPack_t.
 *   This pointer should be empty if the deconvolution operation is not fused.
 * @param[out] filter_reorder_size
 *   Output. The size in bytes of memory storing the reordered filter on host.
 * @param[out] bias_reorder_size
 *   Output. The size in bytes of memory storing the reordered bias on host.
 *
 * @par API Dependency
 * - Before calling this function, you should call the ::cnnlSetDeconvolutionDescriptorReorderType
 *   function to set the reorder type of filter and bias. In addition, if the deconvolution
 *   operation is fused, you should call the ::cnnlMakeFusedOpsPlan and
 *   ::cnnlSetFusedOpsVariantParamPackAttribute functions to get fusion_plan and vparam_pack
 *   in advance.
 * - After calling this function, you can call the ::cnnlHostReorderDeconvolutionData function to
 *   perform the reordering operation for filter and bias on host.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for the filter tensor
 *   and bias tensor:
 *   - filter tensor: int8, int16, half, float.
 *   - bias tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layouts of the filter tensor and bias tensor are as follows:
 *   - filter tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, \p CNNL_LAYOUT_NCHW, and \p CNNL_LAYOUT_NDHWC.
 *   - bias tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, \p CNNL_LAYOUT_NCHW, and \p CNNL_LAYOUT_NDHWC.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlGetReorderDeconvolutionDataSize(cnnlDeviceType_t dev,
                                    const cnnlTensorDescriptor_t filter_desc,
                                    const cnnlTensorDescriptor_t bias_desc,
                                    const cnnlDataType_t input_dtype,
                                    const cnnlDataType_t input_onchip_dtype,
                                    const cnnlDataType_t output_dtype,
                                    const cnnlDataType_t output_onchip_dtype,
                                    const cnnlDeconvolutionDescriptor_t deconv_desc,
                                    const cnnlDeconvolutionCastMode_t cast_mode,
                                    const cnnlDeconvolutionAlgo_t algo,
                                    const cnnlFusedOpsPlan_t fusion_plan,
                                    const cnnlFusedOpsVariantParamPack_t vparam_pack,
                                    size_t *filter_reorder_size,
                                    size_t *bias_reorder_size);
// Group:Deconvolution
/*!
 * @brief Performs the reordering operation for filter and bias of the deconvolution operation
 *        on host.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] dev
 *   Input. The enum indicating the MLU device. For detailed information,
 *   see ::cnnlDeviceType_t.
 * @param[in] filter_desc
 *   Input. The descriptor of the filter tensor in the deconvolution operation.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] bias_desc
 *   Input. The descriptor of the bias tensor in the deconvolution operation.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input_dtype
 *   Input. The offchip data type of input tensor in the deconvolution operation.
 *   For detailed information, see ::cnnlDataType_t.
 * @param[in] input_onchip_dtype
 *   Input. The onchip data type of input tensor in the deconvolution operation.
 *   For detailed information, see ::cnnlDataType_t.
 * @param[in] output_dtype
 *   Input. The offchip data type of output tensor in the deconvolution operation.
 *   For detailed information, see ::cnnlDataType_t.
 * @param[in] output_onchip_dtype
 *   Input. The onchip data type of output tensor in the deconvolution operation.
 *   For detailed information, see ::cnnlDataType_t.
 * @param[in] deconv_desc
 *   Input. The descriptor of the deconvolution operation. For detailed information,
 *   see ::cnnlDeconvolutionDescriptor_t.
 * @param[in] cast_mode
 *   Input. The quantization mode used for the deconvolution operation. For detailed information,
 *   see ::cnnlDeconvolutionCastMode_t.
 * @param[in] algo
 *   Input. The algorithm used to perform the deconvolution operation. The algorithms are
 *   defined in the ::cnnlDeconvolutionAlgo_t enumeration. You can get the best
 *   suited algorithm with the ::cnnlGetDeconvolutionAlgo function.
 * @param[in] fusion_plan
 *   Input. Pointer to a previously created and initialized fusion plan used in the fusion
 *   operation. For detailed information, see ::cnnlFusedOpsPlan_t.
 *   This pointer should be empty if the deconvolution operation is not fused.
 * @param[in] vparam_pack
 *   Input. Pointer to the variant parameter pack in the fusion operation.
 *   For detailed information, see ::cnnlFusedOpsVariantParamPack_t.
 *   This pointer should be empty if the deconvolution operation is not fused.
 * @param[in] filter
 *   Input. Pointer to the CPU memory that stores the filter data before the reordering operation.
 * @param[in] filter_reorder_size
 *   Input. The size in bytes of memory storing the reordered filter on host.
 * @param[out] reordered_filter
 *   Output. Pointer to the CPU memory that stores the reordered filter data after the reordering
 *   operation.
 * @param[in] bias
 *   Input. Pointer to the CPU memory that stores the bias data before the reordering operation.
 * @param[in] bias_reorder_size
 *   Input. The size in bytes of memory storing the reordered bias on host.
 * @param[out] reordered_bias
 *   Output. Pointer to the CPU memory that stores the reordered bias data after the reordering
 *   operation.
 *
 * @par API Dependency
 * - Before calling this function, you should call the ::cnnlGetReorderDeconvolutionDataSize function
 *   to get the size in bytes of memory which stores the reordered filter and bias on host.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for the filter tensor
 *   and bias tensor:
 *   - filter tensor: int8, int16, half, float.
 *   - bias tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layouts of the filter tensor and bias tensor are as follows:
 *   - filter tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, \p CNNL_LAYOUT_NCHW, and \p CNNL_LAYOUT_NDHWC.
 *   - bias tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, \p CNNL_LAYOUT_NCHW, and \p CNNL_LAYOUT_NDHWC.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlHostReorderDeconvolutionData(cnnlDeviceType_t dev,
                                 const cnnlTensorDescriptor_t filter_desc,
                                 const cnnlTensorDescriptor_t bias_desc,
                                 const cnnlDataType_t input_dtype,
                                 const cnnlDataType_t input_onchip_dtype,
                                 const cnnlDataType_t output_dtype,
                                 const cnnlDataType_t output_onchip_dtype,
                                 const cnnlDeconvolutionDescriptor_t deconv_desc,
                                 const cnnlDeconvolutionCastMode_t cast_mode,
                                 const cnnlDeconvolutionAlgo_t algo,
                                 const cnnlFusedOpsPlan_t fusion_plan,
                                 const cnnlFusedOpsVariantParamPack_t vparam_pack,
                                 const void *filter,
                                 const size_t filter_reorder_size,
                                 void *reordered_filter,
                                 const void *bias,
                                 const size_t bias_reorder_size,
                                 void *reordered_bias);
// Group:Svd
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra
 * workspace to optimize the SVD operation.
 *
 * The size of the extra workspace is based on the given information of the SVD operation,
 * including the input tensor descriptor \p input_desc, the parameter \p some and
 * the parameter \p compute_uv. For more information about the workspace,
 * see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the SVD operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] some
 *   Input. Boolean value. When \p some is true, ::cnnlSvd will
 *   compute only the leading min(m, n) singular vectors (min(m, n) means the minimum
 *   of m and n), i.e., the returned \p u and \p v matrices will contain only min(m,n)
 *   orthonormal columns. When \p some is false, ::cnnlSvd will compute full-sized
 *   \p u and \p v matrices.
 * @param[in] compute_uv
 *   Input. Boolean value. When \p compute_uv is true, left and
 *   right singular matrices will be computed and returned in \p u and \p v,
 *   respectively. When \p compute_uv is false, the returned \p u and \p v matrices
 *   will be zero matrices of shape [...,m, m] and [..., n, n] respectively, and
 *   \p some will be ignored here.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that
 *   is used in the SVD operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par API Dependency
 * - You need to call the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor
 *   functions to create and set the tensor descriptors \p input_desc before calling
 *   this function.
 * - The allocated extra workspace should be passed to the ::cnnlSvd function to
 *   perform the SVD operation.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetSvdWorkspaceSize(cnnlHandle_t handle,
                                                  const cnnlTensorDescriptor_t input_desc,
                                                  const bool some,
                                                  const bool compute_uv,
                                                  size_t *workspace_size);
// Group:Svd
/*!
 * @brief Computes the singular value decomposition (SVD) of batches of real matrices
 * \p input such that \p input[..., :, :] = \p u[..., :, :] * diag(\p s[..., :]) *
 * \p transpose(v[..., :, :]).
 *
 * This function needs extra MLU memory as the workspace to improve the SVD
 * performance. You can get the workspace size
 * with the ::cnnlGetSvdWorkspaceSize function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the SVD operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor with the
 *   shape of [..., m, n].
 * @param[in] some
 *   Input. Boolean value. When \p some is true, the returned \p u and \p v matrices
 *   will contain only the min(m, n) orthonormal columns (min(m, n) means the minimum
 *   of m and n). When \p some is false, ::cnnlSvd will compute full-sized \p u and
 *   \p v matrices.
 * @param[in] compute_uv
 *   Input. Boolean value. When \p compute_uv is true, left and
 *   right singular matrices will be computed and returned in \p u and \p v,
 *   respectively. When \p compute_uv is false, the returned \p u and \p v matrices
 *   will be zero matrices of shape [...,m, m] and [..., n, n] respectively, and
 *   \p some will be ignored here.
 * @param[in] is_trans_u
 *   Input. Boolean value. When \p is_trans_u is true, the returned \p u matrices
 *   will be transposed. When \p is_trans_u is false, the returned \p u matrices
 *   will not be transposed.
 * @param[in] is_trans_v
 *   Input. Boolean value. When \p is_trans_v is true, the returned \p v matrices
 *   will be transposed. When \p is_trans_v is false, the returned \p v matrices
 *   will not be transposed.
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the
 *   SVD backward operation. You can get the size of the workspace with the
 *   ::cnnlGetSvdWorkspaceSize function.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   SVD operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] u_desc
 *   Input. The descriptor of the output left singular matrices \p u tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] u
 *   Output. Pointer to the MLU memory that stores the output left singular
 *   matrices \p u tensor.
 * @param[in] s_desc
 *   Input. The descriptor of the output singular values \p s tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] s
 *   Output. Pointer to the MLU memory that stores the output singular values \p s tensor.
 * @param[in] v_desc
 *   Input. The descriptor of the output right singular matrices \p u tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] v
 *   Output. Pointer to the MLU memory that stores the output right singular
 *   matrices \p v tensor.
 * @param[in] infos_desc
 *   Input. The descriptor of the output infos tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] infos
 *   Output. Pointer to the MLU memory that stores the output infos tensor, which
 *   indicates whether SVD computation converges or not. If infos[i] == 0, then the computation
 *   of the i-th input matrix SVD is success. If infos[i] > 0, then the i-th input
 *   matrix SVD computation do not converge, the precision of the results might be low.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "SVD Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor
 *   \p input and output tensor \p u, \p s, \p v and \p infos.
 *   Note that \p input, \p u, \p s and \p v should have the same data type.
 *   - input tensor: float.
 *   - u tensor: float.
 *   - s tensor: float.
 *   - v tensor: float.
 *   - infos tensor: int32.
 *
 * @par Scale Limitation
 * - The input tensor with shape [...,m, n] must meet the following requirements:
 *   - The maximum of m and n should be less than 150.
 *
 * @note
 *   - When m or n is over 20, the precision of the results of ::cnnlSvd might be low.
 *   - This operation is not supported on the 1V platforms.
 *
 * @par Example
 * - The example of the SVD operation is as follows:
     @verbatim
       input tensor:
         input array by 1 * 4 * 3 -->
           [[0.22589493 0.20218787 0.69084793]
            [0.52529126 0.8826841  0.6898656 ]
            [0.11243416 0.86975896 0.9611005 ]
            [0.8365484  0.5822014  0.8437774 ]]

       parameters:
         some: true
         compute_uv: true
         is_trans_u: false
         is_trans_v: false

       output tensor:
         u array by 1 * 4 * 3 -->
           [[-0.3076, -0.0010, -0.7794],
            [-0.5386, -0.0253,  0.6153],
            [-0.5479,  0.7277, -0.0696],
            [-0.5614, -0.6854, -0.0954]]
         s array by 1 * 3 -->
           [2.2559, 0.5571, 0.3801]
         v array by 1 * 3 * 3 -->
           [[-0.3917, -0.9067,  0.1566],
            [-0.5944,  0.3793,  0.7091],
            [-0.7023,  0.1846, -0.6875]]
         infos array by 1 * 1 -->
           [0]
   @endverbatim
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.svd.html
 * - https://tensorflow.org/api_docs/python/tf/raw_ops/Svd
 */
cnnlStatus_t CNNL_WIN_API cnnlSvd(cnnlHandle_t handle,
                                  const cnnlTensorDescriptor_t input_desc,
                                  const void *input,
                                  const bool some,
                                  const bool compute_uv,
                                  const bool is_trans_u,
                                  const bool is_trans_v,
                                  const size_t workspace_size,
                                  void *workspace,
                                  const cnnlTensorDescriptor_t u_desc,
                                  void *u,
                                  const cnnlTensorDescriptor_t s_desc,
                                  void *s,
                                  const cnnlTensorDescriptor_t v_desc,
                                  void *v,
                                  const cnnlTensorDescriptor_t infos_desc,
                                  void *infos);
// Group:Threshold
/*!
 * @brief Computes threshold operation on input tensor \p x by the threshold value \p threshold and
 *        replacement value \p value, and returns the result in the output tensor \p y.
 *        If \p x is greater than \p threshold, \p y is equal to \p x, otherwise \p y is equal to \p value.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the threshold
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] threshold
 *   Input. Pointer to the host memory that stores the threshold which is used to compare with input tensor \p x
 *   and to decide the return value \p y.
 * @param[in] value
 *   Input. Pointer to the host memory that stores the value to be returned in \p y if \p x is less than
 *   or equal to the threshold.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_EXECUTION_FAILED.
 *
 * @par Formula
 * - See "Threshold Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor \p x, output tensor \p y, threshold \p threshold and
 *   \p value must be the same.
 *   The supported data types of input tensor \p x, output tensor \p y, threshold \p threshold
 *   and \p value are as follows:
 *   - input tensor: half, bfloat16, float, uint8, int8, int16, int32.
 *   - output tensor: half, bfloat16, float, uint8, int8, int16, int32.
 *   - threshold: half, bfloat16, float, uint8, int8, int16, int32.
 *   - value: half, bfloat16, float, uint8, int8, int16, int32.
 *
 *   The bfloat16 data type is supported only on MLU500 series.
 *
 * @note
 * - Before calling this function, if data type of threshold \p threshold or value \p value is not
 *   the same as that of input tensor \p x, users need to cast data types of threshold \p threshold
 *   and value \p value to be consistent with input tensor \p x.
 * - When the input data or parameter contains NaN or infinity:
 *   - On CE3226:
 *     - If \p x is negative infinity, then \p y is NaN.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
     input tensor x : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
     param:
       threshold: 4
       value: 666
     output tensor y : [666, 666, 666, 666, 666, 5, 6, 7, 8, 9]
     @endverbatim
 *
 * @par Reference
 * - http://pytroch.org/docs/1.6.0/generated/torch.nn.Threshold.html?highlight=threshold#torch.nn.Threshold
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlThreshold(cnnlHandle_t handle,
                                        const cnnlTensorDescriptor_t x_desc,
                                        const void *x,
                                        const void *threshold,
                                        const void *value,
                                        const cnnlTensorDescriptor_t y_desc,
                                        void *y);
// Group:ThresholdBackward
/*!
 * @brief Computes threshold_backward operation on input tensor \p x and input tensor \p diff_y
 *        by the threshold value \p threshold, and returns the result in the output tensor \p diff_x.
 *        If \p x is greater than \p threshold, \p diff_x is equal to \p diff_y, otherwise \p diff_x
 *        is equal to 0.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the threshold
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] diff_y_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] diff_y
 *   Input. Pointer to the MLU memory that stores the input tensor which is the gradient.
 *   The value of this parameter is returned in \p diff_x if \p x is greater than \p threshold.
 * @param[in] threshold
 *   Input. Pointer to the host memory that stores the threshold which is used to compare with input tensor \p x
 *   and to decide the return value \p diff_x.
 * @param[in] diff_x_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the output tensor which is the gradient.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_EXECUTION_FAILED.
 *
 * @par Formula
 * - See "Threshold Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor \p x, input tensor \p diff_y, threshold \p threshold and
 *   \p diff_x must be the same.
 *   The supported data types of input tensor \p x, input tensor \p diff_y, threshold \p threshold
 *   and output tensor \p diff_x are as follows:
 *   - input tensor: half, bfloat16, float, uint8, int8, int16, int32.
 *   - output tensor: half, bfloat16, float, uint8, int8, int16, int32.
 *   - threshold: half, bfloat16, float, uint8, int8, int16, int32.
 *
 *   The bfloat16 data type is supported only on MLU500 series.
 *
 * @note
 * - Before calling this function, if data type of threshold \p threshold is not same as that of
 *   input tensor \p x and input tensor \p diff_y, users need to cast the data type of threshold
 *   \p threshold to be consistent with input tensor \p x and input tensor \p diff_y.
 * - When the input data or parameter contains NaN or infinity:
 *   - On CE3226:
 *     - If \p x is NaN and \p diff_y is finite value, then \p diff_x is zero.
 *     - If \p x is NaN and \p diff_y is negative infinity or positive infinity, then \p diff_x is NaN.
 *     - If \p x is finite value and less than \p threshold and \p diff_y is NaN, then \p diff_x is NaN.
 *     - If \p x is negative infinity and \p diff_y is negative infinity, positive infinity or NaN,
 *       then \p diff_x is NaN.
 *
 * Example:
 * @verbatim
 *  - On MLU300 series and CE3226:
 *   input tensor x :      [0.0  1.0, 2.0,  inf, -inf, 5.0,  6.0, 7.0, 8.0, NaN, NaN,   NaN, NaN,  inf, inf, inf, -inf,   -inf, -inf]
 *   input tensor diff_y : [NaN, inf, -inf, 3.0, 16.4, 14.3, inf, -inf, NaN, NaN, 19.0, -inf, inf, -inf, inf, NaN,  -inf,  inf,  NaN]
 *   param:
 *     threshold: 5.0
 *   output tensor diff_x : [NaN, NaN, NaN, 3.0, 0.0,  0.0,  inf, -inf, NaN, NaN, 0.0,  NaN, NaN,  -inf, inf,  NaN, NaN,   NaN, NaN]
 *   @endverbatim
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - http://pytroch.org/docs/1.6.0/generated/torch.nn.Threshold.html?highlight=threshold#torch.nn.Threshold
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlThresholdBackward(cnnlHandle_t handle,
                                                const cnnlTensorDescriptor_t x_desc,
                                                const void *x,
                                                const cnnlTensorDescriptor_t diff_y_desc,
                                                const void *diff_y,
                                                const void *threshold,
                                                const cnnlTensorDescriptor_t diff_x_desc,
                                                void *diff_x);

// Group:Det
/*!
 * @brief Computes determinants, log(determinants) or slog(determinants) of square matrices on input tensor
 * \p input depending on \p mode, and returns the results in the output tensor \p output. When the mode is
 * CNNL_DET_MODE_SLOGDET, it will return \p sign as well.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   determinant computing operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] mode
 *   Input. The mode used to compute the determinant. For detailed information, see ::cnnlDetMode_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in]  workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   Det operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in]  workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the Det operation. You can get the size of the workspace with
 *   the ::cnnlGetDetWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] sign_desc
 *   Input. The descriptor of the sign tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] sign
 *   Output. Pointer to the MLU memory that stores the sign tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @par Formula
 * - See "Det" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor \p input and output tensor \p output must be the same.
 * - The supported data types are as follows:
 *   - input: float, half.
 *   - output: float, half.
 *
 * @par Limitation
 * - The input tensor must have at least 2 dimensions.
 *
 *   - For input tensor with exactly 2 dimensions, the output tensor should have 1 dimension.
 *   - For input tensors with more than 2 dimensions, the output tensor should have 2 fewer dimensions than the input
 *    tensor, with the size of each corresponding dimension remaining the same.
 * - The length of the last 2 dimensions of input tensor should be the same.
 * - sign_desc and sign should be NULL when mode is CNNL_DET_MODE_DET and CNNL_DET_MODE_LOGDET.
 *
 * @par API Dependency
 * - Before calling this function, call ::cnnlGetDetWorkspaceSize.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the determinant computing is as follows:
     @verbatim
      input one array by 1 * 2 * 2
      --> [[[1, 2],
            [3, 4]]]

      param:
        mode: 0

      output one array by 1
      --> output: [-2]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.det.html
 */
cnnlStatus_t CNNL_WIN_API cnnlDet_v2(cnnlHandle_t handle,
                                     cnnlDetMode_t mode,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const void *input,
                                     void *workspace,
                                     size_t workspace_size,
                                     const cnnlTensorDescriptor_t output_desc,
                                     void *output,
                                     const cnnlTensorDescriptor_t sign_desc,
                                     void *sign);

// Group:Det
/*!
 * @brief Returns in \p size the size of the MLU memory that is used to save the
 * intermediate result of ::cnnlDet_v2 operation.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the ::cnnlGetDetWorkspaceSize operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  input_desc
 *   Input. The descriptor of a tensor. The value must be the same as \p input_desc in ::cnnlDet_v2.
 *   This parameter is used to calculate the workspace size.
 * @param[out]  size
 *   Output. A host pointer to the returned size of the extra workspace in bytes
 *   that is used in ::cnnlDet_v2 operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - The \p input_desc must be the same as \p input_desc in ::cnnlDet_v2 function.
 *
 * @par API Dependency
 * - This function must be called before the ::cnnlDet_v2 function.
 *
 * @note
 * - None.
 *
 * @par Example
 * - None.
 */

cnnlStatus_t CNNL_WIN_API cnnlGetDetWorkspaceSize(cnnlHandle_t handle,
                                                  const cnnlTensorDescriptor_t input_desc,
                                                  size_t *size);

// Group:Det
/*!
 * @brief Computes determinants or log(determinants) of square matrices on input tensor
 * \p input depending on \p mode, and returns the results in the output tensor \p output.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   determinant computing operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] mode
 *   Input. The mode used to compute the determinant. The modes are defined in the
 *   ::cnnlDetMode_t enum.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @par Formula
 * - See "Det" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor \p input and output tensor \p output must be the same.
 * - The supported data types are as follows:
 *   - input: float, half.
 *   - output: float, half.
 *
 * @par Limitation
 * - The input square matrices should not be greater than 330 * 330.
 * - The dimension of input tensor should not be less than 2. When the dimension of input
 *   tensor is 2, the dimension of output tensor should be 1. When the dimension of input
 *   tensor is greater than 2, the dimension of output tensor should be 2 less than
 *   that of input tensor, and each dimension number of output tensor and input tensor must
 *   be the same.
 * - The length of the last 2 dimensions of input tensor should be the same.
 * - The mode cannot be CNNL_DET_MODE_SLOGDET.
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the determinant computing is as follows:
     @verbatim
      input one array by 1 * 2 * 2
      --> [[[1, 2],
            [3, 4]]]

      param:
        mode: 0

      output one array by 1
      --> output: [-2]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.det.html
 */
CNNL_DEPRECATED_FOR(cnnlDet_v2)
cnnlStatus_t CNNL_WIN_API cnnlDet(cnnlHandle_t handle,
                                  cnnlDetMode_t mode,
                                  const cnnlTensorDescriptor_t input_desc,
                                  const void *input,
                                  const cnnlTensorDescriptor_t output_desc,
                                  void *output);

// Group:SlogDet
/*!
 * @brief Computes the sign of determinants of the input and the logrithm of the absolute value of
 * determinants, and returns the results in the sign tensor \p sign and output tensor \p output.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   SlogDet operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] sign_desc
 *   Input. The descriptor of the sign tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] sign
 *   Output. Pointer to the MLU memory that stores the sign tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @par Formula
 * - See "SlogDet" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor \p input, output tensors \p output and \p sign must be the same.
 * - The supported data types are as follows:
 *   - input: float, half.
 *   - output: float, half.
 *
 * @par Data Layout
 * - The supported data layouts are as follows:
 *   - \p input_desc: \p CNNL_LAYOUT_ARRAY.
 *   - \p output_desc: \p CNNL_LAYOUT_ARRAY.
 *   - \p sign_desc: \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The input square matrices should be no greater than 330 * 330 on MLU300 series and MLU500 series.
 * - On 1V, the input square matrices should be no greater than 443 * 443, and
 *   on CE3226, the input square matrices should be no greater than 330 * 330.
 * - The dimension of input tensor should be no less than 2. When the dimension of input
 *   tensor is 2, the dimension of output tensor should be 1. When the dimension of input
 *   tensor is greater than 2, the dimension number of output tensor should be 2 less than
 *   that of input tensor and the dimension length of output tensor and input tensor must
 *   be the same except the last two dimensions.
 * - The length of the last 2 dimensions of input tensor should be the same.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
      input one array by 1 * 2 * 2
      --> [[[1, 2],
            [3, 4]]]

      param:
        mode: 2

      output two array by 1
      --> output1(log of absolute det value): [0.6931]
      --> output2(sign of det value): [-1.]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.linalg.slogdet.html
 */
CNNL_DEPRECATED_FOR(cnnlDet_v2)
cnnlStatus_t CNNL_WIN_API cnnlSlogDet(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t input_desc,
                                      const void *input,
                                      const cnnlTensorDescriptor_t output_desc,
                                      void *output,
                                      const cnnlTensorDescriptor_t sign_desc,
                                      void *sign);

/*****************************************************************************
 * Cambricon CNNL OP: RNN
 ******************************************************************************/
/*!
 * @brief Enumeration variables describing the RNN cell implementation modes.
 *
 */
typedef enum {
  CNNL_RNN_RELU  = 0,  /*!< Basic RNN cell with ReLu activation. It is not supported now.*/
  CNNL_RNN_TANH  = 1,  /*!< Basic RNN cell with Tanh activation. It is not supported now.*/
  CNNL_LSTM      = 2,  /*!< LSTM cell. */
  CNNL_LSTM_TANH = 3, /*!< LSTM cell with Tanh activation before peephole. */
} cnnlRNNMode_t;

/*!
 * @brief Enumeration variables describing RNN forward modes.
 */
typedef enum {
  CNNL_FWD_MODE_INFERENCE = 0,
  /*!< The RNN forward operation for the inference scenario. */
  CNNL_FWD_MODE_TRAINING,
  /*!< The RNN forward operation for the training scenario. */
} cnnlForwardMode_t;

/*!
 * @brief Enumeration variables describing the number of bias vectors in the RNN cell.
 */
typedef enum {
  CNNL_RNN_NO_BIAS          = 0,
  /*!< No bias is used in the RNN cell formulas. */
  CNNL_RNN_SINGLE_INP_BIAS  = 1,
  /*!< One bias is used after input GEMM in the RNN cell formulas.*/
  CNNL_RNN_DOUBLE_BIAS      = 2,
  /*!< Two biases are used in the RNN cell formulas. This value is used by default.*/
  CNNL_RNN_SINGLE_REC_BIAS  = 3,
  /*!< One bias is used after recurrent GEMM in the RNN cell formulas. */
} cnnlRNNBiasMode_t;

/*!
 * @brief Enumeration variables describing how the RNN cell processes the input sequence data.
 */
typedef enum {
  CNNL_RNN_UNIDIRECTIONAL = 0,
  /*!< The RNN network iteratively processes the input sequence data from front to back.*/
  CNNL_RNN_BIDIRECTIONAL  = 1,
  /*!< Two RNN networks iteratively process the input sequence data from two directions, one from
       front to back, and the other from back to front. Finally, the results of the two RNN
       network are concatenated at each iteration as the output.
       Note: two RNN networks have different model parameters.*/
} cnnlDirectionMode_t;

/*!
 * @brief Enumeration variables describing whether to use the input gemm
 *        in the first RNN layer.
 *
 */
typedef enum {
  CNNL_RNN_LINEAR_INPUT = 0,
  /*!< Enables the input gemm in the first recurrent layer.*/
  CNNL_RNN_SKIP_INPUT   = 1,
  /*!< Omits the input gemm in the first recurrent layer.*/
} cnnlRNNInputMode_t;

/*!
 * @brief Enumeration variables describing whether to pad the input and output sequence data.
 * It is used in the ::cnnlSetRNNPaddingMode and ::cnnlGetRNNPaddingMode.
 * Default is ::CNNL_RNN_PADDED_IO_DISABLED. Currently, this enumeration
 * is not supported for RNN inference.
 */
typedef enum {
  CNNL_RNN_PADDED_IO_DISABLED = 0, /*!< Disables the padded input and output. */
  CNNL_RNN_PADDED_IO_ENABLED  = 1, /*!< Enables the padded  input and output. */
} cnnlRNNPaddingMode_t;

/*!
 * @brief Enumeration variables describing whether to make a clipping in an RNN cell. It is
 *        used in the ::cnnlSetRNNClip and ::cnnlGetRNNClip. Default is ::CNNL_RNN_CLIP_NONE.
 *
 */
typedef enum {
  CNNL_RNN_CLIP_NONE   = 0,  /*!< Disables clipping operation in the RNN cell. */
  CNNL_RNN_CLIP_MINMAX = 1,  /*!< Enables clipping operation in the RNN cell. */
} cnnlRNNClipMode_t;

/*!
 * @brief Enumeration variables describing the filter order of the recurrent unit in LSTM cell. It is
 *        used in ::cnnlSetRNNWeightOrder and ::cnnlGetRNNWeightOrder. Default is
 *        ::CNNL_LSTM_IFGO.
 */
typedef enum {
  CNNL_LSTM_IFGO = 0,  /*!< The order is input gate, forget gate, update gate and output gate. */
  CNNL_LSTM_IFOG = 1,  /*!< The order is input gate, forget gate, output gate and update gate. */
  CNNL_LSTM_IOFG = 2,  /*!< The order is input gate, output gate, forget gate and update gate. */
} cnnlRNNWeightOrder_t;

/*!
 * @brief Enumeration variables used to choose whether to make a peephole connection in LSTM
 *        cell. Peephole connections allow the gates to use the previous internal state as well
 *        as the previous hidden state.
 *        It is used in the ::cnnlSetRNNPeepholeMode(), ::cnnlGetRNNPeepholeMode().
 *        Default is ::CNNL_LSTM_PEEPHOLE_DISABLED.
 */
typedef enum {
  CNNL_LSTM_PEEPHOLE_DISABLED = 0, /*!< Disables LSTM peephole. */
  CNNL_LSTM_PEEPHOLE_ENABLED  = 1, /*!< Enables LSTM peephole. */
} cnnlRNNPeepholeMode_t;

/*!
 * @brief Enumeration variables describing whether to enable the mask operation in LSTM/RNN cell.
 *        It is used in the ::cnnlSetRNNMaskMode and ::cnnlGetRNNMaskMode.
 *        Default is ::CNNL_LSTM_MASK_DISABLED.
 */
typedef enum {
  CNNL_LSTM_MASK_DISABLED  = 0,
  /*!< Disables the mask operation in the LSTM/RNN cell. */
  CNNL_LSTM_MASK_ENABLED   = 1,
  /*!< Enables the mask operation in the LSTM/RNN cell. */
  CNNL_LSTM_MASK_CONST     = 2,
  /*!< Enables the mask const operation in the LSTM/RNN cell. */
  CNNL_LSTM_MASK_SEQUENCES = 3,
  /*!< Enables the mask sequence length operation in the LSTM/RNN cell. */
} cnnlRNNMaskMode_t;

/*!
 * @brief Enumeration variables describing whether RNN cell has output layer.
 *        It is used in the ::cnnlSetRNNOutputMode and ::cnnlGetRNNOutputMode.
 *        It only works for rnn mode with CNNL_RNN_TANH/CNNL_RNN_RELU.
 *        Default is ::CNNL_RNN_NO_OUT_LAYER.
 */
typedef enum {
  CNNL_RNN_NO_OUT_LAYER = 0, /*!< Output layer is not included in the RNN cell. */
  CNNL_RNN_HAS_OUT_LAYER  = 1, /*!< Output layer is included in the RNN cell. */
} cnnlRNNOutputMode_t;

/*!
 * @brief Enumeration variables describing whether the calculation of each gate that corresponds to the
 *  input and hidden state is combined.
 *
 *  This enumeration is reserved.
 *
 */
typedef enum {
  CNNL_LSTM_GATE_SEPARATION = 0, /*!< Performs GEMM once per gate. It is the default value. */
  CNNL_LSTM_GATE_MERGING    = 1, /*!< Performs a GEMM after merging the filter of the four gate.*/
} cnnlLSTMGateMode_t;

#define CNNL_LSTM_MAX_ACTIVATION_NUM 6
/*!
 * @brief Enumeration variables describing the positions of activation operations applied
 * in the LSTM.
 * The default activation function is the implementation in the standard LSTM cell.
 *
 * Currently this enumeration is reserved.
 *
 */
typedef enum {
  CNNL_LSTM_GATE_FORGET = 0, /*!< The activation operation is applied at the forget gate. */
  CNNL_LSTM_GATE_INPUT  = 1, /*!< The activation operation is applied at the input gate. */
  CNNL_LSTM_GATE_UPDATE = 2, /*!< The activation operation is applied at the cell gate. */
  CNNL_LSTM_GATE_OUTPUT = 3, /*!< The activation operation is applied at the output gate. */
  CNNL_LSTM_CY  = 4, /*!< The activation operation is applied at the output cell state. */
  CNNL_LSTM_PROJECTION  = 5, /*!< The activation operation is applied after projection. */
} cnnlLSTMActivationLocation_t;

// Group:RNN
/*!
 * @brief Computes the forward process of RNN network in the inference scenario. The specific
 *        network structure is determined by the descriptor \p rnn_desc set by the user.
 *        Using the input data \p x, \p hx, \p cx, \p filter, \p bias, and \p mask, according to
 *        the specific network structure, writes the calculation result into the output memory \p y,
 *        \p hy, and \p cy.
 *
 * This function requires two additional MLU memory as the extra input and the workspace to improve
 * the RNN network performance. You can get the workspace size
 * with the ::cnnlGetRNNWorkspaceSize function, and the size of the extra input \p extra_input_size
 * with the ::cnnlGetRNNExtraInputSize function.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlRNNForwardInference_v2 instead, which uses \p w_desc and \p b_desc in type of
 * ::cnnlTensorDescriptor_t instead of ::cnnlTensorSetDescriptor_t.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the RNN operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] rnn_desc
 *   Input. The descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[in] x_desc
 *   Input. The descriptor of input sequence data.
 *   For detailed information, see ::cnnlSeqDataDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores input sequence data.
 * @param[in] hx_desc
 *   Input. The descriptor of hidden state tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] hx
 *   Input. Pointer to the MLU memory that stores hidden state tensor.
 * @param[in] cx_desc
 *   Input. The descriptor of cell state tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] cx
 *   Input. Pointer to the MLU memory that stores cell state tensor.
 * @param[in] w_desc
 *   Input. The descriptor of the filter tensor set.
 *   For detailed information, see ::cnnlTensorSetDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores filter.
 * @param[in] b_desc
 *   Input. The descriptor of the bias tensor set.
 *   For detailed information, see ::cnnlTensorSetDescriptor_t.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores bias data and peephole data.
 * @param[in] mask_desc
 *   Input. The descriptor of mask sequence data.
 *   For detailed information, see ::cnnlSeqDataDescriptor_t.
 * @param[in] mask
 *   Input. Pointer to the MLU memory that stores mask data.
 * @param[in] extra_input
 *   Input. Pointer to the MLU memory that stores the extra input data.
 *   You need to copy the extra input data to MLU from the host that is
 *   initialized with ::cnnlInitRNNExtraInput. For more information
 *   about extra input data, see Cambricon CNNL User Guide.
 * @param[in] y_desc
 *   Input. The descriptor of output sequence data.
 *   For detailed information, see ::cnnlSeqDataDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores output sequence data.
 * @param[in] hy_desc
 *   Input. The descriptor of output hidden state tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] hy
 *   Output. Pointer to the MLU memory that stores output hidden state tensor.
 * @param[in] cy_desc
 *   Input. The descriptor of output cell state tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] cy
 *   Output. Pointer to the MLU memory that stores output cell state tensor.
 * @param[in] k_desc
 *   Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] keys
 *   Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] c_desc
 *   Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] c_attn
 *   Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] i_desc
 *   Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] i_attn
 *   Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] q_desc
 *   Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] queries
 *   Reserved for future use. Set the value of these parameters to NULL.
 * @param[in]  workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   RNN operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in]  workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the RNN operation. You can get the size of the workspace with
 *   the ::cnnlGetRNNWorkspaceSize function.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "RNN Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for the input tensor
 *   \p x_desc, and \p hx_desc, filter tensor \p w_desc, bias tensor \p cx_desc, \p cy_desc,
 *   \p b_desc and \p mask_desc, output_tensor \p y_desc, and \p hy_desc.
 *    - input tensor: int8, int16, half, float.
 *    - filter tensor: int8, int16, int31, half, float.
 *    - bias tensor: half, float.
 *    - output tensor: int8, int16, half, float.
 *
 * - If \p rnn_mode is ::CNNL_RNN_RELU or ::CNNL_RNN_TANH:
 *   - Onchip and offchip data type of filter tensor must be the same, and can be int8, int16, or int31.
 *   - If the offchip data type of input tensor is float or half, then bias tensor should have the
 *     same offchip data type with input tensor.
 *   - If offchip data type of input tensor is int8 or int16, then filter tensor should have the
 *     same onchip data type with input tensor.
 *   - Input tensor and filter tensor should have the same onchip data type, besides input tensor
 *     and output tensor should have the same offchip data type.
 *
 * - If rnn_mode is ::CNNL_LSTM or ::CNNL_LSTM_TANH:
 *   - Onchip and offchip data type of filter tensor must be the same, which can be int8, int16,
 *     or int31 on MLU200/MLU270/MLU290/CE3226, and can be int8, int16, int31, half or float on
 *     MLU300 series.
 *   - If onchip data type of filter tensor is float or half, then the input tensor, bias tensor and
 *     output tensor should have the same onchip and offchip data type.
 *   - If the offchip data type of input tensor is float or half, then bias tensor should have the
 *     same offchip data type with input tensor.
 *   - If offchip data type of input tensor is int8 or int16, then filter tensor should have the
 *     same onchip data type with input tensor.
 *   - Input tensor and filter tensor should have the same onchip data type, besides input tensor
 *     and output tensor should have the same offchip data type.
 *   - If \p rec_proj_size is greater than zero, then onchip data type of filter tensor must be
 *     int8 or int16 on CE3226.
 *
 * - If \p mask_mode is ::CNNL_LSTM_MASK_SEQUENCES:
 *   - Mask data type must be int32.
 *
 * @par Data Layout
 * - The supported data layouts set in descriptors are as follows:
 *   - \p x_desc must be set to \p CNNL_SEQDATA_TNC.
 *   - \p hx_desc must be set to \p CNNL_LAYOUT_ARRAY and \p dimNb in \p hx_desc must be 3.
 *   - \p cx_desc must be set to \p CNNL_LAYOUT_ARRAY and \p dimNb in \p cx_desc must be 3.
 *   - \p w_desc must be set to \p CNNL_LAYOUT_NHWC.
 *   - \p b_desc must be set to \p CNNL_LAYOUT_NHWC.
 *   - \p mask_desc must be set to \p CNNL_SEQDATA_TN.
 *   - \p y_desc must be set to \p CNNL_SEQDATA_TNC.
 *   - \p hy_desc must be set to \p CNNL_LAYOUT_ARRAY and \p dimNb in \p hy_desc must be 3.
 *   - \p cy_desc must be set to \p CNNL_LAYOUT_ARRAY and \p dimNb in \p cy_desc must be 3.
 *
 * @par Scale Limitation
 * - \p rnn_mode supports ::CNNL_RNN_RELU, ::CNNL_RNN_TANH, ::CNNL_LSTM and ::CNNL_LSTM_TANH.
 * - Layout of \p x_desc and \p y_desc only supports ::CNNL_SEQDATA_TNC.
 * - If \p rnn_mode is not ::CNNL_LSTM or ::CNNL_LSTM_TANH, then \p cx_desc, \p cx, \p cy_desc and
 *   \p cy are NULL.
 * - The descriptor of filter tensor set \p w_desc, contains all filter tensor related information
 *   of RNN. Its dimensions are (\p num_layers, direction_num, cell_filter_num).
 * - When \p direction is ::CNNL_RNN_BIDIRECTIONAL, \p direction_num must be 2, else
 *   \p direction_num must be 1.
 * - The descriptor of filter tensor sets \p w_desc contains all filter tensor related information
 *   of RNN. The dimensions are (\p num_layers, direction_num, cell_filter_num).
 * - The descriptor of bias tensor set \p b_desc, containing all bias and peephole data.
 *   Its dimensions are (layer_num, direction_num, cell_data_num).
 *   The principle is the same as above w_desc, the difference is that addition to bias, peephole
 *   tensor is included.
 *
 * - If rnn_mode is ::CNNL_RNN_RELU or ::CNNL_RNN_TANH:
 *   - If \p input_mode is ::CNNL_RNN_LINEAR_INPUT and \p output_mode is ::CNNL_RNN_NO_OUT_LAYER,
 *     \p cell_filter_num should be 2.
 *   - If \p input_mode is ::CNNL_RNN_LINEAR_INPUT and \p output_mode is ::CNNL_RNN_HAS_OUT_LAYER,
 *     \p cell_filter_num should be 3.
 *   - If \p input_mode is ::CNNL_RNN_SKIP_INPUT and \p output_mode is ::CNNL_RNN_NO_OUT_LAYER, the
 *     first layer \p cell_filter_num should be 1.
 *     Other layers \p cell_filter_num should be 2.
 *   - If \p input_mode is ::CNNL_RNN_SKIP_INPUT and \p output_mode is ::CNNL_RNN_HAS_OUT_LAYER,
 *     the first layer \p cell_filter_num should be 2.
 *     Other layers \p cell_filter_num should be 3.
 *   - The overall filter order of \p w_desc is the matrix multiplication with input,
 *     and the matrix multiplication with the hidden state. The specific location is as follows:
 *     - Value 0 corresponds to the input gate.
 *     - Value 1 corresponds to the recursive gate.
 *     - Value 2 corresponds to the output gate.
 *     some filter may disappear according to RNN configurations.
 *
 *   - If \p bias_mode is ::CNNL_RNN_DOUBLE_BIAS and \p output_mode is ::CNNL_RNN_NO_OUT_LAYER,
 *     \p cell_data_num is 2.
 *   - If \p bias_mode is ::CNNL_RNN_SINGLE_INP_BIAS or ::CNNL_RNN_SINGLE_REC_BIAS and
 *     \p output_mode is
 *     ::CNNL_RNN_NO_OUT_LAYER, \p cell_data_num is 1.
 *   - If \p bias_mode is ::CNNL_RNN_NO_BIAS and output_mode is ::CNNL_RNN_NO_OUT_LAYER,
 *     \p cell_data_num is 0.
 *   - If \p bias_mode is ::CNNL_RNN_DOUBLE_BIAS and \p output_mode is ::CNNL_RNN_HAS_OUT_LAYER,
 *     \p cell_data_num is 3.
 *   - If \p bias_mode is ::CNNL_RNN_SINGLE_INP_BIAS or ::CNNL_RNN_SINGLE_REC_BIAS and
 *     \p output_mode is ::CNNL_RNN_HAS_OUT_LAYER, \p cell_data_num is 2.
 *   - If \p bias_mode is ::CNNL_RNN_NO_BIAS and \p output_mode is ::CNNL_RNN_HAS_OUT_LAYER,
 *     \p cell_data_num is 1.
 *   - The overall bias order of \p b_desc is the input related and hidden state related.
 *     The specific location is as follows:
 *     - Value 0 corresponds to the input gate.
 *     - Value 1 corresponds to the recursive gate.
 *     - Value 2 corresponds to the output gate.
 *     some bias may disappear according to RNN configurations.
 *   - For \p y_desc, its \p layout and \p dtype need to match that of x_desc.
 *   - For \p hy_desc, its \p layout and \p dtype and \p shape must be set the same way as
 *     \p hx_desc.
 *   - If the shape of the input sequence data \p x_desc is (\p T, \p N, \p C), \p layer_num must
 *     be greater than or equal to 1,
 *     \p T must be greater than or equal to 1, and \p C must be greater than or equal to 1.
 *     If \p input_mode is ::CNNL_RNN_SKIP_INPUT, then input_size must equal to hidden_size.
 *
 * - If rnn_mode is ::CNNL_LSTM or ::CNNL_LSTM_TANH:
 *   - If input_mode is ::CNNL_RNN_LINEAR_INPUT and LSTM projection is enabled, \p cell_filter_num
 *     should be 9.
 *   - If input_mode is ::CNNL_RNN_LINEAR_INPUT and LSTM projection is disabled, \p cell_filter_num
 *     should be 8.
 *   - If input_mode is ::CNNL_RNN_SKIP_INPUT and LSTM projection is enabled, the first layer
 *     \p cell_filter_num should be 5.
 *     Other layers \p cell_filter_num should be 9.
 *   - If input_mode is ::CNNL_RNN_SKIP_INPUT and LSTM projection is disabled, the first layer
 *     \p cell_filter_num should be 4.
 *     Other layers \p cell_filter_num should be 8.
 *   - If \p filter_order is CNNL_LSTM_IFGO, then the overall filter order of \p w_desc is the
 *     matrix multiplication
 *     with input, and the matrix multiplication with the hidden state.
 *     The specific location is as follows:
 *     - Value 0 and value 4 correspond to the input gate.
 *     - Value 1 and value 5 correspond to the forget gate.
 *     - Value 2 and value 6 correspond to the update gate.
 *     - Value 3 and value 7 correspond to the output gate.
 *     - Value 8 correspond to the projection filter.
 *     some filter may disappear and filter orders may be different according to RNN configurations.
 *
 *   - If \p bias_mode is ::CNNL_RNN_DOUBLE_BIAS, and \p peephole is enabled, then \p cell_data_num
 *     is 11.
 *   - If \p bias_mode is ::CNNL_RNN_SINGLE_INP_BIAS or ::CNNL_RNN_SINGLE_REC_BIAS, and \p peephole
 *     is enabled,
 *     then \p cell_data_num is 7.
 *   - If \p bias_mode is ::CNNL_RNN_NO_BIAS, and \p peephole is enabled, then \p cell_data_num
 *     is 3.
 *   - If \p bias_mode is ::CNNL_RNN_DOUBLE_BIAS, and \p peephole is disabled, then
 *     \p cell_data_num is 8.
 *   - If \p bias_mode is ::CNNL_RNN_SINGLE_INP_BIAS or ::CNNL_RNN_SINGLE_REC_BIAS, and
 *     \p peephole is disabled,
 *     then \p cell_data_num is 4.
 *   - If \p bias_mode is ::CNNL_RNN_NO_BIAS, and \p peephole is disabled, then
 *     \p cell_data_num is 0.
 *   - If \p filter_order is CNNL_LSTM_IFGO, then the overall bias order of \p b_desc
 *     is the input related,
 *     hidden state related and peephole connection related. The specific location is as follows:
 *     - Value 0 and value 4 correspond to bias of input gate.
 *     - Value 1 and value 5 correspond to bias of forget gate.
 *     - Value 2 and value 6 correspond to bias of update gate.
 *     - Value 3 and value 7 correspond to bias of output gate.
 *     - Value 8 coreespond to peephole of input gate.
 *     - Value 9 coreespond to peephole of forget gate.
 *     - Value 10 coreespond to peephole of output gate.
 *     some bias may disappear and bias orders may be different according to RNN configurations.
 *   - For \p y_desc, its \p layout and \p dtype need to match that of x_desc.
 *   - For \p hy_desc, its \p layout and \p dtype and \p shape must be set the same way as
 *     \p hx_desc.
 *   - If the shape of the input sequence data \p x_desc is (\p T, \p N, \p C),
 *     then the input sequence \p x_desc and the RNN descriptor must meet the following
 *     requirements:
 *     - \p state_size and \p rec_proj_size must be less than or equal to \p hidden_size.
 *     - Size of \p rec_filter 4 *  hidden_size *  state_size must be less than or equal to \f$2^{23}-1\f$.
 *     - If \p input_mode is ::CNNL_RNN_SKIP_INPUT, then input_size must meet the requirements:
 *       - input_size = 4 * hidden_size.
 *     - On CE3226:
 *       - Pad(\p hidden_size, 64) * 4 * 64 * sizeof(\p filter_type) <= \p wram_size / 2.
 *       - layer num must be less than or equal to 1.
 *       - \p N must be less than or equal to 64.
 *       - \p C must be less than or equal to 1024.
 *
 * - If mask_mode is ::CNNL_LSTM_MASK_SEQUENCES:
 *   - The mask tensor stores the valid sequence length for each token, and the shape should be
 *     (1, \p N).
 *
 * @par API Dependency
 * - Before calling this function to implement RNN, you need to prepare
 *   all the parameters passed to this function. See each parameter description
 *   for details.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance,
 *   set the layout of the input sequence data \p x_desc and output sequence data \p y_desc
 *   to ::CNNL_SEQDATA_TNC, set the layout of the filter tensor and bias tensor to NHWC.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Reference
 * - Long Short-Term Memory, Hochrereiter, 1997.
 *
 */
CNNL_DEPRECATED_FOR(cnnlRNNForwardInference_v2)
cnnlStatus_t CNNL_WIN_API
cnnlRNNForwardInference(cnnlHandle_t handle,
                        const cnnlRNNDescriptor_t rnn_desc,
                        const cnnlSeqDataDescriptor_t x_desc,
                        const void *x,
                        const cnnlTensorDescriptor_t hx_desc,
                        const void *hx,
                        const cnnlTensorDescriptor_t cx_desc,
                        const void *cx,
                        const cnnlTensorSetDescriptor_t w_desc,
                        const void *filter,
                        const cnnlTensorSetDescriptor_t b_desc,
                        const void *bias,
                        const cnnlSeqDataDescriptor_t mask_desc,
                        const void *mask,
                        const void *extra_input,
                        const cnnlSeqDataDescriptor_t y_desc,
                        void *y,
                        const cnnlTensorDescriptor_t hy_desc,
                        void *hy,
                        const cnnlTensorDescriptor_t cy_desc,
                        void *cy,
                        const cnnlSeqDataDescriptor_t k_desc, /* reserved, should pass NULL */
                        const void *keys,                     /* reserved, should pass NULL */
                        const cnnlSeqDataDescriptor_t c_desc, /* reserved, should pass NULL */
                        void *c_attn,                          /* reserved, should pass NULL */
                        const cnnlSeqDataDescriptor_t i_desc, /* reserved, should pass NULL */
                        void *i_attn,                          /* reserved, should pass NULL */
                        const cnnlSeqDataDescriptor_t q_desc, /* reserved, should pass NULL */
                        void *queries,                        /* reserved, should pass NULL */
                        void *workspace,
                        size_t workspace_size);

// Group:RNN
/*!
 * @brief Computes the forward process of RNN network in the inference scenario. The specific
 *        network structure is determined by the descriptor \p rnn_desc set by the user.
 *        Using the input data \p x, \p hx, \p cx, \p filter, \p bias, and \p mask, according to
 *        the specific network structure, this function writes the calculation result into the
 *        output memory \p y, \p hy, and \p cy.
 *
 * This function requires two additional MLU memory as the extra input and the workspace to improve
 * the RNN network performance. You can get the workspace size
 * with the ::cnnlGetRNNWorkspaceSize function, and the size of the extra input \p extra_input_size
 * with the ::cnnlGetRNNExtraInputSize function.
 *
 * Compared with ::cnnlRNNForwardInference, ::cnnlRNNForwardInference_v2 uses \p w_desc and \p b_desc
 * in type of ::cnnlTensorDescriptor_t instead of ::cnnlTensorSetDescriptor_t.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the RNN operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] rnn_desc
 *   Input. The descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[in] x_desc
 *   Input. The descriptor of input sequence data.
 *   For detailed information, see ::cnnlSeqDataDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores input sequence data.
 * @param[in] hx_desc
 *   Input. The descriptor of hidden state tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t
 * @param[in] hx
 *   Input. Pointer to the MLU memory that stores hidden state tensor.
 * @param[in] cx_desc
 *   Input. The descriptor of cell state tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t
 * @param[in] cx
 *   Input. Pointer to the MLU memory that stores cell state tensor.
 * @param[in] w_desc
 *   Input. The descriptor of the filter tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores filter.
 * @param[in] b_desc
 *   Input. The descriptor of the bias tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores bias data and peephole data.
 * @param[in] mask_desc
 *   Input. The descriptor of mask sequence data.
 *   For detailed information, see ::cnnlSeqDataDescriptor_t.
 * @param[in] mask
 *   Input. Pointer to the MLU memory that stores mask data.
 * @param[in] extra_input
 *   Input. Pointer to the MLU memory that stores the extra input data.
 *   You need to copy the extra input data to MLU from the host that is
 *   initialized with ::cnnlInitRNNExtraInput. For more information
 *   about extra input data, see Cambricon CNNL User Guide.
 * @param[in] y_desc
 *   Input. The descriptor of output sequence data.
 *   For detailed information, see ::cnnlSeqDataDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores output sequence data.
 * @param[in] hy_desc
 *   Input. The descriptor of output hidden state tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] hy
 *   Output. Pointer to the MLU memory that stores output hidden state tensor.
 * @param[in] cy_desc
 *   Input. The descriptor of output cell state tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] cy
 *   Output. Pointer to the MLU memory that stores output cell state tensor.
 * @param[in] k_desc
 *   Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] keys
 *   Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] c_desc
 *   Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] c_attn
 *   Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] i_desc
 *   Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] i_attn
 *   Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] q_desc
 *   Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] queries
 *   Reserved for future use. Set the value of these parameters to NULL.
 * @param[in]  workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   RNN operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in]  workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the RNN operation. You can get the size of the workspace with
 *   the ::cnnlGetRNNWorkspaceSize function.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "RNN Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for the input tensor
 *   \p x_desc, and \p hx_desc, filter tensor \p w_desc, bias tensor \p cx_desc, \p cy_desc,
 *   \p b_desc and \p mask_desc, output_tensor \p y_desc, and \p hy_desc.
 *    - input tensor: int8, int16, half, float.
 *    - filter tensor: int8, int16, int31, half, float.
 *    - bias tensor: half, float.
 *    - output tensor: int8, int16, half, float.
 *
 * - If \p rnn_mode is ::CNNL_RNN_RELU or ::CNNL_RNN_TANH:
 *   - Onchip and offchip data type of filter tensor must be the same, and can be int8, int16, or int31.
 *   - If the offchip data type of input tensor is float or half, then bias tensor should have the
 *     same offchip data type with input tensor.
 *   - If offchip data type of input tensor is int8 or int16, then filter tensor should have the
 *     same onchip data type with input tensor.
 *   - Input tensor and filter tensor should have the same onchip data type, besides input tensor
 *     and output tensor should have the same offchip data type.
 *
 * - If rnn_mode is ::CNNL_LSTM or ::CNNL_LSTM_TANH:
 *   - Onchip and offchip data type of filter tensor must be the same, which can be int8, int16,
 *     or int31 on MLU200/MLU270/MLU290/CE3226, and can be int8, int16, int31, half or float on
 *     MLU300 series.
 *   - If onchip data type of filter tensor is float or half, then the input tensor, bias tensor and
 *     output tensor should have the same onchip and offchip data type.
 *   - If the offchip data type of input tensor is float or half, then bias tensor should have the
 *     same offchip data type with input tensor.
 *   - If offchip data type of input tensor is int8 or int16, then filter tensor should have the
 *     same onchip data type with input tensor.
 *   - Input tensor and filter tensor should have the same onchip data type, besides input tensor
 *     and output tensor should have the same offchip data type.
 *   - If \p rec_proj_size is greater than zero, then onchip data type of filter tensor must be
 *     int8 or int16 on CE3226.
 *
 * - If \p mask_mode is ::CNNL_LSTM_MASK_SEQUENCES:
 *   - Mask data type must be int32.
 *
 * @par Data Layout
 * - The supported data layouts set in descriptors are as follows:
 *   - \p x_desc must be set to \p CNNL_SEQDATA_TNC.
 *   - \p hx_desc must be set to \p CNNL_LAYOUT_ARRAY and \p dimNb in \p hx_desc must be 3.
 *   - \p cx_desc must be set to \p CNNL_LAYOUT_ARRAY and \p dimNb in \p cx_desc must be 3.
 *   - \p w_desc must be set to \p CNNL_LAYOUT_NHWC.
 *   - \p b_desc must be set to \p CNNL_LAYOUT_NHWC.
 *   - \p mask_desc must be set to \p CNNL_SEQDATA_TN.
 *   - \p y_desc must be set to \p CNNL_SEQDATA_TNC.
 *   - \p hy_desc must be set to \p CNNL_LAYOUT_ARRAY and \p dimNb in \p hy_desc must be 3.
 *   - \p cy_desc must be set to \p CNNL_LAYOUT_ARRAY and \p dimNb in \p cy_desc must be 3.
 *
 * @par Scale Limitation
 * - \p rnn_mode supports ::CNNL_RNN_RELU, ::CNNL_RNN_TANH, ::CNNL_LSTM and ::CNNL_LSTM_TANH.
 * - Layout of \p x_desc and \p y_desc only supports ::CNNL_SEQDATA_TNC.
 * - If \p rnn_mode is not ::CNNL_LSTM or ::CNNL_LSTM_TANH, then \p cx_desc, \p cx, \p cy_desc and
 *   \p cy are NULL.
 * - The descriptor of filter tensor set \p w_desc, contains all filter tensor related information
 *   of RNN. Its dimensions are (\p num_layers, direction_num, cell_filter_num).
 * - When \p direction is ::CNNL_RNN_BIDIRECTIONAL, \p direction_num must be 2, else
 *   \p direction_num must be 1.
 * - The descriptor of filter tensor sets \p w_desc contains all filter tensor related information
 *   of RNN. The dimensions are (\p num_layers, direction_num, cell_filter_num).
 * - The descriptor of bias tensor set \p b_desc, containing all bias and peephole data.
 *   Its dimensions are (layer_num, direction_num, cell_data_num).
 *   The principle is the same as above w_desc, the difference is that addition to bias, peephole
 *   tensor is included.
 *
 * - If rnn_mode is ::CNNL_RNN_RELU or ::CNNL_RNN_TANH:
 *   - If \p input_mode is ::CNNL_RNN_LINEAR_INPUT and \p output_mode is ::CNNL_RNN_NO_OUT_LAYER,
 *     \p cell_filter_num should be 2.
 *   - If \p input_mode is ::CNNL_RNN_LINEAR_INPUT and \p output_mode is ::CNNL_RNN_HAS_OUT_LAYER,
 *     \p cell_filter_num should be 3.
 *   - If \p input_mode is ::CNNL_RNN_SKIP_INPUT and \p output_mode is ::CNNL_RNN_NO_OUT_LAYER, the
 *     first layer \p cell_filter_num should be 1.
 *     Other layers \p cell_filter_num should be 2.
 *   - If \p input_mode is ::CNNL_RNN_SKIP_INPUT and \p output_mode is ::CNNL_RNN_HAS_OUT_LAYER,
 *     the first layer \p cell_filter_num should be 2.
 *     Other layers \p cell_filter_num should be 3.
 *   - The overall filter order of \p w_desc is the matrix multiplication with input,
 *     and the matrix multiplication with the hidden state. The specific location is as follows:
 *     - Value 0 corresponds to the input gate.
 *     - Value 1 corresponds to the recursive gate.
 *     - Value 2 corresponds to the output gate.
 *     some filter may disappear according to RNN configurations.
 *
 *   - If \p bias_mode is ::CNNL_RNN_DOUBLE_BIAS and \p output_mode is ::CNNL_RNN_NO_OUT_LAYER,
 *     \p cell_data_num is 2.
 *   - If \p bias_mode is ::CNNL_RNN_SINGLE_INP_BIAS or ::CNNL_RNN_SINGLE_REC_BIAS and
 *     \p output_mode is
 *     ::CNNL_RNN_NO_OUT_LAYER, \p cell_data_num is 1.
 *   - If \p bias_mode is ::CNNL_RNN_NO_BIAS and output_mode is ::CNNL_RNN_NO_OUT_LAYER,
 *     \p cell_data_num is 0.
 *   - If \p bias_mode is ::CNNL_RNN_DOUBLE_BIAS and \p output_mode is ::CNNL_RNN_HAS_OUT_LAYER,
 *     \p cell_data_num is 3.
 *   - If \p bias_mode is ::CNNL_RNN_SINGLE_INP_BIAS or ::CNNL_RNN_SINGLE_REC_BIAS and
 *     \p output_mode is ::CNNL_RNN_HAS_OUT_LAYER, \p cell_data_num is 2.
 *   - If \p bias_mode is ::CNNL_RNN_NO_BIAS and \p output_mode is ::CNNL_RNN_HAS_OUT_LAYER,
 *     \p cell_data_num is 1.
 *   - The overall bias order of \p b_desc is the input related and hidden state related.
 *     The specific location is as follows:
 *     - Value 0 corresponds to the input gate.
 *     - Value 1 corresponds to the recursive gate.
 *     - Value 2 corresponds to the output gate.
 *     some bias may disappear according to RNN configurations.
 *   - For \p y_desc, its \p layout and \p dtype need to match that of x_desc.
 *   - For \p hy_desc, its \p layout and \p dtype and \p shape must be set the same way as
 *     \p hx_desc.
 *   - If the shape of the input sequence data \p x_desc is (\p T, \p N, \p C), \p layer_num must
 *     be greater than or equal to 1,
 *     \p T must be greater than or equal to 1, and \p C must be greater than or equal to 1.
 *     If \p input_mode is ::CNNL_RNN_SKIP_INPUT, then input_size must equal to hidden_size.
 *
 * - If rnn_mode is ::CNNL_LSTM or ::CNNL_LSTM_TANH:
 *   - If input_mode is ::CNNL_RNN_LINEAR_INPUT and LSTM projection is enabled, \p cell_filter_num
 *     should be 9.
 *   - If input_mode is ::CNNL_RNN_LINEAR_INPUT and LSTM projection is disabled, \p cell_filter_num
 *     should be 8.
 *   - If input_mode is ::CNNL_RNN_SKIP_INPUT and LSTM projection is enabled, the first layer
 *     \p cell_filter_num should be 5.
 *     Other layers \p cell_filter_num should be 9.
 *   - If input_mode is ::CNNL_RNN_SKIP_INPUT and LSTM projection is disabled, the first layer
 *     \p cell_filter_num should be 4.
 *     Other layers \p cell_filter_num should be 8.
 *   - If \p filter_order is CNNL_LSTM_IFGO, then the overall filter order of \p w_desc is the
 *     matrix multiplication
 *     with input, and the matrix multiplication with the hidden state.
 *     The specific location is as follows:
 *     - Value 0 and value 4 correspond to the input gate.
 *     - Value 1 and value 5 correspond to the forget gate.
 *     - Value 2 and value 6 correspond to the update gate.
 *     - Value 3 and value 7 correspond to the output gate.
 *     - Value 8 correspond to the projection filter.
 *     some filter may disappear and filter orders may be different according to RNN configurations.
 *
 *   - If \p bias_mode is ::CNNL_RNN_DOUBLE_BIAS, and \p peephole is enabled, then \p cell_data_num
 *     is 11.
 *   - If \p bias_mode is ::CNNL_RNN_SINGLE_INP_BIAS or ::CNNL_RNN_SINGLE_REC_BIAS, and \p peephole
 *     is enabled,
 *     then \p cell_data_num is 7.
 *   - If \p bias_mode is ::CNNL_RNN_NO_BIAS, and \p peephole is enabled, then \p cell_data_num
 *     is 3.
 *   - If \p bias_mode is ::CNNL_RNN_DOUBLE_BIAS, and \p peephole is disabled, then
 *     \p cell_data_num is 8.
 *   - If \p bias_mode is ::CNNL_RNN_SINGLE_INP_BIAS or ::CNNL_RNN_SINGLE_REC_BIAS, and
 *     \p peephole is disabled,
 *     then \p cell_data_num is 4.
 *   - If \p bias_mode is ::CNNL_RNN_NO_BIAS, and \p peephole is disabled, then
 *     \p cell_data_num is 0.
 *   - If \p filter_order is CNNL_LSTM_IFGO, then the overall bias order of \p b_desc
 *     is the input related,
 *     hidden state related and peephole connection related. The specific location is as follows:
 *     - Value 0 and value 4 correspond to bias of input gate.
 *     - Value 1 and value 5 correspond to bias of forget gate.
 *     - Value 2 and value 6 correspond to bias of update gate.
 *     - Value 3 and value 7 correspond to bias of output gate.
 *     - Value 8 coreespond to peephole of input gate.
 *     - Value 9 coreespond to peephole of forget gate.
 *     - Value 10 coreespond to peephole of output gate.
 *     some bias may disappear and bias orders may be different according to RNN configurations.
 *   - For \p y_desc, its \p layout and \p dtype need to match that of x_desc.
 *   - For \p hy_desc, its \p layout and \p dtype and \p shape must be set the same way as
 *     \p hx_desc.
 *   - If the shape of the input sequence data \p x_desc is (\p T, \p N, \p C),
 *     then the input sequence \p x_desc and the RNN descriptor must meet the following
 *     requirements:
 *     - \p state_size and \p rec_proj_size must be less than or equal to \p hidden_size.
 *     - Size of \p rec_filter 4 *  hidden_size *  state_size must be less than or equal to \f$2^{23}-1\f$.
 *     - If \p input_mode is ::CNNL_RNN_SKIP_INPUT, then input_size must meet the requirements:
 *       - input_size = 4 * hidden_size.
 *     - On CE3226:
 *       - Pad(\p hidden_size, 64) * 4 * 64 * sizeof(\p filter_type) <= \p wram_size / 2.
 *       - layer num must be less than or equal to 1.
 *       - \p N must be less than or equal to 64.
 *       - \p C must be less than or equal to 1024.
 *
 * - If mask_mode is ::CNNL_LSTM_MASK_SEQUENCES:
 *   - The mask tensor stores the valid sequence length for each token, and the shape should be
 *     (1, \p N).
 *
 * @par API Dependency
 * - Before calling this function to implement RNN, you need to prepare
 *   all the parameters passed to this function. See each parameter description
 *   for details.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance,
 *   set the layout of the input sequence data \p x_desc and output sequence data \p y_desc
 *   to ::CNNL_SEQDATA_TNC, set the layout of the filter tensor and bias tensor to NHWC.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Reference
 * - Long Short-Term Memory, Hochrereiter, 1997.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlRNNForwardInference_v2(cnnlHandle_t handle,
                           const cnnlRNNDescriptor_t rnn_desc,
                           const cnnlSeqDataDescriptor_t x_desc,
                           const void *x,
                           const cnnlTensorDescriptor_t hx_desc,
                           const void *hx,
                           const cnnlTensorDescriptor_t cx_desc,
                           const void *cx,
                           const cnnlTensorDescriptor_t w_desc,
                           const void *filter,
                           const cnnlTensorDescriptor_t b_desc,
                           const void *bias,
                           const cnnlSeqDataDescriptor_t mask_desc,
                           const void *mask,
                           const void *extra_input,
                           const cnnlSeqDataDescriptor_t y_desc,
                           void *y,
                           const cnnlTensorDescriptor_t hy_desc,
                           void *hy,
                           const cnnlTensorDescriptor_t cy_desc,
                           void *cy,
                           const cnnlSeqDataDescriptor_t k_desc, /* reserved, should pass NULL */
                           const void *keys,                     /* reserved, should pass NULL */
                           const cnnlSeqDataDescriptor_t c_desc, /* reserved, should pass NULL */
                           void *c_attn,                          /* reserved, should pass NULL */
                           const cnnlSeqDataDescriptor_t i_desc, /* reserved, should pass NULL */
                           void *i_attn,                          /* reserved, should pass NULL */
                           const cnnlSeqDataDescriptor_t q_desc, /* reserved, should pass NULL */
                           void *queries,                        /* reserved, should pass NULL */
                           void *workspace,
                           size_t workspace_size);

// Group:RNN
/*!
 * @brief Computes the forward process of RNN network in the training scenario. The specific
 *        network structure is determined by the descriptor \p rnn_desc set by the user.
 *        Using the input data \p x, \p hx, \p cx, \p filterspace, according to the specific
 *        network structure, writes the calculation result into the output memory \p y,
 *        \p hy, \p cy.
 *
 * This function requires two additional MLU memory as the \p reservespace and the \p workspace to
 * improve the RNN network performance. You can get the size of the \p workspace \p workspace_size and
 * \p reservespace \p reservespace_size with the ::cnnlGetRNNTempSizes function, and the size of the
 * \p filterspace \p filterspace_size with the ::cnnlGetRNNWeightSpaceSize function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the RNN operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] rnn_desc
 *   Input. The descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[in] dev_seq_lengths
 *   Input. A copy of \p seqLengthArray set in \p x_desc or \p y_desc RNN data descriptor.
 *   The dev_seq_lengths array must be stored in MLU memory.
 * @param[in] x_desc
 *   Input. The descriptor of input sequence data.
 *   For detailed information, see ::cnnlSeqDataDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores input sequence data.
 * @param[in] y_desc
 *   Input. The descriptor of output sequence data.
 *   For detailed information, see ::cnnlSeqDataDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores output sequence data.
 * @param[in] h_desc
 *   Input. The descriptor of hidden state tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] hx
 *   Input. Pointer to the MLU memory that stores hidden state tensor.
 * @param[out] hy
 *   Output. Pointer to the MLU memory that stores output hidden state tensor.
 * @param[in] c_desc
 *   Input. The descriptor of cell state tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] cx
 *   Input. Pointer to the MLU memory that stores cell state tensor.
 * @param[out] cy
 *   Output. Pointer to the MLU memory that stores output cell state tensor.
 * @param[in] filterspace
 *   Input. Pointer to the MLU memory that stores filter and bias.
 * @param[in] filterspace_size
 *   Input. Specifies the size of the buffer in bytes that stores filter.
 *   You can call ::cnnlGetRNNWeightSpaceSize to get the size of the buffer to be used.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra \p workspace for the RNN operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the RNN operation.
 *   You can get the size of the workspace with the ::cnnlGetRNNTempSizes function.
 * @param[in] reservespace
 *   Input. Pointer to the MLU memory that is used as an extra memory space for saving
 *   intermediate results of RNN operation.
 * @param[in] reservespace_size
 *   Input. The size of the extra reservespace in bytes that needs to be used in the RNN operation.
 *   You can get the size of the reservespace with the ::cnnlGetRNNTempSizes function.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "RNN Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layouts set in descriptors are as follows:
 *   - \p x_desc must be set to ::CNNL_SEQDATA_TNC, ::CNNL_SEQDATA_NTC or ::CNNL_SEQDATA_TNC_PACKED.
 *   - \p y_desc must be set to be the same layout as \p x_desc.
 *   - \p h_desc must be set to ::CNNL_LAYOUT_ARRAY and \p dimNb in \p h_desc must be 3.
 *   - \p c_desc must be set to ::CNNL_LAYOUT_ARRAY and \p dimNb in \p c_desc must be 3.
 *
 * @par Scale Limitation
 * - The \p rnn_mode of rnn_desc \p only supports ::CNNL_LSTM with projection layer.
 * - The \p input_mode of rnn_desc \p only supports ::CNNL_RNN_LINEAR_INPUT.
 * - The \p padding_mode of rnn_desc \p only supports ::CNNL_RNN_PADDED_IO_DISABLED.
 * - If \p direction is ::CNNL_RNN_UNIDIRECTIONAL, then the first dimension of hidden state
 *   tensor or cell state tensor should match its layer_num argument passed to ::cnnlSetRNNDescriptor_v2.
 * - If dir_mode is ::CNNL_RNN_BIDIRECTIONAL, then the first dimension should be double the
 *   \p layer_num argument passed to ::cnnlSetRNNDescriptor_v2.
 * - The second dimension must match the batch's size and the third dimension must match the hidden's size.
 * - \p layer_num must be set to 1 currently.
 * - The \p layout and \p dtype of \p y_desc need to match that of \p x_desc.
 * - The \p math_prec must be int16 on CE3226 and MLU200 series and it must be the same as
 *   \p data_type or int16 on MLU300 series.
 * - The \p dev_seq_lengths must be batch's sequence and descending order and the length of
 *   the \p dev_seq_lengths must be equal to x_desc->dims[0] when the \p cnnlSeqDataLayout_t
 *   is CNNL_SEQDATA_TNC_PACKED.
 * - The \p layout of \p filterspace should be [wi, wh, whr, bias].
 *
 * @par API Dependency
 * - Before calling this function to implement RNN, you need to prepare
 *   all the parameters passed to this function. See each parameter description
 *   for details.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance,
 *   set the layout of the input sequence data \p x_desc and output sequence data \p y_desc
 *   to ::CNNL_SEQDATA_TNC.
 *
 * @note
 * - The first two dimensions of \p x_desc must be equal to those of \p y_desc.
 */
cnnlStatus_t CNNL_WIN_API
cnnlRNNForwardTraining(cnnlHandle_t handle,
                       const cnnlRNNDescriptor_t rnn_desc,
                       const int dev_seq_lengths[],
                       const cnnlSeqDataDescriptor_t x_desc,
                       const void *x,
                       const cnnlSeqDataDescriptor_t y_desc,
                       void *y,
                       const cnnlTensorDescriptor_t h_desc,
                       const void *hx,
                       void *hy,
                       const cnnlTensorDescriptor_t c_desc,
                       const void *cx,
                       void *cy,
                       const void *filterspace,
                       size_t filterspace_size,
                       void *workspace,
                       size_t workspace_size,
                       void *reservespace,
                       size_t reservespace_size);

// Group:LSTMGatesForward
/*!
 * @brief Computes the forward process of LSTM cell network in the training scenario.
 *        It use the input data \p x_gates, \p x_bias, \p h_gates, \p h_bias and \p cx
 *        according to the specific network structure and writes the calculation result
 *        into the output memory \p cy and \p hy.
 *
 * This function requires an additional MLU memory as the \p reservespace to improve the
 * performance of LSTM cell network. You can get the size of the \p reservespace
 * \p reservespace_size with the ::cnnlGetLSTMGatesTempSize function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the LSTM cell operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] gates_desc
 *   Input. The descriptor of gates tensor, \p x_gates, \p x_bias, \p h_gates and \p h_bias.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x_gates
 *   Input. Pointer to the MLU memory that stores input gates data.
 * @param[in] x_bias
 *   Input. Pointer to the MLU memory that stores bias of input gates data.
 * @param[in] h_gates
 *   Input. Pointer to the MLU memory that stores hidden gates data.
 * @param[in] h_bias
 *   Input. Pointer to the MLU memory that stores bias of hidden gates data.
 * @param[in] c_desc
 *   Input. The descriptor of cell state tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] cx
 *   Input. Pointer to the MLU memory that stores cell state tensor.
 * @param[out] cy
 *   Output. Pointer to the MLU memory that stores output cell state tensor.
 * @param[in] h_desc
 *   Input. The descriptor of hidden state tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] hy
 *   Output. Pointer to the MLU memory that stores output hidden state tensor.
 * @param[in] reservespace
 *   Input. Pointer to the MLU memory that is used as an extra memory space for saving
 *   intermediate results of LSTM cell operation.
 * @param[in] reservespace_size
 *   Input. The size of the extra reservespace in bytes that needs to be used in the LSTM cell operation.
 *   You can get the size of the reservespace with the ::cnnlGetLSTMGatesTempSize function.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - Data types of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layouts set in descriptors are as follows:
 *   - \p gates_desc must be set to ::CNNL_LAYOUT_NGC or ::CNNL_LAYOUT_GNC.
 *   - \p c_desc must be be set to ::CNNL_LAYOUT_NC.
 *   - \p h_desc must be be set to ::CNNL_LAYOUT_NC.
 *   - The gate layout of gates data and bias must be ifgo.
 *
 *@par Note
 * - The \p x_bias and \p h_bias can be set to NULL,
 *   which means no bias participate in operations.
 * - The size of the \p x_bias and \p h_bias is 4 * C, 4 means four gates and
 *   C means hidden.
 * - The dimemsions of the \p x_gates and \p h_gates are [N, 4, C] or [4, N, C], where 4 means
 *   four gates, N means batch and C means hidden.
 * - This operation is not supported on the 1V platforms.
 */

cnnlStatus_t CNNL_WIN_API
cnnlLSTMGatesForward(cnnlHandle_t handle,
                    const cnnlTensorDescriptor_t gates_desc,
                    const void *x_gates,
                    const void *x_bias,
                    const void *h_gates,
                    const void *h_bias,
                    const cnnlTensorDescriptor_t c_desc,
                    const void *cx,
                    void *cy,
                    const cnnlTensorDescriptor_t h_desc,
                    void *hy,
                    void *reservespace,
                    size_t reservespace_size);
// Group:RNN
/*!
 * @brief Returns in \p size_in_bytes the size of the MLU memory that is used as an extra workspace
 * to optimize the RNN forward operation.
 *
 * The size of the extra workspace is based on the given information of the RNN
 * forward operation, including RNN descriptor \p conv_desc,
 * the input sequence data descriptor \p x_desc tht output sequence data descriptor \p y_desc.
 * For more information about the extra workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the convolution operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] rnn_desc
 *   Input.The descriptor of the RNN operation. For detailed information,
 *   see ::cnnlConvolutionDescriptor_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input sequence data. For detailed information,
 *   see ::cnnlSeqDataDescriptor_t.
 * @param[in] y_desc
 *   Input. The descriptor of the output sequence data. For detailed information,
 *   see ::cnnlSeqDataDescriptor_t.
 * @param[out] size_in_bytes
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in
 *   the RNN forward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function,
 *   you need to call ::cnnlCreateRNNDescriptor, ::cnnlSetRNNDescriptor functions to create and
 *   set the RNN operation descriptor,
 *   and call the ::cnnlCreateSeqDataDescriptor and ::cnnlSetSeqDataDescriptor functions
 *   to create and set the tensor descriptors \p x_desc and \p y_desc.
 * - The allocated extra workspace memory should be passed to the ::cnnlRNNForwardInference function
 *   to perform the RNN forward operation.
 *
 * @note
 * - For the RNN descriptor, in addition to the basic information set by calling the function
 *   ::cnnlSetRNNDescriptor, if you want to run the optional variant mode, you need to call the
 *   corresponding function to set it. Such as:
 *   ::cnnlSetRNNProjectionLayers, ::cnnlSetRNNPeepholeMode, ::cnnlSetRNNBiasMode,
 *   ::cnnlSetRNNMaskMode, ::cnnlSetRNNClip, ::cnnlSetRNNPaddingMode.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetRNNWorkspaceSize(cnnlHandle_t handle,
                        const cnnlRNNDescriptor_t rnn_desc,
                        const cnnlSeqDataDescriptor_t x_desc,
                        const cnnlSeqDataDescriptor_t y_desc,
                        size_t *size_in_bytes);

// Group:RNN
/*!
 * @brief Returns in \p extra_input_size the size of the MLU memory and host memory that is used as an extra
 * input data to optimize the RNN forward operation. You need to allocate memory both on host and MLU based on
 * the size returned in \p extra_input_size.
 *
 * The size of extra input data is based on the given information of the RNN
 * forward operation, including RNN descriptor \p rnn_desc,
 * the input sequence data descriptor \p x_desc.
 * For more information about the extra input, see "Cambricon CNNL User Guide".
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the convolution operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] rnn_desc
 *   Input.The descriptor of the RNN operation. For detailed information,
 *   see ::cnnlRNNDescriptor_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input sequence data. For detailed information,
 *   see ::cnnlSeqDataDescriptor_t.
 * @param[out] extra_input_size
 *   Output. A host pointer to the returned size of the extra input data in bytes
 *   that is used in the RNN operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - The parameter \p extra_input_size is a pointer and should not be NULL.
 *
 * @par API Dependency
 * - Before calling this function,
 *   you need to call ::cnnlCreateRNNDescriptor, ::cnnlSetRNNDescriptor functions to create and
 *   set up a complete the RNN operation descriptors,
 *   and call the ::cnnlCreateSeqDataDescriptor and ::cnnlSetSeqDataDescriptor functions
 *   to create and set the tensor descriptors \p x_desc.
 * - After calling this function, you need to call ::cnnlInitRNNExtraInput to initialize the memory on host.
 * - The allocated extra input should be passed to the ::cnnlRNNForwardInference function
 *   to perform the RNN operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetRNNExtraInputSize(cnnlHandle_t handle,
                                                   const cnnlRNNDescriptor_t rnn_desc,
                                                   const cnnlSeqDataDescriptor_t x_desc,
                                                   size_t *extra_input_size);
// Group:RNN
/*!
 * @brief Initializes the extra input data space \p extra_input_host_ptr on host.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlInitRNNWeightPositionAndScale instead, which obtains position and scale
 * from \p weight_position_ptr and \p weight_scale_ptr.
 *
 * @param[out]  extra_input_host_ptr
 *   Output. Pointer to the host memory that is used as an extra input space
 *   for the RNN forward operation.
 * @param[in] rnn_desc
 *   Input.The descriptor of the RNN operation. For detailed information,
 *   see ::cnnlRNNDescriptor_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input sequence data. For detailed information,
 *   see ::cnnlSeqDataDescriptor_t.
 * @param[in] hx_desc
 *   Input. The descriptor of the hidden state tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] w_desc
 *   Input. The descriptor of the filter tensor set. For detailed information,
 *   see ::cnnlTensorSetDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the bias tensor set. For detailed information,
 *   see ::cnnlTensorSetDescriptor_t.
 * @param[in] w_ptr
 *   Input. The pointer to all filter data address on the MLU device memory.
 * @param[in] b_ptr
 *   Input. Pointer to all bias data address on the MLU device memory.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - The parameter \p extra_input_host_ptr is a pointer and should not be NULL.
 *
 * @par API Dependency
 * - You need to get the size of the extra input data with ::cnnlGetRNNExtraInputSize.
 *   The memory of the extra input data should be allocated before calling this function.
 * - This function must be called after ::cnnlCreateSeqDataDescriptor, ::cnnlSetSeqDataDescriptor,
 *   ::cnnlCreateTensorDescriptor, ::cnnlSetTensorDescriptor,
 *   ::cnnlCreateTensorSetDescriptor, ::cnnlInitTensorSetMemberDescriptor and
 *   ::cnnlInitTensorSetMemberDescriptorPositionAndScale.
 *
 * - The ::cnnlCreateSeqDataDescriptor and ::cnnlSetSeqDataDescriptor functions are used to create
 *   and set the sequence data descriptors \p x_desc.
 * - The ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions are used to create and set
 *   the tensor descriptors \p hx_desc.
 * - The ::cnnlCreateTensorSetDescriptor, ::cnnlInitTensorSetMemberDescriptor,
 *   and ::cnnlInitTensorSetMemberDescriptorPositionAndScale functions are used to create and set
 *   the tensor set descriptors \p w_desc and \p b_desc.
 * - This function must be called after allocating MLU memory of \p w_ptr and \p b_ptr.
 * - The allocated extra input should be passed to the ::cnnlRNNForwardInference function
 *   to perform the RNN operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlInitRNNWeightPositionAndScale)
cnnlStatus_t CNNL_WIN_API cnnlInitRNNExtraInput(void *extra_input_host_ptr,
                                                const cnnlRNNDescriptor_t rnn_desc,
                                                const cnnlSeqDataDescriptor_t x_desc,
                                                const cnnlTensorDescriptor_t hx_desc,
                                                const cnnlTensorSetDescriptor_t w_desc,
                                                const cnnlTensorSetDescriptor_t b_desc,
                                                void *w_ptr,
                                                void *b_ptr);

// Group:RNN
/*!
 * @brief Initializes the extra input data space \p extra_input_host_ptr on host.
 * Compared with ::cnnlInitRNNExtraInput, ::cnnlInitRNNWeightPositionAndScale
 * obtains position and scale from \p weight_position_ptr and \p weight_scale_ptr.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in ::cnnlInitRNNWeightPositionAndScale. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] rnn_desc
 *   Input. The descriptor of the RNN operation. For detailed information,
 *   see ::cnnlRNNDescriptor_t.
 * @param[in] weight_num
 *   Input. The number of filters used in the RNN forward operation.
 * @param[in] weight_position_ptr
 *   Input. The pointer to position for filter data address on the host memory.
 * @param[in] weight_scale_ptr
 *   Input. The pointer to scale for filter data address on the host memory.
 * @param[in] weight_offset_ptr
 *   Input. The pointer to offset for filter data address on the host memory.
 * @param[out]  extra_input_host_ptr
 *   Output. Pointer to the host memory that is used as an extra input space
 *   in the RNN forward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - The parameter \p extra_input_host_ptr is a pointer and should not be NULL.
 *
 * @par API Dependency
 * - Before calling this function, you need to get the size of the extra input data with
 *   ::cnnlGetRNNExtraInputSize, and make sure that the memory of the extra input data is allocated.
 * - This function must be called after ::cnnlCreateSeqDataDescriptor, ::cnnlSetSeqDataDescriptor,
 *   ::cnnlCreateTensorDescriptor, ::cnnlSetTensorDescriptor,
 *   ::cnnlCreateTensorSetDescriptor, ::cnnlInitTensorSetMemberDescriptor and
 *   ::cnnlInitTensorSetMemberDescriptorPositionAndScale.
 * - The ::cnnlCreateSeqDataDescriptor and ::cnnlSetSeqDataDescriptor functions are used to create
 *   and set the sequence data descriptor \p x_desc.
 * - The ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions are used to create and set
 *   the tensor descriptor \p hx_desc.
 * - The ::cnnlCreateTensorSetDescriptor, ::cnnlInitTensorSetMemberDescriptor,
 *   and ::cnnlInitTensorSetMemberDescriptorPositionAndScale functions are used to create and set
 *   the tensor set descriptor \p w_desc and \p b_desc.
 * - This function must be called after allocating MLU memory of \p w_ptr and \p b_ptr.
 * - The allocated extra input should be passed to the ::cnnlRNNForwardInference function
 *   to perform the RNN operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlInitRNNWeightPositionAndScale(cnnlHandle_t handle,
                                  const cnnlRNNDescriptor_t rnn_desc,
                                  const int weight_num,
                                  void *weight_position_ptr,
                                  void *weight_scale_ptr,
                                  void *weight_offset_ptr,
                                  void *extra_input_host_ptr);

// Group:RNN
/*!
 * @brief Creates a descriptor pointed by \p rnn_desc for a RNN inference forward
 *        operation, and allocates memory for holding the information about the RNN
 *        operation. The information is defined in ::cnnlRNNDescriptor_t. For more
 *        information about descriptor, see "Cambricon CNNL User Guide".
 *
 * @param[out] rnn_desc
 *   Output. A host pointer to the RNN descriptor that holds information about the RNN operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_NOT_INITIALIZED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetRNNDescriptor function to initialize
 *   and set the basic information to the RNN descriptor. In addition to the basic information,
 *   there are some optional modes that can be set through other functions,
 *   such as ::cnnlSetRNNProjectionLayers and ::cnnlSetRNNPeepholeMode.
 * - At last, you need to call the ::cnnlDestroyRNNDescriptor function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateRNNDescriptor(cnnlRNNDescriptor_t *rnn_desc);

// Group:RNN
/*!
 * @brief Destroys a RNN descriptor \p rnn_desc that was previously created with the
 *        ::cnnlCreateRNNDescriptor function.
 *
 * The RNN descriptor is defined in ::cnnlRNNDescriptor_t
 * and holds the information about the RNN inference forward operation.
 *
 * @param[in] rnn_desc
 *   Input. The convolution descriptor to be destroyed.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - You need to call this function after calling the ::cnnlRNNForwardInference function.
 * - This function should be called to destroy the RNN descriptor.
 *   Otherwise, the memory leak may occur.
 *
 * @par Requirements
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyRNNDescriptor(cnnlRNNDescriptor_t rnn_desc);

// Group:RNN
/*!
 * @brief Initializes the RNN descriptor \p rnn_desc that was previously created
 * with the ::cnnlCreateRNNDescriptor function.
 * The information includes the number of features in the hidden state \p hidden_size,
 * the number of recurrent layers \p num_layers, how the first recurrent layer processes the input
 * \p input_mode, the recurrence pattern \p direction, RNN cell implementation \p mode.
 *
 * @param[in,out] rnn_desc
 *  Input/output. The descriptor of the RNN operation. For detailed information,
 *   see ::cnnlRNNDescriptor_t.
 * @param[in] hidden_size
 *   Input. The number of features in the hidden state input vector.
 * @param[in] num_layers
 *   Input. The Number of stacked layers in the deep RNN mode.
 * @param[in] input_mode
 *   Input. Specifies how the first stacked layer handles the input of the RNN mode.
 *   When the parameter \p input_mode is  ::CNNL_RNN_SKIP_INPUT, no matrix formation operation is
 *   performed on the original input vectors.
 *   For detailed information, see ::cnnlRNNInputMode_t.
 * @param[in] direction
 *   Input. Determines recurrence pattern. In ::CNNL_RNN_BIDIRECTIONAL mode, the result of the
 *   current layer are concatenations of forward and backward hidden states in the feature dimension.
 *   For detailed information, see ::cnnlDirectionMode_t.
 * @param[in] rnn_mode
 *   Input. Determines how to implement the RNN cell. For detailed information, see ::cnnlRNNMode_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetRNNDescriptor(cnnlRNNDescriptor_t rnn_desc,
                                               const int hidden_size,
                                               const int num_layers,
                                               const cnnlRNNInputMode_t input_mode,
                                               const cnnlDirectionMode_t direction,
                                               const cnnlRNNMode_t rnn_mode);
// Group:RNN
/*!
 * @brief Initializes the RNN descriptor \p rnn_desc that was previously created
 * with the ::cnnlCreateRNNDescriptor function.
 *
 * @param[in,out] rnn_desc
 *  Input/output. The descriptor of the RNN operation. For detailed information,
 *   see ::cnnlRNNDescriptor_t.
 * @param[in] hidden_size
 *   Input. The number of features in the hidden state input vector.
 * @param[in] num_layers
 *   Input. The Number of stacked layers in the deep RNN mode.
 * @param[in] input_mode
 *   Input. Specifies how the first stacked layer handles the input of the RNN mode.
 *   When the parameter \p input_mode is  ::CNNL_RNN_SKIP_INPUT, no matrix formation operation is
 *   performed on the original input vectors. For detailed information, see ::cnnlRNNInputMode_t.
 * @param[in] direction
 *   Input. Determines recurrence pattern. In ::CNNL_RNN_BIDIRECTIONAL mode, the result of the
 *   current layer are concatenations of forward and backward hidden states in the feature dimension.
 *   For detailed information, see ::cnnlDirectionMode_t.
 * @param[in] rnn_mode
 *   Input. Determines how to implement the RNN cell. For detailed information, see ::cnnlRNNMode_t.
 * @param[in] bias_mode
 *   Input. Specify bias number of the RNN operation.
 * @param[in] peephole_mode
 *   Input. Specify peephole connections for the RNN operation.
 * @param[in] mask_mode
 *   Input. Specify mask operation for the RNN operation.
 * @param[in] math_prec
 *   Input. The data type of computing precision used in the RNN operation.
 *   Supported values are ::CNNL_DTYPE_HALF and ::CNNL_DTYPE_FLOAT.
 * @param[in] weight_order
 *   Input. The LSTM weight order used in RNN operation defined in ::cnnlRNNWeightOrder_t.
 * @param[in] comp_pref
 *   Input. The RNN computation preference mode defined in ::cnnlComputationPreference_t.
 * @param[in] output_mode
 *   Input. The RNN output mode defined in ::cnnlRNNOutputMode_t.
 * @param[in] padding_mode
 *   Input. Specify padding mode for input and output.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetRNNInferenceDescriptor(cnnlRNNDescriptor_t rnn_desc,
                              const int hidden_size,
                              const int num_layers,
                              const cnnlRNNInputMode_t input_mode,
                              const cnnlDirectionMode_t direction,
                              const cnnlRNNMode_t rnn_mode,
                              const cnnlRNNBiasMode_t bias_mode,
                              const cnnlRNNPeepholeMode_t peephole_mode,
                              const cnnlRNNMaskMode_t mask_mode,
                              const cnnlDataType_t math_prec,
                              const cnnlRNNWeightOrder_t weight_order,
                              const cnnlComputationPreference_t comp_pref,
                              const cnnlRNNOutputMode_t output_mode,
                              const cnnlRNNPaddingMode_t padding_mode);

// Group:RNN
/*!
 * @brief Sets quantization information for the RNN descriptor \p rnn_desc that
 * was previously created with the ::cnnlCreateRNNDescriptor function.
 *
 * @param[in,out] rnn_desc
 *  Input/output. The descriptor of the RNN operation. For detailed information,
 *   see ::cnnlRNNDescriptor_t.
 * @param[in] quant_hx_desc
 *   Input. The quantization information for input \p hx.
 * @param[in] quant_hy_desc
 *   Input. The quantization information for input \p hy.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetRNNDescriptorQuant(cnnlRNNDescriptor_t rnn_desc,
                          const cnnlQuantizeExDescriptor_t quant_hx_desc,
                          const cnnlQuantizeExDescriptor_t quant_hy_desc);

// Group:RNN
/*!
 * @brief Sets quantization information of \p quant_weight_desc for the
 * RNN descriptor \p rnn_desc that was previously created with
 * the ::cnnlCreateRNNDescriptor function.
 *
 * @param[in,out] rnn_desc
 *  Input/output. The descriptor of the RNN operation. For detailed information,
 *   see ::cnnlRNNDescriptor_t.
 * @param[in] quant_weight_desc
 *   Input. The quantization information for input \p filter.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetRNNDescriptorWeightQuant(cnnlRNNDescriptor_t rnn_desc,
                                const cnnlQuantizeExDescriptor_t quant_weight_desc);

// Group:RNN
/*!
 * @brief Retrieves the RNN operation information from \p rnn_desc that was previously created
 * with the ::cnnlCreateRNNDescriptor function and set by ::cnnlSetRNNDescriptor function.
 * The information includes the number of features in the hidden state \p hidden_size,
 * number of recurrent layers \p num_layers, how the first recurrent layer processes the input
 * \p input_mode, the recurrence pattern \p direction, RNN cell implementation \p mode.
 *
 * @param[in] rnn_desc
 *   Input. The descriptor of the RNN operation. For detailed information,
 *   see ::cnnlRNNDescriptor_t.
 * @param[out] hidden_size
 *   Output. The number of features in the hidden state input vector.
 * @param[out] num_layers
 *   Output. Number of stacked layers in the deep RNN mode.
 * @param[out] input_mode
 *   Output. Specify how the first stacked layer handles the input of the RNN mode.
 *   When the parameter \p input_mode is  ::CNNL_RNN_SKIP_INPUT, no matrix formation operation is
 *   performed on the original input vectors.
 * @param[out] direction
 *   Output. Determine recurrence pattern. In ::CNNL_RNN_BIDIRECTIONAL mode, the result of the
 *   current layer are concatenations of forward and backward hidden states in the feature dimension.
 * @param[out] mode
 *   Output. Determine the RNN cell implementation in mode(CNNL_RNN_RELU, CNNL_RNN_TANH, CNNL_LSTM, CNNL_LSTM_TANH).
 *   For detailed information, see ::cnnlRNNMode_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - Output pointer \p hidden_size, \p numLayers, \p inputMode, \p direction, \p mode should not be NULL.
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetRNNDescriptor(const cnnlRNNDescriptor_t rnn_desc,
                     int *hidden_size,
                     int *num_layers,
                     cnnlRNNInputMode_t *input_mode,
                     cnnlDirectionMode_t *direction,
                     cnnlRNNMode_t *mode);
// Group:RNN
/*!
 * @brief Sets bias number for the RNN descriptor \p rnn_desc that was previously created
 * with the ::cnnlCreateRNNDescriptor function.
 *
 * @param[out] rnn_desc
 *   Output. The descriptor of the RNN operation. For detailed information,
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[in] bias_mode
 *   Input. Specify bias number of the RNN operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor function.
 *
 * @note
 * - If this function is not called, the default bias mode is ::CNNL_RNN_DOUBLE_BIAS.
 *
 * @par Requirements
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetRNNBiasMode(cnnlRNNDescriptor_t rnn_desc,
                   const cnnlRNNBiasMode_t bias_mode);

// Group:RNN
/*!
 * @brief Gets bias number from the RNN descriptor \p rnn_desc that was previously created
 * with the ::cnnlCreateRNNDescriptor function.
 *
 * @param[in] rnn_desc
 *   Input. The previously initialized descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[out] bias_mode
 *   Output. Pointer to the host memory where the RNN bias mode should be saved.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor function.
 *
 * @note
 * - If ::cnnlSetRNNBiasMode function is not called, default bias mode is ::CNNL_RNN_DOUBLE_BIAS.
 *
 * @par Requirements
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetRNNBiasMode(const cnnlRNNDescriptor_t rnn_desc,
                   cnnlRNNBiasMode_t *bias_mode);

// Group:RNN
/*!
 * @brief Sets computing precision type for \p rnn_desc which is already created by ::cnnlCreateRNNDescriptor.
 *
 * @param[in] rnn_desc
 *   Input. The previously initialized descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[in] math_prec
 *   Input. The data type of computing precision used in RNN operation. Supported values are ::CNNL_DTYPE_HALF
 *   and ::CNNL_DTYPE_FLOAT.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor function.
 *
 * @note
 * - This function is only used for ::cnnlRNNForwardInference.
 * - This function only works when \p rnn_mode is CNNL_RNN_RELU, CNNL_RNN_TANH,
 *   CNNL_RNN_LSTM or CNNL_RNN_LSTM_TANH.
 *
 * @par Requirements
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetRNNMathPrec(cnnlRNNDescriptor_t rnn_desc,
                   const cnnlDataType_t math_prec);

// Group:RNN
/*!
 * @brief Gets computing precision type of \p rnn_desc set with ::cnnlSetRNNMathPrec.
 *
 * @param[in] rnn_desc
 *   Input. The previously initialized descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[out] math_prec
 *   Output. Pointer to the host memory of \p math_prec.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor and ::cnnlSetRNNMathPrec
 *   functions.
 *
 * @note
 * - If ::cnnlSetRNNMathPrec function is not called, the default computing precision type is uncertain.
 * - This function is only used for ::cnnlRNNForwardInference.
 *
 * @par Requirements
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetRNNMathPrec(const cnnlRNNDescriptor_t rnn_desc,
                   cnnlDataType_t* math_prec);

// Group:RNN
/*!
 * @brief Sets LSTM filter order for \p rnn_desc which is already created by ::cnnlCreateRNNDescriptor.
 *
 * @param[in] rnn_desc
 *   Input. The previously initialized descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[in] filter_order
 *   Input. The LSTM filter order used in RNN operation defined in ::cnnlRNNWeightOrder_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor function.
 *
 * @note
 * - If this function is not called, the default filter order is ::CNNL_LSTM_IFGO.
 * - This function is only used for ::cnnlRNNForwardInference.
 * - This function only works when \p rnn_mode is CNNL_RNN_LSTM or CNNL_RNN_LSTM_TANH.
 *
 * @par Requirements
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetRNNWeightOrder(cnnlRNNDescriptor_t rnn_desc,
                      const cnnlRNNWeightOrder_t filter_order);

// Group:RNN
/*!
 * @brief Gets LSTM filter order of \p rnn_desc set with ::cnnlSetRNNWeightOrder.
 *
 * @param[in] rnn_desc
 *   Input. The previously initialized descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[out] filter_order
 *   Output. The LSTM filter order used in RNN operation defined in ::cnnlRNNWeightOrder_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor and ::cnnlSetRNNWeightOrder
 *   functions.
 *
 * @note
 * - If ::cnnlSetRNNMathPrec function is not called, the default computing precision type is ::CNNL_LSTM_IFGO.
 * - This function is only used for ::cnnlRNNForwardInference.
 *
 * @par Requirements
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetRNNWeightOrder(cnnlRNNDescriptor_t rnn_desc,
                      cnnlRNNWeightOrder_t* filter_order);

// Group:RNN
/*!
 * @brief Sets LSTM/RNN computation preference mode for \p rnn_desc.
 *
 * @param[in] rnn_desc
 *   Input. The RNN operation descriptor created by ::cnnlCreateRNNDescriptor.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[in] comp_pref
 *   Input. The RNN computation preference mode defined in ::cnnlComputationPreference_t.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, you need to call ::cnnlCreateRNNDescriptor to create the RNN operation
 *   descriptor.
 *
 * @note
 * - If this function is not called, the default computation preference mode is ::CNNL_COMPUTATION_FAST.
 * - If computation preference mode is set to ::CNNL_COMPUTATION_HIGH_PRECISION, math_pre must be
 *   CNNL_DTYPE_FLOAT and MLU device cannot be MLU220/MLU270/MLU290.
 * - This function is only used for ::cnnlRNNForwardInference.
 * - This function only works when \p rnn_mode is CNNL_RNN_RELU, CNNL_RNN_TANH,
 *   CNNL_RNN_LSTM or CNNL_RNN_LSTM_TANH.
 *
 * @par Requirements
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetRNNComputationPreference(cnnlRNNDescriptor_t rnn_desc,
                                const cnnlComputationPreference_t comp_pref);

// Group:RNN
/*!
 * @brief Gets LSTM/RNN computation preference mode of \p rnn_desc set with ::cnnlSetRNNComputationPreference.
 *
 * @param[in] rnn_desc
 *   Input. The RNN operation descriptor created by ::cnnlCreateRNNDescriptor.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[out] comp_pref
 *   Output. Pointer to the memory that stores the LSTM/RNN computation preference mode defined in
 *   ::cnnlComputationPreference_t.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, you need to call ::cnnlCreateRNNDescriptor to create the RNN operation
 *   descriptor.
 *
 * @note
 * - If ::cnnlSetRNNComputationPreference function is not called, the default computation preference mode
 *   is ::CNNL_COMPUTATION_FAST.
 * - This function is only used for ::cnnlRNNForwardInference.
 *
 * @par Requirments
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetRNNComputationPreference(const cnnlRNNDescriptor_t rnn_desc,
                                cnnlComputationPreference_t* comp_pref);

// Group:RNN
/*!
 * @brief Sets RNN output mode for \p rnn_desc.
 *
 * @param[in] rnn_desc
 *   Input. The RNN operation descriptor created by ::cnnlCreateRNNDescriptor.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[in] output_mode
 *   Input. The RNN output mode defined in ::cnnlRNNOutputMode_t.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, you need to call ::cnnlCreateRNNDescriptor to create the RNN operation
 *   descriptor.
 *
 * @note
 * - If this function is not called, the default output mode is
 *   ::CNNL_RNN_NO_OUT_LAYER.
 * - This function is only used for ::cnnlRNNForwardInference.
 * - This function only works only if \p rnn_mode is CNNL_RNN_RELU or CNNL_RNN_TANH.
 *
 * @par Requirements
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetRNNOutputMode(cnnlRNNDescriptor_t rnn_desc,
                     const cnnlRNNOutputMode_t output_mode);

// Group:RNN
/*!
 * @brief Gets RNN output mode of \p rnn_desc set with ::cnnlSetRNNOutputMode.
 *
 * @param[in] rnn_desc
 *   Input. The RNN operation descriptor created by ::cnnlCreateRNNDescriptor.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[out] output_mode
 *   Output. Pointer to the memory that stores the RNN output mode defined in
 *   ::cnnlRNNOutputMode_t.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, you need to call ::cnnlCreateRNNDescriptor to create the RNN operation
 *   descriptor.
 *
 * @note
 * - If ::cnnlSetRNNOutputMode function is not called, the default output mode
 *   is CNNL_RNN_NO_OUT_LAYER.
 * - This function is only used for ::cnnlRNNForwardInference.
 *
 * @par Requirments
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetRNNOutputMode(cnnlRNNDescriptor_t rnn_desc,
                     cnnlRNNOutputMode_t* output_mode);

// Group:RNN
/*!
 * @brief Sets the size of the LSTM cell output after the projection layer
 *  for the RNN descriptor \p rnn_desc that was previously
 *  created with the ::cnnlCreateRNNDescriptor function.
 *
 * @param[out] rnn_desc
 *   Output. The descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[in] rec_proj_size
 *   Input. Specify recurrent projection layer size on the RNN operation.
 * @param[in] out_proj_size
 *   Input. Specify output projection layer size on the RNN operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor function.
 *
 * @note
 * - Currently, output projection is not supported.
 *
 * @par Requirements
 * - None.
 *
 * @par Reference
 * - None.
 *
 * @par Example
 * - See the Cambricon CNNL sample code in ``samples/rnn_sample/rnn_sample.cc``
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetRNNProjectionLayers(cnnlRNNDescriptor_t rnn_desc,
                           const int rec_proj_size,
                           const int out_proj_size);

// Group:RNN
/*!
 * @brief Gets projeciton layer parameters from the RNN descriptor \p rnn_desc
 * that was previously set by ::cnnlSetRNNProjectionLayers function.
 *
 * @param[in] rnn_desc
 *   Input. The previously initialized descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[out] rec_proj_size
 *   Output. Pointer to host memory where recurrent projection layer parameter should be saved.
 * @param[out] out_proj_size
 *   Output. Pointer to host memory where output projection layer parameter should be saved.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor function.
 *
 * @note
 * - If ::cnnlSetRNNProjectionLayers function is not call,
 *   \p *rec_proj_size and \p *out_proj_size is 0.
 *
 * @par Example
 * - See the Cambricon CNNL sample code in ``samples/rnn_sample/rnn_sample.cc``
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetRNNProjectionLayers(const cnnlRNNDescriptor_t rnn_desc,
                           int *rec_proj_size,
                           int *out_proj_size);

// Group:RNN
/*!
 * @brief Sets peephole connections with cell state for the RNN operation that was previously
 *  created with the ::cnnlCreateRNNDescriptor function.
 *
 * @param[out] rnn_desc
 *   Output. The descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[in] peephole_mode
 *   Input. Specify peephole connections for the RNN operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor function.
 *
 * @note
 * - If ::cnnlSetRNNPeepholeMode function not call,
 *   \p peephole_mode default is ::CNNL_LSTM_PEEPHOLE_DISABLED.
 * - Currently, ::CNNL_LSTM_PEEPHOLE_ENABLED must be used with projection layer.
 *
 * @par Requirements
 * - None.
 *
 * @par Reference
 * - None.
 *
 * @par Example
 * - See the Cambricon CNNL sample code in ``samples/rnn_sample/rnn_sample.cc``
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetRNNPeepholeMode(cnnlRNNDescriptor_t rnn_desc,
                       const cnnlRNNPeepholeMode_t peephole_mode);

// Group:RNN
/*!
 * @brief Gets peephole connection mode from the RNN descriptor \p rnn_desc
 * that was previously set by ::cnnlSetRNNPeepholeMode function.
 *
 * @param[in] rnn_desc
 *   Input. The previously initialized descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[out] peephole_mode
 *   Output. Pointer to host memory where peephole mode should be saved.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor function.
 *
 * @note
 * - If ::cnnlSetRNNPeepholeMode function is not called,
 *   \p *peephole_mode is ::CNNL_LSTM_PEEPHOLE_DISABLED.
 * - Currently, ::CNNL_LSTM_PEEPHOLE_ENABLED must be used with projection layer.
 *
 * @par Example
 * - See the Cambricon CNNL sample code in ``samples/rnn_sample/rnn_sample.cc``
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetRNNPeepholeMode(cnnlRNNDescriptor_t rnn_desc,
                       cnnlRNNPeepholeMode_t *peephole_mode);

// Group:RNN
/*!
 * @brief Sets mask mode for the RNN operation that was previously created with the
 *        ::cnnlCreateRNNDescriptor function.
 *
 * This function is used to enable mask operation.
 *
 * @param[out] rnn_desc
 *   Output. The descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[in] mask_mode
 *   Input. Specify mask operation for the RNN operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor function.
 *
 * @note
 * - If ::cnnlSetRNNPeepholeMode function not call,
 *   \p mask_mode default is ::CNNL_LSTM_MASK_DISABLED.
 *
 * @par Requirements
 * - None.
 *
 * @par Reference
 * - None.
 *
 * @par Example
 * - See the Cambricon CNNL sample code in ``samples/rnn_sample/rnn_sample.cc``
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetRNNMaskMode(cnnlRNNDescriptor_t rnn_desc,
                   const cnnlRNNMaskMode_t mask_mode);

// Group:RNN
/*!
 * @brief Gets mask mode from the RNN descriptor \p rnn_desc that was previously set by
 *        ::cnnlSetRNNMaskMode function.
 *
 * @param[in] rnn_desc
 *   Input. The previously initialized descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[out] mask_mode
 *   Output. Pointer to host memory where mask mode should be saved.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor function.
 *
 * @note
 * - If ::cnnlSetRNNMaskMode function is not called,
 *   \p *mask_mode is ::CNNL_LSTM_MASK_DISABLED.
 * - Currently, ::CNNL_LSTM_MASK_ENABLED must be used with projection layer.
 *
 * @par Example
 * - See the Cambricon CNNL sample code in ``samples/rnn_sample/rnn_sample.cc``
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetRNNMaskMode(const cnnlRNNDescriptor_t rnn_desc,
                   cnnlRNNMaskMode_t *mask_mode);

// Group:RNN
/*!
 * @brief Sets clip operation for the RNN operation that was previously
 *        created with the ::cnnlCreateRNNDescriptor function. When you want to constrain the cell
 *        state value range within a certain range, you can use this function to set related parameters.
 *
 *
 * @param[out] rnn_desc
 *   Output. The descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[in] clip_mode
 *   Input. Specify clip operation for the RNN operation.
 *   For detailed information, see ::cnnlRNNClipMode_t.
 * @param[in] clip_nan_opt
 *   Input. Specify the output data as NaN, the upper bounds, or lower bounds, when the input data is
 *   not in the [left_clip, right_clip].
 *   For detailed information, see ::cnnlNanPropagation_t.
 * @param[in] left_clip
 *   Input. Specify clip operation lower boundary.
 * @param[in] right_clip
 *   Input. Specify clip operation upper boundary.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor function.
 *
 * @note
 * The clip operation is not supported currently. So this function is not supported.
 *
 * @par Reference
 * - None.
 *
 * @par Example
 * - See the Cambricon CNNL sample code in ``samples/rnn_sample/rnn_sample.cc``
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetRNNClip(cnnlRNNDescriptor_t rnn_desc,
               const cnnlRNNClipMode_t clip_mode,
               const cnnlNanPropagation_t clip_nan_opt,
               const double left_clip,
               const double right_clip);

// Group:RNN
/*!
 * @brief Gets clip operation from the RNN descriptor \p rnn_desc
 * that was previously set by ::cnnlSetRNNClip function.
 *
 * @param[in] rnn_desc
 *   Input. The previously initialized descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[out] clip_mode
 *   Output. Pointer to host memory where clip mode should be saved.
 * @param[out] clip_nan_opt
 *   Output. Pointer to host memory where clip Nan propagation mode should be saved.
 *   For detailed information, see ::cnnlNanPropagation_t.
 * @param[out] left_clip
 *   Output. Pointer to host memory where clip lower boundary should be saved.
 * @param[out] right_clip
 *   Output. Pointer to host memory where clip upper boundary should be saved.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor function.
 *
 * @note
 * The clip operation is not supported currently. So this function is not supported.
 *
 * @par Example
 * - See the Cambricon CNNL sample code in ``samples/rnn_sample/rnn_sample.cc``
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetRNNClip(const cnnlRNNDescriptor_t rnn_desc,
               cnnlRNNClipMode_t *clip_mode,
               cnnlNanPropagation_t *clip_nan_opt,
               double *left_clip,
               double *right_clip);

// Group:RNN
/*!
 * @brief Sets padding mode of input and output for the RNN operation that was previously
 *  created with the ::cnnlCreateRNNDescriptor function.
 *
 * @param[out] rnn_desc
 *   Output. The descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[in] padding_mode
 *   Input. Specify padding mode for input and output.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor function.
 *
 * @note
 * - The padding mode is only supported for ::cnnlRNNForwardTraining.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - See the Cambricon CNNL sample code in ``samples/rnn_sample/rnn_sample.cc``
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetRNNPaddingMode(cnnlRNNDescriptor_t rnn_desc,
                      const cnnlRNNPaddingMode_t padding_mode);

// Group:RNN
/*!
 * @brief Retrieves padding mode from the RNN descriptor \p rnn_desc
 * that was previously set by ::cnnlSetRNNPaddingMode function.
 *
 * @param[in] rnn_desc
 *   Input. The previously initialized descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[out] padding_mode
 *   Output. Pointer to host memory where padding mode should be saved.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor function.
 *
 * @note
 * The padding mode is only supported for ::cnnlRNNForwardTraining.
 *
 * @par Example
 * - See the Cambricon CNNL sample code in ``samples/rnn_sample/rnn_sample.cc``
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetRNNPaddingMode(const cnnlRNNDescriptor_t rnn_desc,
                      cnnlRNNPaddingMode_t *padding_mode);

// Group:BceLoss
/*!
 * @brief Computes binary cross entropy with \p input tensor, \p target tensor,
 * \p filter tensor, and returns the results in the \p output tensor.
 *
 * If \p filter tensor need to be broadcasted, this function needs
 * extra host memory as the workspace to broadcast \p filter tensor.
 * You can get the workspace size with the ::cnnlGetBceLossWorkspaceSize
 * function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in ::cnnlBceLoss. For detailed information, see ::cnnlHandle_t.
 * @param[in] desc_input
 *   Input. The descriptor of input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] desc_target
 *   Input. The descriptor of target tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] target
 *   Input. Pointer to the MLU memory that stores the target tensor which is the target value of input.
 * @param[in] desc_filter
 *   Input. The descriptor of filter tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the filter tensor. The \p filter tensor is
 *   a filter of the binary cross entropy.
 * @param[in] reduction
 *   Input. An enum value that describing the reduction dimension, for detailed information,
 *   see ::cnnlBceLossReduction_t.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlBceLoss. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   ::cnnlBceLoss. You can get the size of the workspace with
 *   the ::cnnlGetBceLossWorkspaceSize function.
 * @param[out] desc_output
 *   Output. The descriptor of output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor. The \p output
 *   tensor is the result of ::cnnlBceLoss.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par Formula
 * - See "BceLoss Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for \p input tensor,
 *   \p target tensor, \p filter tensor, and \p output tensor.
 *   Note that the tensor type of all input and output tensors must be the same.
 *   bfloat16 supported only on MLU500 series.
 *   - input: half, float, bfloat16.
 *   - target: half, float, bfloat16.
 *   - filter: half, float, bfloat16.
 *   - output: half, float, bfloat16.
 *
 * @par Scale Limitation
 * - The precision cannot be ensured for large amount of data if the data types of input, target and filter are half.
 *   Because the maximum value of half is 65504.
 * - The range of \p input, \p target, \p filter should be small enough to prevent
 *   the result from data overflow.
 * - The value of \p input should be between [0, 1].
 * - For better accuracy, the numerical ranges for \p target and \p filter are recommended within [0, 1] and [-100, 100], respectively.
 *
 * @par API Dependency
 * - You need to get the extra space size by ::cnnlGetBceLossWorkspaceSize.
 *
 * @note
 * - The total number of dimensions of \p input tensor, \p target tensor, should be the same.
 * - If \p reduction is CNNL_BCE_LOSS_MEAN or CNNL_BCE_LOSS_SUM,
 *   then the \p output tensor should be a number. Otherwise, the dimension of \p output tensor and
 *   \p input tensor should be the same.
 * - For each dimension, the dimension length of the \p filter tensor need to meet the requirements of broadcasting.
 * - \p filter tensor are optional. When you do not need them, set them to NULL.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the ::cnnlBceLoss function is as follows:
     @verbatim
       input:  shape=(4) -> [0.5, 0.5, 0.5, 0.5],

       target:  shape=(4) -> [1.0, 1.0, 1.0, 1.0],

       filter: NULL,

       reduction: CNNL_BCE_LOSS_MEAN.

       Then we will get the output.

       output  -->: [0.6931].

   @endverbatim
 *
 *  @par Reference
 *  - https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html?highlight=bceloss#torch.nn.BCELoss.
 */
cnnlStatus_t CNNL_WIN_API
cnnlBceLoss(cnnlHandle_t handle,
            const cnnlTensorDescriptor_t desc_input,
            const void *input,
            const cnnlTensorDescriptor_t desc_target,
            const void *target,
            const cnnlTensorDescriptor_t desc_filter,
            const void *filter,
            const cnnlBceLossReduction_t reduction,
            void *workspace,
            size_t workspace_size,
            const cnnlTensorDescriptor_t desc_output,
            void *output);

// Group:BceLoss
/*!
 * @brief Returns in \p size the size of the host memory that is used as an extra workspace
 * in the ::cnnlBceLoss operation.
 *
 * The size of the extra workspace is based on the given information of ::cnnlBceLoss,
 * including the \p target tensor descriptor \p target_desc, \p filter tensor descriptor \p filter_desc.
 * For more information about the workspace,
 * see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in ::cnnlGetBceLossWorkspaceSize. For detailed information, see ::cnnlHandle_t.
 * @param[in] desc_input
 *   Input. The descriptor of the \p input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] desc_filter
 *   Input. The descriptor of \p filter tensor. The \p filter tensor is a filter
 *   of the binary cross entropy.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra MLU workspace in bytes that is used in
 *   the ::cnnlBceLoss operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetBceLossWorkspaceSize(cnnlHandle_t handle,
                            const cnnlTensorDescriptor_t desc_input,
                            const cnnlTensorDescriptor_t desc_filter,
                            size_t *size);
// Group:BceLossBackward
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace
 * in the ::cnnlBceLossBackward operation.
 *
 * The size of the extra workspace is based on the given information of ::cnnlBceLossBackward,
 * including the \p target tensor descriptor \p target_desc, \p filter tensor descriptor \p filter_desc.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the ::cnnlGetBceLossBackwardWorkspaceSize. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] target_desc
 *   Input. The descriptor of \p target tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] filter_desc
 *   Input. The descriptor of \p filter tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in
 *   the ::cnnlBceLossBackward operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlBceLossBackward function
 *   to broadcast the \p filter tensor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetBceLossBackwardWorkspaceSize(
                                                    cnnlHandle_t handle,
                                                    const cnnlTensorDescriptor_t target_desc,
                                                    const cnnlTensorDescriptor_t filter_desc,
                                                    size_t *size);
// Group:BceLossBackward
/*!
 *  @brief Computes the gradients of ::cnnlBceLoss with \p grad tensor, \p input tensor,
 *  \p target tensor, \p filter tensor, and returns the results in the \p output tensor.
 *
 *  If \p filter tensor needs to be broadcasted, this function needs extra MLU memory
 *  as the workspace to broadcast \p filter tensor. You can get the workspace size
 *  with the ::cnnlGetBceLossBackwardWorkspaceSize function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in ::cnnlBceLossBackward. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] grad_desc
 *   Input. The descriptor of grad tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] grad
 *    Input. Pointer to the MLU memory that stores the gradient tensor.
 * @param[in] input_desc
 *   Input. The descriptor of input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] target_desc
 *   Input. The descriptor of target tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] target
 *   Input. Pointer to the MLU memory that stores the target tensor. The \p target tensor
 *   has the same shape as \p input tensor.
 * @param[in] filter_desc
 *   Input. The descriptor of filter tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the filter tensor. The \p filter tensor is
 *   a filter of the result of binary cross entropy, and must match \p target tensor shape.
 * @param[in] reduction
 *   Input. An enum value describing the reduction dimension.
 *   See the ::cnnlBceLossReduction_t enum definition.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlBceLossBackward. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   ::cnnlBceLossBackward. You can get the size of the workspace with
 *   the ::cnnlGetBceLossBackwardWorkspaceSize function.
 * @param[out] output_desc
 *   Output. The descriptor of output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor. The \p output
 *   tensor is the gradients of ::cnnlBceLoss.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for
 *   \p grad tensor, \p input tensor, \p target tensor, \p filter tensor
 *   and \p output tensor.
 *   Note that the data type of these tensors must be the same.
 *   - grad tensor: half, float.
 *   - input tensor: half, float.
 *   - target tensor: half, float.
 *   - filter tensor: half, float.
 *   - output tensor: half, float.
 * - On MLU500 series or above, this function also supports bfloat16 data type.
 *
 * @par Scale Limitation
 * - If the data type of \p input tensor is half, the values of tensor should be
 *   greater than 1.6e-5 and less than 1 to prevent the data overflow.
 *
 * @par API Dependency
 * - You need to get the extra space size by ::cnnlGetBceLossBackwardWorkspaceSize.
 *
 * @note
 * - The number of dimensions of \p input tensor, \p target tensor, \p output tensor should be the same.
 * - If reduction is CNNL_BCE_WITH_LOGITS_MEAN or CNNL_BCE_WITH_LOGITS_SUM,
 *   the \p grad tensor should have one element. Otherwise, the number of dimensions of \p grad tensor and
 *   \p input tensor should be the same.
 * - The dimension values of the \p filter tensor and the \p target tensor need to meet the requirements of broadcasting.
 * - The \p filter tensor tensor is optional. When you do not need them, set them to NULL.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the ::cnnlBceLossBackward function is as follows:
     @verbatim
       grad:  [2.],

       input:  [0.1,0.2,0.3,0.4.]

       target: [1.,1.,1.,1.]

       filter: NULL,

       reduction: CNNL_BCE_WITH_LOGITS_MEAN.

       Then we will get the output.

       output  -->: [-4.999.,-2.500.,-1.666,-1.250].
     @endverbatim
 *
 * @par Reference
 * - None.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlBceLossBackward(cnnlHandle_t handle,
                    const cnnlTensorDescriptor_t grad_desc,
                    const void *grad,
                    const cnnlTensorDescriptor_t input_desc,
                    const void *input,
                    const cnnlTensorDescriptor_t target_desc,
                    const void *target,
                    const cnnlTensorDescriptor_t filter_desc,
                    const void *filter,
                    const cnnlBceLossReduction_t reduction,
                    void *workspace,
                    const size_t workspace_size,
                    const cnnlTensorDescriptor_t output_desc,
                    void *output);

// Group:KthValue
/*!
 * @brief Computes the k-th smallest element of each row of the input tensor in the given dimension \p dim.
 *
 * If there are more than one valid indices for the k-th smallest values, this function returns any one of the
 * indices that the corresponding value is k-th smallest value.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlKthValue_v2 instead, which is achieved by calling ::cnnlTopKTensor_v3 to support the condition
 * while the input \p input contains NaN or infinity value.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the k-th smallest
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_values_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output_values
 *   Output. Pointer to the MLU memory that stores tensor for the k-th smallest elements in the given dim.
 * @param[in] output_indices_desc
 *   Input. The descriptor of the index tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output_indices
 *   Output. Pointer to the MLU memory that stores the index tensor for the k-th smallest elements.
 * @param[in] k
 *   Input. An integer value that determines the k-th smallest element to get.
 * @param[in] dim
 *   Input. An integer value that determines the dimension of input tensor to get k-th smallest element.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH,
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "k-th_value Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \p input, output tensor
 *   \p output_values, and index tensor \p output_indices.
 *   Data type of input tensor and output tensor should be the same.
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *   - index tensor: int32.
 *
 * @par Scale Limitation
 * - The input tensor, output tensor, and index tensor must meet the following requirement:
 *   - The shape of \p output_values and \p output_indices should be the same.
 *   - The total size of \p input in dimension \p dim should be less than 167936 bytes.
 *   - There is no dimension limit for the input tensor, output tensor and index tensor.
 *   - The output shape should be [1] if the \p input is a one-dimensional tensor.
 *
 * @par Example
 * The example of kth_value is as follows:
   @verbatim
    input one array by 1*5 -->
        input: [1.0, 2.0, 3.0, 4.0, 5.0]

    param:

    k: [4], dim: [0],

    output array by 1*1 and 1*1:
        --> values: [4.0]

        --> indices: [3]
   @endverbatim
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.kthvalue.html
 */
CNNL_DEPRECATED_FOR(cnnlKthValue_v2)
cnnlStatus_t CNNL_WIN_API
cnnlKthValue(cnnlHandle_t handle,
             const cnnlTensorDescriptor_t input_desc,
             const void *input,
             const uint32_t k,
             const int dim,
             const cnnlTensorDescriptor_t output_values_desc,
             void *output_values,
             const cnnlTensorDescriptor_t output_indices_desc,
             void *output_indices);

// Group:KthValue
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace
 * in ::cnnlKthValue_v2 operation.
 *
 * The size of the extra workspace is based on the given information of ::cnnlKthValue_v2,
 * including the \p input tensor descriptor \p input_desc, the parameter \p k and the parameter \p dim.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in ::cnnlGetKthValueWorkspaceSize. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of \p input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] output_indices_desc
 *   Input. The descriptor of \p output_indices tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] k
 *   Input. An integer value that determines the k-th smallest element to get.
 * @param[in] dim
 *   Input. An integer value that determines the dimension of input tensor to get k-th smallest element.
 * @param[out] size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in
 *   ::cnnlKthValue_v2 operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to ::cnnlKthValue_v2 function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetKthValueWorkspaceSize(cnnlHandle_t handle,
                             const cnnlTensorDescriptor_t input_desc,
                             const cnnlTensorDescriptor_t output_indices_desc,
                             const uint64_t k,
                             const int32_t dim,
                             size_t *size);

// Group:KthValue
/*!
 * @brief Computes the k-th smallest element of each row of the input tensor in the given dimension \p dim.
 *
 * If there are more than one valid indices for the k-th smallest value, this function returns any one of the
 * indices whose corresponding value is the k-th smallest value.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the k-th smallest
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for
 *   ::cnnlKthValue_v2. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   ::cnnlKthValue_v2. You can get the size of the workspace with
 *   ::cnnlGetKthValueWorkspaceSize function.
 * @param[in] k
 *   Input. An integer value that determines the k-th smallest element to get.
 * @param[in] dim
 *   Input. An integer value that determines the dimension of input tensor to get k-th smallest element.
 * @param[in] output_values_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output_values
 *   Output. Pointer to the MLU memory that stores tensor for the k-th smallest elements in the given dim.
 * @param[in] output_indices_desc
 *   Input. The descriptor of the index tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output_indices
 *   Output. Pointer to the MLU memory that stores the index tensor for the k-th smallest elements.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH,
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "k-th_value Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \p input, output tensor
 *   \p output_values, and index tensor \p output_indices.
 *   <b>Data type of input tensor and output tensor should be the same.</b>
 *   - input tensor: half, float, bfloat16.
 *   - output tensor: half, float, bfloat16.
 *   - index tensor: int32, int64.
 *
 *   - The data type bfloat16 is only supported on MLU500 series.
 *
 * @par Scale Limitation
 * - The input tensor, output tensor, and index tensor must meet the following requirement:
 *   - The shape of \p output_values and \p output_indices should be the same.
 *   - The total size of \p input in dimension \p dim should be less than 167936 bytes.
 *   - There is no dimension limit for the input tensor, output tensor and index tensor.
 *   - The output shape should be [1] if the \p input is a one-dimensional tensor.
 *
 * @par Example
 * - The example of kth_value is as follows:
   @verbatim
    input one array by 1*5 -->
        input: [1.0, 2.0, 3.0, 4.0, 5.0]

    param:

    k: [4], dim: [0],

    output array by 1*1 and 1*1:
        --> values: [4.0]

        --> indices: [3]
   @endverbatim
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.kthvalue.html
 */
cnnlStatus_t CNNL_WIN_API
cnnlKthValue_v2(cnnlHandle_t handle,
                const uint64_t k,
                const int dim,
                const cnnlTensorDescriptor_t input_desc,
                const void *input,
                void *workspace,
                size_t workspace_size,
                const cnnlTensorDescriptor_t output_values_desc,
                void *output_values,
                const cnnlTensorDescriptor_t output_indices_desc,
                void *output_indices);

// Group:AdvancedIndex
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 *        workspace to optimize the advanced index operation.
 *
 * The size of the extra workspace is based on the given information of the advanced
 * index operation, including the input descriptor \p input_desc and indices descriptor
 * array \p indices_desc.
 * For more information about the workspace,
 * see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor for the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] indices_desc[]
 *   Input. An array of descriptor for the all indices tensors. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is
 *   used in ::cnnlAdvancedIndex.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - The array size of \p indices_desc should be equal to 8, in which unused elements
 *   should be empty.
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlAdvancedIndex function
 *   to perform the advanced index operation.
 *
 * @note
 * - If the data types and scale of input tensor and indices tensor described with \p input_desc and
 *   \p indices_desc do not match the requirements, ::CNNL_STATUS_BAD_PARAM would be returned when
 *   you call this function. For detailed information on data type and scale limitation,
 *   see ::cnnlAdvancedIndex.
 *
 * @par Requirements
 * - The input tensor \p input_desc and indices tensors \p indices_desc should all be smaller than
 *   2 GBytes.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetAdvancedIndexWorkspaceSize(
    cnnlHandle_t handle,
    const cnnlTensorDescriptor_t input_desc,
    const cnnlTensorDescriptor_t indices_desc[],
    size_t *workspace_size);

// Group:AdvancedIndex
/*!
 * @brief Returns the dimension number \p output_desc_dim and shape \p output_desc_dims of the
 * output descriptor of the advanced index operation with the given input descriptor \p input_desc
 * and an array of indices tensors \p indices_desc.
 *
 * The shape of the output tensor can be set in the output tensor descriptor based on
 * the return value of this function.
 *
 * If the tensor size is equal to or larger than 2 GBytes, use
 * ::cnnlGetAdvancedIndexOutputDim_v2.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlGetAdvancedIndexOutputDim_v2 instead, which supports
 * tensor size that is equal to or larger than 2 GBytes.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor for the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] indices_desc[]
 *   Input. An array of descriptor for the all indices tensors. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output_desc_dim
 *    Output. An integer number that stores the number of dimensions of the output tensor of the
 *    advanced index. The number is within range of [0, 8].
 * @param[out] output_desc_dims[]
 *    Output. An array that stores the shape of the output tensor of the advanced index
 *    operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - The array size of \p indices_desc should be equal to 8, in which unused elements
 *   should be empty.
 * - The array size of \p output_desc_dims should be equal to 8.
 *
 * @par API Dependency
 * - The output descriptor that is created according to \p output_desc_dim and \p output_desc_dims
 *   should be passed to the ::cnnlAdvancedIndex function to perform the advanced index operation.
 *
 * @note
 * - If the data types and scale of input tensor and indices tensor described with \p input_desc and
 *   \p indices_desc do not match the requirements, ::CNNL_STATUS_BAD_PARAM would be returned when
 *   you call this function. For detailed information on data type and scale limitation,
 *   see ::cnnlAdvancedIndex.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlGetAdvancedIndexOutputDim_v2)
cnnlStatus_t CNNL_WIN_API cnnlGetAdvancedIndexOutputDim(
    cnnlHandle_t handle,
    const cnnlTensorDescriptor_t input_desc,
    const cnnlTensorDescriptor_t indices_desc[],
    int *output_desc_dim,
    int output_desc_dims[]);

// Group:AdvancedIndex
/*!
 * @brief Returns the dimension number \p output_desc_dim and shape \p output_desc_dims of the
 * output descriptor of the advanced index operation with the given input descriptor \p input_desc
 * and an array of indices tensors \p indices_desc.
 *
 * The shape of the output tensor can be set in the output tensor descriptor based on
 * the return value of this function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor for the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] indices_desc[]
 *   Input. An array of descriptor for the all indices tensors. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output_desc_dim
 *    Output. An integer number that stores the number of dimensions of the output tensor of the
 *    advanced index. The number is within range of [0, 8].
 * @param[out] output_desc_dims[]
 *    Output. An array that stores the shape of the output tensor of the advanced index
 *    operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - The array size of \p indices_desc should be equal to 8, in which unused elements
 *   should be empty.
 * - The array size of \p output_desc_dims should be equal to 8.
 *
 * @par API Dependency
 * - The output descriptor that is created according to \p output_desc_dim and \p output_desc_dims
 *   should be passed to the ::cnnlAdvancedIndex_v2 function to perform the advanced index operation.
 *
 * @note
 * - If the data types and scale of input tensor and indices tensor described with \p input_desc and
 *   \p indices_desc do not match the requirements, ::CNNL_STATUS_BAD_PARAM would be returned when
 *   you call this function. For detailed information on data type and scale limitation,
 *   see ::cnnlAdvancedIndex_v2.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetAdvancedIndexOutputDim_v2(
    cnnlHandle_t handle,
    const cnnlTensorDescriptor_t input_desc,
    const cnnlTensorDescriptor_t indices_desc[],
    int *output_desc_dim,
    int64_t output_desc_dims[]);

// Group:AdvancedIndex
/*!
 * @brief Selects data from \p input according to index information in \p indices and stores the
 * result into \p output.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlAdvancedIndex_v2 instead, which supports int64 data type and
 * supports tensor size that is equal to or larger than 2 GBytes.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor for the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor \p input which to be indexed.
 * @param[in] indices_desc[]
 *   Input. An array of descriptors for the all indices tensors. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] indices[]
 *   Input. An array of pointers to the MLU memory that stores all indices tensors. Based on indices
 *   tensors, output data is selected from the input tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   advanced index operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the advanced index operation. The workspace size can be get by calling
 *   the ::cnnlGetAdvancedIndexWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. The output tensor dimension information can be got
 *   by calling the ::cnnlGetAdvancedIndexOutputDim function. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[out] output_dim
 *   Output. Pointer to the MLU memory that stores the total number of output dimensions. The data
 *   is int32. The pointer should not be a nullptr.
 * @param[out] output_dims
 *   Output. Pointer to the MLU memory that stores the tensor of output dimension shape. The data
 *   is int32. The pointer should not be a nullptr.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for the
 *   input tensor \p input and indices tensors \p indices.
 *   - input tensor: uint8, int8, bool, int16, int32, half, bfloat16, float.
 *   - indices tensors: int32, bool, uint8.
 *   - output tensor: data type is set the same as input in each case.
 * - When \p indices array contains multiple valid tensors, the data types of all the tensors should
 *   be the same. The valid tensor means the tensor element is not a nullptr.
 *
 * @par Scale Limitation
 * - The array size of \p indices_desc or \p indices should be equal to 8, in which unused elements
 *   should be empty.
 * - The dimension of \p input tensor or \p indices tensor cannot be larger than 8.
 * - The \p indices tensor should be zero elements if the corresponding dimension size of the
 *   \p input tensor is zero.
 * - The output dimension number that is calculated by \p input_desc and \p indices_desc should be
 *   no larger than 8, and it should be the same as the output dimension (dim) from \p output_desc.
 * - The output shape that is calculated by \p input_desc and \p indices_desc should be the same as
 *   the output shape (dims) from \p output_desc.
 * - The \p input tensor and \p indices tensors support stride feature, but the \p output tensor
 *   does not support stride feature.
 *
 * @par API Dependency
 * - Before calling this function to implement advanced index operation, all the parameters should
 *   be prepared to be passed to this function. See each parameter description for details.
 *
 * @note
 * - When the data type of \p indices is int32, this function supports tensor broadcasting as long as
 *   all valid tensors in \p indices satisfy the broadcast conditions. The valid tensor means the
 *   tensor element is not a nullptr.
 * - When the data type of \p indices is int32, \p Indices data can be minus numbers. The data
 *   range of n-th \p indices tensor is [- input_dim_n, input_dim_n - 1], where input_dim_n is
 *   the size of the n-th \p input dimension.
 * - When the data type of \p indices is uint8, \p Indices data value can only be either 0 or 1.
 * - When the data type of \p indices is uint8 or bool, the number of valid tensors in \p indices can
 *   only be 1, and the dimension number of the valid \p indices tensor should be no larger than the
 *   dimension number of the \p input.
 * - When the data type of \p indices is uint8 or bool, the shape of valid tensors in \p indices
 *   should be the same as the corresponding shape of the \p input.
 * - The valid index descriptors in \p indices_desc should be set to the first input_dim
 *   elements of \p indices_desc, where input_dim is the dimension number of the \p input.
 * - If the element in \p indices_desc is nullptr, the corresponding element in \p indices should be
 *   nullptr. If the element in \p indices_desc is a valid pointer and the tensor shape is not zero,
 *   the corresponding element in \p indices should be a valid pointer. If the element in
 *   \p indices_desc is a valid pointer but the tensor shape is zero, the corresponding element in
 *   \p indices should be nullptr.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlAdvancedIndex_v2)
cnnlStatus_t CNNL_WIN_API cnnlAdvancedIndex(
    cnnlHandle_t handle,
    const cnnlTensorDescriptor_t input_desc,
    const void *input,
    const cnnlTensorDescriptor_t indices_desc[],
    const void *const indices[],
    void *workspace,
    size_t workspace_size,
    const cnnlTensorDescriptor_t output_desc,
    void *output,
    void *output_dim,
    void *output_dims);

// Group:AdvancedIndex
/*!
 * @brief Selects data from \p input according to index information in \p indices and stores the
 * result into \p output.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor for the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor \p input which to be indexed.
 * @param[in] indices_desc[]
 *   Input. An array of descriptors for the all indices tensors. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] indices[]
 *   Input. An array of pointers to the MLU memory that stores all indices tensors. Based on indices
 *   tensors, output data is selected from the input tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   advanced index operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the advanced index operation. The workspace size can be get by calling
 *   the ::cnnlGetAdvancedIndexWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor \p output. The output tensor dimension information can be got
 *   by calling the ::cnnlGetAdvancedIndexOutputDim_v2 function. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] output_dim_desc
 *   Input. The descriptor of the output dim tensor \p output_dim. The output dim tensor shape
 *   is fixed as [1]. For detailed information, see ::cnnlTensorDescriptor_t. This tensor can be set to
 *   NULL if the data type of \p indices is int32.
 * @param[out] output_dim
 *   Output. Pointer to the MLU memory that stores the total number of output dimensions. This pointer should
 *   be set to NULL if and only if \p output_dim_desc is NULL.
 * @param[in] output_dims_desc
 *   Input. The descriptor of the output dims tensor \p output_dims. The output dims tensor shape
 *   is fixed as [8]. For detailed information, see ::cnnlTensorDescriptor_t. This tensor should be set to
 *   NULL if and only if \p output_dim_desc is NULL.
 * @param[out] output_dims
 *   Output. Pointer to the MLU memory that stores the tensor of output dimension shape. This pointer should
 *   be set to NULL if and only if \p output_dims_desc is NULL.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for the
 *   input tensor \p input and indices tensors \p indices.
 *   - input tensor: uint8, int8, bool, int16, int32, int64, half, bfloat16, float.
 *   - indices tensors: int32, int64, bool, uint8.
 *   - output tensor: data type is set the same as input in each case.
 *   - output dim tensor: int32.
 *   - output dims tensor: int32, int64.
 * - When \p indices array contains multiple valid tensors, the data types of all the tensors should
 *   be the same. The valid tensor means the tensor element is not a nullptr.
 *
 * @par Scale Limitation
 * - The array size of \p indices_desc or \p indices should be equal to the dim size of \p input,
 *   in which unused elements should be empty.
 * - The dimension of \p input tensor or \p indices tensor cannot be larger than 8.
 * - The \p indices tensor should be zero elements if the corresponding dimension size of the
 *   \p input tensor is zero.
 * - The output dimension number that is calculated by \p input_desc and \p indices_desc should be
 *   no larger than 8, and it should be the same as the output dimension (dim) from \p output_desc.
 * - The output shape that is calculated by \p input_desc and \p indices_desc should be the same as
 *   the output shape (dims) from \p output_desc.
 * - The \p input tensor and \p indices tensors support stride feature, but the \p output tensor
 *   does not support stride feature.
 *
 * @par API Dependency
 * - Before calling this function to implement advanced index operation, all the parameters should
 *   be prepared to be passed to this function. See each parameter description for details.
 *
 * @note
 * - When the data type of \p indices is int32, this function supports tensor broadcasting as long as
 *   all valid tensors in \p indices satisfy the broadcast conditions. The valid tensor means the
 *   tensor element is not a nullptr.
 * - When the data type of \p indices is int32, \p Indices data can be minus numbers. The data
 *   range of n-th \p indices tensor is [- input_dim_n, input_dim_n - 1], where input_dim_n is
 *   the size of the n-th \p input dimension.
 * - When the data type of \p indices is uint8, \p Indices data value can only be either 0 or 1.
 * - When the data type of \p indices is uint8 or bool, the number of valid tensors in \p indices can
 *   only be 1, and the dimension number of the valid \p indices tensor should be no larger than the
 *   dimension number of the \p input.
 * - When the data type of \p indices is uint8 or bool, the shape of valid tensors in \p indices
 *   should be the same as the corresponding shape of the \p input.
 * - The valid index descriptors in \p indices_desc should be set to the first input_dim
 *   elements of \p indices_desc, where input_dim is the dimension number of the \p input.
 * - If the element in \p indices_desc is nullptr, the corresponding element in \p indices should be
 *   nullptr. If the element in \p indices_desc is a valid pointer and the tensor shape is not zero,
 *   the corresponding element in \p indices should be a valid pointer. If the element in
 *   \p indices_desc is a valid pointer but the tensor shape is zero, the corresponding element in
 *   \p indices should be nullptr.
 * - If the data type of \p output_dims is int32, this function only supports tensors whose sizes
 *   are smaller than 2 GBytes.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlAdvancedIndex_v2(cnnlHandle_t handle,
                                               const cnnlTensorDescriptor_t input_desc,
                                               const void *input,
                                               const cnnlTensorDescriptor_t *indices_desc,
                                               const void *const *indices,
                                               void *workspace,
                                               size_t workspace_size,
                                               const cnnlTensorDescriptor_t output_desc,
                                               void *output,
                                               const cnnlTensorDescriptor_t output_dim_desc,
                                               void *output_dim,
                                               const cnnlTensorDescriptor_t output_dims_desc,
                                               void *output_dims);

// Group:RoiAlignBackward
/*!
 * @brief Computes the gradients of images \p grads_image based on the gradients \p grads and
 * bounding boxes \p boxes to perform the backpropagation of ::cnnlRoiAlign operation. To use
 * maximum pooling mode or average pooling mode in this operation, call ::cnnlRoiAlignBackward_v2.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlRoiAlignBackward_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   ::cnnlRoiAlignBackward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] grads_desc
 *   Input. The descriptor of the gradient tensor in the backpropagation process. For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @param[in] grads
 *   Input. Pointer to the MLU memory that stores the gradient tensor.
 * @param[in] boxes_desc
 *   Input. The descriptor of the bounding boxes tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] boxes
 *   Input. Pointer to the MLU memory that stores the bounding boxes tensor.
 * @param[in] spatial_scale
 *   Input. A scaling factor that specifies how to map the box coordinates in the origin image to
 *   the coordinates in the output.
 * @param[in] sampling_ratio
 *   Input. The number of sampling points in the grid used to compute the output.
 * @param[in] aligned
 *   Input. A Boolean value that determines whether to shift the boxes by 0.5 pixel.
 * @param[in] grads_image_desc
 *   Input. The descriptor of the gradients tensor of the original images.
 * @param[out] grads_image
 *   Output. Pointer to the MLU memory that stores the gradients tensor of the original images.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH.
 *
 * @par Formula
 * - See "RoiAlignBackward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for gradient tensor \p grads, boxes tensor
 *   \p boxes, and output tensor \p grads_image. Data type of all tensors should be the same.
 *   - gradient tensor: half, float.
 *   - boxes tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layouts of \p grads, \p boxes, \p grads_images are as follows:
 *   - grads tensor: \p CNNL_LAYOUT_NHWC.
 *   - boxes tensor: \p CNNL_LAYOUT_ARRAY, only supports 2D tensor.
 *   - grads_image tensor: \p CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - The gradient tensor and output tensor must have four dimensions.
 * - Size of the fourth dimension of gradient tensor and output tensor must be the same.
 * - The bounding boxes tensor \p boxes must have two dimensions.
 * - Size of the first dimension of gradient tensor and bounding boxes tensor must be the same.
 * - The shape of \p boxes should be [boxes_num, 5].
 * - \p boxes[i] consists of [image_id, x1, y1, x2, y2]. \p image_id specifies which image this box
 *   is in, and should be in range of [0, batch_num - 1]. \p x1 and \p y1 specify the start
 *   coordinate of this box in origin image. \p x2 and \p y2 specify the end coordinate of this box
 *   in origin image. \p x1 and \p y1 should be greater than or equal to 0. \p x2 should be greater
 *   than \p x1. \p y2 should be greater than \p y1.
 * - \p spatial_scale should be in range of (0, 1].
 *
 * @par API Dependency
 * - None.
 *
 * @par Performance Optimization
 * - None.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the roi_align_backward operation is as follows:
     @verbatim
     input two arrays by 1 * 1 * 1 * 1 and 1 * 5 --> grads: [[[[1.0]]]]

     --> boxes: [[0.0, 0.0, 0.0, 1.0, 1.0]]

     param:
         spatial_scale: 1.0, sampling_ratio: 2, aligned: false

     output array by 1 * 2 * 2 * 1 -->
         output: [[[[0.25]], [[0.25]]], [[[0.25]], [[0.25]]]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/vision/stable/ops.html#torchvision.ops.roi_align
 */
CNNL_DEPRECATED_FOR(cnnlRoiAlignBackward_v2)
cnnlStatus_t CNNL_WIN_API cnnlRoiAlignBackward(cnnlHandle_t handle,
                                               const float spatial_scale,
                                               const int sampling_ratio,
                                               const bool aligned,
                                               const cnnlTensorDescriptor_t grads_desc,
                                               const void *grads,
                                               const cnnlTensorDescriptor_t boxes_desc,
                                               const void *boxes,
                                               const cnnlTensorDescriptor_t grads_image_desc,
                                               void *grads_image);
// Group:RoiAlignBackward
/*!
 * @brief Computes the gradients of images \p grads_image based on the gradients \p grads,
 * bounding boxes \p boxes, the coordinate of x axis \p argmax_x and the coordinate of y axis
 * \p argmax_y to perform this operation. Compared with ::cnnlRoiAlignBackward, in addition to
 * supporting average pooling mode, ::cnnlRoiAlignBackward_v2 also supports maximum pooling mode
 * defined in \p pool_mode with two more inputs \p argmax_x and \p argmax_y.
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   ::cnnlRoiAlignBackward_v2 operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] grads_desc
 *   Input. The descriptor of the gradient tensor in the backpropagation process. For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @param[in] grads
 *   Input. Pointer to the MLU memory that stores the gradient tensor.
 * @param[in] boxes_desc
 *   Input. The descriptor of the bounding boxes tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] boxes
 *   Input. Pointer to the MLU memory that stores the bounding boxes tensor.
 * @param[in] argmax_x_desc
 *   Input. The descriptor of the \p argmax_x tensor that stores the coordinate of x axis. For detailed
 * information, see ::cnnlTensorDescriptor_t.
 * @param[in] argmax_x
 *   Input. Pointer to the MLU memory that stores the \p argmax_x tensor. \p argmax_x represents
 *   \p output coordinate of x axis returned by ::cnnlRoiAlign_v2 when \p pool_mode is maximum
 *   pooling mode. When \p pool_mode is average pooling mode, \p argmax_x is NULL.
 * @param[in] argmax_y_desc
 *   Input. The descriptor of the \p argmax_y tensor that stores the coordinate of y axis. For detailed
 * information, see ::cnnlTensorDescriptor_t.
 * @param[in] argmax_y
 *   Input. Pointer to the MLU memory that stores the \p argmax_y tensor. \p argmax_y represents
 *   \p output coordinate of y axis returned by ::cnnlRoiAlign_v2 when \p pool_mode is maximum
 *   pooling mode. When \p pool_mode is average pooling mode, \p argmax_y is NULL.
 * @param[in] spatial_scale
 *   Input. A scaling factor that specifies how to map the box coordinates in the original image to
 *   the coordinates in the output.
 * @param[in] sampling_ratio
 *   Input. The number of sampling points in the grid used to compute the output.
 * @param[in] aligned
 *   Input. A Boolean value that determines whether to shift the boxes by 0.5 pixel. If the value
 *   of \p aligned is set to true, the boxes are shifted by 0.5. If the value of \p aligned is set
 *   to false, the boxes are not shifted.
 * @param[in] pool_mode
 *   Input. The pooling mode which determines to use maximum pooling mode or average
 *   pooling mode. If the value of \p pool_mode is set to 1, the average pooling mode is used. If
 *   the value of \p pool_mode is set to 0, the maximum pooling mode is used.
 * @param[in] grads_image_desc
 *   Input. The descriptor of the gradients tensor of the original images. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] grads_image
 *   Output. Pointer to the MLU memory that stores the \p grads_image tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH.
 *
 * @par Formula
 * - See "RoiAlignBackward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for gradient tensor \p grads, boxes tensor
 *   \p boxes, argmax_x tensor \p argmax_x, argmax_y tensor \p argmax_y and output tensor \p
 *   grads_image. Data type of all tensors should be the same.
 *   - gradient tensor: half, float.
 *   - boxes tensor: half, float.
 *   - argmax_x tensor: half, float.
 *   - argmax_y tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layouts of gradient tensor \p grads, boxes tensor \p boxes, argmax_x tensor
 *   \p argmax_x, argmax_y tensor \p argmax_y and output tensor \p grads_images are as follows:
 *   - grads tensor: \p CNNL_LAYOUT_NHWC.
 *   - boxes tensor: \p CNNL_LAYOUT_ARRAY, only supports 2D tensor.
 *   - argmax_x tensor: \p CNNL_LAYOUT_NHWC.
 *   - argmax_y tensor: \p CNNL_LAYOUT_NHWC.
 *   - grads_image tensor: \p CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - The gradient tensor \p grads, argmax_x tensor \p argmax_x, argmax_y tensor \p argmax_y and
 *   output tensor \p grads_images must have four dimensions.
 * - Size of the fourth dimension of gradient tensor \p grads, argmax_x tensor \p argmax_x,
 *   argmax_y tensor \p argmax_y and output tensor \p grads_images must be the same.
 * - The bounding boxes tensor \p boxes must have two dimensions.
 * - Size of the first dimension of gradient tensor \p grads, argmax_x tensor \p argmax_x, argmax_y
 *   tensor \p argmax_y and bounding boxes tensor \p boxes must be the same.
 * - Size of each dimension of gradient tensor \p grads, argmax_x tensor \p argmax_x and argmax_y
 *   tensor \p argmax_y must be the same.
 * - The shape of \p boxes should be [boxes_num, 5].
 * - \p boxes[i] consists of [image_id, x1, y1, x2, y2]. \p image_id specifies which image this box
 *   is in, and should be in range of [0, batch_num - 1]. \p x1 and \p y1 specify the starting
 *   coordinate of this box in origin image. \p x2 and \p y2 specify the ending coordinate of this box
 *   in origin image. \p x1 and \p y1 should be greater than or equal to 0. \p x2 should be greater
 *   than \p x1. \p y2 should be greater than \p y1.
 *
 * @par API Dependency
 * - This function should be used with ::cnnlRoiAlign_v2.
 *
 * @par Performance Optimization
 * - None.
 *
 * @note
 * - Set the values of \p argmax_x and \p argmax_y according to the result returned by
 *   ::cnnlRoiAlign_v2.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the RoiAlignBackward_v2 operation is as follows:
     @verbatim
     input four arrays by 1 * 1 * 1 * 1, 1 * 5, 1 * 1 * 1 * 1 and 1 * 1 *1 *1--> grads: [[[[1.0]]]]

     --> boxes: [[0.0, 0.0, 0.0, 1.0, 1.0]]

     --> argmax_x:[[[[0.5]]]]

     --> argmax_y:[[[[0.5]]]]

     param:
         spatial_scale: 1.0, sampling_ratio: 0, aligned: false

     output array by 1 * 1 * 1 * 1 -->
         output: [[[[1.0]]]]
     @endverbatim
 *
 * @par Reference
 * - http://github.com/open-mmlab/mmcv/blob/master/mmcv/ops/csrc/pytorch/roi_align_cuda.cu
 */

cnnlStatus_t CNNL_WIN_API cnnlRoiAlignBackward_v2(cnnlHandle_t handle,
                                                  const cnnlTensorDescriptor_t grads_desc,
                                                  const void *grads,
                                                  const cnnlTensorDescriptor_t boxes_desc,
                                                  const void *boxes,
                                                  const cnnlTensorDescriptor_t argmax_x_desc,
                                                  const void *argmax_x,
                                                  const cnnlTensorDescriptor_t argmax_y_desc,
                                                  const void *argmax_y,
                                                  const float spatial_scale,
                                                  const int sampling_ratio,
                                                  const bool aligned,
                                                  const int pool_mode,
                                                  const cnnlTensorDescriptor_t grads_image_desc,
                                                  void *grads_image);


/*!
 * The descriptor of roialign_function that holds the information required in the
 * roialign operation. You need to call ::cnnlCreateRoiAlignDescriptor to create
 * a descriptor, and call ::cnnlSetRoiAlignDescriptor or ::cnnlSetRoiAlignDescriptor_v2
 * to set the basic information of the roialign operation to the descriptor. Also, you need to
 * destroy the descriptor at the end with the ::cnnlDestroyRoiAlignDescriptor function.
 */
typedef struct cnnlRoiAlignStruct *cnnlRoiAlignDescriptor_t;

// Group:RoiAlign
/*!
 * @brief Creates a descriptor pointed by \p desc for a roialign operation, and allocates
 *        memory for holding the information about the roialign operation. The information
 *        is defined in ::cnnlRoiAlignDescriptor_t. For more information about descriptor,
 *        see "Cambricon CNNL User Guide".
 *
 * @param[out] desc
 *  Output. A host pointer to the roialign descriptor that holds information about the roialign
 *  operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetRoiAlignDescriptor function or
 * ::cnnlSetRoiAlignDescriptor_v2 function to initialize and set the information to the roialign descriptor.
 * - You need to call the ::cnnlDestroyRoiAlignDescriptor function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */

cnnlStatus_t CNNL_WIN_API
cnnlCreateRoiAlignDescriptor(cnnlRoiAlignDescriptor_t *desc);

// Group:RoiAlign
/*!
 * @brief Initializes the roialign descriptor \p desc that was previously created with
 * the ::cnnlCreateRoiAlignDescriptor function, and sets the information about the
 * roialign to the roialign descriptor \p desc. The information includes the number
 * of the roialign feature map height \p pooled_height, the roialign feature map width
 * \p pooled_width, the sampling_ratio for each boxes \p sampling_ratio, the of the
 * spatial_scale for each boxes \p spatial_scale, the spatial_scale mode \p aligned.
 *
 * @deprecated
 * This function is deprecated and will be removed in future
 *   release. Use ::cnnlSetRoiAlignDescriptor_v2 instead, which
 *   supports both maximum and average modes.
 *
 * @param[in,out] desc
 *   Input/output. The descriptor of the roialign operation. For detailed information,
 *   see ::cnnlRoiAlignDescriptor_t.
 * @param[in] pooled_height
 *   Input. The height of output feature map.
 * @param[in] pooled_width
 *   Input. The width of output feature map.
 * @param[in] sampling_ratio
 *   Input. The number of sampling points in the grid used to compute the output.
 * @param[in] spatial_scale
 *   Input. The spatial scale of each regions of interest in the output.
 * @param[in] aligned
 *   Input. A Boolean value that determines whether to shift the boxes by 0.5 pixel.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */

CNNL_DEPRECATED_FOR(cnnlSetRoiAlignDescriptor_v2)
cnnlStatus_t CNNL_WIN_API
cnnlSetRoiAlignDescriptor(cnnlRoiAlignDescriptor_t desc,
                          const int pooled_height,
                          const int pooled_width,
                          const int sampling_ratio,
                          const float spatial_scale,
                          const bool aligned);
// Group:RoiAlign
/*!
 * @brief Initializes the roialign descriptor \p roialign_desc that was previously created with
 * the ::cnnlCreateRoiAlignDescriptor function, and sets the information about the roialign to the
 * roialign descriptor \p roialign_desc. Compared with ::cnnlSetRoiAlignDescriptor, this function
 * supports both maximum and average modes defined in \p pool_mode. The information includes the height
 * \p pooled_height, and width \p pooled_width of output feature map, the pooling mode \p pool_mode
 * for the roialign operation, the sampling ratio \p sampling_ratio, the spatial scale
 * \p spatial_scale and the flag of pixel shift \p aligned for each box.
 *
 * @param[in,out] roialign_desc
 *   Input/output. The descriptor of the roialign operation. For detailed information,
 *   see ::cnnlRoiAlignDescriptor_t.
 * @param[in] pooled_height
 *   Input. The height of output feature map. The value of this parameter should be greater than 0.
 * @param[in] pooled_width
 *   Input. The width of output feature map. The value of this parameter should be greater than 0.
 * @param[in] sampling_ratio
 *   Input. The number of sampling points in the grid used to compute the output.
 * @param[in] spatial_scale
 *   Input. The spatial scale of each regions of interest in the output.
 * @param[in] pool_mode
 *   Input. If the value of \p pool_mode is set to 1, the average pooling mode is used. If the value
 * of \p pool_mode is set to 0, the maximum pooling mode is used.
 * @param[in] aligned
 *   Input. A Boolean value that determines whether to shift the boxes by 0.5 pixel. If the value of \p aligned
 * is set to true, the boxes is shifted by 0.5. If the value of \p aligned is set to false, the boxes is not shifted.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetRoiAlignDescriptor_v2(cnnlRoiAlignDescriptor_t roialign_desc,
                             const int pooled_height,
                             const int pooled_width,
                             const int sampling_ratio,
                             const float spatial_scale,
                             const int pool_mode,
                             const bool aligned);

// Group:RoiAlign
/*!
 * @brief Destroys a roialign descriptor \p desc that was previously created with the
 *        ::cnnlCreateRoiAlignDescriptor function.
 *
 * The roialign descriptor is defined in ::cnnlRoiAlignDescriptor_t
 * and holds the information about the roialign operation.
 *
 * @param[in] desc
 *   Input. The roialign descriptor to be destroyed.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - You need to call this function after calling the ::cnnlRoiAlign,
 *   Otherwise, ::CNNL_STATUS_BAD_PARAM is returned.
 * - This function should be called to destroy the roialign descriptor.
 *   Otherwise, the memory leak may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */

cnnlStatus_t CNNL_WIN_API
cnnlDestroyRoiAlignDescriptor(cnnlRoiAlignDescriptor_t desc);

// Group:RoiAlign
/*!
 * @brief Computes the output of images \p output_image based on the input \p input_tensor and
 * bounding boxes \p boxes to perform ::cnnlRoiAlign operation. To use maximum pooling mode
 * or average pooling mode in this operation, call ::cnnlRoiAlign_v2.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlRoiAlign_v2 instead, which supports both maximum and average modes.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   ::cnnlRoiAlign operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] roialign_desc
 *   Input. The descriptor of the roialign operation. For detailed information, see
 *   ::cnnlRoiAlignDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor in the roialign process. For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] boxes_desc
 *   Input. Descriptor of boxes, containing dimension and the layout of boxes.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] boxes
 *   Input. Pointer to the MLU memory that stores the boxes tensor.
 * @param[out] output_desc
 *   Input.  Descriptor of output, containing dimension and the layout of output.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "RoiAlignForward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \p input, boxes tensor
 *   \p boxes, and output tensor \p output tensor. Data type of all tensors should be the same.
 *   - input tensor: half, float.
 *   - boxes tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layouts of \p input, \p boxes, \p output are as follows:
 *   - input tensor: \p CNNL_LAYOUT_NHWC.
 *   - boxes tensor: \p CNNL_LAYOUT_ARRAY, only supports 2D tensor.
 *   - output tensor: \p CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor must have four dimensions.
 * - data type of half is not recommended due to low precision.
 * - Size of the lowest dimension of input tensor and output tensor must be the same.
 * - The boxes tensor must have two dimensions.
 * - Size of the highest dimension of output tensor and boxes tensor must be the same.
 * - The shape of \p boxes should be [boxes_num, 5].
 * - \p boxes[i] consists of [batch_id, x1, y1, x2, y2]. \p batch_id should be in range of
 *   [0, batch_num - 1]. \p x1 and \p y1 should be greater than or equal to 0. \p x2 should be
 *   greater than \p x1. \p y2 should be greater than \p y1.
 *
 * @par API Dependency
 * - None.
 *
 * @par Performance Optimization
 * - None.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the roialign_forward operation is as follows:
     @verbatim
     input two arrays by 1 * 1 * 1 * 1 and 1 * 5 --> input: [[[[1.0]]]]

     --> boxes: [[1.0, 0.0, 0.0, 1.0, 1.0]]

     param:
            pooled_height: 1.0, pooled_width: 1.0, spatial_scale: 1.0,
            sampling_ratio: 2, aligned: false

     output array by 1 * 1 * 1 * 1 -->
         output: [[[[1]]]]
     @endverbatim
 *
 *
 * @par Reference
 * - https://pytorch.org/vision/stable/ops.html#torchvision.ops.roi_align
 */
CNNL_DEPRECATED_FOR(cnnlRoiAlign_v2)
cnnlStatus_t CNNL_WIN_API
cnnlRoiAlign(cnnlHandle_t handle,
             const cnnlRoiAlignDescriptor_t roialign_desc,
             const cnnlTensorDescriptor_t input_desc,
             const void *input,
             const cnnlTensorDescriptor_t boxes_desc,
             const void *boxes,
             const cnnlTensorDescriptor_t output_desc,
             void *output);
// Group:RoiAlign
/*!
 * @brief Computes the output feature map \p output based on the input feature map \p input and
 * bounding boxes \p boxes to perform this operation. Compared with ::cnnlRoiAlign, in addition to
 * supporting average pooling mode, ::cnnlRoiAlign_v2 also supports maximum pooling mode with two
 * more output \p argmax_x and \p argmax_y.
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   ::cnnlRoiAlign_v2 operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] roialign_desc
 *   Input. The descriptor of the roialign operation. For detailed information, see
 *   ::cnnlRoiAlignDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor in the roialign process. For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] boxes_desc
 *   Input. The descriptor of the region proposals tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] boxes
 *   Input. Pointer to the MLU memory that stores the region proposals tensor.
 * @param[in] output_desc
 *   Input.  The descriptor of the \p output tensor of the original images.  For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the \p output tensor.
 * @param[in] argmax_x_desc
 *   Input.  The descriptor of the \p argmax_x tensor that stores the coordinate of x axis. For detailed
 * information, see ::cnnlTensorDescriptor_t.
 * @param[out] argmax_x
 *   Output. Pointer to the MLU memory that stores the \p argmax_x tensor. \p argmax_x represents
 * \p output coordinate of x axis when \p pool_mode is maximum pooling mode. When \p pool_mode is average pooling mode, \p argmax_x is NULL.
 * @param[in] argmax_y_desc
 *   Input.  The descriptor of the \p argmax_y tensor that stores the coordinate of y axis. For detailed
 * information, see ::cnnlTensorDescriptor_t.
 * @param[out] argmax_y
 *   Output. Pointer to the MLU memory that stores the \p argmax_y tensor. \p argmax_y represents \p output
 * coordinate of y axis when \p pool_mode is maximum pooling mode. When \p pool_mode is average pooling mode, \p argmax_y is NULL.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "RoiAlignForward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \p input, boxes tensor
 *   \p boxes, output tensor \p output tensor, argmax_x tensor \p argmax_x and argmax_y tensor
 *   \p argmax_y. Data type of all tensors should be the same.
 *   - input tensor: half, float.
 *   - boxes tensor: half, float.
 *   - output tensor: half, float.
 *   - argmax_x tensor: half, float.
 *   - argmax_y tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layouts of \p input, \p boxes, \p output, \p argmax_x and \p argmax_y are as
 *   follows:
 *   - input tensor: \p CNNL_LAYOUT_NHWC.
 *   - boxes tensor: \p CNNL_LAYOUT_ARRAY, only supports 2D tensor.
 *   - output tensor: \p CNNL_LAYOUT_NHWC.
 *   - argmax_x tensor: \p CNNL_LAYOUT_NHWC.
 *   - argmax_y tensor: \p CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - The \p input tensor, \p output tensor, \p argmax_x tensor and \p argmax_y tensor must have four dimensions.
 * - \p input data type of half is not recommended due to low precision.
 * - Size of the lowest dimension of \p input tensor and \p output tensor must be the same.
 * - Size of the lowest dimension of \p input tensor and \p argmax_x tensor must be the same.
 * - Size of the lowest dimension of \p input tensor and \p argmax_y tensor must be the same.
 * - The \p boxes tensor must have two dimensions.
 * - Size of the highest dimension of \p output tensor and \p boxes tensor must be the same.
 * - Size of the highest dimension of \p argmax_x tensor and \p boxes tensor must be the same.
 * - Size of the highest dimension of \p argmax_y tensor and \p boxes tensor must be the same.
 * - The shape of \p boxes should be [boxes_num, 5].
 * - \p boxes[i] consists of [batch_id, x1, y1, x2, y2]. \p batch_id specifies which image this box
 *   is in, and should be in range of [0, batch_num - 1]. \p x1 and \p y1 specify the starting
 *   coordinate of this box in origin image. \p x2 and \p y2 specify the ending coordinate of this box
 *   in origin image. \p x1 and \p y1 should be greater than or equal to 0. \p x2 should be greater
 *   than \p x1. \p y2 should be greater than \p y1. If the input boxes have negative height or width,
 *   no calculation will be performed. When \p pool_mode is set to 1, the output \p output will be set to zero.
 *   When \p pool_mode is set to 0, the output \p output will be set to zero, and both \p argmax_x and
 *   \p argmax_y will be set to -FLT_MAX.
 *
 * @par API Dependency
 * - This function should be used with ::cnnlSetRoiAlignDescriptor_v2.
 *
 * @par Performance Optimization
 * - None.
 *
 * @note
 * - When \p input contains Nan:
 *   - If \p pool_mode is maximum pooling_mode:
 *     - \p output is positive saturation value on MLU200 series
 *     - \p output gets more Nan than IEEE 754 on MLU300 series and CE3226.
 *
 * - When \p boxes contains Nan or infinity:
 *   The operator does not provide support for bounding boxes containing Nan and infinity values.
 *   Such occurrences of Nan and infinity within the \p boxes are inconsistent with the intended semantics
 *   of the operator's algorithm.
 *   - The behavior of computation results on MLU500 series is as follows:
 *     - If the \p boxes includes Nan values, the calculation process resorts to employing bilinear interpolation.
 *     - In cases where the \p boxes includes infinity values, an error will be detected and reported. The
 *       nature of this error is described as a "timeout". This error is triggered due to the presence of infinity
 *       values, which deviate from the anticipated input range and disrupt the operator's expected behavior.
 *
 * - This operation is not supported on the 1V platforms.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the roialign_forward operation is as follows:
     @verbatim
     input two arrays by 1 * 1 * 1 * 1 and 1 * 5 --> input: [[[[1.0]]]]

     --> boxes: [[1.0, 0.0, 0.0, 1.0, 1.0]]

     parameters:
            pooled_height: 1.0, pooled_width: 1.0, spatial_scale: 1.0,
            sampling_ratio: 0, aligned: false, pool_mode = 0

     output array by 1 * 1 * 1 * 1 -->
         output: [[[[1]]
     argmax_x array by 1 * 1 * 1 * 1 -->
         argmax_x: [[[[0.5]]
     argmax_y array by 1 * 1 * 1 * 1 -->
         argmag_y: [[[[0.5]]

     @endverbatim
 *
 * @par Reference
 * - http://github.com/open-mmlab/mmcv/blob/master/mmcv/ops/csrc/pytorch/roi_align_cuda.cu
 */
cnnlStatus_t CNNL_WIN_API
cnnlRoiAlign_v2(cnnlHandle_t handle,
                const cnnlRoiAlignDescriptor_t roialign_desc,
                const cnnlTensorDescriptor_t input_desc,
                const void *input,
                const cnnlTensorDescriptor_t boxes_desc,
                const void *boxes,
                const cnnlTensorDescriptor_t output_desc,
                void *output,
                const cnnlTensorDescriptor_t argmax_x_desc,
                void *argmax_x,
                const cnnlTensorDescriptor_t argmax_y_desc,
                void *argmax_y);

// Group:RoiPooling
/*!
 * @brief Generates fixed size feature map and input feature index
 * of argmax for each Roi(Regions of Interest) to perform ::cnnlRoiPoolingForward operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   ::cnnlRoiPoolingForward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] pooling_mode
 *   Input. The pooling mode of roipoolingforward defined in ::cnnlPoolingMode_t
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor in the roipoolingforward process. For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] rois_desc
 *   Input. The descriptor of rois tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] rois
 *   Input. Pointer to the MLU memory that stores the rois tensor. Rois means regions of interest.
 * @param[in] spatial_scale
 *   Input. The spatial scale of each regions of interest in the input feature map.
 * @param[in] output_desc
 *   Input. Descriptor of output tensor and argmax tensor.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[out] argmax
 *   Output. Pointer to the MLU memory that stores the argmax tensor. Pointer may be NULL. Argmax tensor
 *   means input feature index of maximum for each Roi(Regions of Interest).
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "RoiPoolingForward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \p input, rois tensor \p rois,
 *   output tensor \p output, and argmax tensor \p argmax. The data type of the \p input \p rois \p output
 *   should be the same. The data type of the argmax is different from others.
 *   - input tensor: half, float.
 *   - rois tensor: half, float.
 *   - output tensor: half, float.
 *   - argmax tensor: int32.
 *
 * @par Data Layout
 * - The supported data layouts of \p input, \p rois, \p output, \p argmax are as follows:
 *   - input tensor: \p CNNL_LAYOUT_NHWC.
 *   - rois tensor: \p CNNL_LAYOUT_ARRAY, only supports 2D tensor.
 *   - output tensor: \p CNNL_LAYOUT_NHWC.
 *   - argmax tensor: \p CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - The value of \p pooling_mode only supports \p CNNL_POOLING_MAX.
 * - The input tensor, output tensor and argmax tensor must have four dimensions.
 * - Data type of half of \p input and \p rois is not recommended due to low precision.
 * - Size of the lowest dimension of input tensors, output tensor and argmax tensors must be the same.
 * - The total number of dimension of rois tensor must be 2.
 * - Size of the highest dimension of output tensor and rois tensor must be the same.
 * - The shape of \p rois should be [rois_num, 5].\p rois_num means the total number of \p rois.
 * - \p Rois[i] consists of [batch_id, x1, y1, x2, y2]. \p batch_id should be in range of
 * [0, batch_num - 1]. \p x1 and \p y1 should be greater than or equal to 0. \p x2 should be
 * greater than \p x1. \p y2 should be greater than \p y1. \p batch_id means id of batch of \p rois. \p x1, \p y1,
 * \p x2 and \p y2 mean the coordinate values of rois in the input feature map. \p batch_num means the total number
 * of the batch.
 * - \p Spatial_scale should be in range of (0, 1].
 * - \p Output consists of [rois_num, pooled_h, pooled_w, channels]. In the dimensions of the h and w of the input
 * and the output, (\p x2 - \p x1) * (\p y2 - \p y1) * \p spatial_scale * \p spatial_scale / (\p pooled_h * \p
 * pooled_w) < (nram_limitation / 32). Nram_limitation means the limitation of the nram. When the supported MLU platform is 200,
 * the nram_limitation is (98304 - 4 * \p channels) / 2. When the supported MLU platform is 300, the nram_limitation is
 * (163840 - 4 * \p channels) / 2. \p pooled_h means height of output.
 * \p pooled_w means width of output.
 *
 * @par API Dependency
 * - None

 * @par Performance Optimization
 * - None.
 *
 * @note
 * - It is not recommended to set the data type of grads tensor, rois tensor and grads_image tensors
 *   that may cause the low accuracy on MLU200 series.
 * - When the input data or parameter contains NaN or infinity:
 *   - On MLU200 series:
 *     - The \p output is the positive saturation value. The \p argmax is the index of the last NaN in the kernel of the pooling.
 *   - On MLU300 series and CE3226:
 *     - If the last value in the kernel of the pooling is NaN, the \p argmax is the index of the last value, the \p output is the last value,
 *       as shown in example 2 below. Otherwise, the \p argmax is the index of the maximum value after the last NaN, the \p output is the maximum value
 *       after the last NaN, as shown in example 3 below.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example 1 of the roipoolingforward operation is as follows:
     @verbatim
     input two arrays by 1 * 4 * 4 * 1 and 1 * 5 -->input: [[[[1.0],[2.0],[3.0],[4.0]],
                                                             [[5.0],[6.0],[7.0],[8.0]],
                                                             [[9.0],[10.0],[11.0],[12.0]],
                                                             [[13.0],[14.0],[15.0],[16.0]]]]

     --> rois: [[1.0, 0.0, 0.0, 3.0, 3.0]]

     param:
         pooling_mode: 0, spatial_scale: 1.0

     output array by 1 * 2 * 2 * 1 --> output: [[[[6.0], [8.0]],
                                                 [[14.0], [16.0]]]]

     argmax array by 1 * 2 * 2 * 1 --> argmax: [[[[5], [7]],
                                                 [[13], [15]]]]
     @endverbatim

   - The example 2 of the roipoolingforward operation is as follows:
     @verbatim
     input two arrays by 1 * 2 * 2 * 1 and 1 * 5 -->input: [[[[1.0],[2.0]],
                                                             [[3.0],[NaN]]]]

     --> rois: [[1.0, 0.0, 0.0, 1.0, 1.0]]

     param:
         pooling_mode: 0, spatial_scale: 1.0

     output array by 1 * 1 * 1 * 1 --> output: [[[[NaN]]]]

     argmax array by 1 * 1 * 1 * 1 --> argmax: [[[[3]]]]
     @endverbatim

   - The example 3 of the roipoolingforward operation is as follows:
     @verbatim
     input two arrays by 1 * 2 * 2 * 1 and 1 * 5 -->input: [[[[1.0],[NaN]],
                                                             [[3.0],[4.0]]]]

     --> rois: [[1.0, 0.0, 0.0, 1.0, 1.0]]

     param:
         pooling_mode: 0, spatial_scale: 1.0

     output array by 1 * 1 * 1 * 1 --> output: [[[[4.0]]]]

     argmax array by 1 * 1 * 1 * 1 --> argmax: [[[[3]]]]
     @endverbatim
 *
 * @par Reference
 * - https:github.com/pytorch/pytorch/caffe2/operators/roi_pool_op.cu
 */
cnnlStatus_t CNNL_WIN_API cnnlRoiPoolingForward(cnnlHandle_t handle,
                                                cnnlPoolingMode_t pooling_mode,
                                                const cnnlTensorDescriptor_t input_desc,
                                                const void *input,
                                                const cnnlTensorDescriptor_t rois_desc,
                                                const void *rois,
                                                float spatial_scale,
                                                const cnnlTensorDescriptor_t output_desc,
                                                void *output,
                                                int *argmax);

// Group:RoiPoolingBackward
/*!
 * @brief Computes the gradients of images \p grads_image based on the gradients \p grads and
 * region proposals \p rois to perform the backpropagation of ::cnnlRoiPoolingForward operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   ::cnnlRoiPoolingBackward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] grads_desc
 *   Input. The descriptor of the gradient tensor in the backpropagation process. For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @param[in] grads
 *   Input. Pointer to the MLU memory that stores the gradient tensor.
 * @param[in] rois_desc
 *   Input. The descriptor of the region proposals tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] rois
 *   Input. Pointer to the MLU memory that stores the region proposals tensor.
 * @param[in] argmax_desc
 *   Input. The descriptor of the argmax tensor that stores the index returned by
 *   ::cnnlRoiPoolingForward. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] argmax
 *   Input. Pointer to the MLU memory that stores the argmax tensor.
 * @param[in] spatial_scale
 *   Input. A scaling factor that specifies how to map the ROIs coordinates in the origin image to
 *   the coordinates in the output.
 * @param[in] mode
 *   Input. The pooling mode, which is defined in the ::cnnlPoolingMode_t enum.
 * @param[in] grads_image_desc
 *   Input. The descriptor of the gradients tensor of the original images.
 * @param[out] grads_image
 *   Output. Pointer to the MLU memory that stores the gradients tensor of the original images.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH.
 *
 * @par Formula
 * - See "RoiPoolingBackward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for gradient tensor \p grads, region proposals
 *   tensor \p rois, argmax_tensor \p argmax and output tensor \p grads_image. Data type of
 *   gradient tensor \p grads, region proposals tensor \p rois and output tensor \p grads_image
 *   should be the same.
 *   - grads tensor: half, float.
 *   - rois tensor: half, float.
 *   - argmax tensor: int32_t.
 *   - grads_image tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layouts of \p grads, \p rois, \p grads_images are as follows:
 *   - grads tensor: \p CNNL_LAYOUT_NHWC.
 *   - rois tensor: \p CNNL_LAYOUT_ARRAY, only supports 2D tensor.
 *   - argmax tensor: \p CNNL_LAYOUT_NHWC.
 *   - grads_image tensor: \p CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - The value of \p mode only supports \p CNNL_POOLING_MAX.
 * - The grads tensor, argmax tensor and grads_image tensor must have four dimensions.
 * - Size of the fourth dimension of grads tensor, argmax tensor and grads_image tensor must be the
 *   same.
 * - Size of each dimension of grads tensor and argmax tensor must be the same.
 * - The region proposals tensor \p rois must have two dimensions.
 * - Size of the first dimension of grads tensor, rois tensor and argmax tensor must be the same.
 * - The shape of \p rois should be [rois_num, 5].
 * - \p rois[i] consists of [rois_num, x1, y1, x2, y2]. \p rois_num specifies which image this ROI
 *   is in, and should be in range of [0, batch_num - 1]. \p x1 and \p y1 specify the starting
 *   coordinate of this ROI in origin image. \p x2 and \p y2 specify the ending coordinate of this
 *   ROI in origin image. \p x1 and \p y1 should be greater than or equal to 0. \p x2 should be
 *   greater than \p x1. \p y2 should be greater than \p y1.
 * - \p spatial_scale should be in range of (0, 1].
 * - The value of argmax tensors with the data type is int32_t and should be in range of
 *   [0, \f$2^{31}-1\f$].
 *
 * @par API Dependency
 * - None.
 *
 * @par Performance Optimization
 * - None.
 *
 * @note
 * - In general, set the values of \p argmax according to the result returned by
 *   ::cnnlRoiPoolingForward. Otherwise, these values may be regarded as invalid and will not be
 *   used in this operation.
 * - It is not recommended to set the data type of grads tensor, rois tensor and grads_image tensors
 *   as half since it may get a low accuracy result on MLU200 series.
 * - When \p rois contains NaN or infinity, it may cause undefined behavior.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the roi_pooling_backward operation is as follows:
     @verbatim
     input two arrays by 1 * 2 * 2 * 1 and 1 * 5 --> grads: [[[[1.0], [2.0]], [[3.0], [4.0]]]]

     --> rois: [[0.0, 0.0, 0.0, 2.0, 2.0]]
     --> argmax: [[[[0], [2]], [[8], [10]]]]

     param:
         spatial_scale: 1.0

     grads_image array by 1 * 4 * 4 * 1 -->
         grads_image: [[[[1.0], [0.0], [2.0], [0.0]], [[0.0], [0.0], [0.0], [0.0]],
                        [[3.0], [0.0], [4.0], [0.0]], [[0.0]. [0.0], [0.0], [0.0]]]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/vision/stable/ops.html#torchvision.ops.roi_pool
 */
cnnlStatus_t CNNL_WIN_API cnnlRoiPoolingBackward(cnnlHandle_t handle,
                                                 cnnlPoolingMode_t mode,
                                                 const cnnlTensorDescriptor_t grads_desc,
                                                 const void *grads,
                                                 const cnnlTensorDescriptor_t rois_desc,
                                                 const void *rois,
                                                 const cnnlTensorDescriptor_t argmax_desc,
                                                 const int *argmax,
                                                 const float spatial_scale,
                                                 const cnnlTensorDescriptor_t grads_image_desc,
                                                 void *grads_image);
// Group:IsNan
/*!
 *  @brief Returns the Boolean elements in \p output that represent if each element
 *  of \p input tensor is NaN or not.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *    queues in the index select operation. For detailed information, see ::cnnlHandle_t.
 *  @param[in] x_desc
 *    Input. The descriptors of the input tensors. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[in] x
 *    Input. Pointer to the MLU memory that stores the input tensor.
 *  @param[in] y_desc
 *    Input. The descriptors of the \p output tensors. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[out] y
 *    Output. Pointer to the MLU memory that stores the output tensor.
 *    Returns true if the element of \p input tensor is NaN. Otherwise, returns false.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM

 *  @par Data Type
 *  - \p input: float, half, bfloat16.
 *  - \p output: bool.
 *
 *  @par Scale Limitation
 *  - The dimension length of output tensor and input tensor must be the same in the same dimension.
 *
 *  @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/math/is_nan
 *
 *  @par API Dependency
 *  - None.
 *
 *  @note
 *  - bfloat16 is only supported on MLU300 series and MLU500 series.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
      @verbatim
        x:  [1.0, 6.2, NaN, inf, -inf]
        y: [false, false, true, false, false]
        where inf represents infinity
      @endverbatim
 */

cnnlStatus_t CNNL_WIN_API cnnlIsNan(cnnlHandle_t handle,
                                    const cnnlTensorDescriptor_t x_desc,
                                    const void *x,
                                    const cnnlTensorDescriptor_t y_desc,
                                    void *y);

// Group:NanToNum
/*!
 *  @brief Replaces NaN, positive infinity, and negative infinity values in \p input with the
 *  values specified by \p nan, \p pos_inf and \p neg_inf.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *    queues in the nan_to_num operation. For detailed information, see ::cnnlHandle_t.
 *  @param[in] input_desc
 *    Input. The descriptors of the \p input tensors. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[in] input
 *    Input. Pointer to the MLU memory that stores the input tensor.
 *  @param[in] nan
 *    Input. The value used to replace the NaN in the input.
 *  @param[in] pos_inf
 *    Input. The value used to replace the inf value in the input.
 *  @param[in] neg_inf
 *    Input. The value used to replace the -inf value in the input.
 *  @param[in] output_desc
 *    Input. The descriptors of the \p output tensors. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[out] output
 *    Output. Pointer to the MLU memory that stores the output tensor.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH.

 *  @par Data Type
 *  - \p input: float, half, bfloat16, bool.
 *  - \p output: float, half, bfloat16, bool.
 *  - The data type bfloat16 is only supported on MLU500 series.
 *
 *  @par Scale Limitation
 *  - The dimension length of output tensor and input tensor must be the same in the same dimension.
 *
 *  @par Reference
 * - https://www.pytorch.org/docs/stable/generated/torch.nan_to_num.html
 *
 *  @par API Dependency
 *  - None.
 *
 *  @note
 *  - This function is only supported on MLU300 series, MLU500 series, and 1V.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
      @verbatim
        input:  [1.0, 6.2, NaN, inf, -inf]
        nan: 0
        pos_inf: 2.0
        neg_inf: -2.0
        output: [1.0, 6.2, 0, 2.0, -2.0]
        where inf represents infinity
      @endverbatim
 */
cnnlStatus_t CNNL_WIN_API cnnlNanToNum(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t input_desc,
                                       const void *input,
                                       const float nan,
                                       const float pos_inf,
                                       const float neg_inf,
                                       const cnnlTensorDescriptor_t output_desc,
                                       void *output);

/******************************************************************************
 * Cambricon CNNL Data Structure: MSELossReduction
 ******************************************************************************/
/*!
 * @brief Enumeration variables describing the reduction applied to the output in the
 * implementation of the MSELoss functions.
 *
 */
typedef enum {
  CNNL_MSE_LOSS_NONE = 0,
  /*!< No reduction is applied to the output. */
  CNNL_MSE_LOSS_SUM = 1,
  /*!< The reduction that calculates the sum of the output is applied. */
  CNNL_MSE_LOSS_MEAN = 2,
  /*!< The reduction that calculates the mean of the sum of the output is applied. */
} cnnlMSELossReduction_t;

// Group:MSELoss
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the mse_loss operation.
 *
 * The size of the extra workspace is based on the given information of the mse_loss operation,
 * including the input tensor descriptor \p input_desc.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the mse_loss
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the mse_loss
 *   operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - This API is only used along with ::cnnlMSELoss_v2. ::cnnlMSELoss does not require this API.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetMSELossWorkspaceSize(cnnlHandle_t handle,
                                                      const cnnlTensorDescriptor_t input_desc,
                                                      size_t *workspace_size);

// Group:MSELoss
/*!
 * @brief Computes a MSE (Mean Squared Error) Loss, which is used to train a
 * network on classification tasks, based on input tensor \p input with \p target, and
 * returns the results in the output tensor \p output.
 *
 * Compared with cnnlMSELoss, this function allows you to allocate some extra workspace as an input
 * parameter. If you just set \p workspace to NULL and \p workspace_size to 0, this function will
 * perform as same as cnnlMSELoss.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   mse_loss forward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] reduction
 *   Input. The reduction used to compute the mse_loss. The reduction is defined in the
 *   ::cnnlMSELossReduction_t enum.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] target_desc
 *   Input. The descriptor of the target tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] target
 *   Input. Pointer to the MLU memory that stores the target tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlMSELoss_v2.
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   ::cnnlMSELoss_v2. You can get the size of the workspace with
 *   the ::cnnlGetMSELossWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH.
  * @par Data Type
 * - Data type of input tensors and output tensor should be the same.
 *   The supported data types of input and output tensors are as follows:
 *   - \p input: float, half, bfloat16(bf16 is only supported on MLU500 series).
 *   - \p target: float, half, bfloat16(bf16 is only supported on MLU500 series).
 *   - \p output: float, half, bfloat16(bf16 is only supported on MLU500 series).
 *
 * @par Limitations
 * - None.
 *
 * @par API Dependency
 * - Before using this API, you need to get the size of the workspace with
 *   the ::cnnlGetMSELossWorkspaceSize function and pass the required extra workspace to
 *   the ::cnnlMSELoss_v2 function.
 *
 * @note
 * - The dimension of input tensor and target tensor should be the same.
 * - The dimension of the input tensor and output tensor should be the same, when the \p
 *   reduction is \p CNNL_MSE_LOSS_NONE.
 * - The number of elements in output tensor should be one, when the \p reduction is \p
 *   CNNL_MSE_LOSS_SUM or \p CNNL_MSE_LOSS_MEAN.
 * - On MLU300 and eariler hardware platforms, both ::cnnlMSELoss_v2 and ::cnnlMSELoss can be used.
 *   On MLU500 series, ::cnnlMSELoss_v2 should be used instead of ::cnnlMSELoss.
 *
 * @par Requirements
 * - None.
 *
 * @par Formula
 * - See "MSELoss" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Example
 * - The example of the nllloss forward operation is as follows:
     @verbatim
       input two arrays by 2 * 3, and 2 * 3
       --> input: [[1,2,3], [4,5,6]]

       --> target: [[2,3,4], [5,6,7]]

       param:
         reduction: CNNL_MSE_LOSS_NONE

       output one array by 2 * 3
       --> output: [[1,1,1], [1,1,1]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/1.8.0/generated/torch.nn.MSELoss.html#torch.nn.MSELoss
 */
cnnlStatus_t CNNL_WIN_API cnnlMSELoss_v2(cnnlHandle_t handle,
                                         cnnlMSELossReduction_t reduction,
                                         const cnnlTensorDescriptor_t input_desc,
                                         const void *input,
                                         const cnnlTensorDescriptor_t target_desc,
                                         const void *target,
                                         void *workspace,
                                         size_t workspace_size,
                                         const cnnlTensorDescriptor_t output_desc,
                                         void *output);

// Group:MSELoss
/*!
 * @brief Computes a MSE (Mean Squared Error) Loss, which is used to train a
 * network on classification tasks, based on input tensor \p input with \p target, and
 * returns the results in the output tensor \p output.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlMSELoss_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   mse_loss forward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] reduction
 *   Input. The reduction used to compute the mse_loss. The reduction is defined in the
 *   ::cnnlMSELossReduction_t enum.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] target_desc
 *   Input. The descriptor of the target tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] target
 *   Input. Pointer to the MLU memory that stores the target tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH.
  * @par Data Type
 * - Data type of input tensors and output tensor should be the same.
 *   The supported data types of input and output tensors are as follows:
 *   - \p input: float, half, bfloat16(bf16 is only supported on MLU500 series).
 *   - \p target: float, half, bfloat16(bf16 is only supported on MLU500 series).
 *   - \p output: float, half, bfloat16(bf16 is only supported on MLU500 series).
 *
 * @par Limitations
 * - None.
 *
 * @par API Dependency
 * - None.
 *
 * @note
 * - The dimension of input tensor and target tensor should be the same.
 * - The dimension of the inputs tensor and output tensor should be the same, when the \p
 *   reduction is \p CNNL_MSE_LOSS_NONE.
 * - The number of elements in output tensor should be one, when the \p reduction is \p
 *   CNNL_MSE_LOSS_SUM or \p CNNL_MSE_LOSS_MEAN.
 * - On MLU500 series, ::cnnlMSELoss cannot be used and you should
 *   use ::cnnlMSELoss_v2 instead.
 *
 * @par Requirements
 * - None.
 *
 * @par Formula
 * - See "MSELoss" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Example
 * - The example of the nllloss forward operation is as follows:
     @verbatim
       input two arrays by 2 * 3, and 2 * 3
       --> input: [[1,2,3], [4,5,6]]

       --> target: [[2,3,4], [5,6,7]]

       param:
         reduction: CNNL_MSE_LOSS_NONE

       output one array by 2 * 3
       --> output: [[1,1,1], [1,1,1]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/1.8.0/generated/torch.nn.MSELoss.html#torch.nn.MSELoss
 */
CNNL_DEPRECATED_FOR(cnnlMSELoss_v2)
cnnlStatus_t CNNL_WIN_API cnnlMSELoss(cnnlHandle_t handle,
                                      cnnlMSELossReduction_t reduction,
                                      const cnnlTensorDescriptor_t input_desc,
                                      const void *input,
                                      const cnnlTensorDescriptor_t target_desc,
                                      const void *target,
                                      const cnnlTensorDescriptor_t output_desc,
                                      void *output);

// Group:FloorDivTrunc
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the floor_div_trunc operation.
 *
 * The size of the extra workspace is based on the given information of the floor_div_trunc operation,
 * including the input tensor descriptors \p input1_desc and \p input2_desc, and the output
 * tensor descriptor \p output_desc. For more information about the workspace, see "Cambricon
 * CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   floor_div_trunc operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input1_desc
 *   Input. The descriptor of the first input which is dividend tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input2_desc
 *   Input. The descriptor of the second input which is divisor tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   floor_div_trunc operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH.
 *
 * @par Scale Limitation
 * - According to the rule of tensor broadcast, the parameters should satisfy the following
 *   conditions. For example, c1_dim, c2_dim, and c3_dim represent the lowest dimension of
 *   \p input1, \p input2, and \p output respectively:
 *
 *   min(c1_dim, c2_dim) == 1 or c1_dim == c2_dim
 *
 *   max(c1_dim, c2_dim) == c3_dim
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetFloorDivTruncWorkspaceSize(cnnlHandle_t handle,
                                  const cnnlTensorDescriptor_t input1_desc,
                                  const cnnlTensorDescriptor_t input2_desc,
                                  const cnnlTensorDescriptor_t output_desc,
                                  size_t *workspace_size);

// Group:FloorDivTrunc
/*!
 * @brief Divides \p input1 by \p input2, rounding to the zero direction, and returns
 *        the results in the output tensor \p output.
 *
 * This function may need extra MLU memory as the workspace to improve the floor_div_trunc
 * performance.
 * You can get the workspace size with the
 * ::cnnlGetFloorDivTruncWorkspaceSize function.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release. Use ::cnnlDiv_v3 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   floor_div_trunc operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. The algorithm used to compute the output. For detailed information,
 *   see ::cnnlComputationPreference_t.
 *   The default value of this parameter is ::CNNL_COMPUTATION_HIGH_PRECISION.
 * @param[in] input1_desc
 *   Input. The descriptor of the first input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input1
 *   Input. Pointer to the MLU memory that stores the dividend tensor.
 * @param[in] input2_desc
 *   Input. The descriptor of the second input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input2
 *   Input. Pointer to the MLU memory that stores the divisor tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the floor_div_trunc
 *   operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the floor_div_trunc
 *   operation.
 *   You can get the size of the workspace with the ::cnnlGetFloorDivTruncWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "FloorDivTrunc Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported data types of input and output tensors are as follows:
 *   - input1 tensor: half, float, int32, float, int64, int64.
 *   - input2 tensor: half, float, int32, int64, float, int64.
 *   - output tensor: half, float, int32, float, float, int64.
 * - On MLU500 series, this function additionally supports the following data type combinations:
 *   - input tensor: bfloat16.
 *   - output tensor: bfloat16.
 *
 * @par Data Layout
 * - The supported data layout of the input tensors and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - According to the rule of tensor broadcast, the parameters should satisfy the following
 *   conditions. For example, c1_dim, c2_dim, and c3_dim represent the lowest dimension of
 *   \p input1, \p input2, and \p output respectively:
 *
 *   min(c1_dim, c2_dim) == 1 or c1_dim == c2_dim
 *
 *   max(c1_dim, c2_dim) == c3_dim
 *
 * @par API Dependency
 * - You need to call the ::cnnlGetFloorDivTruncWorkspaceSize function to allocate extra workspace
 *   which should be passed to ::cnnlFloorDivTrunc before calling this function.
 *
 * @note
 * - The inputs \p input1 and \p input2 are multi-dimensional arrays, supporting up to
 *   CNNL_DIM_MAX dimensions.
 * - This API returns -1 if the input \p input2 is fixed-point zero.
 * - In the situation when this function is in the COMPUTATION_FAST mode, accuracy problem may occur.
 *
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlDiv_v3)
cnnlStatus_t CNNL_WIN_API cnnlFloorDivTrunc(cnnlHandle_t handle,
                                            cnnlComputationPreference_t prefer,
                                            const cnnlTensorDescriptor_t input1_desc,
                                            const void *input1,
                                            const cnnlTensorDescriptor_t input2_desc,
                                            const void *input2,
                                            const cnnlTensorDescriptor_t output_desc,
                                            void *output,
                                            void *workspace,
                                            size_t workspace_size);

// Group:FloorDivTrunc
/*!
 * @brief Divides \p input1 by \p input2, rounding to the zero direction, and returns
 *        the results in the output tensor \p output.
 *
 * Compared with ::cnnlFloorDivTrunc, this function removes the \p prefer parameter.
 * This function may need extra MLU memory as the workspace to improve the floor_div_trunc
 * performance.
 * You can get the workspace size with the
 * ::cnnlGetFloorDivTruncWorkspaceSize function.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release. Use ::cnnlDiv_v3 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   floor_div_trunc operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input1_desc
 *   Input. The descriptor of the first input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input1
 *   Input. Pointer to the MLU memory that stores the dividend tensor.
 * @param[in] input2_desc
 *   Input. The descriptor of the second input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input2
 *   Input. Pointer to the MLU memory that stores the dividend tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the floor_div_trunc
 *   operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 *   @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the floor_div_trunc
 *   operation.
 *   You can get the size of the workspace with the ::cnnlGetFloorDivTruncWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "FloorDivTrunc Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported data types of input and output tensors are as follows:
 *   - input1 tensor: half, float, int32, float, int64, int64.
 *   - input2 tensor: half, float, int32, int64, float, int64.
 *   - output tensor: half, float, int32, float, float, int64.
 * - On MLU500 series, this function additionally supports the following data type combinations:
 *   - input tensor: bfloat16.
 *   - output tensor: bfloat16.
 *
 * @par Data Layout
 * - The supported data layout of the input tensors and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - According to the rule of tensor broadcast, the parameters should satisfy the following
 *   conditions. For example, c1_dim, c2_dim, and c3_dim represent the lowest dimension of
 *   \p input1, \p input2, and \p output respectively:
 *
 *   min(c1_dim, c2_dim) == 1 or c1_dim == c2_dim
 *
 *   max(c1_dim, c2_dim) == c3_dim
 *
 * @par API Dependency
 * - You need to call the ::cnnlGetFloorDivTruncWorkspaceSize function to allocate extra workspace
 *   which should be passed to ::cnnlFloorDivTrunc before calling this function.
 *
 * @note
 * - The inputs \p input1 and \p input2 are multi-dimensional arrays, supporting up to
 *   CNNL_DIM_MAX dimensions.
 * - This API returns -1 if the input \p input2 is fixed-point zero.
 * - In " COMPUTATION_FAST" mode, this function might experience accuracy issues.
 *
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlDiv_v3)
cnnlStatus_t CNNL_WIN_API cnnlFloorDivTrunc_v2(cnnlHandle_t handle,
                                               const cnnlTensorDescriptor_t input1_desc,
                                               const void *input1,
                                               const cnnlTensorDescriptor_t input2_desc,
                                               const void *input2,
                                               const cnnlTensorDescriptor_t output_desc,
                                               void *output,
                                               void *workspace,
                                               size_t workspace_size);

// Group:MSELossBackward
 /*!
 * @brief Computes the gradient of a MSE (Mean Squared Error) Loss, which is used to
 * train a network on classification tasks, based on input tensor \p input with \p target,
 * and \p grad, returns the results in the output tensor \p output.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   mse_loss_backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] reduction
 *   Input. The reduction used to compute the mse_loss_backward. The reduction is defined in the
 *   ::cnnlMSELossReduction_t enum.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor, which represents
 *   the predicted value.
 * @param[in] target_desc
 *   Input. The descriptor of the target tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] target
 *   Input. Pointer to the MLU memory that stores the target tensor, which represents
 *   the ground truth value.
 * @param[in] grad_desc
 *   Input. The descriptor of the grad tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] grad
 *   Input. Pointer to the MLU memory that stores the grad tensor, which represents
 *   the gradient with respect to \p output.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor, which represents
 *   the gradient with respect to \p input.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH.
  * @par Data Type
 * - Data type of input tensors and output tensor should be the same.
 *   The supported data types of input and output tensors are as follows:
 *   - \p input: float, half, bfloat16.
 *   - \p target: float, half, bfloat16.
 *   - \p grad: float, half, bfloat16.
 *   - \p output: float, half, bfloat16.
 *
 *   bfloat16 is only supported on MLU500 series.
 *
 * @par Limitations
 * - None.
 *
 * @par API Dependency
 * - None.
 *
 * @note
 * - The dimension of the inputs tensor and output tensor should be the same, when the \p
 *   reduction is \p CNNL_MSE_LOSS_NONE.
 * - The dimension of the grad tensor should be one, when the \p reduction is \p
 *   CNNL_MSE_LOSS_SUM or \p CNNL_MSE_LOSS_MEAN.
 *
 * @par Requirements
 * - None.
 *
 * @par Formula
 * - See "MSELossBackward" section in "Cambricon CNNL User Guide" for details.  *
 * @par Example
 * - The example of the mseloss backward operation is as follows:
     @verbatim
       input three arrays by 2 * 3, 2 * 3 and 1
       --> input : [[4,5,6], [7,8,9]]

       --> target: [[1,2,3], [4,5,6]]

       --> grad  : [1]

       param:
         reduction: CNNL_MSE_LOSS_MEAN

       output one array by 2 * 3
       --> output: [[1,1,1], [1,1,1]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/1.8.0/generated/torch.nn.MSELoss.html?highlight=mse#torch.nn.MSELoss
 */
cnnlStatus_t CNNL_WIN_API cnnlMSELossBackward(cnnlHandle_t handle,
                                              cnnlMSELossReduction_t reduction,
                                              const cnnlTensorDescriptor_t input_desc,
                                              const void *input,
                                              const cnnlTensorDescriptor_t target_desc,
                                              const void *target,
                                              const cnnlTensorDescriptor_t grad_desc,
                                              const void *grad,
                                              const cnnlTensorDescriptor_t output_desc,
                                              void *output);

// Group:CdistForward
/*!
 * @brief Batch computes the p-norm distance between each pair of row vectors in the input tensor \p x1,
 * and input tensor \p x2, and returns the result in the output tensor \p output.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the cdist_forward
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x1_desc
 *   Input. The descriptor of the input tensor \p x1. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x1
 *   Input. Pointer to the MLU memory that stores the input tensor \p x1.
 * @param[in] x2_desc
 *   Input. The descriptor of the input tensor \p x2. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x2
 *   Input. Pointer to the MLU memory that stores the input tensor \p x2.
 * @param[in] p
 *   Input. A double value used as the p-norm. \p p must be equal 1.0.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor \p output. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor \p output.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Cdist_forward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - x1 tensor: float.
 *   - x2 tensor: float.
 *   - \p p: double.
 *   - output tensor:float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensors and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - This operation supports MLU300 series and MLU500 series.
 * - This operation is not supported on the 1V platforms.
 * - Only support p = 1.0 now.
 * - The tensor shape of \p x1, \p x2 and \p output should be as follows:
 *   - x1 tensor: [B, P, M].
 *   - x2 tensor: [B, R, M].
 *   - output tensor: [B, P, R].
 *
 * @par Requirements
 * - None.
 *
 * @par Scale Limitation
 * - None.
 *
 * @par API Dependency
 * - None.
 *
 * @par Example
     @verbatim
       input x1: [[[-3.,  4.,  1.]],
                  [[ 6.,  5., -1.]]]
       input x2: [[[ 1.,  2.,  1.],
                   [ 1.,  1.,  2.]],
                  [[-3.,  4.,  1.],
                   [-1.,  1.,  2.]]]
       inpute p: 1.0
       output: [[[ 6., 8.]]
                [[12., 14.]]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/1.9.0/generated/torch.cdist.html
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlCdistForward(cnnlHandle_t handle,
                                           const cnnlTensorDescriptor_t x1_desc,
                                           const void *x1,
                                           const cnnlTensorDescriptor_t x2_desc,
                                           const void *x2,
                                           const double p,
                                           const cnnlTensorDescriptor_t output_desc,
                                           void *output);

// Group:PdistForward
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra workspace
 * to optimize the ::cnnlPdistForward operation.
 *
 * The size of the extra workspace is based on the given information of the ::cnnlPdistForward operation,
 * including the input tensor descriptor \p input_desc, and output tensor descriptor \p output_desc.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the ::cnnlPdistForward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes
 *   that is used in the ::cnnlPdistForward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlPdistForward function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetPdistForwardWorkspaceSize(cnnlHandle_t  handle,
                                 const cnnlTensorDescriptor_t input_desc,
                                 const cnnlTensorDescriptor_t output_desc,
                                 size_t *workspace_size);

// Group:PdistForward
/*!
 * @brief Returns in \p extra_input_size the size of the MLU memory and host memory that is used as an extra
 * input data to optimize the ::cnnlPdistForward operation. You need to allocate memory both on host and MLU based on
 * the size returned in \p extra_input_size.
 *
 * The size of extra input data is based on the given information of the ::cnnlPdistForward operation,
 * including the output tensor descriptor \p output_desc.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the ::cnnlPdistForward operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] extra_input_size
 *   Output. A host pointer to the returned size of the extra input data in bytes
 *   that is used in the ::cnnlPdistForward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - After calling this function, you need to call ::cnnlInitPdistForwardExtraInput to initialize the memory on host.
 * - The allocated extra input should be passed to the ::cnnlPdistForward function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetPdistForwardExtraInputSize(cnnlHandle_t handle,
                                  const cnnlTensorDescriptor_t output_desc,
                                  size_t *extra_input_size);

// Group:PdistForward
/*!
 * @brief Initializes the extra input data space \p extra_host_input on host.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in ::cnnlPdistForward operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] extra_host_input
 *   Output. Pointer to the host memory that is used as an extra input space
 *   for ::cnnlPdistForward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to get the size of the extra input data with ::cnnlGetPdistForwardExtraInputSize.
 *   The memory of the extra input data should be allocated before calling this function.
 * - The allocated extra input should be passed to the ::cnnlPdistForward function
 *   to perform this operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlInitPdistForwardExtraInput(cnnlHandle_t handle,
                                                         const cnnlTensorDescriptor_t input_desc,
                                                         const cnnlTensorDescriptor_t output_desc,
                                                         void *extra_host_input);

// Group:Pdist_forward
/*!
 * @brief Computes the p-norm distance between each two row vectors in the input tensor \p input,
 *  and returns the result in the output tensor \p output.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the pdist_forward
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor \p input. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor \p input.
 * @param[in] extra_device_input
 *   Input. Pointer to the MLU memory that stores the extra input data.
 *   You need to copy the extra input data to MLU from the host that is
 *   initialized with ::cnnlInitPdistForwardExtraInput. For more information
 *   about extra input data, see Cambricon CNNL User Guide.
 * @param[in] extra_input_size
 *   Input. The size of the extra input data in bytes that needs to be used in
 *   the operation. You can get the size of the extra input data with
 *   the ::cnnlGetPdistForwardExtraInputSize function.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for this
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   this operation. You can get the size of the workspace with
 *   the ::cnnlGetPdistForwardWorkspaceSize function.
 * @param[in] p
 *   Input. A double value used as the p-norm.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor \p output. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor \p output.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "PdistForward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: float.
 *   - \p p: double.
 *   - output tensor:float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensors and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - The input tensor \p input is 2D and the output tensor \p output is 1D. If the shape of the input tensor
 *   \p input is represented as [N, M], while the shape of the output tensor \p output is [N * (N-1) / 2].
 *
 * @par Requirements
 * - None.
 *
 * @par Scale Limitation
 * - \p p must be not less than 0.
 * - \p p supports infinity but dost not support NaN.
 *
 * @par API Dependency
 * - None.
 *
 * @par Example
     @verbatim
       input: [[1.0, 2.0],
               [3.0, 5.0],
               [4.0, 7.0]]
       p: 1.0
       output: [5.0, 8.0, 3.0]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/1.9.0/generated/torch.nn.functional.pdist.html#torch.nn.functional.pdist
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlPdistForward(cnnlHandle_t handle,
                                           const cnnlTensorDescriptor_t input_desc,
                                           const void *input,
                                           const void *extra_device_input,
                                           size_t extra_input_size,
                                           const double p,
                                           void *workspace,
                                           size_t workspace_size,
                                           const cnnlTensorDescriptor_t output_desc,
                                           void *output);

// Group:Ceil
/*!
 * @brief Computes the ceil of each element (the smallest integer greater than or
 * equal to each element) in the input tensor \p x and returns the result in the output tensor \p y.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the ceil
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor \p x. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor \p y. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Ceil Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: Half, Float, BFloat16.
 *   - output tensor: Half, Float, BFloat16.
 *
 *   The BFloat16 data type is supported only on MLU500 series.
 *
 * @par Data Layout
 * - Data Layout of input tensor and output tensor should be the same.
 *
 * @note
 * - This operation is supported on MLU300 and MLU500 series.
 * - This operation is not supported on 1V.
 *
 * @par Requirements
 * - None.
 *
 * @par Scale Limitation
 * - The shape of input tensor and output tensor should be the same.
 * - The total number of dimensions of input tensor and output tensor should be the same.
 *
 * @par API Dependency
 * - None.
 *
 * @par Example
     @verbatim
       input:  [-2.1, -1.8, 9.125, 6.9, 4.0]
       output: [-2.0, -1.0, 10.0, 7.0, 4.0]
     @endverbatim
 *
 * @par Reference
 * - https://www.tensorflow.google.cn/api_docs/python/tf/math/ceil
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlCeil(cnnlHandle_t handle,
                                   const cnnlTensorDescriptor_t x_desc,
                                   const void *x,
                                   const cnnlTensorDescriptor_t y_desc,
                                   void *y);
// Group:RNN
/*!
 * @brief Retrieves the RNN operation information from \p rnn_desc that was previously created
 * with the ::cnnlCreateRNNDescriptor function and set by ::cnnlSetRNNDescriptor_v2 function.
 * The information includes the number of RNN cell implementation \p cell_mode, how to use bias
 * after recurrent GEMM in the RNN cell formulas, the recurrence pattern \p direction, features
 * in the input state \p input_size, features in the hidden state \p hidden_size, projection layer
 * size for the RNN projection operation, how the first recurrent layer processes the input
 * \p input_mode, number of recurrent layers \p num_layers. This function is used for training
 * mode only. To initialize RNN descriptor for inference mode, call ::cnnlSetRNNDescriptor.
 *
 * @param[in] rnn_desc
 *   Input. The descriptor of the RNN operation. For detailed information,
 *   see ::cnnlRNNDescriptor_t.
 * @param[out] cell_mode
 *   Output. Determine the RNN cell implementation in mode basic_RNN, LSTM, GRU.
 *   For detailed information, see ::cnnlRNNMode_t.
 * @param[out] bias_mode
 *   Output. Pointer to the host memory where the RNN bias mode should be saved.
 *   For detailed information, see ::cnnlRNNBiasMode_t.
 * @param[out] direction
 *   Output. Determine recurrence pattern. For detailed information, see ::cnnlDirectionMode_t.
 * @param[out] input_mode
 *   Output. Specify how the first stacked layer handles the input of the RNN mode.
 *   When the parameter \p input_mode is ::CNNL_RNN_SKIP_INPUT, no matrix formation operation is
 *   performed on the original input vectors.
 * @param[out] data_type
 *   Output. The data type of \p inputs, \p output and \p filter. See ::cnnlDataType_t.
 * @param[out] math_prec
 *   Output. The data type for computing all matrix multiplications in the RNN operation.
 * @param[out] input_size
 *   Output. The number of features in the input state input vector.
 * @param[out] hidden_size
 *   Output. The number of features in the hidden state input vector.
 * @param[out] proj_size
 *   Input. Specify output projection layer size on the RNN operation.
 * @param[out] layer_num
 *   Output. Number of stacked layers in the deep RNN mode.
 * @param[out] dropout_desc
 *   Input. The dropout rate of the dropout layer after RNN operation output.
 *   Dropout is used only in the training mode.
 * @param[out] padding_mode
 *   Output. Pointer to host memory where padding mode should be saved.
 *   For detailed information, see ::cnnlRNNPaddingMode_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - Output pointer \p cell_mode, \p bias_mode, \p direction, \p input_mode,
 *   \p data_type, \p input_size, \p hidden_size, \p proj_size, \p numLayers,
 *   should not be NULL.
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor function.
 */

cnnlStatus_t CNNL_WIN_API
cnnlGetRNNDescriptor_v2(const cnnlRNNDescriptor_t rnn_desc,
                        cnnlRNNMode_t *cell_mode,
                        cnnlRNNBiasMode_t *bias_mode,
                        cnnlDirectionMode_t *direction,
                        cnnlRNNInputMode_t *input_mode,
                        cnnlDataType_t *data_type,
                        cnnlDataType_t *math_prec,
                        int32_t *input_size,
                        int32_t *hidden_size,
                        int32_t *proj_size,
                        int32_t *layer_num,
                        void **dropout_desc,
                        cnnlRNNPaddingMode_t *padding_mode);
// Group:RNN
/*!
 * @brief Initializes the RNN descriptor \p rnn_desc that was previously created
 * with the ::cnnlCreateRNNDescriptor function. The information includes the number
 * of features in the hidden state \p hidden_size, the number of recurrent layers
 * \p num_layers, how the first recurrent layer processes the input \p input_mode,
 * the recurrence pattern \p direction, RNN cell implementation \p mode. This function
 * is used for training mode only. To initialize RNN descriptor for inference mode,
 * call ::cnnlSetRNNDescriptor.
 *
 * @param[in,out] rnn_desc
 *   Input/output. The descriptor of the RNN operation. For detailed information,
 *   see ::cnnlRNNDescriptor_t.
 * @param[in] cell_mode
 *   Input. Determine the RNN cell implementation in mode basic_RNN, LSTM and GRU.
 *   For detailed information, see ::cnnlRNNMode_t.
 * @param[in] bias_mode
 *   Input. Pointer to the host memory where the RNN bias mode should be saved.
 *   For detailed information, see ::cnnlRNNBiasMode_t.
 * @param[in] direction
 *   Input. Determine recurrence pattern. For detailed information, see ::cnnlDirectionMode_t.
 * @param[in] input_mode
 *   Input. Specify how the first stacked layer handles the input of the RNN mode.
 *   When the parameter \p input_mode is ::CNNL_RNN_SKIP_INPUT, no matrix formation operation is
 *   performed on the original input vectors.
 * @param[in] data_type
 *   Input. The data type of \p inputs, \p output and \p filter. See ::cnnlDataType_t.
 * @param[in] math_prec
 *   Input. The data type for computing all matrix multiplications in the RNN operation.
 * @param[in] input_size
 *   Input. The number of features in the input state input vector.
 * @param[in] hidden_size
 *   Input. The number of features in the hidden state input vector.
 * @param[in] proj_size
 *   Input. Specify output projection layer size on the RNN operation.
 * @param[in] layer_num
 *   Input. Number of stacked layers in the deep RNN mode.
 * @param[in] dropout_desc
 *   Input. The dropout rate of the dropout layer can be applied between
 *   RNN physical layers.Dropout is used only in the training mode.
 * @param[in] padding_mode
 *   input. Pointer to host memory where padding mode should be saved.
 *   For detailed information, see ::cnnlRNNPaddingMode_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor function.
 *
 * @note
 * - No dropout can be applied if the RNN network only has one layer.
 * - Currently, \p cell_mode only supports ::CNNL_LSTM. \p input_mode only supports
 *   ::CNNL_RNN_LINEAR_INPUT. \p dropout_desc only supports NULL. The padding mode
 *   is not supported currently.
 */

cnnlStatus_t CNNL_WIN_API
cnnlSetRNNDescriptor_v2(cnnlRNNDescriptor_t rnn_desc,
                        cnnlRNNMode_t cell_mode,
                        cnnlRNNBiasMode_t bias_mode,
                        cnnlDirectionMode_t direction,
                        cnnlRNNInputMode_t input_mode,
                        cnnlDataType_t data_type,
                        cnnlDataType_t math_prec,
                        int input_size,
                        int hidden_size,
                        int proj_size,
                        int layer_num,
                        void *dropout_desc,
                        cnnlRNNPaddingMode_t padding_mode);
// Group:RNN
/*!
 * @brief Returns in \p workspace_size and \p reserve_size the size of the MLU memory
 * that is used as an extra workspace to optimize the RNN operation, and pass the
 * intermediate results between RNN backward operations respectively.
 *
 * For more information about the extra \p workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the RNN operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] rnn_desc
 *   Input. The descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input sequence data.
 *   For detailed information, see ::cnnlSeqDataDescriptor_t.
 * @param[out] workspace_size
 *   Output. The size of the extra workspace in bytes that needs to be used in
 *   the RNN operation.
 * @param[out] reservespace_size
 *   Output. The size of the reservespace in bytes that needs to be used in
 *   the RNN operation. The reservespace is used to pass intermediate results.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function,
 *   you need to call ::cnnlCreateRNNDescriptor, ::cnnlSetRNNDescriptor_v2 functions to create and
 *   set the RNN operation descriptor,
 *   and call the ::cnnlCreateSeqDataDescriptor and ::cnnlSetSeqDataDescriptor functions
 *   to create and set the tensor descriptors \p x_desc.
 * - The allocated extra workspace memory should be passed to the ::cnnlRNNForwardTraining function,
 *   ::cnnlRNNBackwardData function and ::cnnlRNNBackwardWeights function to perform the RNN
 *   forward or backward operation.
 *
 * @note
 * - For the RNN descriptor, in addition to the basic information set by calling the function
 *   ::cnnlSetRNNDescriptor_v2, if you want to run the optional variant mode, you need to call the
 *   corresponding function to set it, such as: ::cnnlSetRNNMaskMode, ::cnnlSetRNNClip.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */

cnnlStatus_t CNNL_WIN_API
cnnlGetRNNTempSizes(cnnlHandle_t handle,
                    const cnnlRNNDescriptor_t rnn_desc,
                    const cnnlSeqDataDescriptor_t x_desc,
                    size_t *workspace_size,
                    size_t *reservespace_size);
// Group:RNN
/*!
 * @brief Returns in \p weightpace_size, the size of MLU memory that is used to
 * store RNN filter and bias.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the RNN operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] rnn_desc
 *   Input. The descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[out] weightspace_size
 *   output. The minimum size in bytes of MLU memory needed for RNN trainable
 *   parameters in the RNN operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function,
 *   you need to call ::cnnlCreateRNNDescriptor, ::cnnlSetRNNDescriptor_v2 functions
 *   to create and set the RNN operation descriptor,
 *   and call the ::cnnlCreateSeqDataDescriptor and ::cnnlSetSeqDataDescriptor functions
 *   to create and set the tensor descriptors \p x_desc.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetRNNWeightSpaceSize(cnnlHandle_t handle,
                          const cnnlRNNDescriptor_t rnn_desc,
                          size_t *weightspace_size);
// Group:RNN
/*!
 * @brief Gets the starting address and shape information of every RNN filter and bias
 * in each \p pseudo_layer within the RNN network.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the RNN operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] rnn_desc
 *   Input.The descriptor of the RNN operation. For detailed information,
 *   see ::cnnlRNNDescriptor_t.
 * @param[in] pseudo_layer
 *   Input. When \p direction is ::CNNL_RNN_UNIDIRECTIONAL, pseudo_layer is
 *   the same as physical layer. When \p direction is ::CNNL_RNN_BIDIRECTIONAL,
 *   the number of pseudo_layers is twice that of physical_layers.
 * - When \p direction is ::CNNL_RNN_UNIDIRECTIONAL:
 *   - If pseudo_layer = 0, this layer is RNN input layer.
 *   - If pseudo_layer = 1, this layer is RNN first hidden layer.
 * - When \p direction is ::CNNL_RNN_BIDIRECTIONAL:
 *   - pseudo_layer = 0, this layer is a sub-layer of input forward layer.
 *   - pseudo_layer = 1, this layer is a sub-layer of input backward layer.
 *   - pseudo_layer = 2, this layer is a sub-layer of first hidden forward layer.
 *   - pseudo_layer = 3, this layer is a sub-layer of first hidden backward layer.
 *   and so on.
 * @param[in] filterspace_size
 *   Input. Specifies the size of the buffer in bytes that stores filter. You can
 *   call ::cnnlGetRNNWeightSpaceSize to get the size of the buffer to be used.
 * @param[in] filter_space
 *   Input. Pointer to the MLU memory that stores filter and bias.
 * @param[in] lin_layer_id
 *   Input. The filter or bias linear id index.
 * - For \p rnn_mode only supports ::CNNL_LSTM, The supported values of lin_layer_id are
 *   as follows:
 *   - value 0, 1, 2 and 3 reference filter or bias used in computing with input tensor.
 *   - value 4, 5, 6 and 7 reference filter or bias used in computing with hidden state tensor.
 *   - value 8 corresponds to the projection matrix, but there is no bias in RNN projection
 *   operation.
 *   - value 0 and 4 correspond to the input gate.
 *   - value 1 and 5 correspond to the forget gate.
 *   - value 2 and 6 correspond to the update gate.
 *   - value 3 and 7 correspond to the output gate.
 * @param[out] m_desc
 *   Input. The descriptor of filter data. The shape of this filter is returned in
 *   this descriptor in the format of {hidden_size, input_size, 1, 1}. If the filter
 *   of the specified pseudo_layer and lin_layer_id does not exist, the return dim is 0.
 * @param[out] m_addr
 *   Output. Pointer to starting address of the MLU memory that stores filter data. If
 *   the filter of the specified pseudo_layer and lin_layer_id does not exist, the return
 *   address is NULL.
 * @param[out] b_desc
 *   Input. The descriptor of bias data. The shape of this bias is returned in
 *   this descriptor in the format of {hidden_size, 1, 1, 1}. If the bias of the specified
 *   pseudo_layer and lin_layer_id does not exist, the return dim is 0.
 * @param[out] b_addr
 *   Output. Pointer to starting address of the MLU memory that stores bias data. If
 *   the bias of the specified pseudo_layer and lin_layer_id does not exist, the return address
 *   is NULL.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function to implement RNN, you need to prepare
 *   all the parameters passed to this function. You need to call
 *   ::cnnlCreateRNNDescriptor, ::cnnlSetRNNDescriptor_v2 functions to create and
 *   set the RNN operation descriptor.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetRNNWeightParams(cnnlHandle_t handle,
                       const cnnlRNNDescriptor_t rnn_desc,
                       int32_t pseudo_layer,
                       size_t filterspace_size,
                       const void *filter_space,
                       int32_t lin_layer_id,
                       cnnlTensorDescriptor_t m_desc,
                       void **m_addr,
                       cnnlTensorDescriptor_t b_desc,
                       void **b_addr);

// Group:FusedDropout
/*!
 *
 * @brief Randomly set the value of input elements to zero and returns the processed
 * output and the state of each element.
 * @deprecated
 * This function is deprecated and will be removed in future release.
 * Use ::cnnlFusedDropout_v3 instead, which supports \p philox mode only.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the fused_dropout operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] p
 *   Input. A float value used as the probability that the value of input element is
 *   set to zero, \p p must be in [0, 1].
 * @param[in] seed
 *   Input. An unsigned int pointer pointing to the random seed.
 * @param[out] mask_desc
 *   Output. The descriptor of the mask tensor. The element of mask tensor is the
 *   state of the element of input tensor after deactivation. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] mask
 *   Output. Pointer to the MLU memory that stores the mask tensor.
 * @param[out] output_desc
 *   Output. The descriptor of the output tensor. The value of the output element is
 *   the value of the input element is randomly zeroed. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Data Type
 * - Data Type of input tensor \p input, output tensor \p output must be the same.
 * - The supported data types of \p input, \p mask and \p output are as follows:
 *   - \p input: int8, uint8, int16, int32, half, float.
 *   - \p mask: uint8.
 *   - \p output: int8, uint8, int16, int32, half, float.
 * @par Data Layout
 * - The supported data layouts of \p input, \p mask and \p output tensor are as follows:
 *   - input tensor: \p CNNL_LAYOUT_ARRAY.
 *   - mask tensor: \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_ARRAY.
 * @par Scale Limitation
 * - The input tensor, mask tensor and output tensor must have the same shapes.
 * @par Requirements
 * - None.
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
      input: an array [0, 1, 2, 3, ...]
      p: 0.5.
      mask: an array [0, 1, 0, 1, ...]
      output: an array [0, 2, 0, 6, ...]
     @endverbatim
 *
 * @note
 * - The int8, int16 and int32 data types of input are deprecated and will be removed in future
 *   release.
 */
CNNL_DEPRECATED_FOR(cnnlFusedDropout_v3)
cnnlStatus_t CNNL_WIN_API
cnnlFusedDropout(cnnlHandle_t handle,
                 const cnnlTensorDescriptor_t input_desc,
                 const void *input,
                 const float p,
                 uint32_t *seed,
                 const cnnlTensorDescriptor_t mask_desc,
                 const void *mask,
                 const cnnlTensorDescriptor_t output_desc,
                 void *output);

// Group:FusedDropout
/*!
 * @brief Randomly set the value of input elements to zero and returns the processed
 * output and the state of each element. On the basis of the cnnlFusedDropout,
 * cnnlFusedDropout_v2 adds generator to determine the mode of generating random
 * number and uses state instead of seed to generate random numbersequences.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlFusedDropout_v3 instead, which supports
 *   \p philox mode only.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the fused_dropout operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] generator
 *   Input. The descriptor of random \p generator. For detailed information,
 *   see ::cnnlRandGenerator_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] p
 *   Input. A float value used as the probability that the value of input element is
 *   set to zero, \p p must be in [0, 1].
 * @param[in,out] state
 *   Input and output. Pointer to device state data, which is used to generate random sequence.
 * Set NULL if you use \p CNNL_RAND_RNG_FAST \p generator type.
 * @param[out] mask_desc
 *   Output. The descriptor of the mask tensor. The element of mask tensor is the
 *   state of the element of input tensor after deactivation. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] mask
 *   Output. Pointer to the MLU memory that stores the mask tensor.
 * @param[out] output_desc
 *   Output. The descriptor of the output tensor. The value of the output element is
 *   the value of the input element is randomly zeroed. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Data Type
 * - Data Type of input tensor \p input, output tensor \p output must be the same.
 * - The supported data types of \p input, \p mask and \p output are as follows:
 *   - \p input: int8, uint8, int16, int32, half, float, bfloat16. Bfloat16 is
 *   supported only on MLU500 series.
 *   - \p mask: uint8, bool.
 *   - \p output: int8, uint8, int16, int32, half, float, bfloat16. Bfloat16 is
 *   supported only on MLU500 series.
 * @par Data Layout
 * - The supported data layouts of \p input, \p mask and \p output tensor are as follows:
 *   - input tensor: \p CNNL_LAYOUT_ARRAY.
 *   - mask tensor: \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_ARRAY.
 * @par Scale Limitation
 * - The input tensor, mask tensor and output tensor must have the same shapes.
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlRandCreateGenerator function.
 *   Additionally, if using ::CNNL_RAND_RNG_MTGP32 random generator type, you also need to
 *   call the ::cnnlRandGetMTGP32StateSize function and the ::cnnlRandMakeMTGP32KernelState
 *   function before calling this function.
 *
 * @par Requirements
 * - None.
 * @par Example
 * - The example of this operation is as follows:
    @verbatim
     input: an array [0, 1, 2, 3, ...]
     p: 0.5.
     mask: an array [0, 1, 0, 1, ...]
     output: an array [0, 2, 0, 6, ...]
    @endverbatim
 * @note
 * - On MLU200 series, users should generate random numbers with the \p CNNL_RAND_RNG_FAST
 *   generator type; on MLU300 series, users should generate random numbers with
 *   the \p CNNL_RAND_RNG_MTGP32 generator type, otherwise, the performance will be very poor.
 * - The int8, int16 and int32 data types of input are deprecated and will be deprecated in future
 *   release.
 */
CNNL_DEPRECATED_FOR(cnnlFusedDropout_v3)
cnnlStatus_t CNNL_WIN_API
cnnlFusedDropout_v2(cnnlHandle_t handle,
                    const cnnlRandGenerator_t generator,
                    const cnnlTensorDescriptor_t input_desc,
                    const void *input,
                    const float p,
                    void *state,
                    const cnnlTensorDescriptor_t mask_desc,
                    const void *mask,
                    const cnnlTensorDescriptor_t output_desc,
                    void *output);

// Group:FusedDropout
/*!
 * @brief Randomly sets the value of input elements to zero and returns the processed
 * output and the state of each element.
 *
 * It only supports \p philox mode. To use other modes, call
 * ::cnnlFusedDropout_v3.
 * The Philox algorithm has three parameters: seed, offset and subsequence. Seed specifies the key
 * in the algorithm, and offset and subsequence specify the counter in the algorithm. On MLU devices
 * , we use index [0, THREAD_NUM) to bind to subsequence, so only seed and offset arguments are needed.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the fused_dropout operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] rng_type
 *   Input. The rng type of random generator. Only support \p CNNL_RAND_RNG_PHILOX.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] captured
 *   Input. If \p captured is true, use \p seed_ptr, \p offset_ptr and \p offset_intragraph to specify
 *   seed and offset. If \p captured is false, use \p seed and \p offset.
 * @param[in] seed
 *   Input. Specify the seed of the Philox algorithm when \p captured is false.
 * @param[in] offset
 *   Input. Specify the offset of the Philox algorithm when \p captured is false.
 * @param[in] seed_ptr
 *   Input. Pointer to device seed data. Specify the seed of the Philox algorithm when \p captured is true.
 * @param[in] offset_ptr
 *   Input. Pointer to device offset data. Specify the offset of the Philox algorithm when \p captured is true.
 * @param[in] offset_intragraph
 *   Input. The offset in a graph. Specify the offset of the Philox algorithm when \p captured is true.
 * @param[in] p
 *   Input. A float value used as the probability that the value of input element is
 *   set to zero, and \p p must be in [0, 1].
 * @param[in] mask_desc
 *   Input. The descriptor of the mask tensor. The element of mask tensor is the
 *   state of the element of input tensor after deactivation. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] mask
 *   Output. Pointer to the MLU memory that stores the mask tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to device output data, which is the random sequence.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Data Type
 * - Data Type of input tensor \p input, output tensor \p output must be the same.
 * - The supported data types of \p input, \p mask and \p output are as follows:
 *   - \p input: int8, uint8, int16, int32, half, float, bfloat16.
 *   - \p mask: uint8, bool.
 *   - \p output: int8, uint8, int16, int32, half, float, bfloat16.
 *   The bfloat16 data type is supported only on MLU500 series.
 *
 * @par Data Layout
 * - The supported data layouts of \p input, \p mask and \p output tensor are as follows:
 *   - input tensor: \p CNNL_LAYOUT_ARRAY.
 *   - mask tensor: \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The input tensor, mask tensor and output tensor must have the same shapes.
 *
 * @par API Dependency
 * - None.
 *
 * @par Requirements
 * - None.
*
 * @par Example
 * - The example of this operation is as follows:
    @verbatim
     input: an array [0, 1, 2, 3, ...]
     p: 0.5.
     mask: an array [0, 1, 0, 1, ...]
     output: an array [0, 2, 0, 6, ...]
    @endverbatim

 * @note
 * - The int8, int16 and int32 data types of input are deprecated and will be removed in future
 *   release.
 */
cnnlStatus_t CNNL_WIN_API
cnnlFusedDropout_v3(cnnlHandle_t handle,
                    cnnlRandRngType_t rng_type,
                    const cnnlTensorDescriptor_t input_desc,
                    const void *input,
                    const bool captured,
                    const uint64_t seed,
                    const uint64_t offset,
                    const int64_t *seed_ptr,
                    const int64_t *offset_ptr,
                    const uint32_t offset_intragraph,
                    const float p,
                    const cnnlTensorDescriptor_t mask_desc,
                    void *mask,
                    const cnnlTensorDescriptor_t output_desc,
                    void *output);

// Group:RNN
/*!
 * @brief Computes the first-order derivatives of filter and bias of RNN network in the training
 *        process. The specific network structure is determined by the descriptor \p rnn_desc
 *        set by the user. Using the input data \p x, \p hx, \p y, \p filterspace and \p reservespace
 *        according to the specific network structure, writes the calculation result into the \p dfilterspace.
 *
 * This function requires two additional MLU memory as the \p reservespace and the \p workspace to
 * improve the RNN network performance. You can get the size of the \p workspace \p workspace_size
 * and \p reservespace \p reservespace_size with the ::cnnlGetRNNTempSizes function, and the size
 * of the \p weightspace \p weightspace_size with the ::cnnlGetRNNWeightSpaceSize function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the RNN operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] rnn_desc
 *   Input. The descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[in] add_grad
 *   Input. The filter gradient mode, see ::cnnlWgradMode_t for more details.
 * @param[in] dev_seq_lengths
 *   Input. A copy of \p seqLengthArray set in \p x_desc or \p y_desc RNN data descriptor.
 *   The dev_seq_lengths array must be stored in MLU memory.
 * @param[in] x_desc
 *   Input. The descriptor of input sequence data.
 *   For detailed information, see ::cnnlSeqDataDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores input sequence data.
 * @param[in] hx_desc
 *   Input. The descriptor of hidden state tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] hx
 *   Input. Pointer to the MLU memory that stores hidden state tensor.
 * @param[in] y_desc
 *   Input. The descriptor of output sequence data.
 *   For detailed information, see ::cnnlSeqDataDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores output sequence data.
 * @param[out] dfilterspace
 *   Output. Pointer to the MLU memory that stores gradient deltas filter and bias.
 * @param[in] filterspace_size
 *   Input. Specifies the size of the buffer in bytes that stores filter.
 *   You can call ::cnnlGetRNNWeightSpaceSize to get the size of the buffer to be used.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   RNN operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the RNN operation.
 *   You can get the size of the workspace with the ::cnnlGetRNNTempSizes function.
 * @param[in] reservespace
 *   Input. Pointer to the MLU memory that is used as an extra memory space for
 *   saving intermediate results of RNN operation.
 *   RNN operation.
 * @param[in] reservespace_size
 *   Input. The size of the extra reservespace in bytes that needs to be used in
 *   the RNN operation.
 *   You can get the size of the reservespace with the ::cnnlGetRNNTempSizes function.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "RNN Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layouts set in descriptors are as follows:
 *   - \p x_desc must be set to ::CNNL_SEQDATA_TNC, ::CNNL_SEQDATA_NTC or ::CNNL_SEQDATA_TNC_PACKED.
 *   - \p hx_desc must be set to ::CNNL_LAYOUT_ARRAY and \p dimNb in \p hx_desc must be 3.
 *   - \p y_desc must be set to be the same layout as \p x_desc.
 *
 * @par Scale Limitation
 * - The \p rnn_mode of \p rnn_desc only supports ::CNNL_LSTM with projection layer.
 * - The \p input_mode of \p rnn_desc only supports ::CNNL_RNN_LINEAR_INPUT.
 * - The \p padding_mode of \p rnn_desc only supports ::CNNL_RNN_PADDED_IO_DISABLED.
 * - The \p math_prec must be int16 on CE3226 and MLU200 series and it must be the same as
 *   \p data_type or int16 on MLU300 series.
 * - The \p dev_seq_lengths must be batch's sequence and descending order and the length of
 *   the \p dev_seq_lengths must be equal to x_desc->dims[0] when the \p cnnlSeqDataLayout_t
 *   is CNNL_SEQDATA_TNC_PACKED.
 *
 * @par API Dependency
 * - Before calling this function to implement RNN, you need to prepare
 *   all the parameters passed to this function. See each parameter description
 *   for details.::cnnlRNNBackwardWeights function should be invoked after
 *   ::cnnlRNNBackwardData.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, set the layout of the input
 *   sequence data \p x_desc and output sequence data \p y_desc to ::CNNL_SEQDATA_TNC.
 *
 * @note
 * - The first two dimensions of \p x_desc must be equal to those of \p y_desc.
 * - \p add_grad only supports ::CNNL_WGRAD_MODE_SET.
 */

cnnlStatus_t CNNL_WIN_API
cnnlRNNBackwardWeights(cnnlHandle_t handle,
                       const cnnlRNNDescriptor_t rnn_desc,
                       cnnlWgradMode_t add_grad,
                       const int32_t dev_seq_lengths[],
                       const cnnlSeqDataDescriptor_t x_desc,
                       const void *x,
                       const cnnlTensorDescriptor_t hx_desc,
                       const void *hx,
                       const cnnlSeqDataDescriptor_t y_desc,
                       const void *y,
                       void *dfilterspace,
                       size_t filterspace_size,
                       void *workspace,
                       size_t workspace_size,
                       void *reservespace,
                       size_t reservespace_size);

// Group:FocalLossSigmoidForward
/*!
 * @brief Computes cross entropy loss with weighting factor and focusing factor to reduce the
 *        filter of samples which are easy to classify.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in ::cnnlFocalLossSigmoidForward. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. An enum to indicate the algorithm used to compute the output.
 *   For detailed information, see ::cnnlComputationPreference_t. Only supports
 *   ::CNNL_COMPUTATION_HIGH_PRECISION currently.
 * @param[in] reduction
 *   Input. An enum to indicate the reduction mode used to compute the operation.
 *   For detailed information, see ::cnnlLossReduction_t. Only supports
 *   \p CNNL_LOSS_REDUCTION_NONE currently.
 * @param[in] input_desc
 *   Input. The descriptor of input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] target_desc
 *   Input. The descriptor of target tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] target
 *   Input. Pointer to the MLU memory that stores the target tensor which is the target
 *   of input.
 * @param[in] filter_desc
 *   Input. The descriptor of filter tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the filter tensor, which is the filter
 *   value of input.
 * @param[in] alpha
 *   Input. A float value that is the filtering factor of the focal loss sigmoid forward.
 * @param[in] gamma
 *   Input. A float value that is the focusing factor of the focal loss sigmoid forward.
 * @param[in] output_desc
 *   in. The descriptor of output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Focal Loss Sigmoid Forward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for
 *   \p input tensor, \p target tensor, \p filter tensor and \p output tensor:
 *   - input: half, float.
 *   - target: int32.
 *   - filter: half, float.
 *   - output: half, float.
 *
 * @par Scale Limitation
 * - The shape of \p input must be [N, C].
 * - The shape of \p input and \p output must be equal.
 * - The shape of \p target is [N] when the shape of \p input is [N, C].
 * - The shape of \p filter is [C] when the shape of \p input is [N, C].
 * - \p input value should be in range of [-20, 20] when the data type of \p input is float.
 * - \p input value should be in range of [-5, 5] when the data type of \p input is half.
 * - \p target value should be in range of [0, C] when \p filter is NULL and the shape of
 *   \p input is [N, C].
 * - \p target value should be in range of [0, C-1] when \p filter is not NULL and the
 *   shape of \p input is [N, C].
 * - \p gamma should be greater than or equal to 0.
 *
 * @note
 * - When the input data or parameter contains NaN or infinity:
 *   - On MLU200 series:
 *     - If \p input, \p filter, \p alpha or \p gamma is NaN, then \p output is finite value.
 *     - If \p input, \p filter or \p alpha is infinity, but \p gamma is finite value,
 *       then \p output is finite value.
 *     - If \p gamma is positive infinity, but \p input, \p filter and \p alpha are finite value,
 *       then \p output is finite value.
 *   - On MLU300 series and CE3226:
 *     - If \p input is infinity, but \p filter, \p alpha and \p gamma are finite value,
 *       then \p output is NaN or finite value.
 *     - If \p filter is positive infinity, but \p input, \p alpha and \p gamma are finite value,
 *       then \p output is NAN or positive infinity.
 *     - If \p filter is negative infinity, but \p input, \p alpha and \p gamma are finite value,
 *       then \p output is NAN or negative infinity.
 *     - If \p alpha is infinity and data type of \p input is float,
 *       but \p input, \p filter and \p gamma are finite value,
 *       then \p output is NAN or infinity.
 *     - If \p alpha is infinity and data type of \p input is half,
 *       but \p input, \p filter and \p gamma are finite value,
 *       then \p output is NAN or finite value.
 *     - If \p gamma is positive infinity, but \p input, \p filter and \p alpha are finite value,
 *       then \p output is NAN or 0.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Dollar. Proceedings of the IEEE
 *   International Conference on Computer Vision(ICCV), 2017, oo.2980-2988.
 * - http://github.com/open-mmlab/mmcv/blob/master/mmcv/ops/focal_loss.py
 *
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlFocalLossSigmoidForward(cnnlHandle_t handle,
                                                      const cnnlComputationPreference_t prefer,
                                                      const cnnlLossReduction_t reduction,
                                                      const cnnlTensorDescriptor_t input_desc,
                                                      const void *input,
                                                      const cnnlTensorDescriptor_t target_desc,
                                                      const void *target,
                                                      const cnnlTensorDescriptor_t filter_desc,
                                                      const void *filter,
                                                      const float alpha,
                                                      const float gamma,
                                                      const cnnlTensorDescriptor_t output_desc,
                                                      void *output);

// Group:FocalLossSigmoidBackward
/*!
 *  @brief Computes the gradients of ::cnnlFocalLossSigmoidBackward with \p input tensor,
 *  \p target tensor, \p filter tensor, \p grad_output tensor, and returns the results in the \p grad_input tensor.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in ::cnnlFocalLossSigmoidBackward. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. An enum to indicate the algorithm used to compute the output.
 *   For detailed information, see ::cnnlComputationPreference_t. Only supports
 *   ::CNNL_COMPUTATION_FAST currently.
 * @param[in] reduction
 *   Input. An enum to indicate the reduction mode used to compute the operation.
 *   For detailed information, see ::cnnlLossReduction_t. Only supports
 *   ::CNNL_LOSS_REDUCTION_NONE currently.
 * @param[in] input_desc
 *   Input. The descriptor of input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] target_desc
 *   Input. The descriptor of target tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] target
 *   Input. Pointer to the MLU memory that stores the target tensor which is the target
 *   of input.
 * @param[in] filter_desc
 *   Input. The descriptor of filter tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the filter tensor, which is the filter
 *   value of input.
 * @param[in] grad_output_desc
 *   Input. The descriptor of \p grad_output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] grad_output
 *    Input. Pointer to the MLU memory that stores the gradient tensor.
 * @param[in] alpha
 *   Input. A float value that is the filtering factor of the focal loss sigmoid backward.
 * @param[in] gamma
 *   Input. A float value that is the focusing factor of the focal loss sigmoid backward.
 * @param[in] grad_input_desc
 *   Input. The descriptor of \p grad_input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] grad_input
 *   Output. Pointer to the MLU memory that stores the \p grad_input tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED.
 *
 * @par Formula
 * - See "Focal Loss Sigmoid Backward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for
 *   \p input tensor, \p target tensor, \p filter tensor and \p output tensor:
 *   - input: float.
 *   - target: int32.
 *   - filter: float.
 *   - grad_input: float.
 *   - grad_output: float.
 *
 * @par Scale Limitation
 * - The shape of \p input must be [N, C].
 * - The shape of \p input and \p grad_output must be consistent.
 * - The shape of \p input and \p grad_input must be consistent.
 * - The shape of \p target is [N] when the shape of \p input is [N, C].
 * - The shape of \p filter is [C] when the shape of \p input is [N, C].
 * - \p target value should be in range of [0, C] when \p filter is NULL and the shape of
 *   \p input is [N, C].
 * - \p target value should be in range of [0, C-1] when \p filter is not NULL and the
 *   shape of \p input is [N, C].
 * - prefer only supports ::CNNL_COMPUTATION_FAST currently.
 * - reduction only supports CNNL_LOSS_REDUCTION_NONE currently.
 * - The layout of \p input, \p target, \p filter, \p grad_output and \p grad_input must be ARRAY.
 *
 * @note
 * - If the shape of \p input is set to [N, C]. The length of C should be in range of [0, 13615] when
 *   \p filter is NULL on MLU300 series. The length of C should be in range of [0, 12544] when
 *   \p filter is not NULL on MLU300 series.
 * - If the shape of \p input is set to [N, C]. The length of C should be in range of [0, 8154] when
 *   \p filter is NULL on MLU200 series. The length of C should be in range of [0, 7520] when
 *   \p filter is not NULL on MLU200 series.
 * - \p filter does not support positive infinity and negative infinity currently.
 * - \p grad_output does not support positive infinity and negative infinity currently.
 * - \p gamma should be in range of [0, 8] on MLU200 series, and should be in range of [0, 10000]
 *   on MLU300 series.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - http://github.com/open-mmlab/mmcv/blob/master/mmcv/ops/focal_loss.py
 *
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlFocalLossSigmoidBackward(cnnlHandle_t handle,
                             const cnnlComputationPreference_t prefer,
                             const cnnlLossReduction_t reduction,
                             const cnnlTensorDescriptor_t input_desc,
                             const void *input,
                             const cnnlTensorDescriptor_t target_desc,
                             const void *target,
                             const cnnlTensorDescriptor_t filter_desc,
                             const void *filter,
                             const cnnlTensorDescriptor_t grad_output_desc,
                             const void *grad_output,
                             const float alpha,
                             const float gamma,
                             const cnnlTensorDescriptor_t grad_input_desc,
                             void *grad_input);

// Group:Log1p
/*!
 * @brief Computes the natural logarithm of the \p input plus one, element-wise.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlLog1p_v2 instead, which removes the \p prefer parameter.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the log1p
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. The \p prefer mode defined in ::cnnlComputationPreference_t enum.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \p x.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor \p y.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH,
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Log1p Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, bfloat16, float.
 *   - output tensor: half, bfloat16, float.
 *
 *   bfloat16 is only supported on MLU500 series.
 *
 * @par Data Layout
 * - None.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor should have the same shape, and the input data must meet
 *   the following input data range:
 *   - If log1p works on on MLU300 and higher platforms:
 *     - float: [-0.99999, 0][1e-20, 6e10].
 *     - half:
 *        - ::CNNL_COMPUTATION_HIGH_PRECISION: [-0.999, 0][1e-20, 6e4].
 *        - ::CNNL_COMPUTATION_FAST: [-0.999, 0][1e-16, 6e4].
 *
 * @note
 * - ::CNNL_COMPUTATION_HIGH_PRECISION mode or ::CNNL_COMPUTATION_FAST mode is only for half.
 * - This operation is not supported on the 1V platforms.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://tensorflow.google.cn/api_docs/python/tf/math/log1p
 */
CNNL_DEPRECATED_FOR(cnnlLog1p_v2)
cnnlStatus_t CNNL_WIN_API cnnlLog1p(cnnlHandle_t handle,
                                    const cnnlComputationPreference_t prefer,
                                    const cnnlTensorDescriptor_t x_desc,
                                    const void *x,
                                    const cnnlTensorDescriptor_t y_desc,
                                    void *y);

// Group:Log1p
/*!
 * @brief Computes the natural logarithm of the \p input plus one, element-wise.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 * log1p operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 * ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \p x.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 * ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor \p y.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH,
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Log1p Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, bfloat16, float.
 *   - output tensor: half, bfloat16, float.
 *
 *   bfloat16 is only supported on MLU500 series.
 *
 * @par Data Layout
 * - None.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor should have the same shape, and the input data must meet
 *   the following input data ranges:
 *   - If log1p works on on MLU300 and higher platforms:
 *     - float: [-0.99999, 0][1e-20, 6e10].
 *     - bfloat16: [-0.99, 0][1e-20, 6e10].
 *     - half: [-0.999, 0][1e-16, 6e4].
 *
 * @note
 * - This operation is not supported on the 1V platforms.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://tensorflow.google.cn/api_docs/python/tf/math/log1p
 */
cnnlStatus_t CNNL_WIN_API cnnlLog1p_v2(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t x_desc,
                                       const void *x,
                                       const cnnlTensorDescriptor_t y_desc,
                                       void *y);

// Group:GatherTree
/*!
 * @brief Calculates the full beams from the \p step_ids tensor and \p parent_ids tensor.
 *   If the value of parent id is out of range [0, beam_width), a \p -1 is stored
 *   in the corresponding output value and the execution for that beam returns early.
 *   For a given beam, past the time step containing the first decoded \p end_token
 *   all values are filled in with \p end_token.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.

 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in ::cnnlGatherTree. For detailed information, see ::cnnlHandle_t.
 * @param[in] step_ids_desc
 *   Input. The descriptor \p of step_ids tensor. For detailed information,
 *   see ::cnnlSeqDataDescriptor_t.
 * @param[in] step_ids
 *   Input. Pointer to the MLU memory that stores the \p step_ids tensor which is
 *   the predicted token IDs.
 * @param[in] parent_ids_desc
 *   Input. The descriptor of \p parent_ids tensor. For detailed information,
 *   see ::cnnlSeqDataDescriptor_t.
 * @param[in] parent_ids
 *   Input. Pointer to the MLU memory that stores the \p parent_ids tensor which is
 *   the parent beam indices.
 * @param[in] max_seq_len_desc
 *   Input. The descriptor of \p max_sequence_lengths tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] max_sequence_lengths
 *   Input. Pointer to the MLU memory that stores the \p max_sequence_lengths tensor
 *   which is the maximum sequence length of each batch.
 * @param[in] end_token
 *   Input. An int32 value that is end token ID.
 * @param[in] beams_desc
 *   Input. The descriptor of \p beams tensor. For detailed information,
 *   see ::cnnlSeqDataDescriptor_t.
 * @param[out] beams
 *   Output. Pointer to the MLU memory that stores the \p beams tensor which is
 *   the reorderd token IDs based on \p parent_ids.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED.
 *
 * @par Formula
 * - See "Gather Tree Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for
 *   \p step_ids tensor, \p parent_ids tensor, \p max_sequence_lengths tensor and \p beams tensor:
 *   - step_ids: int32.
 *   - parent_ids: int32.
 *   - max_sequence_lengths: int32.
 *   - beams: int32.
 *
 * @par Data Layout
 *   The supported data layouts are as follows:
 *   - \p step_ids_desc must be set to ::CNNL_SEQDATA_TNC.
 *   - \p parent_ids_desc and \p beams_desc must be set to be the same layout as \p step_ids.
 *   - \p max_seq_len_desc must be set to ::CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The shape of \p step_ids must be [max_time, batch_size, beam_width].
 * - The shape of \p parent_ids must be [max_time, batch_size, beam_width].
 * - The shape of \p beams must be [max_time, batch_size, beam_width].
 * - The shape of \p max_sequence_lengths must be [batch_size]
 * - The value of max_time * beam_width should be in range of [0, 32746] on MLU200 series.
 *   The value of max_time * beam_width should be in range of [0, 54591] on MLU300 series.
 *
 * @note
 * - The shape of \p step_ids and \p parent_ids must be consistent.
 * - The shape of \p step_ids and \p beams must be consistent.
 * - This operation is not supported on the 1V platforms.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - http://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/contrib/seq2seq/kernels/beam_search_ops.cc
 * - http://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/contrib/seq2seq/kernels/beam_search_ops_gpu.cu.cc
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlGatherTree(cnnlHandle_t handle,
                                         const cnnlSeqDataDescriptor_t step_ids_desc,
                                         const void *step_ids,
                                         const cnnlSeqDataDescriptor_t parent_ids_desc,
                                         const void *parent_ids,
                                         const cnnlTensorDescriptor_t max_seq_len_desc,
                                         const void *max_sequence_lengths,
                                         const int32_t end_token,
                                         const cnnlSeqDataDescriptor_t beams_desc,
                                         void *beams);

// Group:TinShiftForward
/*!
 * @brief Shifts data from \p input according to shift information in \p shifts and stores the
 * result into \p output.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor for the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor \p input to be shifted.
 * @param[in] shifts_desc
 *   Input. The descriptor for the shifts tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] shifts
 *   Input. Pointer to the MLU memory that stores all shifts tensor. Based on shifts
 *   tensor, output data is from the input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "TinShift Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for the
 *   input tensor \p input and shifts tensor \p shifts.
 *   - input tensor: half, float.
 *   - shifts tensor: int32.
 *   - output tensor: data type is set the same as input in each case.
 *
 * @par Scale Limitation
 * - The input batch size and shifts batch size should be the same.
 * - The input channel size should be multiple of shifts group size. For example,
 *   if the input shape is [N, T, C, HW], the shifts shape is [N, G], C should be multiple of G, C and G should not be zero.
 *
 * @note
 * - This operation is not supported on 1V platforms.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - http://github.com/open-mmlab/mmcv/tree/master/mmcv/ops/tin_shift.py
 *
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlTinShiftForward(cnnlHandle_t handle,
                                              const cnnlTensorDescriptor_t input_desc,
                                              const void *input,
                                              const cnnlTensorDescriptor_t shifts_desc,
                                              const void *shifts,
                                              const cnnlTensorDescriptor_t output_desc,
                                              void *output);

// Group:TinShiftBackward
/*!
 * @brief Shifts gradients from \p grad_output according to shift information in \p shifts and stores the
 * result into \p grad_input.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] grad_output_desc
 *   Input. The descriptor for the grad_output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] grad_output
 *   Input. Pointer to the MLU memory that stores the input tensor \p grad_output to be shifted.
 * @param[in] shifts_desc
 *   Input. The descriptor the shifts tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] shifts
 *   Input. Pointer to the MLU memory that stores all shifts tensor. Based on shifts
 *   tensor, grad_input data is from the grad_output tensor.
 * @param[in] grad_input_desc
 *   Input. The descriptor of the grad_input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] grad_input
 *   Output. Pointer to the MLU memory that stores the grad_input tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "TinShift Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for the
 *   input tensor \p grad_output and shifts tensor \p shifts.
 *   - input tensor: half, float.
 *   - shifts tensor: int32.
 *   - output tensor: data type is set the same as input in each case.
 *
 * @par Scale Limitation
 * - The grad_output batch size and shifts batch size should be the same.
 * - The grad_output channel size should be multiple of shifts group size. For example,
 *   if the grad_output shape is [N, T, C, HW], the shifts shape is [N, G], C should be multiple of G, C and G should not be zero.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - http://github.com/open-mmlab/mmcv/tree/master/mmcv/ops/tin_shift.py
 *
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlTinShiftBackward(cnnlHandle_t handle,
                                               const cnnlTensorDescriptor_t grad_output_desc,
                                               const void *grad_output,
                                               const cnnlTensorDescriptor_t shifts_desc,
                                               const void *shifts,
                                               const cnnlTensorDescriptor_t grad_input_desc,
                                               void *grad_input);
// Group:PsamaskForward
/*!
 * @brief Moves the \p x tensor to \p y tensor according to \p h_mask,
 *        \p w_mask and \p psa_type.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in ::cnnlPsamaskForward. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] psa_type
 *   Input. Type of the psamask computation, including COLLECT and DISTRIBUTE.
 * @param[in] x_desc
 *   Input. The descriptor of data of input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the data of input tensor.
 * @param[in] h_mask
 *   Input. An integer value that is the h_mask factor of the psamask.
 * @param[in] w_mask
 *   Input. An integer value that is the w_mask factor of the psamask.
 * @param[in] y_desc
 *   Input. The descriptor of data of output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the data of output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED.
 *
 * @par Formula
 * - See "Psamask Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for
 *   data of input tensor \p x and data of output tensor \p y:
 *   - x: float.
 *   - y: float.
 *
 * @par Data Layout
 * - This function supports the following data layout for
 *   data of input tensor \p x and data of output tensor \p y:
 *   - x: NHWC.
 *   - y: NHWC.
 *
 * @par Scale Limitation
 * - The shape of \p x must be [N, H, W, C].
 * - The shape of \p y must be [N, H, W, C].
 * - All dimensions size of \p x and \p y must be the same, except the C dimension.
 * - If the shape of \p x is set to [N, H, W, C], the size of C dimension should be \p h_mask * \p w_mask.
 * - If the shape of \p y is set to [N, H, W, C], the size of C dimension should be H * W.
 * - On MLU200 series:
 *   - When psa_type is COLLECT, the size of \p x channels ci and \p y channels co should be satisfied:
 *     ci + co <= 6144.
 *   - When psa_type is DISTRIBUTE, the size of \p x channels ci and \p y channels co should be satisfied:
 *     ci + 2 * co <= 6144.
 * - On MLU300 series:
 *   - When psa_type is COLLECT, the size of \p x channels ci and \p y channels co should be satisfied:
 *     ci + co <= 10240.
 *   - When psa_type is DISTRIBUTE, the size of \p x channels ci and \p y channels co should be satisfied:
 *     ci + 2 * co <= 10240.
 *
 * @note
 * - This operation is not supported on 1V platforms.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - http://github.com/open-mmlab/mmcv/blob/master/mmcv/ops/psa_mask.py
 *
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlPsamaskForward(cnnlHandle_t handle,
                   const int psa_type,
                   const cnnlTensorDescriptor_t x_desc,
                   const void* x,
                   const int h_mask,
                   const int w_mask,
                   const cnnlTensorDescriptor_t y_desc,
                   void* y);

// Group:PsamaskBackward
/*!
 * @brief Computes the gradients of input tensor \p dx with the gradients of output tensor \p dy
 *        according to \p h_mask, \p w_mask and \p psa_type.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in ::cnnlPsamaskBackward. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] psa_type
 *   Input. Type of the psamask computation, including COLLECT and DISTRIBUTE.
 * @param[in] dy_desc
 *   Input. The descriptor of gradient of output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] dy
 *   Input. Pointer to the MLU memory that stores the gradient of output tensor.
 * @param[in] h_mask
 *   Input. An integer value that is the h_mask factor of the psamask.
 * @param[in] w_mask
 *   Input. An integer value that is the w_mask factor of the psamask.
 * @param[in] dx_desc
 *   Input. The descriptor of gradient of input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] dx
 *   Output. Pointer to the MLU memory that stores the gradient of input tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED.
 *
 * @par Formula
 * - See "Psamask Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for
 *   gradient of output tensor \p dy and gradient of input tensor \p dx:
 *   - dy: float.
 *   - dx: float.
 *
 * @par Data Layout
 * - This function supports the following data layout for
 *   gradient of output tensor \p dy and gradient of input tensor \p dx:
 *   - dy: NHWC.
 *   - dx: NHWC.
 *
 * @par Scale Limitation
 * - The shape of \p dy must be [N, H, W, C].
 * - The shape of \p dx must be [N, H, W, C].
 * - All dimensions size of \p dy and \p dx must be the same, except the C dimension.
 * - If the shape of \p dx is set to [N, H, W, C], the size of C dimension should be \p h_mask * \p w_mask.
 * - If the shape of \p dy is set to [N, H, W, C], the size of C dimension should be H * W.
 * - On MLU200 series:
 *   - When psa_type is COLLECT, the size of \p dx channels ci and \p dy channels co should be satisfied:
 *     ci + co <= 6144.
 *   - When psa_type is DISTRIBUTE, the size of \p dx channels ci and \p dy channels co should be satisfied:
 *     ci + 2 * co <= 6144.
 * - On MLU300 series:
 *   - When psa_type is COLLECT, the size of \p dx channels ci and \p dy channels co should be satisfied:
 *     ci + co <= 10240.
 *   - When psa_type is DISTRIBUTE, the size of \p dx channels ci and \p dy channels co should be satisfied:
 *     ci + 2 * co <= 10240.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - http://github.com/open-mmlab/mmcv/blob/master/mmcv/ops/psa_mask.py
 *
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlPsamaskBackward(cnnlHandle_t handle,
                    const int psa_type,
                    const cnnlTensorDescriptor_t dy_desc,
                    const void *dy,
                    const int h_mask,
                    const int w_mask,
                    const cnnlTensorDescriptor_t dx_desc,
                    void *dx);

// Group:ApplyAddSign
/*!
 * @brief Updates filter using AddSign method.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in ::cnnlApplyAddSign. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  grad_desc
 *   Input. The descriptor of the \p grad tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  grad
 *   Input. Pointer to the MLU memory that stores the \p grad tensor. It is the gradient of \p var.
 *   With the \p grad value, ::cnnlApplyAddSign function can be used to calculate \p m and \p var.
 * @param[in] var_desc
 *   Input. The descriptor of the \p var tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out] var
 *   Input/Output. Pointer to the MLU memory that stores the \p var tensor.
 *   The \p var value is the optimization goal of whole algorithm.
 * @param[in] m_desc
 *   Input. The descriptor of the \p m tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out] m
 *   Input/Output. Pointer to the MLU memory that stores the \p m tensor.
 *   The \p m is the accumulation of gradient.
 * @param[in] lr
 *   Input. Pointer to the MLU memory that stores the \p lr parameter.
 *   It is the learning rate of this optimizer.
 * @param[in] alpha
 *   Input. Pointer to the MLU memory that stores the \p alpha parameter.
 *   It is base bias.
 * @param[in] sign_decay
 *   Input. Pointer to the MLU memory that stores the \p sign_decay parameter.
 *   It is the decay rate for sign.
 * @param[in] beta
 *   Input. Pointer to the MLU memory that stores the \p beta parameter.
 *   It is the exponential decay rate.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for
 *   \p grad tensor, \p var tensor and \p m tensor.
 *   Note that the combinations of these tensors must be half-half-half or float-float-float.
 *   - grad tensor: half, float
 *   - var tensor: half, float
 *   - m tensor: half, float
 *
 * @par Scale Limitation
 * - The dimensions of \p grad, \p var and \p m should be the same.
 * - The number of dimensions is no more than CNNL_DIM_MAX.

 * - The example of this operation is as follows:
     @verbatim
     input three arrays by 4, 4, 4
     --> grad: [0.3, 0.7, 0.1, 0.8]
     --> var:  [0.6, 0.4, 0.1, 0.5]
     --> m:    [0.6, 0.5, 0.2, 0.6]

     param:
        lr: 0.01
        alpha: 1.0
        sign_decay: 0.99
        beta: 0.9

     output array by 4, 4
      --> var: [0.599403, 0.398607,0.0999808,0.498048]
      --> m: [0.57,0.52,0.19,0.62]
     @endverbatim
 *
 * @note
 * - This operation is not supported on the 1V platforms.
 *
 * @par Reference
 * - https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/core/kernels/training_ops.cc
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlApplyAddSign(cnnlHandle_t handle,
                 const cnnlTensorDescriptor_t grad_desc,
                 const void *grad,
                 const cnnlTensorDescriptor_t var_desc,
                 void *var,
                 const cnnlTensorDescriptor_t m_desc,
                 void *m,
                 const void *lr,
                 const void *alpha,
                 const void *sign_decay,
                 const void *beta);
// Group:UnpoolForward
/*!
 * @brief Computes a partial inverse of pooling_forward operation. This operation
 *        supports the CNNL_POOLING_FIXED and CNNL_POOLING_MAX mode now. In the
 *        CNNL_POOLING_FIXED mode, each input pixel will be put to the center of the
 *        pooling kernel regardless of the index. In the CNNL_POOLING_MAX mode, each
 *        input piexl will be put to the corresponding positon of index.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the unpool_forward operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] pooling_desc
 *   Input. The descriptor of the unpool_forward operation. For detailed information,
 *   see ::cnnlPoolingDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of \p input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the \p input tensor.
 * @param[in] index_desc
 *   Input. The descriptor of the \p index tensor. The \p index_desc is used when
 *   ::cnnlPoolingMode_t is CNNL_POOLING_MAX. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] index
 *   Input. Pointer to the MLU memory that stores the index of the maxpool.
 *   Index is used when ::cnnlPoolingMode_t is CNNL_POOLING_MAX.
 * @param[in] output_desc
 *   Input. The descriptor of the \p output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the \p output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "UnpoolForward operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for
 *   \p input, \p output and \p index:
 *   - input  : float, half
 *   - output : float, half
 *   - index  : int16, int32, int64
 *   Note that the data types of \p input tensor and \p output tensor must be the same.
 *
 * @par Data Layout
 * - Note that the layout of input and output must be the same.
 * - In the unpool_forward operation, the layout of input and output must be CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - According to the definition of pooling, the parameters in the \p pooling_desc should satisfy the following conditions:
 *   padding >= 0, stride >= 1, kernel >= 1.
 * - kw * kh < 1500. The kw and kh represent the width and the height of the pooling kernel size respectively.
 * - The \p mode in the \p pooling_desc supports the CNNL_POOLING_FIXED and CNNL_POOLING_MAX.
 *
 * @note
 * - In the CNNL_POOLING_MAX mode, ensure that the output size of the ::cnnlUnpoolForward is equal to the input size of
 *   ::cnnlPoolingForwardWithIndex.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
     @verbatim
     input : [1, 2, 2, 1]
     output : [1, 4, 4, 1]
     padding : [0, 0, 0, 0]
     kernel : [2, 2]
     stride : [2, 2]
     mode : CNNL_POOLING_FIXED
     Unpoolforward:
           input                               output
     ------------------                -------------------------
     |       |        |                |  1  |  0  | 0.8 |  0  |
     |   1   |  0.8   |                |-----+-----+-----+-----|
     |       |        |                |  0  |  0  |  0  |  0  |
     |-------+--------|  --unpool-->   |-----+-----+-----+-----|
     |       |        |                | 0.4 |  0  | 0.6 |  0  |
     |  0.4  |  0.6   |                |-----+-----+-----+-----|
     |       |        |                |  0  |  0  |  0  |  0  |
     ------------------                -------------------------
     @endverbatim
 *
 */

cnnlStatus_t CNNL_WIN_API cnnlUnpoolForward(cnnlHandle_t handle,
                                            const cnnlPoolingDescriptor_t pooling_desc,
                                            const cnnlTensorDescriptor_t input_desc,
                                            const void *input,
                                            const cnnlTensorDescriptor_t index_desc,
                                            const void *index,
                                            const cnnlTensorDescriptor_t output_desc,
                                            void *output);

// Group:UnpoolBackward
/*!
 * @brief Computes the gradient of ::cnnlUnpoolForward according the \p index.
 * This operation only supports the CNNL_POOLING_MAX mode now.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the unpool_backward operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] pooling_desc
 *   Input. The descriptor of the unpool_backward operation. For detailed information,
 *   see ::cnnlPoolingDescriptor_t.
 * @param[in] grad_desc
 *   Input. The descriptor of \p grad tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] grad
 *   Input. Pointer to the MLU memory that stores the \p grad tensor.
 * @param[in] index_desc
 *   Input. The descriptor of the \p index tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] index
 *   Input. Pointer to the MLU memory that stores the index of the maxpool.
 *   It is the output of ::cnnlPoolingForwardWithIndex.
 * @param[in] output_desc
 *   Input. The descriptor of the \p output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the \p output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "UnpoolBackward operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of \p grad tensor and \p output tensor must be the same.
 * - The supported data types of grad, index and output tensors are as follows:
 *   - grad   : float, half
 *   - output : float, half
 *   - index  : int16, int32, int64
 *
 * @par Data Layout
 * - The supported layouts of \p grad, \p index and \p output are CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - padding >= 0, stride >= 1, kernel >= 1.
 * - kw * kh <= 1500. The kw and kh represent the width and the height of the pooling kernel size respectively.
 * - The \p mode in the \p pooling_desc only supports the CNNL_POOLING_MAX.
 *
 * @note
 * - In the CNNL_POOLING_MAX mode, ensure that the grad(input) size of ::cnnlUnpoolBackward is equal to
 *   the input size of ::cnnlPoolingForwardWithIndex.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
     @verbatim
     grad : [1, 4, 4, 1]
     index : [1, 2, 2, 1]
     output : [1, 2, 2, 1]
     padding : [0, 0, 0, 0]
     kernel : [2, 2]
     stride : [2, 2]
     mode : CNNL_POOLING_MAX
     Unpoolbackward:

              grad                    index                output
      -------------------------  ------------------  ------------------
      |  1  |  2  |  3  |  4  |  |       |        |  |       |        |
      |-----+-----+-----+-----|  |   1   |   2    |  |   2   |   7    |
      |  5  |  6  |  7  |  8  |  |       |        |  |       |        |
      |-----+-----+-----+-----|  |-------+--------|  |-------+--------|
      |  9  |  10 |  11 |  12 |  |       |        |  |       |        |
      |-----+-----+-----+-----|  |   3   |   1    |  |   14  |   12   |
      |  13 |  14 |  15 |  16 |  |       |        |  |       |        |
      -------------------------  ------------------  ------------------
     @endverbatim
 *
 */

cnnlStatus_t CNNL_WIN_API cnnlUnpoolBackward(cnnlHandle_t handle,
                                            const cnnlPoolingDescriptor_t pooling_desc,
                                            const cnnlTensorDescriptor_t grad_desc,
                                            const void *grad,
                                            const cnnlTensorDescriptor_t index_desc,
                                            const void *index,
                                            const cnnlTensorDescriptor_t output_desc,
                                            void *output);

// Group:ClipGradNorm
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra workspace
 *        to optimize the ::cnnlClipGradNorm_v2 operation.
 *
 * The size of the extra workspace is based on the given information of the ::cnnlClipGradNorm
 * operation, including the number of input tensors \p input_num and the list of descriptors of
 * input tensors \p input_desc. For more information about the workspace, see "Cambricon CNNL
 * User Guide".
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGetClipGradNormWorkspaceSize_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *          ::cnnlClipGradNorm operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_num
 *   Input. The number of input tensors.
 * @param[in] input_desc
 *   Input. The list of descriptors of input tensors. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in the
 *           ::cnnlClipGradNorm operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - Parameters must meet the following requirements:
 *   - The parameter \p input_num should be greater than 0.
 *
 * @par API Dependency
 * - This function must be called before the ::cnnlClipGradNorm function.
 *
 * @note
 * - The length of the list of input tensors should be equal to \p input_num.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlGetClipGradNormWorkspaceSize_v2)
cnnlStatus_t CNNL_WIN_API
cnnlGetClipGradNormWorkspaceSize(cnnlHandle_t handle,
                                 const int input_num,
                                 const cnnlTensorDescriptor_t input_desc[],
                                 size_t *workspace_size);

// Group:ClipGradNorm
/*!
 * @brief Clips gradient norm of input tensors. The norm is computed over all gradients together.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlClipGradNorm_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *          ::cnnlClipGradNorm operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_num
 *   Input. The number of input tensors.
 * @param[in] input_desc
 *   Input. The list of descriptors of input tensors. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. A host pointer to a list of MLU pointers, which points to the MLU memory that store the
 *          input tensors.
 * @param[in] max_norm
 *   Input. A float value that is the max norm of the clip grad norm.
 * @param[in] norm_type
 *   Input. A float value that indicates the type of used p-norm.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the ::cnnlClipGradNorm
 *          operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the
 *          ::cnnlClipGradNorm operation. You can get the size of the workspace with the
 *          ::cnnlGetClipGradNormWorkspaceSize function.
 * @param[in] total_norm_desc
 *   Input. The descriptor of total norm. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] total_norm
 *   Output. Pointer to the MLU memory that stores the total norm.
 * @param[in] output_desc
 *   Input. The list of descriptors of output tensors. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. A host pointer to a list of MLU pointers, which points to the MLU memory that store
 *           the output tensors.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par Formula
 * - See "Clip Grad Norm Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of \p input tensor, \p total_norm, and \p output tensor should be the same.
 * - The supported data types of \p input, \p total_norm, and \p output tensors are as follows:
 *   - \p input: half, bfloat16, float.
 *   - \p total_norm: half, bfloat16, float.
 *   - \p output: half, bfloat16, float.
 *
 *   The data type bfloat16 is only supported on MLU500 series or above.
 *
 * @par Scale Limitation
 * - Parameters must meet the following conditions:
 *   - \p input_num should be greater than 0.
 *   - \p max_norm should be in range of [-65504, 65504] on MLU200 series when
 *     the data type of \p input is half.
 *   - \p norm_type should be equal to 2.0.
 *
 * @par API Dependency
 * - Before calling this function to implement clip grad norm, you need to call
 *   ::cnnlGetClipGradNormWorkspaceSize to get the extra space size needed in ::cnnlClipGradNorm
 *   operation.
 *
 * @note
 * - Only supports Euclidean-Norm currently.
 * - This function supports in-place operation, which means that the input tensor list
 *   \p input and the output tensor list \p output can be the same one.
 * - The dimension size of the i-th \p input tensor and the i-th \p output tensor should be less
 *   than or equal to CNNL_DIM_MAX.
 * - The length of the list of \p input tensors should be equal to \p input_num.
 * - The length of the list of \p output tensors should be equal to \p input_num.
 * - The shapes of the i-th \p input tensor and the i-th \p output tensor must match.
 * - The strides of the i-th \p input tensor and the i-th \p output tensor must match.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - http://github.com/pytorch/pytorch/blob/master/torch/nn/utils/clip_grad.py
 */
CNNL_DEPRECATED_FOR(cnnlClipGradNorm_v2)
cnnlStatus_t CNNL_WIN_API cnnlClipGradNorm(cnnlHandle_t handle,
                                           const int input_num,
                                           const cnnlTensorDescriptor_t input_desc[],
                                           const void *input[],
                                           const float max_norm,
                                           const float norm_type,
                                           void *workspace,
                                           size_t workspace_size,
                                           const cnnlTensorDescriptor_t total_norm_desc,
                                           void *total_norm,
                                           const cnnlTensorDescriptor_t output_desc[],
                                           void *output[]);

// Group:ClipGradNorm
/*!
 * @brief Returns in \p extra_input_size the size of the MLU memory that is used as an extra
 *        input data to optimize the ::cnnlClipGradNorm_v2 operation.
 *
 * The size of the extra input is based on the given information of the ::cnnlClipGradNorm_v2
 * operation, including the list of descriptors of input tensors. For more information about
 * the \p extra_input_size, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *          the ::cnnlClipGradNorm_v2 operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_num
 *   Input. The number of input tensors.
 * @param[in] input_desc
 *   Input. The list of descriptors of input tensors. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[out] extra_input_size
 *   Output. A host pointer to the returned size of the extra input in bytes that is used in the
 *           ::cnnlClipGradNorm_v2 operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - Parameters must meet the following requirements:
 *   - The parameter \p input_num should be greater than 0.
 *
 * @par API Dependency
 * - This function must be called before the ::cnnlClipGradNorm_v2 function.
 * - After calling this function, you need to call ::cnnlInitClipGradNormExtraInput to initialize
 *   the memory on host.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetClipGradNormExtraInputSize(cnnlHandle_t handle,
                                  const int input_num,
                                  const cnnlTensorDescriptor_t *input_desc,
                                  size_t *extra_input_size);

// Group:ClipGradNorm
/*!
 * @brief Initializes the extra input data space \p extra_input on host.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *          ::cnnlClipGradNorm_v2 operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_num
 *   Input. The number of input tensors.
 * @param[in] input_desc
 *   Input. The list of descriptors of input tensors. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. A host pointer to a list of MLU pointers, which points to the MLU memory that store the
 *          input tensors.
 * @param[in] output_desc
 *   Input. The list of descriptors of output tensors. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[in] output
 *   Input. A host pointer to a list of MLU pointers, which points to the MLU memory that store
 *          the output tensors.
 * @param[out] extra_input
 *   Output. Pointer to the host memory that is used as an extra input space for the
 *           ::cnnlClipGradNorm_v2 operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, you need to get the size of the extra input data with
 *   ::cnnlGetClipGradNormExtraInputSize, and make sure that the memory of the extra input data
 *   is allocated.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlInitClipGradNormExtraInput(cnnlHandle_t handle,
                               const int input_num,
                               const cnnlTensorDescriptor_t *input_desc,
                               const void **input,
                               const cnnlTensorDescriptor_t *output_desc,
                               void **output,
                               void *extra_input);

// Group:ClipGradNorm
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra workspace
 *        to optimize the ::cnnlClipGradNorm_v2 operation.
 *
 * The size of the extra workspace is based on the given information of the ::cnnlClipGradNorm_v2
 * operation. For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *          ::cnnlClipGradNorm_v2 operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_num
 *   Input. The number of input tensors.
 * @param[in] input_desc
 *   Input. The list of descriptors of input tensors. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in the
 *           ::cnnlClipGradNorm_v2 operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - Parameters must meet the following requirements:
 *   - The parameter \p input_num should be greater than 0.
 *
 * @par API Dependency
 * - This function must be called before the ::cnnlClipGradNorm_v2 function.
 *
 * @note
 * - The length of the list of \p input_desc should be equal to \p input_num.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetClipGradNormWorkspaceSize_v2(cnnlHandle_t handle,
                                    const int input_num,
                                    const cnnlTensorDescriptor_t *input_desc,
                                    size_t *workspace_size);

// Group:ClipGradNorm
/*!
 * @brief Clips gradient norm of input tensors. The norm is computed over all gradients together.
 *        Compared with ::cnnlClipGradNorm, ::cnnlClipGradNorm_v2 provides better performance with
 *        extra input space.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *          ::cnnlClipGradNorm_v2 operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_num
 *   Input. The number of input tensors.
 * @param[in] input_desc
 *   Input. The list of descriptors of input tensors. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. A host pointer to a list of MLU pointers, which points to the MLU memory that store the
 *          input tensors.
 * @param[in] max_norm
 *   Input. A float value that is the max norm of the clip grad norm.
 * @param[in] norm_type
 *   Input. A float value that indicates the type of used p-norm.
 * @param[in] extra_input
 *   Input. Pointer to the MLU memory that is used as an extra input for the ::cnnlClipGradNorm_v2
 *          operation. You need to copy the extra input data to MLU from the host that is initialized
 *          with ::cnnlInitClipGradNormExtraInput.
 * @param[in] extra_input_size
 *   Input. The size of the extra input in bytes that needs to be used in the ::cnnlClipGradNorm_v2
 *          operation. You can get the size of the extra input with the
 *          ::cnnlGetClipGradNormExtraInputSize function.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the ::cnnlClipGradNorm_v2
 *          operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the
 *          ::cnnlClipGradNorm_v2 operation. You can get the size of the workspace with the
 *          ::cnnlGetClipGradNormWorkspaceSize_v2 function.
 * @param[in] total_norm_desc
 *   Input. The descriptor of total norm. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] total_norm
 *   Output. Pointer to the MLU memory that stores the total norm.
 * @param[in] output_desc
 *   Input. The list of descriptors of output tensors. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. A host pointer to a list of MLU pointers, which points to the MLU memory that store
 *           the output tensors.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par Formula
 * - See "Clip Grad Norm Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of \p input tensor, \p total_norm, and \p output tensor should be the same.
 * - The supported data types of \p input, \p total_norm, and output tensors are as follows:
 *   - \p input: half, bfloat16, float.
 *   - \p total_norm: half, bfloat16, float.
 *   - \p output: half, bfloat16, float.
 *
 *   The data type bfloat16 is only supported on MLU500 series or above.
 *
 * @par Scale Limitation
 * - Parameters must meet the following conditions:
 *   - \p input_num should be greater than 0.
 *   - \p norm_type should be equal to 2.0.
 *
 * @par API Dependency
 * - Before calling this function to implement clip grad norm, you need to prepare
 *   all the parameters passed to this function. See each parameter description
 *   for details.
 *
 * @note
 * - Only supports Euclidean-Norm currently.
 * - This function supports in-place operation, which means that the input tensor list
 *   \p input and the output tensor list \p output can be the same one.
 * - The dimension size of the i-th \p input tensor and the i-th \p output tensor should be less
 *   than or equal to CNNL_DIM_MAX.
 * - The length of the list of \p input tensors should be equal to \p input_num.
 * - The length of the list of \p output tensors should be equal to \p input_num.
 * - The shapes of the i-th \p input tensor and the i-th \p output tensor must match.
 * - The strides of the i-th \p input tensor and the i-th \p output tensor must match.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - http://github.com/pytorch/pytorch/blob/master/torch/nn/utils/clip_grad.py
 */
cnnlStatus_t CNNL_WIN_API cnnlClipGradNorm_v2(cnnlHandle_t handle,
                                              const int input_num,
                                              const cnnlTensorDescriptor_t *input_desc,
                                              const void **input,
                                              const float max_norm,
                                              const float norm_type,
                                              const void *extra_input,
                                              size_t extra_input_size,
                                              void *workspace,
                                              size_t workspace_size,
                                              const cnnlTensorDescriptor_t total_norm_desc,
                                              void *total_norm,
                                              const cnnlTensorDescriptor_t *output_desc,
                                              void **output);

// Group:IndexCopy
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the index_copy operation.
 *
 * The size of the extra workspace is based on the given information of the index_copy operation,
 * including the input tensor descriptors \p index_desc. For more information about the workspace,
 * see "Cambricon CNNL User Guide".
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGetIndexCopyWorkspaceSize_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   index_copy operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] index_desc
 *   Input. The descriptor of the input index tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   index_copy operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
CNNL_DEPRECATED_FOR(cnnlGetIndexCopyWorkspaceSize_v2)
cnnlStatus_t CNNL_WIN_API cnnlGetIndexCopyWorkspaceSize(cnnlHandle_t handle,
                                                        const cnnlTensorDescriptor_t index_desc,
                                                        size_t *workspace_size);

// Group:IndexCopy
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the index_copy operation.
 *
 * The size of the extra workspace is based on the given information of the index_copy operation.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   index_copy operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] dim
 *   Input. The dimension of the \p input_a and \p input_b to be indexed.
 * @param[in] input_a_desc
 *   Input. The descriptor of the \p input_a. For more detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input_b_desc
 *   Input. The descriptor of the \p input_b. For more detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] index_desc
 *   Input. The descriptor of the \p index tensor. For more detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of the \p output tensor. For more detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   index_copy operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetIndexCopyWorkspaceSize_v2(cnnlHandle_t handle,
                                 int32_t dim,
                                 const cnnlTensorDescriptor_t input_a_desc,
                                 const cnnlTensorDescriptor_t input_b_desc,
                                 const cnnlTensorDescriptor_t index_desc,
                                 const cnnlTensorDescriptor_t output_desc,
                                 size_t *workspace_size);

// Group:IndexCopy
/*!
 * @brief Copies vectors or scalars from \p input_b into \p input_a along \p dim according to the
 * entries in \p index and stores the result into \p output. Supports in-place operation: \p output = \p input_a.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the index_copy operation. For more detailed information, see ::cnnlHandle_t.
 * @param[in] dim
 *   Input. The dimension of the \p input_a and \p input_b to be indexed.
 * @param[in] deterministic_mode
 *   Input. The mode that decides if index_copy operation uses deterministic algorithms. That is whether to
 *   always produce the same output based on the same input when running on the same software and hardware.
 *   When \p index contains duplicate entries, multiple elements from \p input_b will be copied to the same index,
 *   and the result is nondeterministic since it depends on which copy occurs last.
 *   True mode will cost some time to make sure the output is deterministic according to the order of \p index.
 *   False mode is not supported yet.
 * @param[in] input_a_desc
 *   Input. The descriptor of the \p input_a. For more detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input_a
 *   Input. Pointer to the MLU memory that stores the input tensor \p input_a to be indexed and copied into.
 * @param[in] input_b_desc
 *   Input. The descriptor of the \p input_b. For more detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input_b
 *   Input. Pointer to the MLU memory that stores the input tensor \p input_b to be copied from.
 * @param[in] index_desc
 *   Input. The descriptor of the \p index tensors. For more detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] index
 *   Input. Pointer to the MLU memory that stores the input tensor \p index which maps vectors or
 *   scalars from \p input_b to \p input_a.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the index
 *   copy operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the index
 *   copy operation. You can get the size of the workspace with the
 *   ::cnnlGetIndexCopyWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the \p output tensor. For more detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Index Copy Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \p input_a, \p input_b and
 *   output tensor \p output.
 *   Note that the data type of input tensor and output tensor should be the same.
 *   - input tensor: uint8, int8, uint16, int16, uint32, int31, int32, uint64, int64, bool, half, float, bfloat16.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int31, int32, uint64, int64, bool, half, float, bfloat16.
 *
 * - \p dim: int32
 * - \p index: int64, int32
 *
 * @par Scale Limitation
 * - The \p input_a tensor, \p index tensor, \p input_b tensor and \p dim must meet the following
 *   requirements:
 *   - The dim-th dimension of \p input_b must have the same size as the length of
 *        \p index.
 *   - \p index must be a vector.
 *   - The length of all dimensions of \p input_b apart from the dim-th dimension must match
 *        the length of all dimensions of \p input_a apart from the dim-th dimension.
 *   - The value of \p index must be less than the length of the dim-th dimension of \p input_a, and
 *        every element of \p index must be greater than or equal to zero.
 *   - The value of \p dim must be less than the dimension of \p input_a and not less than 0.
 *
 * @note
 * - This operation is not supported on the 1V platforms.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the index add operation is as follows:
     @verbatim
     dim = 0
     input_a array
       input_a = [[1, 1, 1],
                  [1, 1, 1],
                  [1, 1, 1],
                  [1, 1, 1],
                  [1, 1, 1]]
     index array
       index = [0, 4, 2]
     input_b array
       input_b = [[1, 2, 3],
                  [4, 5, 6],
                  [7, 8, 8]]
     output array
       output = [[1, 2, 3],
                 [1, 1, 1],
                 [7, 8, 8],
                 [1, 1, 1],
                 [4, 5, 6]]
     @endverbatim
 */
cnnlStatus_t CNNL_WIN_API
cnnlIndexCopy(cnnlHandle_t handle,
              int32_t dim,
              bool deterministic_mode,
              const cnnlTensorDescriptor_t input_a_desc,
              const void *input_a,
              const cnnlTensorDescriptor_t input_b_desc,
              const void *input_b,
              const cnnlTensorDescriptor_t index_desc,
              const void *index,
              void *workspace,
              size_t workspace_size,
              const cnnlTensorDescriptor_t output_desc,
              void *output);

// Group:Diagonal
/*!
 * @brief Obtains the designated diagonal on designated 2D plane of multidimensional tensor \p input.
 * \p offset determines which diagonal on the plane. \p dim1 and \p dim2 determine the plane to be considered.
 * Diagonal obtained will be appended as the last dimension of \p output.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the diagonal operation. For more detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the \p input. It contains information including dimension number N and tensor shape SHAPE_IN.
 *   For more detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor \p input.
 * @param[in] offset
 *   Input. The index of the diagonal to be obtained. If \p offset is positive, diagonal above main diagonal will be considered.
 *   If \p offset is negative, diagonal below main diagonal will be considered. If offset equals 0, main diagonal will be considered.
 * @param[in] dim1
 *   Input. The first dimension of the designated 2D plane to take diagonal.
 * @param[in] dim2
 *   Input. The second dimension of the designated 2D plane to take diagonal.
 * @param[in] output_desc
 *   Input. The descriptor of the \p output. It contains information including dimension number M and tensor shape SHAPE_OUT.
 *   M should be equal to N-1. To get SHAPE_OUT, two dimensions represented by \p dim1 and \p dim2 should be removed from SHAPE_IN.
 *   Then, between the removed dimensions, the one has minor length should be appended at the end of SHAPE_OUT.
 *   For more detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "Diagonal Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The \p input data types this operator supports are as follows. Data types such as int31, all complex are not supported.
 *   Data types of \p input and \p output must be the same.
 *   - input tensor: bool, int8, int16, int32, int64, half, float32, double
 *   - output tensor: bool, int8, int16, int32, int64, half, float32, double
 * - Other parameters only support int32.
 *   - \p offset: int32
 *   - \p dim1: int32
 *   - \p dim2: int32
 *
 * @par Scale Limitation
 * - \p dim1 and \p dim2 should be greater than -N-1 and less than N, where N is the dimension number of \p input.
 * - \p dim1 and \p dim2 should not be the same.
 *
 * @note
 * - Users should define output tensor correctly in \p output_desc, otherwise,  CNNL_STATUS_BAD_PARAM may be returned.
 * - \p offset has no scale limitation. But when absolute value of \p offset is greater than the minor one between the
 *   the lengths of \p dim1 and \p dim2 dimensions, \p output_desc SHAPE_OUT elements should be all 0 and the function
 *   returns 0 element tensor.
 * - Currently this function only supports \p offset = 0 and \p input tensor with the same length in all dimensions for
 *   trace computations.
 *
 * @par Requirements
 * - None
 *
 * @par Example
    @verbatim
    input shape: [2, 3, 4, 5]
    offset: 0
    dim1: 1
    dim2: 2
    output shape: [2, 5, 3]
    input: tensor([[[[  0,   1,   2,   3,   4],
                     [  5,   6,   7,   8,   9],
                     [ 10,  11,  12,  13,  14],
                     [ 15,  16,  17,  18,  19]],

                    [[ 20,  21,  22,  23,  24],
                     [ 25,  26,  27,  28,  29],
                     [ 30,  31,  32,  33,  34],
                     [ 35,  36,  37,  38,  39]],

                    [[ 40,  41,  42,  43,  44],
                     [ 45,  46,  47,  48,  49],
                     [ 50,  51,  52,  53,  54],
                     [ 55,  56,  57,  58,  59]]],


                   [[[ 60,  61,  62,  63,  64],
                     [ 65,  66,  67,  68,  69],
                     [ 70,  71,  72,  73,  74],
                     [ 75,  76,  77,  78,  79]],

                    [[ 80,  81,  82,  83,  84],
                     [ 85,  86,  87,  88,  89],
                     [ 90,  91,  92,  93,  94],
                     [ 95,  96,  97,  98,  99]],

                    [[100, 101, 102, 103, 104],
                     [105, 106, 107, 108, 109],
                     [110, 111, 112, 113, 114],
                     [115, 116, 117, 118, 119]]]])
    output: tensor([[[  0,  25,  50],
                    [  1,  26,  51],
                    [  2,  27,  52],
                    [  3,  28,  53],
                    [  4,  29,  54]],

                   [[ 60,  85, 110],
                    [ 61,  86, 111],
                    [ 62,  87, 112],
                    [ 63,  88, 113],
                    [ 64,  89, 114]]])
   @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.diagonal.html
 */

cnnlStatus_t CNNL_WIN_API cnnlDiagonal(const cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t input_desc,
                                       const void* input,
                                       const int32_t offset,
                                       const int32_t dim1,
                                       const int32_t dim2,
                                       const cnnlTensorDescriptor_t output_desc,
                                       void* output);

/******************************************************************************
 * CNNL Data Structure: DiagPart
 ******************************************************************************/
/*!
 * @brief Enumeration variables describing the \p align modes that can be used to define
 * how superdiagonals and subdiagonals are aligned in the ::cnnlDiagPart function.
 *
 */
typedef enum {
  CNNL_DIAG_PART_RIGHT_LEFT = 0,
  /*!< Aligns superdiagonals to the right and subdiagonals to the left for ::cnnlDiagPart. */
  CNNL_DIAG_PART_LEFT_RIGHT = 1,
  /*!< Aligns superdiagonals to the left and subdiagonals to the right for ::cnnlDiagPart. */
  CNNL_DIAG_PART_LEFT_LEFT = 2,
  /*!< Aligns superdiagonals to the left and subdiagonals to the left for ::cnnlDiagPart. */
  CNNL_DIAG_PART_RIGHT_RIGHT = 3
  /*!< Aligns superdiagonals to the right and subdiagonals to the right for ::cnnlDiagPart. */
} cnnlDiagPartMode_t;

// Group:DiagPart
/*!
 * @brief Returns a batched diagonal tensor with the \p lower_diag_index to \p upper_diag_index
 * diagonals of \p input. This operation may need padding with \p padding_value and is usually
 * used in Conformer network for TensorFlow.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the diag_part
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in]  lower_diag_index
 *   Input. A host pointer to the \p lower_diag_index that represents the lower bound of the diagonal
 *   index of \p output.
 * @param[in]  upper_diag_index
 *   Input. A host pointer to the \p upper_diag_index that represents the upper bound of the diagonal
 *   index of \p output.
 * @param[in]  padding_value
 *   Input. A host pointer to the \p padding_value that holds the constant value to be added for the
 *   diagonal of \p output with padding operation.
 * @param[in]  align
 *   Input. The padding alignment, see ::cnnlDiagPartMode_t for more details.
 * @param[in]  output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out]  output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 *  - Data types of input tensors \p input, \p padding_value and output tensor \p output must be the same.
 *   The supported data types are as follows:
 *   - input tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *   - lower_diag_index: int32.
 *   - upper_diag_index: int32.
 *   - padding_value: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor must meet the following requirements:
 *   - The number of dimensions of \p input should be no less than 2 and no more than CNNL_DIM_MAX.
 *   - \p lower_diag_index > \p -rows && \p lower_diag_index < cols. \p rows represents the length of the
 *   second to last dimension of the input tensor. \p cols represents the length of the last dimension of
 *   the input tensor.
 *   - \p upper_diag_index > \p -rows && \p upper_diag_index < cols. \p rows represents the length of the
 *   second to last dimension of the input tensor. \p cols represents the length of the last dimension of
 *   the input tensor.
 *   - \p lower_diag_index <= \p upper_diag_index.
 *   - If \p lower_diag_index == \p upper_diag_index, \p rank_output == \p rank_input - 1.
 *   \p rank_output represents the number of dimensions of the output tensor. \p rank_input represents the
 *   number of dimensions of the input tensor.
 *   - If \p lower_diag_index < \p upper_diag_index, \p rank_output == \p rank_input. \p rank_output represents
 *   the number of dimensions of the output tensor. \p rank_input represents the number of dimensions of the
 *   input tensor.
 *   - \p max_diag_len = min(\p rows + min(\p upper_diag_index, 0), \p cols - max(\p lower_diag_index, 0)).
 *   \p max_diag_len is the maximum length of the batched diagonals of the output tensor. \p rows represents
 *   the length of the second to last dimension of the input tensor. \p cols represents the length of the
 *   last dimension of the input tensor.
 *   - \p output[rank_output - 1] = \p max_diag_len. \p rank_output represents the number of dimensions of
 *   the output tensor. \p max_diag_len is the maximum length of the batched diagonals of the output tensor.
 *   - \p num_diags = \p upper_diag_index - \p lower_diag_index + 1. \p num_diags represents the number of
 *   diagonals of each batch of the output tensor.
 *   - If \p rank_input > 2 and \p rank_output == \p rank_input, output[\p rank_output - 2] == \p num_diags.
 *   \p rank_input represents the number of dimensions of the input tensor. \p rank_output represents the
 *   number of dimensions of the output tensor. \p num_diags represents the number of diagonals of each batch
 *   of the output tensor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of diag_part is as follows:
   @verbatim
   input: tensor([[[[0., 1., 2.],
                    [3., 4., 5.],
                    [6., 7., 8.],
                    [9., 0., 1.]]]])
   lower_diag_index: -2
   upper_diag_index: 2
   padding_value: 0
   align: CNNL_DIAG_PART_RIGHT_LEFT
   output: tensor([[[[0., 0., 2.]
                     [0., 1., 5.]
                     [0., 4., 8.]
                     [3., 7., 1.]
                     [6., 0., 0.]]]])
   @endverbatim
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/linalg/diag_part
 * - https://www.tensorflow.org/versions/r2.5/api_docs/python/tf/linalg/diag_part
 * - https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/ops/array_ops.py
 */
cnnlStatus_t CNNL_WIN_API cnnlDiagPart(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t input_desc,
                                       const void *input,
                                       const int lower_diag_index,
                                       const int upper_diag_index,
                                       const void *padding_value,
                                       const cnnlDiagPartMode_t align,
                                       const cnnlTensorDescriptor_t output_desc,
                                       void *output);

// Group: MatrixDiag
/*!
 * @brief Returns a tensor with the contents in \p diagonal as lower_diag_index-th to upper_diag_index-th diagonal of a matrix, with everything else padded with \p padding_value.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the matrix_diag
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  diagonal_desc
 *   Input. The descriptor of the \p diagonal tensor whose dimensions are equal to or greater than 1. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  diagonal
 *   Input. Pointer to the MLU memory that stores the \p diagonal tensor.
 * @param[in]  lower_diag_index
 *   Input. An integer number that represents the lower bound of the \p diagonal.
 *   index of \p output.
 * @param[in]  upper_diag_index
 *   Input. An integer number that represents the upper bound of the \p diagonal.
 *   index of \p output.
 * @param[in]  padding_value
 *   Input.  Pointer to the host memory that stores the padding values to be added for the area outside the specified diagonal band. Default is 0.
 * @param[in]  output_desc
 *   Input. The descriptor of the \p output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out]  output
 *   Output. Pointer to the MLU memory that stores the \p output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Data Type
 *   The supported data types are as follows:
 *   - \p diagonal tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float, complex_float.
 *   - \p lower_diag_index: int32.
 *   - \p upper_diag_index: int32.
 *   - \p padding_value: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float, complex_float.
 *   - \p output tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float, complex_float.
 *   - Data types of input tensor \p diagonal, \p padding_value and output tensor \p output must be the same.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor must meet the following requirements:
 *   Let diagonal have R dimensions [I, J, K, ..., L, M, N].
 *   The output tensor has rank R+1 with shape [I, J, K, ..., L, M, output_rows, output_cols], when lower_diag_index == upper_diag_index; Otherwise, it has rank R with shape [I, J, K, ..., L,  output_rows, output_cols].
 * - lower_diag_index <= upper_diag_index.
 * - If lower_diag_index != upper_diag_index, the dim of diagonal_desc should be >= 2.
 *
 * @note
 * - None.
 *
 * @par Requirements
 *
 * @par Example
 * - The example of matrix_diag is as follows:
   @verbatim
    diagonal: tensor([[1, 2, 3, 4],
                      [5, 6, 7, 8],
                      [9, 10, 11, 12]]) # input shape (3, 4)
    # case1. The main diagonal
    lower_diag_index: 0
    upper_diag_index: 0
    padding_value: 0
    output: tensor([[[1, 0, 0, 0]
                     [0, 2, 0, 0]
                     [0, 0, 3, 0]
                     [0, 0, 0, 4]],

                    [[5, 0, 0, 0]
                     [0, 6, 0, 0]
                     [0, 0, 7, 0]
                     [0, 0, 0, 8]],

                    [[9, 0, 0, 0]
                     [0, 10, 0, 0]
                     [0, 0, 11, 0]
                     [0, 0, 0, 12]]]) # output shape (3, 4, 4)

    # case2. The superdiagonal
    lower_diag_index: 1
    upper_diag_index: 3
    padding_value: 0
    output: tensor([[0, 9, 5, 1, 0],
                    [0, 0, 10, 6, 2],
                    [0, 0, 0, 11, 7],
                    [0, 0, 0, 0, 4]]) # output shape (4, 5)
    # case3. The subdigonal
    lower_diag_index: -2
    upper_diag_index: 0
    padding_value: 0
    output: tensor([[1, 0, 0, 0],
                    [5, 2, 0, 0],
                    [9, 6, 3, 0],
                    [0, 10, 7, 4],
                    [0, 6, 11, 8]]) # output shape (5, 4)
    # case4. The tridiagonal
    lower_diag_index: -1
    upper_diag_index: 1
    padding_value: 0
    output: tensor([[5, 1, 0, 0],
                    [9, 6, 2, 0],
                    [0, 10, 7, 3],
                    [0, 0, 11, 8]]) # output shape (4, 4)
   @endverbatim
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/matrix-diag-v2
 * - https://www.tensorflow.org/api_docs/python/tf/raw_ops/MatrixDiagV2
 * - https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/ops/array_ops.py#L2400
 */
cnnlStatus_t CNNL_WIN_API cnnlMatrixDiag(cnnlHandle_t handle,
                                         const cnnlTensorDescriptor_t diagonal_desc,
                                         const void *diagonal,
                                         const int lower_diag_index,
                                         const int upper_diag_index,
                                         const void *padding_value,
                                         const cnnlTensorDescriptor_t output_desc,
                                         void *output);

// Group: FFT
/*!
 *  @brief Creates a descriptor pointed by \p fft_plan for the FFT operation, and allocates memory
 *  for holding the information about the FFT operation. The information is defined in ::cnnlFFTPlan_t.
 *
 *  @deprecated
 *  This function is deprecated and will be removed in future release.
 *
 *  @param[out] fft_plan
 *    Output. Pointer to the FFT descriptor that holds information about the FFT operation.
 *
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 *  @par API Dependency
 *  - After calling this function, you can call the ::cnnlMakeFFTPlanMany function to initialize and set the
 *    information to the created descriptor.
 *  - You need to call the ::cnnlDestroyFFTPlan to destroy the descriptor.
 *    Otherwise, the memory leak may occur.
 *
 *  @note
 *  - This function only supports 1D FFT currently. 2D FFT and 3D FFT
 *    will be supported in the future.
 *  - When the data type of input is float or complex_float, the 1D FFT length should be equal to:
      length = \f$base * 2^{m}\f$, and the base should be less than or equal to 4096.
 *  - When the data type of input is half or complex_half, the 1D FFT length should be equal to:
      length = \f$2^{m}\f$.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t cnnlCreateFFTPlan(cnnlFFTPlan_t *fft_plan);

// Group:FFT
/*!
 *  @brief Initializes the FFT descriptor pointed by \p fft_plan that was previously created
 *  with the ::cnnlCreateFFTPlan function, and sets the information about the
 *  tensor descriptors of input tensor and output tensor, the rank of FFT, and the FFT size on each
 *  dimension.
 *
 *  @deprecated
 *  This function is deprecated and will be removed in future release.
 *
 *  This function also gets the size of MLU memory buffers for FFT execution, including \p reservespace_size and
 *  \p workspace_size. The size of the extra workspace is based on the given information of the
 *  \p fft_plan.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *    in the FFT operation. For detailed information, see ::cnnlHandle_t.
 *  @param[in,out] fft_plan
 *    Input/output. The descriptor of FFT. For detailed information, see ::cnnlFFTPlan_t.
 *  @param[in] input_desc
 *    Input. The descriptor of input signals. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[in] output_desc
 *    Input. The descriptor of output signals. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[in] rank
 *    Input. The dimensionality of the FFT operation. It can be 1D, 2D or 3D.
 *  @param[in] n
 *    Input. An array of size \p rank describing the FFT size of each dimension. n[0]
 *    is the size of the outermost dimension and n[rank - 1] is the innermost dimension
 *    of FFT operation. If n[i] is greater than the size of input on dimension i, the input
 *    signal will be zero-padded on that dimension. Otherwise, input signal is trimmed
 *    on the dimension i.
 *  @param[out] reservespace_size
 *    Output. The size of the extra reserved space in bytes that needs to be used in FFT operation.
 *  @param[out] workspace_size
 *    Output. The size of the extra workspace in bytes that needs to be used in FFT operation.
 *
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED, ::CNNL_STATUS_NOT_INITIALIZED
 *
 *  @par Data Type
 *  - The supported data types of \p input and \p output tensors are as follows:
 *    - real-to-complex fft:
 *      - half(input offchip)-complex_half(output offchip)-int16(input onchip)
 *      - float(input offchip)-complex_float(output offchip)-int31(input onchip)
 *    - complex-to-real fft:
 *      - complex_half(input offchip)-half(output offchip)-int16(input onchip)
 *      - complex_float(input offchip)-float(output offchip)-int31(input onchip)
 *    - complex-to-complex fft:
 *      - complex_half(input offchip)-complex_half(output offchip)-int16(input onchip)
 *      - complex_float(input offchip)-complex_float(output offchip)-int31(input onchip)
 *  - On MLU300 series or above, this function also supports the combinations of
 *    data types as follows:
 *    - real-to-complex fft:
 *      - half(input offchip)-complex_half(output offchip)-half(input onchip)
 *      - float(input offchip)-complex_float(output offchip)-float(input onchip)
 *    - complex-to-real fft:
 *      - complex_half(input offchip)-half(output offchip)-half(input onchip)
 *      - complex_float(input offchip)-float(output offchip)-float(input onchip)
 *    - complex-to-complex fft:
 *      - complex_half(input offchip)-complex_half(output offchip)-half(input onchip)
 *      - complex_float(input offchip)-complex_float(output offchip)-float(input onchip)
 *
 *  @par API Dependency
 *  - Before calling this function, you need to call the ::cnnlCreateFFTPlan function to
 *  create an FFT descriptor firstly, call the ::cnnlSetTensorDescriptor or
 *  ::cnnlSetTensorDescriptorEx function to set the input and output tensor descriptor,
 *  and then call the ::cnnlSetTensorDescriptorOnchipDataType to set the onchip data type
 *  of input tensor descriptor.
 *
 *  @note
 *  - The advanced data layout parameters, (i/o)nembed, (i/o)istride, (i/o)idist, are set through
 *    ::cnnlSetTensorDescriptorEx. If stride information is not needed, you can set the simple data layout
 *    through ::cnnlSetTensorDescriptor.
 *  - The dimension size of input / output should be equal to \p rank or \p rank + 1. In the former case,
 *    the batch size is considered as 1. Otherwise, the outermost dimension is the batch size.
 *  - For real-to-complex FFTs, the innermost dimension of FFT length and output arrays are not the same.
 *    For a x-length 1D real-to-complex FFT, the output is x/2 + 1 complex numbers (the non-redundant outputs).
 *    For a N-D real-to-complex FFT with n=[z, y, x], the output shape will be [z, y, x/2+1].
 *  - For complex-to-real FFTs, the input tensor only holds the non-redundant part of the Fourier coefficients.
 *    And the output tensor stores the real output values.
 *  - When n[0] is greater than 4096, the data type of input only supports float or complex_float.
 *
 *  @par Scale Limitation
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t cnnlMakeFFTPlanMany(cnnlHandle_t handle,
                                 cnnlFFTPlan_t fft_plan,
                                 const cnnlTensorDescriptor_t input_desc,
                                 const cnnlTensorDescriptor_t output_desc,
                                 const int rank,
                                 const int n[],
                                 size_t *reservespace_size,
                                 size_t *workspace_size);

// Group:FFT
/*!
 *  @brief Bond the \p reservespace to the \p fft_plan. The size of reserved space can be derived
 *  through ::cnnlMakeFFTPlanMany.
 *
 *  @deprecated
 *  This function is deprecated and will be removed in future release.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *    ::cnnlExecFFT. For detailed information, see ::cnnlHandle_t.
 *  @param[in, out] fft_plan
 *    Input/Output. The descriptor of FFT. For detailed information, see ::cnnlFFTPlan_t.
 *  @param[in] reservespace
 *    Input. Pointer to the MLU memory that is used as an extra memory space for saving
 *    intermediate results of FFT operation.
 *
 *  @par API Dependency
 *  - Before calling this function, you need to call the ::cnnlCreateFFTPlan function
 *  to create an FFT descriptor firstly, call the ::cnnlMakeFFTPlanMany function to set the
 *  FFT descriptor and get the size of reserved space, and then call the
 *  cnrtMalloc function to create MLU memory according to the rservespace_size given.
 *
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_INTERNAL_ERROR
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t cnnlSetFFTReserveArea(cnnlHandle_t handle,
                                   cnnlFFTPlan_t fft_plan,
                                   void *reservespace);

// Group:FFT
/*!
 *  @brief Executes any FFT. In case of complex-to-real and real-to-complex
 *  transforms \p direction parameter is ignored. This function stores the Fourier coefficients
 *  in the output array. If the address of input and output are the same, an in-place FFT
 *  is adopted.
 *
 *  @deprecated
 *  This function is deprecated and will be removed in future release.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *    in the FFT execution. For detailed information, see ::cnnlHandle_t.
 *  @param[in] fft_plan
 *    Input. The plan for FFT execution. For detailed information, see ::cnnlFFTPlan_t.
 *  @param[in] input
 *    Input. Pointer to the MLU memory that stores the input tensor.
 *  @param[in] scale_factor
 *    Input. A float-point scalar used to multiply the FFT output.
 *  @param[in, out] workspace
 *    Input/Output. Pointer to the MLU memory that is used as an extra workspace for the
 *    ::cnnlExecFFT.
 *  @param[out] output
 *    Output. Pointer to the MLU memory that stores the output tensor.
 *  @param[in] direction
 *    Input. The transform direction: 0 means FFT forward and 1 means FFT inverse.
 *    Direction is ignored for real-to-complex and complex-to-real transforms.
 *
 *  @note
 *  - For in-place 1D real-to-complex FFTs, the input is a batch of n real numbers, and the
 *    output is n/2 + 1 non-redundant complex numbers. This requires a padding of input array.
 *  - For in-place N-D real-to-complex FFTs, extra padding of the real-data array on the innermost
 *    dimension is necessary to accommodate the size of the complex-data output.
 *  - When \p input contains NaN or infinity and the input onchip data type of FFT is not quantized
 *    data type, the output is computed through the fft formula with computation rules of NaN or
 *    infinity based on IEEE 754.
 *  - When \p input contains NaN or infinity and the input onchip data type of FFT is quantized
 *    data type, i.e. int31 or int16, the output will be unpredictable.
 *  - The range of \p input is recommended to be in range of [-10, 10] with uniform
 *    distribution for higher precision.
 *  - The range of \p scale_factor is recommended to be in range of [-1, 1] to avoid exceeding
 *    the data representation range.
 *  - Half data type of \p input is not recommended due to low precision. The first element of the
 *    FFT result is the sum of all input elements, and it is likely to overflow.
 *  - This operation is not supported on the 1V platforms.
 *
 *  @par API Dependency
 *  - Before calling this function, you need to call the ::cnnlCreateFFTPlan
 *  function to create an FFT descriptor firstly, call the ::cnnlMakeFFTPlanMany
 *  function to set the FFT descriptor and the size of reserved space and work space,
 *  and then call the ::cnnlSetFFTReserveArea to bond the reservespace area to the descriptor.
 *
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_INTERNAL_ERROR
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t cnnlExecFFT(cnnlHandle_t handle,
                         const cnnlFFTPlan_t fft_plan,
                         const void *input,
                         const float scale_factor,
                         void *workspace,
                         void *output,
                         int direction);

// Group:FFT
/*!
 *  @brief Destroys an FFT plan \p fft_plan that is created with the
 *  ::cnnlCreateFFTPlan function.
 *
 *  The fft plan is defined in ::cnnlFFTPlan_t and holds the information about the
 *  FFT operation.
 *
 *  @deprecated
 *  This function is deprecated and will be removed in future release.
 *
 *  @param[in] fft_plan
 *    Input. The fft plan to be destroyed.
 *
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED
 *
 *  @note
 *  - You need to call this function after calling the ::cnnlExecFFT.
 *    Otherwise, memory leak may occur.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example.
 *  - None.
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t cnnlDestroyFFTPlan(cnnlFFTPlan_t fft_plan);

// Group:ReflectionPadBackward
/*!
 * @brief Computes the gradients of reflection_pad.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the reflection_pad_backward
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  x_grad_desc
 *   Input. The descriptor of the \p x_grad tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  x_grad
 *   Input. Pointer to the MLU memory that stores the \p x_grad tensor. \p x_grad is the output gradient of ReflectionPad.
 * @param[in]  paddings
 *   Input. Pointer to the host memory that stores the padding parameter. In 2D mode, the pointer holds the padding size to
 *   be added for certain dimensions of \p x_grad in the order of left, right, top and bottom. Positive and zero padding values
 *   represent the padding size and negative padding values are not supported now. In 1D mode, the pointer holds the padding
 *   size to be added for certain dimensions of \p x_grad in the order of left, right, padding values can be positive, zero
 *   and negative.
 * @param[out]  y_grad_desc
 *   Output. The descriptor of the \p y_grad tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out]  y_grad
 *   Output. Pointer to the MLU memory that stores the \p y_grad tensor. \p y_grad is the input gradient of ReflectionPad.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \p x_grad and output tensor
 *   \p y_grad. Data types of both tensors should be the same.
 *   - input tensor: half, float.
 *   - paddings: int32.
 *   - output tensor: half, float.
 * - On MLU500 series or above, this function also supports bfloat16 data type for both \p input and output.
 *
 * @par Data Layout
 * - Data layouts of input tensor \p x_grad and output tensor \p y_grad must be the same. The supported data
 *  layouts are as follows:
 *   - input tensor: CNNL_LAYOUT_NCHW(2D mode), CNNL_LAYOUT_NLC(1D mode).
 *   - output tensor: CNNL_LAYOUT_NCHW(2D mode), CNNL_LAYOUT_NLC(1D mode).
 *
 * @par Scale Limitation
 * - The input tensor and output tensor must meet the following requirements:
 *   - The number of dimensions of \p input equals 3, and element number of \p paddings equals 2 in 1D mode.
 *   - The number of dimensions of \p input equals 4, and element number of \p paddings equals 4 in 2D mode.
 *   - \p pad_top < \p y_h && \p pad_bottom < \p y_h. \p y_h represents the length of the H dimension in
 *   the output tensor. \p pad_top and \p pad_bottom represent the length of the corresponding dimension in \p paddings
 *   respectively(2D mode).
 *   - \p pad_left < \p y_w and \p pad_right < \p y_w. \p y_w represents the length of the W dimension in
 *   the output tensor. \p pad_left and \p pad_right represent the length of the corresponding dimension in \p paddings
 *   respectively(2D mode).
 *   - \p x_h = \p y_h + \p pad_top + \p pad_bottom && \p x_w = \p y_w + \p pad_left + \p pad_right. \p x_h represents
 *   the length of the H dimension in the input tensor. \p x_w represents the length of the W dimension in
 *   the input tensor(2D mode).
 *   - \p pad_left < \p y_l and \p pad_right < \p y_l. \p y_l represents the length of the L dimension in
 *   the output tensor. \p pad_left and \p pad_right represent the length of the corresponding dimension in \p paddings
 *   respectively(1D mode).
 *   - \p x_l = \p y_l + \p pad_left + \p pad_right. \p x_l represents the length of the L dimension in the input tensor.
 *   (1D mode).
 *   - Negative padding values are not supported in 2D mode now and padding values can be positive, zero
 *   and negative in 1D mode.
 *
 * @note
 * - ReflectionPadBackward has 3 modes: 1D, 2D and 3D. When the dimension of input is equal to 3, the mode
 *   of ReflectionPadBackward is 1D; When the dimension of input is equal to 4, the mode of ReflectionPadBackward
 *   is 2D; When the dimension of input is equal to 5, the mode of ReflectionPadBackward is 3D.
 *   It only supports 1D and 2D now.
 * - The example of ReflectionPadBackward is as follows:
   @verbatim
   The following is an example of 2D mode:
     input: tensor([[[[1., 1., 1., 1., 1., 1.]
                      [1., 1., 1., 1., 1., 1.]
                      [1., 1., 1., 1., 1., 1.]
                      [1., 1., 1., 1., 1., 1.]
                      [1., 1., 1., 1., 1., 1.]
                      [1., 1., 1., 1., 1., 1.]]]])
     output: tensor([[[[2., 1., 1., 2.],
                       [4., 2., 2., 4.],
                       [4., 2., 2., 4.],
                       [2., 1., 1., 2.]]]])
     paddings: array([1., 1., 1., 1.])
   The following is an example of 1D mode:
     input: tensor([[[1., 1., 1., 1., 1., 1.]]])
     output: tensor([[[1., 2., 2., 1.]]])
     paddings: array([1., 1., 1., 1.])
   @endverbatim
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlReflectionPadBackward(cnnlHandle_t handle,
                          const cnnlTensorDescriptor_t x_grad_desc,
                          const void *x_grad,
                          const int *paddings,
                          const cnnlTensorDescriptor_t y_grad_desc,
                          void *y_grad);

// Group:ReplicationPadBackward
/*!
 * @brief Computes the gradients of replication_pad.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the replication_pad_backward operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in]  x_grad_desc
 *   Input. The descriptor of the \p x_grad tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  x_grad
 *   Input. Pointer to the MLU memory that stores the \p x_grad tensor. \p x_grad is the output gradient of ReplicationPad.
 * @param[in]  padding
 *   Input. Pointer to the host memory that stores the padding parameter which holds the padding size to be added for certain
 *   dimensions of \p x_grad in the order of left, right, top and bottom. Padding
 *   values represent the padding size.
 * @param[in]  y_grad_desc
 *   Input. The descriptor of the \p y_grad tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out]  y_grad
 *   Output. Pointer to the MLU memory that stores the \p y_grad tensor. \p y_grad is the input gradient of ReplicationPad.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \p x_grad and output tensor
 *   \p y_grad. Data types of both tensors should be the same.
 *   - input tensor: half, float, bfloat16.
 *   - paddings: int32.
 *   - output tensor: half, float, bfloat16.
 * The bfloat16 data type is supported only on MLU500 series.
 *
 * @par Data Layout
 * - Data layouts of input tensor \p x_grad and output tensor \p y_grad must be the same. The supported data
 *  layouts are as follows:
 *   - input tensor: \p CNNL_LAYOUT_NLC or \p CNNL_LAYOUT_NHWC.
 *   - output tensor: \p CNNL_LAYOUT_NLC or \p CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor must meet the following requirements:
 *   - For 1D, the number of dimensions of \p input equals 3, and element number of \p paddings
 *   equals 2.
 *   - \p x_w = y_w + pad_left + pad_right
 *   - For 2D, the number of dimensions of \p input equals 4, and element number
 *   of \p paddings equals 4.
 *   - \p x_h = y_h + pad_top + pad_bottom &&  x_w = y_w + pad_left + pad_right.
 *   \p x_h represents the length of the H dimension in the input tensor.
 *   \p x_w represents the length of the W dimension in the input tensor.
 *   \p y_h represents the length of the H dimension in the output tensor.
 *   \p y_w represents the length of the W dimension in the output tensor.
 *   \p pad_top and \p pad_bottom represent the length of the corresponding dimension in \p paddings respectively.
 *   \p pad_left and \p pad_right represent the length of the corresponding dimension in \p paddings respectively.
 *
 * @note
 * - ReplicationPadBackward has 3 modes: 1D, 2D and 3D. When the dimension of input is equal to 3, the mode
 *   of ReplicationPadBackward is 1D; When the dimension of input is equal to 4, the mode of ReplicationPadBackward
 *   is 2D; When the dimension of input is equal to 5, the mode of ReplicationPadBackward is 3D.
 *   It only supports 1D and 2D now.
 * @par Example
 *  - The example of ReplicationPadBackward is as follows:
   @verbatim
     1D:
     input: tensor([[[1., 1., 1., 1., 1., 1.]]])
     paddings:(1, 1)
     output: tensor([[[2., 1., 1., 2.]]])
     2D:
     input: tensor([[[[1., 1., 1., 1., 1., 1.],
                      [1., 1., 1., 1., 1., 1.],
                      [1., 1., 1., 1., 1., 1.],
                      [1., 1., 1., 1., 1., 1.],
                      [1., 1., 1., 1., 1., 1.],
                      [1., 1., 1., 1., 1., 1.],
                      [1., 1., 1., 1., 1., 1.],
                      [1., 1., 1., 1., 1., 1.],
                      [1., 1., 1., 1., 1., 1.],
                      [1., 1., 1., 1., 1., 1.]]]])
     paddings: (0, 1, 2, 3)
     output: tensor([[[[3., 3., 3., 3., 6.],
                       [1., 1., 1., 1., 2.],
                       [1., 1., 1., 1., 2.],
                       [1., 1., 1., 1., 2.],
                       [4., 4., 4., 4., 8.]]]])
   @endverbatim
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlReplicationPadBackward(cnnlHandle_t handle,
                           const cnnlTensorDescriptor_t x_grad_desc,
                           const void* x_grad,
                           const int padding[],
                           const cnnlTensorDescriptor_t y_grad_desc,
                           void* y_grad);
// Group:VarForward
/*!
 * @brief Calculates the variance for each row of the input tensor in a given
 * dimension.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlStdVarMean instead, which supports multiple axes, var and mean output.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the var forward operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] dim
 *   Input. The dimension of \p input to reduce.
 * @param[in] unbiased
 *   Input. Whether to use the unbiased estimation or not. If unbiased is false, then the
 *   variance will be calculated via the biased estimator.
 *   Otherwise, Bessel's correction will be used.
 * @param[in] input_desc
 *   Input. The descriptor of the \p input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the \p output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - This function supports the following data types for \p unbiased, \p dim,
 *   input tensor \p input and output tensor \p output. Data types of both tensors should be the
 *   same.
 *   - \p unbiased: bool
 *   - \p dim: int32
 *   - \p input: float, half, bfloat16
 *   - \p output: float, half, bfloat16
 *
 *   The bfloat16 data type is supported only on MLU500 series.
 *
 * @par Reference
 * - https://pytorch.org/docs/1.6.0/generated/torch.var.html#torch.var
 *
 * @par API Dependency
 * - None.
 *
 * @par Example
 *  The example of the var forward operation is as follows:
 * @verbatim
 * input: a tensor with shape by 4 * 4  --> [[-0.3567, 1.7385, -1.3042, 0.7423,
 *                                            [ 1.3436, -0.1015, -0.9834, -0.8438],
 *                                            [ 0.6056, 0.1089, -0.3112, -1.4085],
 *                                            [-0.7700, 0.6074, -0.1469, 0.7777]]
 *  param: dim = 1, unbiased = True
 *
 *  Then get the output:
 *
 *  output: a tensor by 4 * 1             --> [ 1.7444, 1.1363, 0.7356, 0.5112]
 * @endverbatim
 *
 */
CNNL_DEPRECATED_FOR(cnnlStdVarMean)
cnnlStatus_t CNNL_WIN_API cnnlVarForward(cnnlHandle_t handle,
                                        int dim,
                                        bool unbiased,
                                        const cnnlTensorDescriptor_t input_desc,
                                        const void *input,
                                        const cnnlTensorDescriptor_t output_desc,
                                        const void *output);

// Group:RepeatInterleave
/*!
 * @brief Returns a new tensor with the same shape as \p input by repeating
 * elements of the \p input tensor on the specied dimension.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in ::cnnlRepeatInterleave. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] repeats_desc
 *   Input. The descriptor of the repeats tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] repeats
 *   Input. Pointer to the MLU memory that stores the repeats tensor. It means
 *   the number of repetitions for each element.
 * @param[in] dim
 *   Input. An integer value that is the dimension of the input tensor along
 *   which the elements to be repeated.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "RepeatInterleave operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of \p input tensor and \p output tensor must be the same.
 * - The supported data types of input, repeats, and output tensors are as follows:
 *   - input: uint8, int8, int16, int32, int64, half, float, double, bfloat16.
 *   - repeats: int32.
 *   - output: uint8, int8, int16, int32, int64, half, float, double, bfloat16.
 *
 * @par Scale Limitation
 * - Parameters must meet the following conditions:
 *   - \p dim must be smaller than the number of dimension of \p input and not less than
 *     negative value of the number of dimension of \p input.
 *   - \p repeats must be 1D tensor.
 *   - The 0-th dimension of \p repeats must be equal to the dim-th dimension of \p input
 *     or 1.
 *   - The number of dimensions of \p output and \p input should be the same.
 *   - The shape of \p input must be the same as the shape of \p output, except the
 *     dimension \p dim.
 *   - The dim-th dimension of \p output must be equal to the product of the first value
 *     of \p repeats and the dim-th dimension of \p input when the 0-th dimension of
 *     \p repeat is equal to 1.
 *   - The dim-th dimension of \p output and the sum of \p repeats must be equal when
 *     the 0-th dimension of \p repeat is greater than 1.
 *   - The value of \p repeats should be greater than or equal to 0.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html
 * - https://github.com/pytorch/pytorch/blob/1.6/aten/src/ATen/native/Repeat.cpp
 * - https://github.com/pytorch/pytorch/blob/release/1.9/aten/src/ATen/native/Repeat.cpp
 */
cnnlStatus_t CNNL_WIN_API cnnlRepeatInterleave(cnnlHandle_t handle,
                                               const cnnlTensorDescriptor_t input_desc,
                                               const void *input,
                                               const cnnlTensorDescriptor_t repeats_desc,
                                               const void *repeats,
                                               const int32_t dim,
                                               const cnnlTensorDescriptor_t output_desc,
                                               void *output);

/*****************************************************************************
 * Cambricon CNNL OP: grid_sample
 * ***************************************************************************/
/*!
 * @brief Enumeration variables describing which padding mode to be used when the grid value
 * is out of the coordinate range. It is used in the ::cnnlSetGridSampleDescriptor.
 * The coordinate value in the grid tensor is normalized by the input dimensions,
 * therefore, the grid value should be in range of [-1, 1]. If the grid tensor has values
 * outside the range of [-1, 1], the corresponding outputs are handled as defined
 * by padding mode.
 */
typedef enum {
  CNNL_GRIDSAMPLE_PADDING_ZEROS = 0,
  /*!< A type of padding mode, which uses zero value for out-of-bound grid locations. */
  CNNL_GRIDSAMPLE_PADDING_BORDER  = 1,
  /*!< A type of padding mode, which uses border values for out-of-bound grid locations. */
  CNNL_GRIDSAMPLE_PADDING_REFLECTION  = 2
  /*!< A type of padding mode, which uses values at locations reflected by the border
   * for out-of-bound grid locations.
   */
} cnnlGridSamplePaddingMode_t;

// Group:GridSampleForward
/*!
 * @brief Creates a descriptor pointed by \p grid_sample_desc for ::cnnlGridSampleForward
 * operation, and allocates memory for holding the information about the operation.
 * The information is defined in ::cnnlGridSampleDescriptor_t.
 *
 * @param[out] grid_sample_desc
 *   Output. A host pointer to the ::cnnlGridSampleForward descriptor
 *   that holds information about the operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_NOT_INITIALIZED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetGridSampleDescriptor function
 *   to initialize and set the information to the descriptor.
 * - You need to call the ::cnnlDestroyGridSampleDescriptor function at the end
 *   to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t
CNNL_WIN_API cnnlCreateGridSampleDescriptor(cnnlGridSampleDescriptor_t *grid_sample_desc);

// Group:GridSampleForward
/*!
 * @brief Initializes the ::cnnlGridSampleForward operation descriptor \p grid_sample_desc
 * that was previously created with the ::cnnlCreateGridSampleDescriptor function,
 * and sets the information about the ::cnnlGridSampleForward operation to the descriptor
 * \p grid_sample_desc. The information includes
 * the interpolation mode, padding mode and align_corners mode.
 *
 * @param[in,out] grid_sample_desc
 *   Input/output. The descriptor of the ::cnnlGridSampleForward operation.
 *   For detailed information, see ::cnnlGridSampleDescriptor_t.
 * @param[in] interp_mode
 *   Input. The interpolation mode, for detailed information, see ::cnnlInterpMode_t.
 * @param[in] padding_mode
 *   Input. The padding mode, for detailed information, see ::cnnlGridSamplePaddingMode_t.
 * @param[in] align_corners
 *   Input. Boolean variable determines the method to align 4 corner pixels between output
 *   and original input images. Generally, pixels of input and output images are considered to be
 *   squares. If \p align_corners is set to true, the input and output images are aligned by the
 *   center points of 4 corner pixels. Otherwise, the input and output images are aligned
 *   by the upper-left corner points of the corresponding corner pixels.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t
CNNL_WIN_API cnnlSetGridSampleDescriptor(cnnlGridSampleDescriptor_t grid_sample_desc,
                                         const cnnlInterpMode_t interp_mode,
                                         const cnnlGridSamplePaddingMode_t padding_mode,
                                         const bool align_corners);

// Group:GridSampleForward
/*!
 * @brief Destroys a ::cnnlGridSampleForward descriptor \p grid_sample_desc that is
 * previously created with the ::cnnlCreateGridSampleDescriptor function.
 *
 * The descriptor is defined in ::cnnlGridSampleDescriptor_t and holds the
 * information about the ::cnnlGridSampleForward operation.
 *
 * @param[in] grid_sample_desc
 *   Input. The ::cnnlGridSampleForward descriptor to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - After a grid_sample operation descriptor is created using ::cnnlCreateGridSampleDescriptor,
 *   if it is not used any more, this function should be called to destroy the descriptor,
 *   otherwise, the memory leak may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t
CNNL_WIN_API cnnlDestroyGridSampleDescriptor(cnnlGridSampleDescriptor_t grid_sample_desc);

// Group:GridSampleForward
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the ::cnnlGridSampleForward operation.
 *
 * The size of the extra workspace is based on the given information of the operation,
 * including the input tensor descriptor \p input_desc, the grid tensor descriptor \p grid_desc,
 * and the output tensor descriptor \p output_desc. For more information about the workspace,
 * see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   index_copy operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] grid_desc
 *   Input. The descriptor of the grid tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   ::cnnlGridSampleForward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetGridSampleForwardWorkspaceSize(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t input_desc,
                                      const cnnlTensorDescriptor_t grid_desc,
                                      const cnnlTensorDescriptor_t output_desc,
                                      size_t *workspace_size);

// Group:GridSampleForward
/*!
 * @brief Using the coordinate value in the grid, and the pixel value of the coordinate in the
 * input tensor, computes the corresponding pixel value of output tensor. An interpolation
 * algorithm is selected in the computing.
 *
   @verbatim
       ___________________
      |input              |
      |                   |
      |         _         |
      |        |_|--------+-----------------
      |      (x0, y0)     |                |
      |                   |                |
      |                   |                |
      |___________________|                |
                                           |
      ---------------------                |
      |grid  _           y|                |
      |     |_|y0         |                |
    --------------------- |         -------V-------------
    |grid  _           x| |         |      _      output|
    |     |_|x0         | |         |     |_|           |
    |      |            | |         |      ^            |
    |      |            | |         |      |            |
    |      |            |_|         |      |            |
    |      |            |           |      |            |
    |      |            |           |      |            |
    |______|____________|           |______|____________|
           |_______________________________|
   @endverbatim
 *
 * As shown in the schematic diagram, given an input tensor with shape [n, h_in, w_in, c],
 * and a grid tensor with shape [n, h_out, w_out, 2], the output tensor will have shape
 * [n, h_out, w_out, c]. For each output pixel `c_nhw`, get the location information from
 * grid tensor pixel `xy_nhw`. The corresponding pixel of grid has two numbers, which indicates
 * the coordinate [x, y] in the input tensor.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] grid_sample_desc
 *   Input. The descriptor of grid_sample operation. For detailed information,
 *   see ::cnnlGridSampleDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] grid_desc
 *   Input. The descriptor of the grid tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] grid
 *   Input. Pointer to the MLU memory that stores the grid tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlGridSampleForward operation.
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the operation.
 *   You can get the size of the workspace with
 *   the ::cnnlGetGridSampleForwardWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "GridSampleForward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input and out tensors. Note
 *   that the two input(input, grid) tensors and output tensor should have the same data type.
 *   - input tensors: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - Data layouts of input tensors \p input and \p grid, and output tensor \p output
 *   must be \p CNNL_LAYOUT_NHWC or \p CNNL_LAYOUT_NDHWC.
 *
 * @par Scale Limitation
 * - The N-dimension of input tensor must be equal to N-dimension of grid and output.
 * - The channel dimension of grid tensor must be 2 or 3.
 * - The channel dimension of input tensor must be equal to the channel dimension of output.
 * - The height and width dimensions of grid must be equal to those of output tensor.
 *
 * @note
 * - The operation supports NaN/infinity in input data only on MLU300 series and CE3226 platform.
 * - Currently, only spatial (4-D) and volumetric (5-D) inputs are supported.
 * - The input tensor with stride feature is supported while the output tensor
 *   with stride feature is not supported now.
 * - When the input is 4-D, \p CNNL_INTERP_BILINEAR and \p CNNL_INTERP_NEAREST interpolation modes are supported.
 * - When the input is 5-D, only \p CNNL_INTERP_BILINEAR interpolation mode is supported.
 * - The \p CNNL_INTERP_NEAREST interpolation mode is only supported
 *   on MLU300 series and MLU500 series.
 * - When the input is 4-D, in the \p CNNL_INTERP_BILINEAR interpolation mode, this operation only supports
 *   \p CNNL_GRIDSAMPLE_PADDING_ZEROS and \p CNNL_GRIDSAMPLE_PADDING_REFLECTION padding modes.
 * - When the input is 5-D, in the \p CNNL_INTERP_BILINEAR interpolation mode, this operation only supports
 *   \p CNNL_GRIDSAMPLE_PADDING_ZEROS padding mode.
 * - When the input is 5-D, align_corners mode must be false.
 * - In the \p CNNL_INTERP_NEAREST interpolation mode, this operation only supports
 *   \p CNNL_GRIDSAMPLE_PADDING_ZEROS padding mode.
 * - The \p CNNL_GRIDSAMPLE_PADDING_REFLECTION padding mode is only supported
 *   on MLU300 series and MLU500 series.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html
 * - https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/GridSample.h
 */
cnnlStatus_t CNNL_WIN_API cnnlGridSampleForward(cnnlHandle_t handle,
                                                const cnnlGridSampleDescriptor_t grid_sample_desc,
                                                const cnnlTensorDescriptor_t input_desc,
                                                const void *input,
                                                const cnnlTensorDescriptor_t grid_desc,
                                                const void *grid,
                                                const cnnlTensorDescriptor_t output_desc,
                                                void *output,
                                                void *workspace,
                                                size_t workspace_size);

// Group:GridSampleBackward
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the ::cnnlGridSampleBackward operation.
 *
 * The size of the extra workspace is based on the given information of the operation,
 * including the \p grad_output tensor descriptor \p grad_output_desc, the \p input tensor
 * descriptor \p input_desc, and the \p grid tensor descriptor \p grid_desc.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a CNNL context that is used to manage MLU devices and queues in the
 *   GridSampleBackward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] grad_output_desc
 *   Input. The descriptor of the \p grad_output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the \p input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] grid_desc
 *   Input. The descriptor of the \p grid tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   ::cnnlGridSampleBackward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetGridSampleBackwardWorkspaceSize(const cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t grad_output_desc,
                                       const cnnlTensorDescriptor_t input_desc,
                                       const cnnlTensorDescriptor_t grid_desc,
                                       size_t *workspace_size);

// Group:GridSampleBackward
/*!
 * @brief Implements the backward propagation for the grid_sample function.
 *
 * @param[in] handle
 *   Input. Handle to a CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] grid_sample_desc
 *   Input. The descriptor of grid_sample operation. For detailed information,
 *   see ::cnnlGridSampleDescriptor_t.
 * @param[in] grad_output_desc
 *   Input. The descriptor of \p grad_output tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] grad_output
 *   Input. Pointer to the MLU memory that stores the gradient tensor.
 *   \p grad_output is the input gradient of cnnlGridSampleBackward.
 * @param[in] input_desc
 *   Input. The descriptor of \p input tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the \p input tensor.
 * @param[in] grid_desc
 *   Input. The descriptor of \p grid tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] grid
 *   Input. Pointer to the MLU memory that stores the \p grid tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlGridSampleBackward operation.
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the operation.
 *   You can get the size of the workspace with
 *   the ::cnnlGetGridSampleBackwardWorkspaceSize function.
 * @param[in] grad_input_desc
 *   Input. The descriptor of \p grad_input tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] grad_input
 *   Output. Pointer to the MLU memory that stores the \p grad_input tensor.
 *   \p grad_input is the calculated gradient result of \p input.
 * @param[in] grad_grid_desc
 *   Input. The descriptor of \p grad_grid tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] grad_grid
 *   Output. Pointer to the MLU memory that stores the \p grad_grid tensor.
 *   \p grad_grid is the calculated gradient result of \p grid.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "GridSampleBackward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input and output tensor. Note that the
 *   three input(grad_output, input, grid) tensors and two output(grad_input, grad_grid)
 *   tensors should have the same data type.
 *   - input tensors: half, float.
 *   - output tensors: half, float.
 *
 * @par Data Layout
 * - Data layouts of input tensors \p grad_output, \p input and \p grid, and output tensors
 *   \p grad_input and \p grad_grid must be \p CNNL_LAYOUT_NHWC or \p CNNL_LAYOUT_NDHWC.
 *
 * @par Scale Limitation
 *   - The N-dimension of input tensor must be equal to N-dimension of grad_output, grid,
 *     grad_input and grad_grid.
 *   - The channel dimension of grid and grad_grid tensor must be 2 or 3.
 *   - The channel dimension of input tensor must be equal to the channel dimension of grad_output
 *     and grad_input tensor.
 *   - The height and width dimensions of grid tensor must be equal to those of grad_grid and
 *     grad_output tensor.
 *   - The height and width dimensions of input tensor must be equal to those of grad_input tensor.
 *
 * @note
 * - The operation does not support NaN/infinity in input data.
 * - Currently, only spatial (4-D) and volumetric (5-D) inputs are supported.
 * - The input tensor with stride feature is supported while the output tensor
 *   with stride feature is not supported now.
 * - When the input is 4-D, \p CNNL_INTERP_BILINEAR and \p CNNL_INTERP_NEAREST interpolation modes are supported.
 * - When the input is 5-D, only \p CNNL_INTERP_BILINEAR interpolation mode is supported.
 * - The \p CNNL_INTERP_NEAREST interpolation mode is only supported
 *   on MLU300 series and MLU500 series.
 * - Only supports \p CNNL_GRIDSAMPLE_PADDING_ZEROS, \p CNNL_GRIDSAMPLE_PADDING_BORDER
 *   and \p CNNL_GRIDSAMPLE_PADDING_REFLECTION padding modes.
 * - The \p CNNL_GRIDSAMPLE_PADDING_BORDER and \p CNNL_GRIDSAMPLE_PADDING_REFLECTION
 *   padding modes are only supported on MLU300 series and MLU500 series.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html
 * - https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/GridSampler.cu
 */
cnnlStatus_t CNNL_WIN_API
cnnlGridSampleBackward(const cnnlHandle_t handle,
                       const cnnlGridSampleDescriptor_t grid_sample_desc,
                       const cnnlTensorDescriptor_t grad_output_desc,
                       const void *grad_output,
                       const cnnlTensorDescriptor_t input_desc,
                       const void *input,
                       const cnnlTensorDescriptor_t grid_desc,
                       const void *grid,
                       void *workspace,
                       size_t workspace_size,
                       const cnnlTensorDescriptor_t grad_input_desc,
                       void *grad_input,
                       const cnnlTensorDescriptor_t grad_grid_desc,
                       void *grad_grid);

// Group:RoiAlignRotated
/*!
 * @brief According to the \p rois with rotation, extracts the corresponding \p features information to \p output
 * by bilinear interpolation.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   ::cnnlRoiAlignRotatedForward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] features_desc
 *   Input. The descriptor of the features tensor.
 * @param[in] features
 *   Input. Pointer to the MLU memory that stores the features tensor. The shape of \p features is [batch_num, H, W, C].
 * @param[in] rois_desc
 *   Input. Descriptor of rois tensor, containing dimension and the layout of rois.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] rois
 *   Input. Pointer to the MLU memory that stores rois tensors. \p rois[i] consists of [batch_id, x, y, w, h, theta], where \p batch_id
 *   is the ID of the batch, \p x and \p y are the coordinate of center point, \p w and \p h are the width and height of rois, and \p theta
 *   is the rotated angle.
 * @param[in] pooled_height
 *   Input. The height of output.
 * @param[in] pooled_width
 *   Input. The width of output.
 * @param[in] sample_ratio
 *   Input. The number of sampling points in the bin used to compute the output.
 * @param[in] spatial_scale
 *   Input. The spatial scale of each regions of interest in the output.
 * @param[in] aligned
 *   Input. A Boolean value that determines whether to shift the roi by 0.5 pixel. If the value of \p aligned
 *   is set to true, the roi is shifted by 0.5. If the value of \p aligned is set to false, the roi is not shifted.
 * @param[in] clockwise
 *   Input. A Boolean value that determines whether the rotation of roi is clockwise.
 * @param[out] output_desc
 *   Input.  Descriptor of output, containing dimension and the layout of output.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "RoiAlignRotatedForward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \p features, \p rois,
 *   and output tensor \p output. Data type of all tensors should be the same.
 *   - input tensor: half, float.
 *   - rois tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layouts of \p features, \p rois, and \p output are as follows:
 *   - input tensor: \p CNNL_LAYOUT_NHWC.
 *   - rois tensor: \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - The \p features tensor and \p output tensor should be 4D.
 * - The half data type is not recommended due to low precision.
 * - Size of the lowest dimension of \p features tensor and \p output tensor should be the same.
 * - The \p rois tensor should be 2D array.
 * - Size of the highest dimension of \p output tensor and \p rois tensor should be the same.
 * - The shape of \p rois should be [rois_num, 6].
 * - \p batch_id should be in range of [0, \p batch_num - 1], \p x and \p y should be greater than or
 *   equal to 0 and less than \p H and \p W respectively. Both of \p h and \p w should be greater than zero
 *   and less than \p H and \p W respectively.
 * - \p spatial_scale and \p sample_ratio should not be less than zero.
 *
 * @par API Dependency
 * - None.
 *
 * @par Performance Optimization
 * - None.
 *
 * @note
 * - NaN and infinity are not supported for all parameters in \p boxes, except for the \p x and \p y parameters
 *   that support infinity.
 * - The values of the parameters \p x, \p y, \p w and \p h in \p rois multiplied by \p spatial_scale cannot exceed
 *   the range that can be represented by the parameter type.
 * - This operation is not supported on the 1V platforms.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the roi_align_rotated_forward operation is as follows:
     @verbatim
     input two arrays by 1 * 3 * 3 * 1 and 1 * 6 --> input: [[[[1.0],[1.0],[1.0]],[[1.0],[1.0],[1.0]],[[1.0],[1.0],[1.0]]]]

     --> rois: [[0.0, 1.0, 1.0, 1.0, 1.0, 0.0]]

     param:
            pooled_height: 2, pooled_width: 2, spatial_scale: 1.0,
            sampling_ratio: 2, aligned: false, clockwise: false

     output array by 1 * 2 * 2 * 1 -->
         output: [[[[1],[1]],[[1],[1]]]]
     @endverbatim
 *
 * @par Reference
 * - http://github.com/open-mmlab/mmcv/blob/master/mmcv/ops/roi_align_rotated.py
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlRoiAlignRotatedForward(cnnlHandle_t handle,
                           const cnnlTensorDescriptor_t features_desc,
                           const void* features,
                           const cnnlTensorDescriptor_t rois_desc,
                           const void* rois,
                           const int pooled_height,
                           const int pooled_width,
                           const int sample_ratio,
                           const float spatial_scale,
                           const bool aligned,
                           const bool clockwise,
                           const cnnlTensorDescriptor_t output_desc,
                           void* output);

// Group:RoiAlignRotatedBackward
/*!
 * @brief Computes the gradients of feature map \p bottom_grad based on the input \p top_grad and
 * \p rois to perform the backpropagation of the ::cnnlRoiAlignRotatedForward operator.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   ::cnnlRoiAlignRotatedBackward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] top_grad_desc
 *   Input. The descriptor of the gradient tensor in the backpropagation process.
 * @param[in] top_grad
 *   Input. Pointer to the MLU memory that stores the top_grad tensor.
 * @param[in] rois_desc
 *   Input. Descriptor of rois tensor, containing dimension and the layout of rois.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] rois
 *   Input. Pointer to the MLU memory that stores rois tensors. \p rois[i] consists of [batch_id, x, y, w, h, theta], where \p batch_id
 *   is the ID of the batch, \p x and \p y are the coordinate of center point, \p w and \p h are the width and height of rois, and \p theta
 *   is the rotated angle.
 * @param[in] pooled_height
 *   Input. The height of output.
 * @param[in] pooled_width
 *   Input. The width of output.
 * @param[in] sample_ratio
 *   Input. The number of sampling points in the bin used to compute the output.
 * @param[in] spatial_scale
 *   Input. The spatial scale of each regions of interest in the output.
 * @param[in] aligned
 *   Input. A Boolean value that determines whether to shift the roi by 0.5 pixel. If the value of \p aligned
 *   is set to true, the roi is shifted by 0.5. If the value of \p aligned is set to false, the roi is not shifted.
 * @param[in] clockwise
 *   Input. A Boolean value that determines whether the rotation of roi is clockwise.
 * @param[in] bottom_grad_desc
 *   Input. Descriptor of the gradient tensor of the origin feature map.
 * @param[out] bottom_grad
 *   Output. Pointer to the MLU memory that stores the bottom_grad tensor. The shape of bottom_grad is [batch_num, H, W, C].
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "RoiAlignRotatedBackward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \p top_grad, \p rois,
 *   and output tensor \p bottom_grad. Data type of all tensors should be the same.
 *   - top_grad tensor: half, float.
 *   - rois tensor: half, float.
 *   - bottom_grad tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layouts of \p top_grad, \p rois, and \p bottom_grad are as follows:
 *   - top_grad tensor: \p CNNL_LAYOUT_NHWC.
 *   - rois tensor: \p CNNL_LAYOUT_ARRAY.
 *   - bottom_grad tensor: \p CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - The \p bottom_grad tensor and \p top_grad tensor should be 4D.
 * - The half data type is not recommended due to low precision.
 * - Size of the lowest dimension of \p bottom_grad tensor and \p top_grad tensor should be the same.
 * - The \p rois tensor should be 2D array.
 * - Size of the highest dimension of \p top_grad tensor and \p rois tensor should be the same.
 * - \p batch_id should be in range of [0, \p batch_num - 1], \p x and \p y should be greater than or
 *   equal to 0 and less than \p H and \p W respectively. Both of \p h and \p w should be greater than zero
 *   and less than \p H and \p W respectively.
 * - \p spatial_scale and \p sample_ratio should not be less than zero.
 *
 * @par API Dependency
 * - None.
 *
 * @par Performance Optimization
 * - None.
 *
 * @note
 * - NaN and infinity are not supported for all parameters in \p boxes, except for the \p x and \p y parameters
 *   that support infinity.
 * - The values of the parameters \p x, \p y, \p w and \p h in \p rois multiplied by \p spatial_scale cannot exceed
 *   the range that can be represented by the parameter type.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the roi_align_rotated_backward operation is as follows:
     @verbatim
     input two arrays by 1 * 1 * 1 * 1 and 1 * 6 --> input: [[[[1.0]]]]

     --> rois: [[0.0, 0.0, 0.0, 1.0, 1.0, 0.0]]

     param:
            pooled_height: 1.0, pooled_width: 1.0, spatial_scale: 1.0,
            sampling_ratio: 2, aligned: false, clockwise: false

     output array by 1 * 2 * 2 * 1 -->
         output: [[[[0.25], [0.25]], [[0.25], [0.25]]]]
     @endverbatim
 *
 * @par Reference
 * - http://github.com/open-mmlab/mmcv/blob/master/mmcv/ops/roi_align_rotated.py
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlRoiAlignRotatedBackward(cnnlHandle_t handle,
                            const cnnlTensorDescriptor_t top_grad_desc,
                            const void* top_grad,
                            const cnnlTensorDescriptor_t rois_desc,
                            const void* rois,
                            const int pooled_height,
                            const int pooled_width,
                            const int sample_ratio,
                            const float spatial_scale,
                            const bool aligned,
                            const bool clockwise,
                            const cnnlTensorDescriptor_t bottom_grad_desc,
                            void* bottom_grad);

// Group:ApplyProximalAdagrad
/*!
 * @brief Updates \p var and \p accum tensor according to FOBOS with Adagrad learning rate.
 * This function only supports in-place operation when the pointer to the input and output
 * tensor are the same tensor.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in ::cnnlApplyProximalAdagrad. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] var_desc
 *   Input. The descriptor of the \p var tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out] var
 *   Input and output. Pointer to the MLU memory that stores the \p var tensor.
 *   The \p var value is the optimization goal of whole algorithm.
 * @param[in] accum_desc
 *   Input. The descriptor of the \p accum tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out] accum
 *   Input and output. Pointer to the MLU memory that stores the \p accum tensor.
 *   The \p accum is the accumulation of gradient.
 * @param[in]  grad_desc
 *   Input. The descriptor of the \p grad tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  grad
 *   Input. Pointer to the MLU memory that stores the \p grad tensor. It is the gradient of \p var.
 *   With the \p grad value, ::cnnlApplyProximalAdagrad function can be used to calculate \p accum and \p var.
 * @param[in] lr
 *   Input. Pointer to the MLU memory that stores the \p lr parameter.
 *   It is the learning rate of this optimizer.
 * @param[in] l1
 *   Input. Pointer to the MLU memory that stores the \p l1 parameter.
 *   It is the L1 regularization of this optimizer.
 * @param[in] l2
 *   Input. Pointer to the MLU memory that stores the \p l2 parameter.
 *   It is the L2 regularization of this optimizer.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "Apply Proximal Adagrad" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for
 *   \p grad tensor, \p accum tensor, and \p var tensor.
 *   Note that the combinations of these tensors must be half-half or float-float.
 *   - grad tensor: half, float
 *   - accum tensor: half, float
 *   - var tensor: half, float
 *
 * @par Scale Limitation
 * - The dimensions of \p grad, \p accum and \p var should be the same.
 *
 * @note
 * - Use data type of float for \p var, \p accum and \p grad for higher
 * precision.
 * - On MLU200 series, if data type is half, the value of (\p accum + \p grad * \p grad)
 *   should be greater than 0.000367 and less than 65504; if data type is float, the value of
 *   (\p accum + \p grad * \p grad) should be greater than 0.000367 and less than 1e6.
 *
 * @par Requirements
 * - None
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
     input three arrays by 2, 2, 2
     --> var:   [1.0, 1.0]
     --> accum: [4.0, 2.0]
     --> grad:  [1.0, 2.0]

     param:
        lr: 1.0
        l1: 1.0
        l2: 1.0

     output array by 2, 2
      --> var: [1.5147185, 0.45491502]
      --> accum: [2.0, 5.0]
     @endverbatim
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlApplyProximalAdagrad(cnnlHandle_t handle,
                                                   const cnnlTensorDescriptor_t var_desc,
                                                   void *var,
                                                   const cnnlTensorDescriptor_t accum_desc,
                                                   void *accum,
                                                   const cnnlTensorDescriptor_t grad_desc,
                                                   const void *grad,
                                                   const void *lr,
                                                   const void *l1,
                                                   const void *l2);


// Group:Atan2
/*!
 * @brief Computes atan2 on input tensor \p input1 and \p input2, and returns the results
 *        in the output tensor \p output.
 *
 * This function may need extra MLU memory as the workspace to improve the operation performance.
 * You can get the workspace size with the ::cnnlGetAtan2WorkspaceSize
 * function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the atan2
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. The \p prefer modes defined in ::cnnlComputationPreference_t enum.
 * @param[in] input1_desc
 *   Input. The descriptor of the input1 tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input1
 *   Input. Pointer to the MLU memory that stores the atan2 first input tensor.
 * @param[in] input2_desc
 *   Input. The descriptor of the input2 tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input2
 *   Input. Pointer to the MLU memory that stores the atan2 second input tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the atan2 operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the atan2 operation.
 *   You can get the size of the workspace with the ::cnnlGetAtan2WorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Atan2 Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensors and output tensor must be the same.
 * - The data types of input and output tensors are as follows:
 *   - input1 tensor: half, float, bfloat16.
 *   - input2 tensor: half, float, bfloat16.
 *   - output tensor: half, float, bfloat16.
 *
 * @par Data Layout
 * - The data layout of the input tensors and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - None.
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlAtan2 function to perform the atan2 operation.
 *
 * @note
 * - The inputs \p input1 and \p input2 are multi-dimensional arrays, supporting up to CNNL_DIM_MAX dimensions.
 * - This operation supports tensor broadcasting. \p input1, \p input2 and \p output must be broadcastable.
 * - You can specify the stride of all dimensions for input1_desc, input2_desc and output_desc with ::cnnlSetTensorDescriptorEx.
 * - No matter what \p prefer is, the calculation method for half and float is the same.
 * - On MLU370 and MLU500 series:
 *   - The absolute value of (\p input1 / \p input2) is recommended to be greater than 1e-6 for higher precision.
 *   - The inputs \p input1 and \p input2 with NaN or infinity are supported.
 *
 * @par Requirements
 * - Stride and broadcast cannot be supported at the same time.
 * - Data type bfloat16 is unsupported on MLU370.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.google.cn/api_docs/python/tf/math/atan2
 */
cnnlStatus_t CNNL_WIN_API cnnlAtan2(cnnlHandle_t handle,
                                    cnnlComputationPreference_t prefer,
                                    const cnnlTensorDescriptor_t input1_desc,
                                    const void *input1,
                                    const cnnlTensorDescriptor_t input2_desc,
                                    const void *input2,
                                    void *workspace,
                                    size_t workspace_size,
                                    const cnnlTensorDescriptor_t output_desc,
                                    void *output);

// Group:Atan2
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the atan2 operation.
 *
 * The size of the extra workspace is based on the given information of the atan2 operation,
 * including the input tensor descriptors \p input1_desc and \p input2_desc, and the output
 * tensor descriptor \p output_desc. For more information about the workspace, see "Cambricon
 * CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the atan2
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input1_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input2_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the atan2
 *   operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetAtan2WorkspaceSize(cnnlHandle_t handle,
                                                    const cnnlTensorDescriptor_t input1_desc,
                                                    const cnnlTensorDescriptor_t input2_desc,
                                                    const cnnlTensorDescriptor_t output_desc,
                                                    size_t *workspace_size);

// Group:LogAddExp
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the logaddexp operation.
 *
 * The size of the extra workspace is based on the given information of the logaddexp operation,
 * including the input tensor descriptors \p input1_desc and \p input2_desc, and the output
 * tensor descriptor \p output_desc. For more information about the workspace, see "Cambricon
 * CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   the logaddexp operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input1_desc
 *   Input. The descriptor of the first input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input2_desc
 *   Input. The descriptor of the second input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the logaddexp
 *   operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetLogAddExpWorkspaceSize(cnnlHandle_t handle,
                                                        const cnnlTensorDescriptor_t input1_desc,
                                                        const cnnlTensorDescriptor_t input2_desc,
                                                        const cnnlTensorDescriptor_t output_desc,
                                                        size_t *workspace_size);

// Group:LogAddExp
/*!
 * @brief Computes the logarithm of the sum of exponentiations of the input tensors,
 * and returns the results in the output tensors \p output.
 *
 * This function may need extra MLU memory as the workspace to improve the operation performance.
 * You can get the workspace size with the ::cnnlGetLogAddExpWorkspaceSize
 * function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   the logaddexp operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input1_desc
 *   Input. The descriptor of the first input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input1
 *   Input. Pointer to the MLU memory that stores the logaddexp first input tensor.
 * @param[in] input2_desc
 *   Input. The descriptor of the second input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input2
 *   Input. Pointer to the MLU memory that stores the logaddexp second input tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the logaddexp operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the logaddexp operation.
 *   You can get the size of the workspace with the ::cnnlGetLogAddExpWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "LogAddExp Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensors and output tensor must be the same.
 * - The data types of input and output tensors are as follows:
 *   - input1 tensor: float, bfloat16.
 *   - input2 tensor: float, bfloat16.
 *   - output tensor: float, bfloat16.
 *
 * @par Data Layout
 * - The data layout of the input tensors and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - None.
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlLogAddExp function to perform the logaddexp operation.
 *
 * @note
 * - The inputs \p input1 and \p input2 are multi-dimensional arrays, supporting up to CNNL_DIM_MAX dimensions.
 * - This operation supports tensor broadcasting. \p input1, \p input2 and \p output must be broadcastable.
 * - You can specify the strides of all dimensions for input1_desc, input2_desc and output_desc with ::cnnlSetTensorDescriptorEx.
 * - On MLU300 series and CE3226:
 *   - The absolute value of (\p input1 / \p input2) is recommended to be greater than 1e-6 for higher precision.
 *   - The inputs \p input1 and \p input2 with NaN or infinity are supported.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of logaddexp operation is as follows:
     @verbatim
     the first input array
       input1 = [-100.0 -200.0, -300.0]
     the second input array
       input2 = [-1.0, -2.0, -3.0]
     output array
       output = [-1.0, -2.0 -3.0]
     @endverbatim
 *
 * @par Reference
 * - https://www.pytorch.org/docs/stable/generated/torch.logaddexp.html
 */
cnnlStatus_t CNNL_WIN_API cnnlLogAddExp(cnnlHandle_t handle,
                                        const cnnlTensorDescriptor_t input1_desc,
                                        const void *input1,
                                        const cnnlTensorDescriptor_t input2_desc,
                                        const void *input2,
                                        void *workspace,
                                        size_t workspace_size,
                                        const cnnlTensorDescriptor_t output_desc,
                                        void *output);


// Group:DeformRoiPool
/*!
 * @brief Computes deformable roi pooling over \p input tensor. This function firstly divides the obtained
 * candidate region  into regions with the same size according to the specified pooling width and pooling height,
 * then add offsets to rois, and finally calculates the mean value of the sampling points in each bin as output.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   ::cnnlDeformRoiPoolForward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] rois_desc
 *   Input. Descriptor of rois, containing the dimension and layout of rois tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] rois
 *   Input. Pointer to the MLU memory that stores the rois tensor.
 * @param[in] offset_desc
 *   Input. Descriptor of offset, containing the dimension and layout of offset tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] offset
 *   Input. Pointer to the MLU memory that stores the offset tensor.
 * @param[in] pooled_height
 *   Input. An integer value that is the height of the output after pooling.
 * @param[in] pooled_width
 *   Input. An integer value that is the width of the output after pooling.
 * @param[in] spatial_scale
 *   Input. A float value that is the scale factor of coordinates of rois.
 * @param[in] sampling_ratio
 *   Input. An integer value that is the number of sample in one bin. This parameter
 *   only works when it is greater than zero.
 * @param[in] gamma
 *   Input. A float value that is the scale factor of offset.
 * @param[in] output_desc
 *   Input.  Descriptor of output tensor, containing the dimension and layout of output tensor.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED.
 *
 * @par Formula
 * - See "DeformRoiPoolForward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \p input, rois tensor
 *   \p rois, offset tensor \p offset and output tensor \p output. Data types of all tensors should be the same.
 *   - input tensor: half, float.
 *   - rois tensor: half, float.
 *   - offset tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layouts of \p input, \p rois, \p offset and \p output are as follows:
 *   - input tensor: \p CNNL_LAYOUT_NHWC.
 *   - rois tensor: \p CNNL_LAYOUT_ARRAY.
 *   - offset tensor: \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor must be 4D.
 * - Size of the lowest dimension of input tensor and output tensor must be the same.
 * - The rois tensor must be 2D.
 * - The offset tensor must be 4D.
 * - Size of the highest dimension of output tensor, rois tensor and offset tensor must be the same.
 * - Size of the middle two dimensions of output tensor and the lower two dimensions of offset tensor must be the same.
 * - The shape of \p input should be [batch_num, height, width, channels].
 * - The shape of \p rois should be [rois_num, 5].
 * - The shape of \p offset should be [rois_num, 2, pooled_height, pooled_width].
 * - The shape of \p output should be [rois_num, pooled_height, pooled_width, channels].
 * - \p rois[i] consists of [batch_id, x1, y1, x2, y2]. \p batch_id should be in range of [0, batch_num - 1].
 *
 * @par API Dependency
 * - None.
 *
 * @par Performance Optimization
 * - None.
 *
 * @note
 * - The inputs \p rois and \p offset with NaN or infinity are not supported.
 * - This operation is not supported on the 1V platforms.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the deform_roi_pool_forward operation is as follows:
     @verbatim
     input three arrays by 1 * 2 * 2 * 1, 1 * 5 and 1 * 2 * 1 * 1
     --> input: [[[[1.0], [2.0]], [[2.0], [4.0]]]]
     --> rois: [[0.0, 0.0, 0.0, 1.0, 1.0]]
     --> offset: [[[[0.5]], [[0.5]]]]

     param:
            pooled_height: 1.0, pooled_width: 1.0, spatial_scale: 1.0,
            sampling_ratio: 1, gamma: 1

     output array by 1 * 1 * 1 * 1 -->
         output: [[[[2.25]]]]
     @endverbatim
 *
 *
 * @par Reference
 * - http://github.com/open-mmlab/mmcv/tree/master/mmcv/ops/deform_roi_pool.py
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlDeformRoiPoolForward(const cnnlHandle_t handle,
                                                   const cnnlTensorDescriptor_t input_desc,
                                                   const void *input,
                                                   const cnnlTensorDescriptor_t rois_desc,
                                                   const void *rois,
                                                   const cnnlTensorDescriptor_t offset_desc,
                                                   const void *offset,
                                                   const int pooled_height,
                                                   const int pooled_width,
                                                   const float spatial_scale,
                                                   const int sampling_ratio,
                                                   const float gamma,
                                                   const cnnlTensorDescriptor_t output_desc,
                                                   void *output);

// Group:DeformRoiPoolBackward
/*!
 * @brief Computes the gradient of input \p grad_input and the gradient of offset \p grad_offset
 * based on the gradient of output \p grad_output, input \p input, regions of interest \p rois
 * and offset \p offset.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   ::cnnlDeformRoiPoolBackward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] grad_output_desc
 *   Input. The descriptor of the grad_output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] grad_output
 *   Input. Pointer to the MLU memory that stores the grad_output tensor.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] rois_desc
 *   Input. Descriptor of the rois tensor, containing the dimension and layout of rois tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] rois
 *   Input. Pointer to the MLU memory that stores the rois tensor.
 * @param[in] offset_desc
 *   Input. Descriptor of the offset tensor, containing the dimension and layout of offset tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] offset
 *   Input. Pointer to the MLU memory that stores the offset tensor.
 * @param[in] pooled_height
 *   Input. An integer value that is the height of the output after pooling.
 * @param[in] pooled_width
 *   Input. An integer value that is the width of the output after pooling.
 * @param[in] spatial_scale
 *   Input. A float value that is the scale factor of coordinates of rois.
 * @param[in] sampling_ratio
 *   Input. An integer value that is the number of sample in one bin. This parameter
 *   only works when it is greater than zero.
 * @param[in] gamma
 *   Input. A float value that is the scale factor of offset.
 * @param[in] grad_input_desc
 *   Input.  Descriptor of grad_input tensor, containing the dimension and layout of grad_input tensor.
 * @param[out] grad_input
 *   Output. Pointer to the MLU memory that stores the gradient of the input tensor.
 * @param[in] grad_offset_desc
 *   Input.  Descriptor of grad_offset tensor, containing the dimension and layout of grad_offset tensor.
 * @param[out] grad_offset
 *   Output. Pointer to the MLU memory that stores the gradient of the offset tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED.
 *
 * @par Formula
 * - See "DeformRoiPoolBackward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for grad_output tensor \p grad_output, input tensor \p input,
 *   rois tensor \p rois, offset tensor \p offset, grad_input tensor \p grad_input and grad_offset tensor \p grad_offset.
 *   Data types of all tensors should be the same.
 *   - grad_output tensor: half, float.
 *   - input tensor: half, float.
 *   - rois tensor: half, float.
 *   - offset tensor: half, float.
 *   - grad_input tensor: half, float.
 *   - grad_offset tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layouts of grad_output tensor \p grad_output, input tensor \p input, rois tensor \p rois,
 *   offset tensor \p offset, grad_input tensor \p grad_input and grad_offset tensor \p grad_offset are as follows:
 *   - grad_output tensor: \p CNNL_LAYOUT_NHWC.
 *   - input tensor: \p CNNL_LAYOUT_NHWC.
 *   - rois tensor: \p CNNL_LAYOUT_ARRAY.
 *   - offset tensor: \p CNNL_LAYOUT_ARRAY.
 *   - grad_input tensor: \p CNNL_LAYOUT_NHWC.
 *   - grad_offset tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The grad_output tensor, input tensor and grad_input tensor must be 4D.
 * - Size of the lowest dimension of grad_output tensor, input tensor and grad_input tensor must be the same.
 * - The rois tensor must be 2D.
 * - The offset tensor and grad_offset tensor must be 4D.
 * - Size of the highest dimension of output tensor, rois tensor, offset tensor and grad_offset tensor must be the same.
 * - Size of the middle two dimensions of grad_output tensor, the lower two dimensions of offset tensor
 *   and the lower two dimensions of grad_offset tensor must be the same.
 * - The shape of \p grad_output should be [rois_num, pooled_height, pooled_width, channels].
 * - The shape of \p input should be [batch_num, height, width, channels].
 * - The shape of \p rois should be [rois_num, 5].
 * - The shape of \p offset should be [rois_num, 2, pooled_height, pooled_width].
 * - The shape of \p grad_input should be [batch_num, height, width, channels].
 * - The shape of \p grad_offset should be [rois_num, 2, pooled_height, pooled_width].
 * - \p rois[i] consists of [batch_id, x1, y1, x2, y2]. \p batch_id should be in range of [0, batch_num - 1].
 *
 * @par API Dependency
 * - None.
 *
 * @par Performance Optimization
 * - None.
 *
 * @note
 * - The inputs \p rois and \p offset with NaN or infinity are not supported.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - http://github.com/open-mmlab/mmcv/tree/master/mmcv/ops/deform_roi_pool.py
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlDeformRoiPoolBackward(const cnnlHandle_t handle,
                                                    const cnnlTensorDescriptor_t grad_output_desc,
                                                    const void *grad_output,
                                                    const cnnlTensorDescriptor_t input_desc,
                                                    const void *input,
                                                    const cnnlTensorDescriptor_t rois_desc,
                                                    const void *rois,
                                                    const cnnlTensorDescriptor_t offset_desc,
                                                    const void *offset,
                                                    const int pooled_height,
                                                    const int pooled_width,
                                                    const float spatial_scale,
                                                    const int sampling_ratio,
                                                    const float gamma,
                                                    const cnnlTensorDescriptor_t grad_input_desc,
                                                    void *grad_input,
                                                    const cnnlTensorDescriptor_t grad_offset_desc,
                                                    void *grad_offset);

// Group:CosineSimilarity
/*!
 * @brief Returns cosine similarity between \p x1 and \p x2, computed along \p cos_dim.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   cosine similarity operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] desc_x1
 *   Input. The descriptor of the \p x1 tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x1
 *   Input. Pointer to the MLU memory that stores the \p x1 tensor.
 * @param[in] desc_x2
 *   Input. The descriptor of the \p x2 tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x2
 *   Input. Pointer to the MLU memory that stores the \p x2 tensor.
 * @param[in] desc_y
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   cosine similarity operation.
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the cosine similarity operation. You can get the size of the workspace with
 *   the ::cnnlGetCosineSimilarityWorkspaceSize function.
 * @param[in] cos_dim
 *   Input. Dimension where cosine similarity is computed.
 * @param[in] epsilon
 *   Input. Pointer to the MLU memory that stores the \p epsilon parameter.
 *   It is a small positive number just as \f$10^{-8}\f$, to avoid division by 0.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "Cosine Similarity" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *   x1 - x2 - workspace_reduce - y.
 *   - half - half - float - half.
 *   - float - float - float - float.
 *   - bfloat16 - bfloat16 - float - bfloat16.
 * - The data type bfloat16 is only supported on MLU500 series.
 *
 * @par Data Layout
 * - None.
 *
 * @par Scale Limitation
 * - None
 *
 * @note
 * - When \p cos_dim is set to one of the  middle dimensions,
 *   broadcasting is not recommended due to performace limitation.
 *
 * - The element number of two input tensors should satisfy the regular broadcasting rules.
 *
 * - The example of the expand operation is as follows:
     @verbatim
     if cos_dim is zero
     input array by 3 * 2 * 2 --> input: [[[4, 4], [4, 4]],
                                            [[4, 4], [4, 4]],
                                            [[4, 4], [4, 4]]]

     input array by 3 * 2 * 2 --> input: [[[5, 5], [5, 5]],
                                            [[5, 5], [5, 5]],
                                            [[5, 5], [5, 5]]]

     output one array by 2 * 2 --> output: [[1, 1], [1, 1]]

     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/1.6.0/nn.functional.html?highlight=cosine_similarity#torch.nn.functional.cosine_similarity
 */
cnnlStatus_t CNNL_WIN_API cnnlCosineSimilarity(cnnlHandle_t handle,
                                               int32_t cos_dim,
                                               const float epsilon,
                                               const cnnlTensorDescriptor_t desc_x1,
                                               const void *x1,
                                               const cnnlTensorDescriptor_t desc_x2,
                                               const void *x2,
                                               void *workspace,
                                               size_t workspace_size,
                                               const cnnlTensorDescriptor_t desc_y,
                                               void *y);

// Group:CosineSimilarity
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra workspace
 * to optimize the cosine similarity operation.
 *
 * The size of the extra workspace is based on the given information of the cosine similarity operation,
 * including the tensor descriptors \p desc_x1, \p desc_x2 and \p desc_y.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the cosine similarity operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  desc_x1
 *   Input. The descriptor of the input tensor \p x1. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  desc_x2
 *   Input. The descriptor of the input tensor \p x2. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  desc_y
 *   Output. The descriptor of the output tensor \p y. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] cos_dim
 *   Input. Dimension where cosine similarity is computed.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *   the cosine similarity operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlCreateTensorDescriptor and
 *   ::cnnlSetTensorDescriptor functions to create and set the tensor descriptors \p x_desc.
 * - The allocated extra workspace should be passed to the ::cnnlCosineSimilarity function
 *   to perform the cosine similarity operation.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetCosineSimilarityWorkspaceSize(cnnlHandle_t handle,
                                                               int32_t cos_dim,
                                                               const cnnlTensorDescriptor_t desc_x1,
                                                               const cnnlTensorDescriptor_t desc_x2,
                                                               const cnnlTensorDescriptor_t desc_y,
                                                               size_t *workspace_size);

// Group:Unfold
/*!
 * @brief Unfolds the input tensor \p input along the axis \p dimension into slices of
 * size \p size based on the given stride \p step.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the unfold operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in]  dimension
 *   Input.  An int32 scalar, the dimension to unfold.
 * @param[in]  size
 *   Input.  An int32 scalar, the size of each slice.
 * @param[in]  step
 *   Input.  An int32 scalar, the step between two slices.
 * @param[in]  output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out]  output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED, ::CNNL_STATUS_INTERNAL_ERROR.
 *
 * - See "Unfold Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for input tensor \p input and output tensor \p output:
 *   - input tensor: bool, uint8, int8, uint16, int16, uint32, int32, uint64, int64, half, bfloat16, float, double, complex_half, complex_float.
 *   - output tensor: bool, uint8, int8, uint16, int16, uint32, int32, uint64, int64, half, bfloat16, float, double, complex_half, complex_float.
 *   Note that the data types of input tensor and output tensor must be the same.
 *
 * @par Scale Limitation
 * - (-input_dimNb) <= \p dimension < input_dimNb
 * - \p step > 0
 * - if \p dimension < 0: \p dimension = \p dimension + input_dimNb
 * - 0 <= \p size <= input_dims[\p dimension]
 * - output_dimNb = input_dimNb + 1
 * - output_dims should meet the following requirements:
 *   - when 0 <= i < \p dimension: output_dims[i] = input_dims[i]
 *   - when \p dimension < i < input_dimNb: output_dims[i] = input_dims[i]
 *   - output_dims[\p dimension] = floor_div(input_dims[\p dimension] - \p size, \p step) + 1
 *   - output_dims[output_dimNb - 1] = \p size
 *
 * @par Example
 * - The example of unfold operation is as follows:
    @verbatim
    input: an array by 1*7 -->  [0,1,2,3,4,5,6],

    dimension:  an int32 scalar: 0

    size:       an int32 scalar: 3

    step:       an int32 scalar: 2

    Then we will get the output:

    output: an array by 3*3 --> [[0,1,2],
                                 [2,3,4],
                                 [4,5,6]]
    @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.Tensor.unfold.html
 * - https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/TensorShape.cpp
 *
 */

cnnlStatus_t CNNL_WIN_API cnnlUnfold(cnnlHandle_t handle,
                                     const int32_t dimension,
                                     const int32_t size,
                                     const int32_t step,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const void *input,
                                     const cnnlTensorDescriptor_t output_desc,
                                     void *output);

/******************************************************************************
 * Cambricon CNNL OP: Conj
 ******************************************************************************/
// Group:Conj
/*!
 * @brief Computes the conjugate of the input tensor \p x element-wise and returns
 * the results in the output tensor \p y. If an element in the input tensor is a non-complex number,
 * the conjugate of the element is itself.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   Conj operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the x tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the x tensor of complex numbers.
 * @param[in] y_desc
 *   Input. The descriptor of the y tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the y tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED.
 *
 * - See "Conj Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported data types of x and y tensors are as follows:
 *   - x tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, bfloat16, float, double, complex_float and complex_double.
 *   - y tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, bfloat16, float, double, complex_float and complex_double.
 *
 * @par Scale Limitation
 * - None
 *
 * @par Note
 * - Not supported on 520 platforms.
 *
 * @par Example
 * - The example of the conj operation is as follows:
    @verbatim
    input one array by 1 * 3 --> input: [(-1.,1.), (-2.,2.), (3.,-3.)]

    output array by 1 * 3 --> output: [(-1.,-1.), (-2.,-2.), (3.,3.)]
    @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/1.9.0/generated/torch.conj.html?hightlight=conj#torch.conj
 * - https://github.com/pytorch/pytorch/blob/release/1.9/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp#L196
 * - https://github.com/pytorch/pytorch/blob/release/1.9/aten/src/ATen/native/cuda/UnaryComplexKernels.cu#L83
 */
cnnlStatus_t CNNL_WIN_API cnnlConj(cnnlHandle_t handle,
                                   const cnnlTensorDescriptor_t x_desc,
                                   const void *x,
                                   const cnnlTensorDescriptor_t y_desc,
                                   void *y);


// Group:Trunc
/*!
 * @brief Computes the truncation of each element in the input tensor \p x, and returns
 * the result in the output tensor \p y.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the trunc
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor \p x. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor \p y. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "Trunc Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, bfloat16, float.
 *   - output tensor: half, bfloat16, float.
 *
 * @par Data Layout
 * - Data Layout of input tensor and output tensor should be the same.
 *
 * @par Requirements
 * - None.
 *
 * @par Scale Limitation
 * - The shape of input tensor and output tensor should be the same.
 * - The total number of dimensions of input tensor and output tensor should be the same.
 *
 * @par API Dependency
 * - None.
 *
 * @note
 * - This operation is not supported on the 1V platforms.
 *
 * @par Example
     @verbatim
       input:  [-2.1, -1.8, 9.125, 6.9, 4.0]
       output: [-2., -1., 9., 6.0, 4.0]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/1.6.0/generated/torch.trunc.html#torch.trunc
 * - https://pytorch.org/docs/1.9.0/generated/torch.trunc.html#torch.trunc
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlTrunc(cnnlHandle_t handle,
          const cnnlTensorDescriptor_t x_desc,
          const void *x,
          const cnnlTensorDescriptor_t y_desc,
          void *y);

// Group:BorderAlignForward
/*!
 * @brief Extracts the border features of \p input based on the bounding boxes to compute the
 *   maximum border features of \p input with the maximum pooling.
 *   The computing process of this operation is as follows:
 *     1. For each border line of each box (commonly four lines: top, left, bottom and right lines), uniformly samples
 *        pool_size + 1 positions on this line, involving the start and end points.
 *     2. Compute the corresponding features on these points by bilinear interpolation.
 *     3. Perform the max pooling over all pool_size + 1 positions to output the pooled features.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   ::cnnlBorderAlignForward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor. The shape of \p input is [N, H, W, 4C].
 *   Channels ranged in [0,C), [C,2C), [2C,3C), [3C,4C) represent the top, left, bottom and right features
 *   respectively.
 * @param[in] boxes_desc
 *   Input. Descriptor of bounding box tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] boxes
 *   Input. Pointer to the MLU memory that stores boxes tensors. The shape of \p boxes is [N, H * W, 4].
 * @param[in] pool_size
 *   Input. Number of positions sampled over the boxes' borders.
 * @param[in] output_desc
 *   Input.  Descriptor of \p output, containing dimension and the layout of output.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor. The shape of
 *   argmax_idx is [N, H * W, 4, C].
 * @param[in] argmax_idx_desc
 *   Input.  Descriptor of \p argmax_idx, containing dimension and the layout of \p argmax_idx.
 * @param[out] argmax_idx
 *   Output. Pointer to the MLU memory that stores the \p argmax_idx tensor, which is the indices of
 *   maximum values after the max pooling. The shape of argmax_idx is [N, H * W, 4, C].
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "BorderAlignForward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensors \p input, \p boxes, \p pool_size,
 *    \p output and \p argmax_idx. Data type of the \p input, \p boxes and \p output tensors should be the same.
 *   - input tensor: half, float.
 *   - boxes tensor: half, float.
 *   - pool_size: int32_t.
 *   - output tensor: half, float.
 *   - argmax_idx: int32_t.
 *
 * @par Data Layout
 * - The supported data layouts of \p input, \p boxes, \p output and \p argmax_idx are as follows:
 *   - input tensor: \p CNNL_LAYOUT_NHWC.
 *   - boxes tensor: \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_NHWC.
 *   - argmax_idx tensor: \p CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - The \p input, \p output and \p argmax_idx should be 4D.
 * - The \p boxes tensor should be 3D array, and the highest dimension of \p boxes must be 4.
 *
 * @par API Dependency
 * - None.
 *
 * @par Performance Optimization
 * - None.
 *
 * @note
 * - This operation is not supported on the 1V platforms.
 *
 * @par Requirements
 * - None.
 * - The example of the border_align_forward operation is as follows:
     @verbatim
     input: a tensor with shape by 1 * 4 * 3 *4  --> [[[[ 1.,  2.,  3.,  4.],
                                                        [ 5.,  6.,  7.,  8.],
                                                        [ 9., 10., 11., 12.]],

                                                       [[ 6.,  7.,  5.,  8.],
                                                        [ 2.,  1.,  3.,  4.],
                                                        [12.,  9., 11., 10.]],

                                                       [[-2., -3.,  2.,  0.],
                                                        [-4., -5.,  1., -1.],
                                                        [-1., -1., -1., -1.]],

                                                       [[ 0., -1.,  2.,  1.],
                                                        [-4., -3., -2., -1.],
                                                        [-1., -2., -3., -4.]]]]

     boxes: a tensor with shape by 1 * 12 * 4  --> [[[0., 0., 2., 1.],
                                                     [1., 0., 3., 1.],
                                                     [1., 0., 2., 1.],
                                                     [0., 0., 3., 1.],
                                                     [0., 0., 1., 2.],
                                                     [0., 0., 2., 2.],
                                                     [1., 0., 2., 1.],
                                                     [1., 0., 3., 1.],
                                                     [0., 1., 1., 2.],
                                                     [0., 0., 3., 2.],
                                                     [1., 0., 3., 2.],
                                                     [2., 0., 3., 2.]]]

     param: pool_size = 1.
     output: a tensor with shape by 1 * 1 * 12 * 4 --> [[[[ 3.,  6.,  1.,  2.],
                                                          [ 4.,  7., -1.,  1.],
                                                          [ 3.,  7.,  1.,  2.],
                                                          [ 4.,  6., -1.,  1.],
                                                          [ 2., 12., -1., -1.],
                                                          [ 3., 12., -1.,  2.],
                                                          [ 3.,  7.,  1.,  2.],
                                                          [ 4.,  7., -1.,  1.],
                                                          [ 6., 12., -1., -2.],
                                                          [ 4., 12., -1.,  1.],
                                                          [ 4.,  9., -1.,  1.],
                                                          [ 4., 11., -1.,  1.]]]]

     argmax_idxa tensor with shape by 1 * 1 * 12 * 4 -->[[[[1, 0, 0, 1],
                                                             [1, 0, 0, 1],
                                                             [1, 0, 0, 1],
                                                             [1, 0, 0, 1],
                                                             [1, 1, 0, 1],
                                                             [1, 1, 0, 1],
                                                             [1, 0, 0, 1],
                                                             [1, 0, 0, 1],
                                                             [1, 1, 0, 0],
                                                             [1, 1, 0, 1],
                                                             [1, 1, 0, 1],
                                                             [1, 1, 0, 1]]]]
   @endverbatim
 *
 * @par Reference
 * - http://github.com/open-mmlab/mmcv/blob/master/mmcv/ops/border_align.py
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlBorderAlignForward(cnnlHandle_t handle,
                                                 const cnnlTensorDescriptor_t input_desc,
                                                 const void *input,
                                                 const cnnlTensorDescriptor_t boxes_desc,
                                                 const void *boxes,
                                                 const int32_t pool_size,
                                                 const cnnlTensorDescriptor_t output_desc,
                                                 void *output,
                                                 const cnnlTensorDescriptor_t argmax_idx_desc,
                                                 void *argmax_idx);
// Group:CopySign
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the ::cnnlCopySign operation.
 *
 * The size of the extra workspace is based on the given information of the operation,
 * including the input tensor descriptor \p input_desc, the other tensor descriptor \p other_desc,
 * and the output tensor descriptor \p output_desc. For more information about the workspace,
 * see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   copysign operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] other_desc
 *   Input. The descriptor of the other tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   ::cnnlCopySign operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetCopySignWorkspaceSize(cnnlHandle_t handle,
                                                       const cnnlTensorDescriptor_t input_desc,
                                                       const cnnlTensorDescriptor_t other_desc,
                                                       const cnnlTensorDescriptor_t output_desc,
                                                       size_t *workspace_size);

// Group:CopySign
/*!
 * @brief Creates a new floating-point tensor with the magnitude of input tensor
 * and the sign of other tensor, elementwise.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] other_desc
 *   Input. The descriptor of the other tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] other
 *   Input. Pointer to the MLU memory that stores the other tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlCopySign operation.
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the operation.
 *   You can get the size of the workspace with
 *   the ::cnnlGetCopySignWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Copysign Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensors and output tensor. Note
 *   that The two input tensors (input, other) and output tensor should have the same data type.
 *   - input tensors: half, bfloat16, float.
 *   - output tensor: half, bfloat16, float.
 *
 *   bfloat16 is only supported on MLU500 series.
 *
 * @par Data Layout
 * - The supported data layouts of \p input, \p other and \p output tensor are as follows:
 *   - input tensor: \p CNNL_LAYOUT_ARRAY.
 *   - other tensor: \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - None.
 *
 * @note
 * - This operation supports tensor broadcasting. For each dimension,
 *   the dimension length of the \p input tensor and the \p other tensor
 *   need to meet the requirements of broadcasting.
 * - The tensor \p input, \p other and \p output are multi-dimensional arrays, supporting
 *   up to \p CNNL_DIM_MAX dimensions.
 * - This operation is not supported on the 1V platforms.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the copysign operation is as follows:
     @verbatim
     example 1:
     here is an example without tensor broadcasting -->
     input array by 1 * 5 --> input: [0.35, 0.20, 0.45, 0.40, 0.40]

     other array by 1 * 5 --> other: [-1.0, 2.0, 3.0, -4.0, 5.0]

     out array by 1 * 5 --> other: [-0.35, 0.20, 0.45, -0.40, 0.40]

     example 2:
     here is an example with tensor broadcasting -->
     input array by 3 * 5 --> input: [[0.25, 0.45, 0.20, 0.10, 0.01],
                                      [0.40, 0.35, 0.35, 0.50, 0.50],
                                      [0.35, 0.20, 0.45, 0.40, 0.40]]

     other array by 1 * 1 --> other: [-5.0]

     out array by 3 * 5 --> out: [[-0.25, -0.45, -0.20, -0.10, -0.01],
                                  [-0.40, -0.35, -0.35, -0.50, -0.50],
                                  [-0.35, -0.20, -0.45, -0.40, -0.40]]

     example 3:
     here is an example with tensor broadcasting -->
     input array by 3 * 5 --> input: [[0.25, 0.45, 0.20, 0.10, 0.01],
                                      [0.40, 0.35, 0.35, 0.50, 0.50],
                                      [0.35, 0.20, 0.45, 0.40, 0.40]]

     other array by 1 * 5 --> other: [-1.0, 2.0, 3.0, -4.0, 5.0]

     out array by 3 * 5 --> out: [[-0.25, 0.45, 0.20, -0.10, 0.01],
                                  [-0.40, 0.35, 0.35, -0.50, 0.50],
                                  [-0.35, 0.20, 0.45, -0.40, 0.40]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/1.9.0/generated/torch.copysign.html?highlight=copysign#torch.copysign
 */
cnnlStatus_t CNNL_WIN_API cnnlCopySign(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t input_desc,
                                       const void *input,
                                       const cnnlTensorDescriptor_t other_desc,
                                       const void *other,
                                       void *workspace,
                                       size_t workspace_size,
                                       const cnnlTensorDescriptor_t output_desc,
                                       void *output);

// Group:PointsInBoxes
/*!
 * @brief Detects the first 3D box that each point belongs to in  given points cloud data.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the box iou rotated operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] points_desc
 *   Input. The descriptor of the input tensor \p points.
 *   The shape of \p points is [B, npoints, 3], where '3' means
 *   the 3D coordinate position (x, y, z) for each point,
 *   'B' means batch size, and 'npoints' means points number.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] points
 *   Input. Pointer to the MLU memory that stores the input tensor \p points.
 * @param[in] boxes_desc
 *   Input. The descriptor of the input tensor \p boxes.
 *   The shape of \p boxes is [B, N, 7], where ``7`` means
 *   (x, y, z, dx, dy, dz, heading) with
 *   ``x``,``y``, and ``z`` indicating 3D center coordinate for each box, and
 *   ``dx``, ``dy``, ``dz`` indicating max-ranges in x, y and z directions for each box.
 *   Note that ``dx``, ``dy``, ``dz`` is non-negative.
 *   ``B`` means batch size, ``N`` means boxes number.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] boxes
 *   Input. Pointer to the MLU memory that stores the input tensor \p boxes.
 * @param[out] points_indices_desc
 *   Output. The descriptor of the output tensor \p points_indices.
 *   The shape of \p points_indices is [B, N].
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] points_indices
 *   Output. Pointer to the MLU memory that stores the output tensor \p points_indices.
 *   \p points_indices are indexs of the first 3D box that each point belongs to in given points cloud data.
 *   If no corresponding box exists, the output is -1.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 * @par Formula
 * - See "Points In Boxes Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - By the order of \p points - \p boxes - \p points_indices, the supported data types of
 *    \p points, \p boxes and \p points_indices are as follows:
 *   - float - float - int32_t.
 *
 * @par Scale Limitation
 * - On MLU300 series, the number of boxes cannot exceed 23404; on MLU500 series, the number of boxes cannot exceed 14042.
 *
 * @note
 * - Differences between MLU and CPU/GPU may occur when the point is on the edge of the box.
 * - This operation is not supported on the 1V platforms.
 *
 * @par API Dependency
 * - None.
 *
 * @par Requirements
 * - None.
 *
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlPointsInBoxes(cnnlHandle_t handle,
                                            const cnnlTensorDescriptor_t points_desc,
                                            const void *points,
                                            const cnnlTensorDescriptor_t boxes_desc,
                                            const void *boxes,
                                            const cnnlTensorDescriptor_t points_indices_desc,
                                            void *points_indices);

// Group:ApplyFtrlV2
/*!
 * @brief Updates \p var tensor and \p accum tensor by using FTRL-Proximal method.
 * This function contains two modes that are determined by the parameter \p l2_shrinkage.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the ::cnnlApplyFtrlV2 operation. For more detailed information, see ::cnnlHandle_t.
 * @param[in] var_desc
 *   Input. The descriptor of the \p var tensor. For detailed information, see
 * ::cnnlTensorDescriptor_t.
 * @param[in,out] var
 *   Input/Output. Pointer to the MLU memory that stores the \p var tensor.
 *   The \p var value is the optimization goal of whole algorithm.
 * @param[in] accum_desc
 *   Input. The descriptor of the \p accum tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out] accum
 *   Input/Output. Pointer to the MLU memory that stores the \p accum tensor.
 *   The \p accum is the accumulation of gradient.
 * @param[in]  linear_desc
 *   Input. The descriptor of the \p linear tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out] linear
 *   Input/Output. Pointer to the MLU memory that stores the \p linear tensor.
 *   The \p linear is the linear coefficient.
 * @param[in]  grad_desc
 *   Input. The descriptor of the \p grad tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  grad
 *   Input. Pointer to the MLU memory that stores the \p grad tensor. It is the gradient of \p var.
 *   With the \p grad value, ::cnnlApplyFtrlV2 function can be used to calculate \p accum and \p
 *   var.
 * @param[in] lr
 *   Input. Pointer to the MLU memory that stores the \p lr parameter.
 *   It is the learning rate value of this optimizer, must be positive.
 * @param[in] l1
 *   Input. Pointer to the MLU memory that stores the \p l1 parameter.
 *   It is l1 regularization strength, must be greater than or equal to zero.
 * @param[in] l2
 *   Input. Pointer to the MLU memory that stores the \p l2 parameter.
 *   It is l2 regularization strength, must be greater than or equal to zero.
 * @param[in] l2_shrinkage
 *   Input. Pointer to the MLU memory that stores the \p l2_shrinkage parameter.
 *   It is L2 shrinkage regularization.
 *   If the pointer is nullptr, the function will be in ApplyFtrl mode.
 *   If the pointer is non-nullptr, the function will be in ApplyFtrlV2 mode.
 * @param[in] lr_power
 *   Input. Pointer to the MLU memory that stores the \p lr_power parameter.
 *   It is the learning rate power that controls how the learning rate decreases during training.
 *   Use fixed learning rate if \p lr_power is zero.
 * @param[in] multiply_linear_by_lr
 *   Input. A Boolean value that specifies whether the formula to multiply the learning rate.
 *   If the value of this parameter is false, the formula will multiply the learning rate.
 *   If the value of this parameter is true, the formula will not multiply the learning rate.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "ApplyFtrlV2 Operator" section in "Cambricon CNNL User Guide" for more details.
 *
 * @par Data Type
 * - This function supports the following data types for
 *   \p var tensor, \p accum tensor, \p linear tensor and \p grad tensor.
 *   Note that the data type of input tensor and output tensor should be the same.
 *   - var tensor: half, float
 *   - accum tensor: half, float
 *   - linear tensor: half, float
 *   - grad tensor: half, float
 *
 * @par Scale Limitation
 * - The dimensions of \p var, \p accum, \p linear and \p grad should be the same.
 *
 * @note
 * - It is recommended to set \p lr_power a non positive number.
 * - On MLU200 series:
 *   - If lr\_power is integer, \f$(accum + grad*grad)\f$ is recommended to be set in range of (-inf,-1) and (1,+inf) for high precision.
 *   - If data type is half, the input data \p var, \p accum, \p linear and \p grad must be within [-65504, 65504].
 *   - If data type is half, \f$accum^{-lr\_power}\f$ should be within [-65504, 65504].
 *   - If data type is half, \f$(accum + grad*grad)^{-lr\_power}\f$ should be within [-65504, 65504].
 * - On MLU300 series and CE3226:
 *   - If lr\_power is integer, \f$(accum + grad*grad)\f$ is recommended to be set in range of (-inf,-1] and [1,+inf) for high precision.
 *   - If data type is half, \f$accum^{-lr\_power}\f$ should be within [-65504, 65504].
 *   - If data type is half, \f$(accum + grad*grad)^{-lr\_power}\f$ should be within [-65504, 65504].
 *   - If \f$(accum + grad*grad)^{-lr\_power}\f$ and \f$accum^{-lr\_power}\f$ are both positive infinity, then \p var is NaN.
 *   - If \f$(accum + grad*grad)^{-lr\_power}\f$ and \f$accum^{-lr\_power}\f$ are both infinity or NaN, and \p lr_power is 0.0,
 *     then \p linear is NaN.
 * - On MLU300 series, MLU500 series and 1V:
 *   - -32768 <= lr\_power <= 32767.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
     input three arrays by 4, 4, 4
     --> var:   [0.0, 1.0, 2.0, 3.0]
     --> accum: [0.0, 1.0, 2.0, 3.0]
     --> linear: [0.0, 1.0, 2.0, 3.0]
     --> grad:  [0.0, 1.0, 2.0, 3.0]

     param:
        lr: 0.01, l1: 0.01, l2: 0.01, l2_shrinkgae: 0.01, lr_power: -0.01, multiply_linear_by_lr: false,

     output array by 4, 4
      --> accum: [0,2,6,12]
      --> var: [0, -0.01305111, -0.01772886, -0.01770953]
     @endverbatim
 */
cnnlStatus_t CNNL_WIN_API
cnnlApplyFtrlV2(cnnlHandle_t handle,
                const cnnlTensorDescriptor_t var_desc,
                void *var,
                const cnnlTensorDescriptor_t accum_desc,
                void *accum,
                const cnnlTensorDescriptor_t linear_desc,
                void *linear,
                const cnnlTensorDescriptor_t grad_desc,
                const void *grad,
                const void *lr,
                const void *l1,
                const void *l2,
                const void *l2_shrinkage,
                const void *lr_power,
                const bool multiply_linear_by_lr);

/*****************************************************************************
 * Cambricon CNNL OP: histogram
 *****************************************************************************/
/*!
 * @brief Enumeration variables describing the operation mode that histogram series operations
 * run on.
 *
 * These Enumeration variables are member of cnnlHistogramStruct.
 */
typedef enum {
  CNNL_HISTOGRAM_MODE_HISTO_COUNT = 0,
  /*!< The ::cnnlHistc mode. In this mode, the operation computes the histogram of \p input. */
  CNNL_HISTOGRAM_MODE_BIN_COUNT = 1,
  /*!< The Bincount mode. In this mode, the operation counts the frequencies of
   *   non-negative integers in \p input. */
  CNNL_HISTOGRAM_MODE_HISTO_GRAM = 2,
  /*!< The Histogram mode. In this mode, the operation computes the histogram of
   *   \p input with advanced options. This mode is not supported currently. */
} cnnlHistogramMode_t;

/*! The descriptor of histogram series operations that hold the information
 *  including the histogram mode, bins, min_length, max_length, range_min, range_max, density,
 *  axis, binary_out, ext_range and is_index.
 *
 * You need to call the ::cnnlCreateHistogramDescriptor function to create a descriptor,
 * and call the ::cnnlSetHistogramDescriptorHistoCountMode function to set the information of the histogram
 * operation to the descriptor. Also, you need to destroy the Cambricon CNNL context at the end with the
 * ::cnnlDestroyHistogramDescriptor function.
 */
typedef struct cnnlHistogramStruct *cnnlHistogramDescriptor_t;

// Group:Histogram
/*!
 * @brief Creates a descriptor pointed by \p histogam_desc for histogram series operations,
 *        and allocates memory for holding the information about the histogram operation.
 *        The information is defined in ::cnnlHistogramDescriptor_t. For more information
 *        about descriptor, see "Cambricon CNNL User Guide".
 *
 * @param[out] histogram_desc
 *  Output. A host pointer to the histogram descriptor that holds information about the operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetHistogramDescriptorHistoCountMode function
 *   to initialize and set the information to the histogram descriptor.
 * - You need to call the ::cnnlDestroyHistogramDescriptor function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateHistogramDescriptor(cnnlHistogramDescriptor_t *histogram_desc);

// Group:Histogram
/*!
 * @brief Initializes the ::cnnlHistc descriptor \p histogram_desc that was previously created
 * with the ::cnnlCreateHistogramDescriptor function, and sets the information
 * about the ::cnnlHistc operation to the histogram descriptor \p histogram_desc.
 *
 * @param[in,out] histogram_desc
 *   Input/output. The descriptor of the ::cnnlHistc operation. The value of member ::cnnlHistogramMode_t
 *   of this parameter will be set to CNNL_HISTOGRAM_MODE_HISTO_COUNT. For detailed information,
 *   see ::cnnlHistogramDescriptor_t.
 * @param[in] bins
 *   Input. The number of bins to put input tensor in. Histogram operation is to put \p input
 *   elements in bins, and counts how many elements there are in each bin as \p output.
 *   Total number of bins is given by \p bins. Each bin has same length,
 *   and length equals (\p range_max - \p range_min) / \p bins. The value of this parameter
 *   should be the same as the element number of \p output and should be always positive.
 * @param[in] range_min
 *   Input.  A float number to specify the minimum range for \p input element to be counted.
 *   the value of this parameter should not be NaN/infinity or greater than \p range_max.
 *   when \p range_min equals \p range_max, ::cnnlHistc searches for max and min of \p input.
 * @param[in] range_max
 *   Input.  A float number to specify the maximum range for \p input element to be counted.
 *   If \p input elements equal range_max, they will be put in last bin.
 *   The value of this parameter should not be NAN/INF or less than \p range_min.
 *   When \p range_min equals \p range_max, ::cnnlHistc searches for max and min of \p input.
 * @param[in] ext_range
 *   Input. A bool to switch on extended range function. When switched on, \p input element
 *   less than \p range_min and greater than \p range_max will be put in first bin and last
 *   bin respectively. The value of this parameter is FALSE by default.
 * @param[in] is_index
 *   Input. A bool to switch on bin index function. When switched on, \p output will contain
 *   the index of bin for each \p input element to be put in, and the element number of \p output
 *   will be the same as element number of \p input. The value of this parameter is FALSE by default.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - Currently, \p ext_range and \p is_index are not supported.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetHistogramDescriptorHistoCountMode(
                            cnnlHistogramDescriptor_t histogram_desc,
                            int bins,
                            float range_min,
                            float range_max,
                            bool ext_range,
                            bool is_index);

// Group:Histogram
/*!
 * @brief Destroys a histogram descriptor \p histogram_desc that was previously created with the
 *        ::cnnlCreateHistogramDescriptor function.
 *
 * The histogram descriptor is defined in ::cnnlHistogramDescriptor_t
 * and holds the information about the histogram series operations.
 *
 *
 * @param[in] histogram_desc
 *   Input. The histogram descriptor to be destroyed.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - You need to call this function after calling the ::cnnlHistc function.
 *   Otherwise, ::CNNL_STATUS_BAD_PARAM is returned.
 * - This function should be called to destroy the histogram descriptor.
 *   Otherwise, the memory leak may be occurred.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyHistogramDescriptor(cnnlHistogramDescriptor_t histogram_desc);

// Group:Histogram
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra workspace
 * to reorder \p input tensor with stride feature before entering histogram series operations.
 *
 * The size of the extra workspace is based on the given information of the histogram series
 * operations, including the input tensor descriptor \p input_desc,
 * and filter tensor descriptor \p weight_desc.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the histogram series operations. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] weight_desc
 *   Input. The descriptor of the filter tensor in the
 *   Bincount and Histogram operation. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *   the histogram series operations.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlCreateTensorDescriptor and
 *   ::cnnlSetTensorDescriptor functions to create and set the tensor descriptors
 *   \p input_desc and \p weight_desc before calling this function.
 * - The allocated extra workspace should be passed to the ::cnnlHistc function to
 *   store the reordered input with stride feature before entering computing.
 *
 * @note
 * - None
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetHistogramWorkspaceSize(cnnlHandle_t handle,
                                                        const cnnlTensorDescriptor_t input_desc,
                                                        const cnnlTensorDescriptor_t weight_desc,
                                                        size_t *workspace_size);

// Group:Histogram
/*!
 * @brief Computes a histogram of input tensor \p input with configuration information
 * stored in \p histogram_desc, and returns the results in the output tensor \p output.
 * This operation is the ::CNNL_HISTOGRAM_MODE_HISTO_COUNT mode of histogram series operations,
 * thus involving the related parameters defined in the descriptor of the histogram operation.
 *
 * When input tensor \p input has stride feature, this function needs extra MLU memory
 * as the workspace to store reordered input tensor before entering computing.
 * You can get the workspace size with the
 * ::cnnlGetHistogramWorkspaceSize function.
 *
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the histc operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] histogram_desc
 *   Input. The descriptor of the histogram operation. For detailed information,
 *   see ::cnnlHistogramDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace to store
 *   reordered input tensor. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the histc operation. You can get the size of the workspace with
 *   the ::cnnlGetHistogramWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. The output
 *   tensor is always \p bins long, one-dimension tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - None.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \p input
 *   and output tensor \p output.
 * - The data type of \p input and \p output should be the same.
 *   - input tensor: uint8, int8, int16, int32, float.
 *   - output tensor: uint8, int8, int16, int32, float.
 *
 * @par Scale Limitation
 * - The input tensor, output tensor, and the histogram descriptor
 *   (including bins, range_min, range_max) must meet the following
 *   requirements:
 *   - \p bins is greater than 0.
 *   - \p bins equals \p output element number.
 *   - \p range_max is no less than range_min.
 *   - \p range_max and \p range_min cannot be NAN/INF.
 *   - When \p range_max equals \p range_min, \p input cannot contain NaN/infinity.
 *   - When \p range_max equals \p range_min, \p input cannot be all the minimum
 *     or maximum of its data type.
 *   - When the data type of \p input is int32, the elements of the input tensor should be in range of [-2^24, 2^24].
 *
 * @par API Dependency
 * - Before calling this function, you need to prepare
 *   all the parameters passed to this function. See each parameter description
 *   for details.
 *
 * @note
 * - When \p input element number is large enough, \p output may overflow the limit of data type.
 * - When \p bins is too large, precision error may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * * - The example of the histc operation is as follows:
     @verbatim
      input array by 1 * 10  --> input: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]

      param:
        bins: 3, range_min: 0, range_max: 0,

      output array by 1 * 3 --> output: [3, 3, 4]
     @endverbatim
 *
 * @par Reference
 * - https://www.pytorch.org/docs/1.9.0/generated/torch.histc
 * - https://www.pytorch.org/docs/1.11/generated/torch.bincount
 * - https://www.pytorch.org/docs/1.11/generated/torch.histogram
 * - http://tensorflow.org/api_docs/python/tf/histogram_fixed_width
 * - http://tensorflow.org/api_docs/python/tf/histogram_fixed_width_bins
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlHistc(cnnlHandle_t handle,
                               const cnnlHistogramDescriptor_t histogram_desc,
                               const cnnlTensorDescriptor_t input_desc,
                               const void *input,
                               void *workspace,
                               size_t workspace_size,
                               const cnnlTensorDescriptor_t output_desc,
                               void *output);

// Group:Histogram
/*!
 * @brief Initializes the ::cnnlBincount descriptor \p histogram_desc that was previously
 * created with the ::cnnlCreateHistogramDescriptor function, and sets the information
 * about the ::cnnlBincount operation to the histogram descriptor \p histogram_desc.
 *
 * @param[in,out] histogram_desc
 *   Input/output. The descriptor of the ::cnnlBincount operation. The value of member
 *   ::cnnlHistogramMode_t of this parameter will be set to CNNL_HISTOGRAM_MODE_BIN_COUNT.
 *   For detailed information, see ::cnnlHistogramDescriptor_t.
 *
 * @param[in] min_length
 *   Input.  A long integer to specify the minimum length of \p output.
 *   Negative \p min_length will be recognized as invalid.
 *   The value of this parameter is -1 by default.
 *
 * @param[in] max_length
 *   Input.  A long integer to specify the maximum length of \p output.
 *   Negative \p max_length will be recognized as invalid.
 *   When \p max_length is smaller than \p min_length, \p min_length will be disabled.
 *   The value of this parameter is -1 by default.
 *
 * @param[in] axis
 *   Input.  A long integer to specify which dimension of \p input to carry out bincount
 *   operation. Negative \p axis will be recognized as dimension order from backwards.
 *   The value of this parameter is 0 by default.
 *
 * @param[in] binary_out
 *   Input.  A Boolean to specify the mode of \p output. When \p binary_out is True,
 *   bincount will count whether the output element is present in \p input. When
 *   \p binary_out is False, bincount will count how many times each \p output element
 *   shows in \p input. The value of this parameter is False by default.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - Currently, \p axis and \p binary_out are not supported.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetHistogramDescriptorBinCountMode(cnnlHistogramDescriptor_t histogram_desc,
                                       const int64_t min_length,
                                       const int64_t max_length,
                                       const int64_t axis,
                                       const bool binary_out);

// Group:Histogram
/*!
 * @brief Counts the number of occurrences of each elements in \p input with
 * configuration information stored in \p histogram_desc, and returns the
 * results in the output tensor \p output. This operation is
 * the ::CNNL_HISTOGRAM_MODE_BIN_COUNT mode of histogram series operations,
 * thus involving the related parameters defined in the descriptor of the
 * histogram operation.
 *
 * When input tensor \p input and weight tensor \p weight has stride feature,
 * this function needs extra MLU memory as the workspace to store reordered
 * input tensor and weight tensor before entering computing. You can get the size of the
 * workspace \p workspace_size with the ::cnnlGetHistogramWorkspaceSize function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the bincount operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] histogram_desc
 *   Input. The descriptor of the histogram operation.
 *   You need to call ::cnnlSetHistogramDescriptorBinCountMode function to set the information
 *   of the ::cnnlBincount operation. For detailed information,
 *   see ::cnnlHistogramDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] weight_desc
 *   Input. The descriptor of the weight tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] weight
 *   Input. Pointer to the MLU memory that stores the weight tensor.
 *   If \p weight_desc and \p weight are not NULL, each number of occurrence counted
 *   in \p input will be increased by weight instead of 1, and \p binary_out is disabled.
 *   When \p weight and \p weight_desc are NULL, weight tensor will be set as a one-dimensional tensor
 *   of int32 or int64 data type with all values equal to one by default.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace to store
 *   reordered input tensor and temporary computation result.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the bincount operation. You can get the size of the workspace with
 *   the ::cnnlGetHistogramWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. The output tensor is
 *   1D and equals [MAX(\p input)+1], which means that the maximum element
 *   of \p input should be found to compute the length of output prior to
 *   invoking bincount operation.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - None.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \p input,
 *   weight tensor \p weight and output tensor \p output. The data type of \p output
 *   and \p weight should be the same.
 *   - input tensor: uint8, int8, int16, int32, int64.
 *   - weight tensor: uint8, int8, int16, int32, int64, half, float.
 *   - output tensor: uint8, int8, int16, int32, int64, half, float.
 *
 * @par Scale Limitation
 *  - The shape of \p input and \p weight should be consistent.
 *  - \p output should be a 1-d tensor.
 *  - The layout of \p input, \p weight and \p output should be ARRAY.
 *  - The elements of \p input tensor should be non-negative values.
 *  - When the data type of \p input is int64, the elements of the input tensor should be in range of [0, 2^24].
 *  - When the data type of \p weight is int64, the elements of the weight tensor should be in range of [0, 2^24].
 *  - \p min_length smaller than 0 will be recognized as invalid and ignored.
 *  - \p max_length smaller than 0 will be recognized as invalid and ignored.
 *  - \p max_length and \p min_length are long integer, thus cannot be NaN/infinity.
 *  - when \p min_length and \p max_length are both greater than 0, \p min_length
 *    greater than \p max_length will be recognized as invalid and ignored.
 *  - when \p max_length and \p min_length are valid, length of \p output should be
 *    smaller than \p max_length, and greater than \p min_length.
 *
 * @par API Dependency
 * - Before calling this function, you need to prepare
 *   all the parameters passed to this function. See each parameter description
 *   for details.
 *
 * @note
 * - The operation does not support NaN/infinity in weight data.
 * - The output tensor with stride feature is not supported currently.
 * - When \p weight is NULL, the data type of \p output should be int32 or int64.
 * - When \p weight is not NULL, the data type of \p output and \p weight should be the same.
 * - When min_length and max_length are both invalid, the dimension size of output tensor equals [MAX(\p input)+1].
 * - When min_length or max_length is valid, you need to adjust the dimension size of output tensor
 *   by (MAX(\p input)+1) and min_length/max_length.
 *   - If only min_length is valid, the dimention size of output tensor shoule be MAX((MAX(\p input) + 1), min_length).
 *   - If only max_length is valid, the dimention size of output tensor shoule be MIN((MAX(\p input) + 1), max_length).
 *   - When min_length and max_length are both valid, if (MAX(\p input) + 1) is in range of [min_length, max_length],
 *     the dimention size of output tensor shoule be MAX(\p input) + 1).
 *   - When min_length and max_length are both valid, if (MAX(\p input) + 1) is smaller than min_length and max_length,
 *     the dimention size of output tensor shoule be min_length.
 *   - When min_length and max_length are both valid, if (MAX(\p input) + 1) is greater than min_length and max_length,
 *     the dimention size of output tensor shoule be max_length.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * * - The example of the bincount operation is as follows:
     @verbatim
      input:   [1,1,2,3,2,4,4,5]
      weights: [1,5,0,1,0,5,4,6]

      param:
        min_length: 10, max_length: -1, axis: 0, binary_out: False

      output: [0 6 0 1 9 6 0 0 0 0]
     @endverbatim
 *
 * @par Reference
 * - https://www.pytorch.org/docs/1.9.0/generated/torch.histc
 * - https://www.pytorch.org/docs/1.11/generated/torch.bincount
 * - https://www.pytorch.org/docs/1.11/generated/torch.histogram
 * - http://tensorflow.org/api_docs/python/tf/histogram_fixed_width
 * - http://tensorflow.org/api_docs/python/tf/histogram_fixed_width_bins
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlBincount(cnnlHandle_t handle,
             const cnnlHistogramDescriptor_t histogram_desc,
             const cnnlTensorDescriptor_t input_desc,
             const void *input,
             const cnnlTensorDescriptor_t weight_desc,
             const void *weight,
             void *workspace,
             size_t workspace_size,
             const cnnlTensorDescriptor_t output_desc,
             void *output);

// Group:CdistBackward
/*!
 * @brief Computes the gradient of input \p grad_x1 based on the input \p x1, \p x2,
 *   \p cdist and \p grad_result.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the cdist
 *   backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x1_desc
 *   Input. The descriptor of the input tensor \p x1.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x1
 *   Input. Pointer to the MLU memory that stores the tensor \p x1.
 * @param[in] x2_desc
 *   Input. The descriptor of the input tensor \p x2.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x2
 *   Input. Pointer to the MLU memory that stores the tensor \p x2.
 * @param[in] dist_desc
 *   Input. The descriptor of the input tensor \p dist.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] dist
 *   Input. Pointer to the MLU memory that stores the tensor used to store the p-norm distance
 *   between \p x1 and \p x2.
 * @param[in] grad_result_desc
 *   Input. The descriptor of the input tensor \p grad_result.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] grad_result
 *   Input. Pointer to the MLU memory that stores the tensor used to store the gradient.
 * @param[in] p
 *   Input. A double value used as the p-norm and \p p must be equal 1.0.
 * @param[in] grad_x1_desc
 *   Input. The descriptor of the output tensor \p grad_x1. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] grad_x1
 *   Output. Pointer to the MLU memory that stores the output tensor \p grad_x1.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "CdistBackward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 *   - x1 tensor: float.
 *   - x2 tensor: float.
 *   - dist tensor: float.
 *   - grad_result tensor: float.
 *   - grad_x1 tensor: float.
 *   - p: double.
 *
 * @par Data Layout
 * - The supported data layout of the input and output tensors is CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - None.
 *
 * @par API Dependency
 * - Before calling this function, you need to prepare all the parameters passed to this function.
 *   See each parameter description for details.
 *
 * @par Performance Optimization
 * - None.
 *
 * @note
 * - Only support MLU300 and MLU500 series.
 * - Only support p = 1.0 now.
 * - The tensor shape of \p x1, \p x2, \p cdist, \p grad_result, \p grad_x1 should as follows:
 *   - x1 tensor: [B, P, M].
 *   - x2 tensor: [B, R, M].
 *   - dist tensor: [B, P, R].
 *   - grad_result tensor: [B, P, R].
 *   - grad_x1 tensor: [B, P, M].
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://pytorch.org.docs/1.9.0/generated/torch.cdist.html
 */
cnnlStatus_t CNNL_WIN_API
cnnlCdistBackward(cnnlHandle_t handle,
                  const cnnlTensorDescriptor_t x1_desc,
                  const void *x1,
                  const cnnlTensorDescriptor_t x2_desc,
                  const void *x2,
                  const cnnlTensorDescriptor_t dist_desc,
                  const void *dist,
                  const cnnlTensorDescriptor_t grad_result_desc,
                  const void *grad_result,
                  const double p,
                  const cnnlTensorDescriptor_t grad_x1_desc,
                  void *grad_x1);

// Group:PdistBackward
/*!
 * @brief Computes the gradient of input \p grad_x based on the input \p x,
 *   \p dist and \p grad_result.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor \p x.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the tensor \p x.
 * @param[in] dist_desc
 *   Input. The descriptor of the input tensor \p dist.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] dist
 *   Input. Pointer to the MLU memory that stores the norm distance
 *   between row vector in input tensor \p x.
 * @param[in] grad_result_desc
 *   Input. The descriptor of the input tensor \p grad_result.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] grad_result
 *   Input. Pointer to the MLU memory that stores gradients of the input tensor \p dist.
 * @param[in] p
 *   Input. A double value used as the p-norm.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace to support stride features.
 *   For more information about \p workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in PdistBackward.
 * @param[in] grad_x_desc
 *   Input. The descriptor of the output tensor \p grad_x.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] grad_x
 *   Output. Pointer to the MLU memory that stores gradients of the input tensor \p x.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "PdistBackward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 *   - x tensor: float.
 *   - dist tensor: float.
 *   - grad_result tensor: float.
 *   - grad_x tensor: float.
 *   - p: double.
 *
 * @par Data Layout
 * - The supported data layout of the input and output tensors is CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The dimension N of input tensor \p x must meet the requirements:
 *   - N should not be more than 10174 on MLU300 series.
 *   - N should not be more than 6078 on MLU500 series.
 *
 * @par API Dependency
 * - You need to call the ::cnnlGetPdistBackwardWorkspaceSize function to allocate extra
 *   workspace for \p workspace.
 *
 * @par Performance Optimization
 * - None.
 *
 * @note
 * - Only support MLU300 and MLU500 series.
 * - The shape of the input tensor \p x is 2D. If the shape of \p x is represented as [N, M],
 *   the shape of the output tensor \p grad_x is [N, M], and the shapes of \p dist
 *   and \p grad_result tensors are [N*(N-1)/2].
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/1.9.0/generated/torch.nn.functional.pdist.html
 */
cnnlStatus_t CNNL_WIN_API
cnnlPdistBackward(cnnlHandle_t handle,
                  const cnnlTensorDescriptor_t x_desc,
                  const void *x,
                  const cnnlTensorDescriptor_t dist_desc,
                  const void *dist,
                  const cnnlTensorDescriptor_t grad_result_desc,
                  const void *grad_result,
                  const double p,
                  void *workspace,
                  size_t workspace_size,
                  const cnnlTensorDescriptor_t grad_x_desc,
                  void *grad_x);

// Group: PdistBackward
/*!
 * @brief Returns in \p workspace_size the size of MLU memory that is used as an extra workspace.
 *
 * The size of the extra workspace is based on the given information of PdistBackward operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   PdistBackward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor \p x. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] dist_desc
 *   Input. The descriptor of the input tensor \p dist. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] grad_result_desc
 *   Input. The descriptor of the input tensor \p grad_result. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] grad_x_desc
 *   Input. The descriptor of the input tensor \p grad_x. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *   the PdistBackward operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called before the ::cnnlPdistBackward function.
 * - The allocated extra workspace should be passed to the ::cnnlPdistBackward function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetPdistBackwardWorkspaceSize(cnnlHandle_t handle,
                                  const cnnlTensorDescriptor_t x_desc,
                                  const cnnlTensorDescriptor_t dist_desc,
                                  const cnnlTensorDescriptor_t grad_result_desc,
                                  const cnnlTensorDescriptor_t grad_x_desc,
                                  size_t *workspace_size);

// Group:Im2Col
/*!
 * @brief Extracts sliding local blocks from a batched input tensor.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the Im2Col operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] w_desc
 *   Input. The descriptor of the filter tensor in the Im2Col operation.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] conv_desc
 *   Input. The descriptor of the Im2Col operation. For detailed information,
 *   see ::cnnlConvolutionDescriptor_t.
 * @param[in] input_image_size_desc
 *   Input. The descriptor of the \p input_image_size tensor.
 *   This parameter is only used in PaddlePaddle, and set it NULL in PyTorch and TensorFlow frameworks.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input_image_size
 *   Input. Pointer to the MLU memory that stores the \p input_image_size tensor, which contains image real size.
 *   This parameter is only used in PaddlePaddle, and set it NULL in PyTorch and TensorFlow frameworks.
 * @param[in] out_stride
 *   Input. An array that stores the output stride. The default value of this parameter should be 1.
 *   It is enabled when \p input_image_size is not NULL.
 *   This parameter is only used in PaddlePaddle, and set it NULL in PyTorch and TensorFlow frameworks.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace to store transposed input tensor.
 *   Reserved for future use, set the \p workspace to NULL now.
 *   For more information about \p workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the Im2Col operation.
 *   Reserved for future use, set the \p workspace_size to 0 now.
 * @param[in] col_desc
 *   Input. The descriptor of the output tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] col
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Im2Col Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \p x and output tensor
 *   \p col. Data type of both tensors should be the same. Bfloat16 is not supported on PaddlePaddle.
 *   - input tensor: uint8, int8, uint16, bfloat16, int16, uint32, int32, uint64, int64, bool, half, float,
 *     double, complex_half, complex_float.
 *   - output tensor: uint8, int8, uint16, bfloat16, int16, uint32, int32, uint64, int64, bool, half, float,
 *     double, complex_half, complex_float.
 *
 * @par Data Layout
 * - The supported data layouts of the input tensor \p x, filter tensor, and
 *   output tensor \p col are as follows:
 *   - input tensor: \p CNNL_LAYOUT_NCHW in PyTorch and PaddlePaddle frameworks, \p CNNL_LAYOUT_NHWC and \p CNNL_LAYOUT_NDHWC in TensorFlow framework.
 *   - filter tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NCHW , \p CNNL_LAYOUT_HWCN and \p CNNL_LAYOUT_NDHWC.
 *   - output tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - None.
 *
 * @par API Dependency
 * - Before calling this function, you need to prepare all the parameters passed to this function.
 *   See each parameter description for details.
 *
 * @par Performance Optimization
 * - None.
 *
 * @note
 * - The layout of input tensor \p x only supports \p CNNL_LAYOUT_NCHW in PyTorch and PaddlePaddle frameworks.
 * - The layout of input tensor \p x supports \p CNNL_LAYOUT_NHWC and \p CNNL_LAYOUT_NDHWC in TensorFlow framework.
 * - Users do not need to set data type in \p w_desc, since the data type is not used in this function.
 * - When the layout of input tensor \p x is \p CNNL_LAYOUT_NDHWC, the value of dilation descripted in \p conv_desc should be 1.
 * - When the \p input_image_size_desc is NULL:
 *   - \p input_image_size and \p out_stride should be NULL.
 *   - The shape of \p col should be [N, L, C_col].
 * - When the \p input_image_size_desc is not NULL:
 *   - \p input_image_size and \p out_stride should be not NULL.
 *   - The shape of \p col should be [N * L, C_col]. N is batches, L is the number of the
 *     sliding local blocks, and C_col is the size of each sliding local blocks.
 *   - The shape of \p input_image_size should be [N, 2], and the data type of \p input_image_size should be int32.
 *   - The element value of \p out_stride should be greater than or equal to 1.
 *   - The scale size with \p out_stride should be less than or equal to that of input tensor \p x.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/1.10/generated/torch.nn.Unfold.html?highlight=im2col.
 * - https://www.paddlepaddle.org.cn/documentation/docs/zh/1.8/api_cn/layers_cn/im2sequence_cn.html.
 * - https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/extract-volume-patches.
 * - https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/extract-image-patches.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlIm2Col(cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t x_desc,
                                     const void *x,
                                     const cnnlTensorDescriptor_t w_desc,
                                     const cnnlConvolutionDescriptor_t conv_desc,
                                     const cnnlTensorDescriptor_t input_image_size_desc,
                                     const void *input_image_size,
                                     const int out_stride[],
                                     void *workspace,
                                     size_t workspace_size,
                                     const cnnlTensorDescriptor_t col_desc,
                                     void *col);

// Group:Im2Col
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the Im2Col operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the Im2Col operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor \p x. Reserved for future use, set it NULL now.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] w_desc
 *   Input. The descriptor of the input tensor \p w. Reserved for future use, set it NULL now.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] conv_desc
 *   Input. The descriptor of the Im2Col operation. Reserved for future use, set it NULL now.
 *   For detailed information, see ::cnnlConvolutionDescriptor_t.
 * @param[in] col_desc
 *   Input. The descriptor of the output tensor \p col. Reserved for future use, set it NULL now.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the Im2Col operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetIm2ColWorkspaceSize(cnnlHandle_t handle,
                                                     const cnnlTensorDescriptor_t x_desc,
                                                     const cnnlTensorDescriptor_t w_desc,
                                                     const cnnlConvolutionDescriptor_t conv_desc,
                                                     const cnnlTensorDescriptor_t col_desc,
                                                     size_t *workspace_size);

// Group:Col2Im
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra workspace to
 * optimize the Col2Im operation.
 *
 * The size of the extra workspace is based on the given information of the Col2Im operation,
 * including the input tensor descriptor \p col_input_desc and \p im_output_desc and \p filter_desc. For more information about the workspace,
 * see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a CNNL context that is used to manage MLU devices and queues in the
 *   Col2Im operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] col_input_desc
 *   Input. Descriptor of input data \p col_input, including dimension, data type and data layout.
 * @param[in] im_output_desc
 *   Input. Descriptor of input data \p im_output, including dimension, data type and data layout.
 * @param[in] filter_desc
 *   Input. Descriptor of filter, including dimension and data layout.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *   the Col2Im operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called before the cnnlCol2Im function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */

cnnlStatus_t CNNL_WIN_API
cnnlGetCol2ImWorkspaceSize(cnnlHandle_t handle,
                           const cnnlTensorDescriptor_t col_input_desc,
                           const cnnlTensorDescriptor_t filter_desc,
                           const cnnlTensorDescriptor_t im_output_desc,
                           size_t *workspace_size);

// Group:Col2Im
/*!
 * @brief Combines an array of sliding local blocks into a large containing tensor.
 * @param[in] handle
 *   Input. Handle to a CNNL context that is used to manage MLU devices and queues in the
 *   Col2Im operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] col_input_desc
 *   Input. Descriptor of input data \p col_input, including dimension, data type and data layout.
 * @param[in] col_input
 *   Input. Pointer to the MLU memory that stores the col_input tensor.
 * @param[in] filter_desc
 *   Input. Descriptor of filter, including dimension and data layout.
 * @param[in] conv_desc
 *   Input. The descriptor of the convolution operation. For detailed information,
 *   see ::cnnlConvolutionDescriptor_t.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the Col2Im
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that is used in
 *   the Col2Im operation.
 * @param[in] im_output_desc
 *   Input. Descriptor of input data \p im_output, including dimension, data type and data layout.
 * @param[out] im_output
 *   Input. Pointer to the MLU memory that stores the \p im_output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - None.
 *
 * @par Data Type
 * - This function supports the following data types for col_input tensor \p col_input, and im_output tensor
 *   \p im_output.
 *   Data types of col_input tensor and im_output tensor should be the same.
 *   - \p col_input tensor: half, bfloat16, float.
 *   - \p im_output tensor: half, bfloat16, float.
 *
 *   The bfloat16 data type is supported only on MLU500 series.
 * @par Data Layout
 * - The supported data layouts of \p col_input and  \p im_output are as follows:
 *   - \p col_input tensor: \p CNNL_LAYOUT_ARRAY.
 *   - \p im_output tensor: \p CNNL_LAYOUT_NCHW or \p CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - The shape of \p col_input should be [N, L, C_col]. L is the local number of blocks.
 *   (This is exactly the same specification as the output shape of Im2Col)
 * - L = Oh * Ow.
 *   Oh is the filter sliding times in height_im.
 *   Oh is the filter sliding times in width_im.
 * - C_col = C_im * kernel_h * kernel_w.
 *   C_col is the \p col_input channels and C_im is the \p im_output channels.
 * - The shape of \p im_output should be [N, C_im, H, W] or [N, H, W, C_im].
 * - The dimension of \p col_input and \p im_output dims[0](N) should be the same.
 * - conv_desc.pad should meet the following requirements:
 *   - pad[0] = pad[1], pad[2] = pad[3].
 *
 * @par API Dependency
 * - Before calling this function, you need to call cnnlGetCol2ImWorkspaceSize
 *   to get the extra space size needed in cnnlCol2ImForward operation.
 *
 * @par Performance Optimization
 * - None.
 *
 * @note
 * - This operation is not supported on the 1V platforms.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - http://github.com/pytorch/pytorch/blob/release/1.6/aten/src/ATen/native/cuda/im2col.cu
 */

cnnlStatus_t CNNL_WIN_API
cnnlCol2Im(cnnlHandle_t handle,
           const cnnlTensorDescriptor_t col_input_desc,
           const void *col_input,
           const cnnlTensorDescriptor_t filter_desc,
           const cnnlConvolutionDescriptor_t conv_desc,
           void *workspace,
           const size_t workspace_size,
           const cnnlTensorDescriptor_t im_output_desc,
           void *im_output);

// Group:FractionalMaxPoolForward
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra workspace to
 * optimize the FractionalMaxPoolForward operation.
 *
 * The size of the extra workspace is based on the given information of the FractionalMaxPoolForward operation,
 * including the input tensor descriptor \p input_desc, \p random_samples_desc, \p output_desc and \p indices_desc.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a CNNL context that is used to manage MLU devices and queues in the
 *   FractionalMaxPoolForward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. Descriptor of input data \p input, including dimension, data type and data layout.
 * @param[in] random_samples_desc
 *   Input. Descriptor of input data \p random_samples, including dimension, data type and data layout.
 * @param[in] output_desc
 *   Input. Descriptor of \p output, including dimension and data layout.
 * @param[in] indices_desc
 *   Input. Descriptor of \p indices, including dimension and data layout.
 * @param[in] window
 *   Input. A window consists of (kw, kh), where kw and kh are respectively the width and height of the pooling window.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *   the FractionalMaxPoolForward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called before the cnnlFractionalMaxPoolForward function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetFractionalMaxPoolForwardWorkspaceSize(cnnlHandle_t                   handle,
                                             const cnnlTensorDescriptor_t   input_desc,
                                             const cnnlTensorDescriptor_t   random_samples_desc,
                                             const cnnlTensorDescriptor_t   output_desc,
                                             const cnnlTensorDescriptor_t   indices_desc,
                                             const int                      window[],
                                             size_t                         *workspace_size);

// Group:FractionalMaxPoolForward
/*!
 * @brief Performs fractional maximum pooling over input and returns the results and indices in
 * the output tensor \p output and \p indices.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor \p input.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the tensor \p input.
 * @param[in] random_samples_desc
 *   Input. The descriptor of the input tensor \p random_samples.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] random_samples
 *   Input. Pointer to the MLU memory that stores the random values that determines the sampling
 *   region of pooling for each plane.
 * @param[in] window
 *   Input. A window consists of (kw, kh), where kw and kh are respectively the width and height of the pooling window.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace to support stride features.
 *   For more information about \p workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in FractionalMaxPoolForward.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor \p output.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores output values.
 * @param[in] indices_desc
 *   Input. The descriptor of the output tensor \p indices.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] indices
 *   Output. Pointer to the MLU memory that stores indices of the output values.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "FractionalMaxPoolForward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 *   - input tensor: float/half.
 *   - random_samples tensor: float/half.
 *   - output tensor: float/half.
 *   - indices tensor: int32.
 *   - window: int32.
 *
 * @par Data Layout
 * - The supported data layout of the input, output and indices tensors is CNNL_LAYOUT_NCHW.
 *
 * @par Scale Limitation
 * - The data size of \p input should be smaller than 2G bytes.
 *
 * @par API Dependency
 * - You need to call the ::cnnlGetFractionalMaxPoolForwardWorkspaceSize function to allocate extra
 *   workspace for \p workspace before calling this function.
 *
 * @par Performance Optimization
 * - None.
 *
 * @note
 * - This function is only supported on MLU300 series, CE3226 and MLU500 series.
 * - Currently, 2D pooling have been supported.
 * - When the kernel of the pooling contains NaN:
 *   - On MLU500 series:
 *    - The output value is NaN and the index is the index of the first NaN.
 *   - On MLU300 series and CE3226:
 *    - If the last value in the kernel of the pooling is NaN, the output value is NaN and the output index is
 *      the index of the last value. Otherwise, the output value is the maximum value after the last NaN, and
 *      the output index is that of the corresponding maximum value.
 * - When the value of \p random_samples is too large, the sampled points may exceed the input range and the
 *   result may be unpredictable.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html#FractionalMaxPool2d
 */
cnnlStatus_t CNNL_WIN_API
cnnlFractionalMaxPoolForward(cnnlHandle_t                   handle,
                             const cnnlTensorDescriptor_t   input_desc,
                             const void *                   input,
                             const cnnlTensorDescriptor_t   random_samples_desc,
                             const void *                   random_samples,
                             const int                      window[],
                             void *                         workspace,
                             size_t                         workspace_size,
                             const cnnlTensorDescriptor_t   output_desc,
                             void *                         output,
                             const cnnlTensorDescriptor_t   indices_desc,
                             void *                         indices);

// Group:Angle
/*!
 * @brief Computes the angle of real numbers or complex numbers.
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the Angle operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. The precision and performance preference used in the computation. For detailed information, see ::cnnlComputationPreference_t.
 * @param[in] nan_propagation
 *   Input. The NaN propagation type used in the computation. For detailed information, see ::cnnlNanPropagation_t.
 * @param[in] angle_algo
 *   Input. The algorithm of a supported framework and version used in the computation. For detailed information, see ::cnnlAngleAlgo_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input data.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output data.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED.
 *
 * @par Formula
 * - See "Angle Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: int8, uint8_t, int16_t, int32_t, float, complex_float.
 *   - output tensor: complex_float, float.
 *
 * @par Scale Limitation
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * * @note
 * - The input \p x is a multi-dimensional arrays, supporting up to CNNL_DIM_MAX dimensions.
 * - The stride can be specified for all dimensions for x_desc and y_desc with ::cnnlSetTensorDescriptorEx.
 * - If the input type is real type, the output type must also be real type.
 * - If the input type is complex_float, the output type can be complex_float or float.
 * - If \p angle_algo is CNNL_ANGLE_ALGO_V1, \p nan_propagation cannot be CNNL_PROPAGATE_NAN.
 * - If the input type is complex_float, \p nan_propagation cannot be CNNL_NOT_PROPAGATE_NAN.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */

cnnlStatus_t CNNL_WIN_API
cnnlAngle(const cnnlHandle_t handle,
          const cnnlComputationPreference_t prefer,
          const cnnlNanPropagation_t nan_propagation,
          const cnnlAngleAlgo_t angle_algo,
          const cnnlTensorDescriptor_t x_desc,
          const void *x,
          const cnnlTensorDescriptor_t y_desc,
          void *y);

// Group:LogAddExp2
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the logaddexp2 operation.
 *
 * The size of the extra workspace is based on the given information of the logaddexp2 operation,
 * including the input tensor descriptors \p input1_desc and \p input2_desc, and the output
 * tensor descriptor \p output_desc. For more information about the workspace, see "Cambricon
 * CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   the logaddexp2 operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input1_desc
 *   Input. The descriptor of the input1 tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input2_desc
 *   Input. The descriptor of the input2 tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the logaddexp2
 *   operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetLogAddExp2WorkspaceSize(cnnlHandle_t handle,
                                                         const cnnlTensorDescriptor_t input1_desc,
                                                         const cnnlTensorDescriptor_t input2_desc,
                                                         const cnnlTensorDescriptor_t output_desc,
                                                         size_t *workspace_size);

// Group:LogAddExp2
/*!
 * @brief Computes logarithm of the sum of exponentiations of the input tensor \p input1 and \p input2
 *       in base-2, and returns the results in the output tensor \p output.
 *
 * This function may need extra MLU memory as the workspace to improve the operation performance.
 * You can get the workspace size with the ::cnnlGetLogAddExp2WorkspaceSize
 * function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   the logaddexp2 operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input1_desc
 *   Input. The descriptor of the input1 tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input1
 *   Input. Pointer to the MLU memory that stores the first input tensor.
 * @param[in] input2_desc
 *   Input. The descriptor of the input2 tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input2
 *   Input. Pointer to the MLU memory that stores the second input tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the logaddexp2 operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the logaddexp2 operation.
 *   You can get the size of the workspace with the ::cnnlGetLogAddExp2WorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "LogAddExp2 Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensors and output tensor must be the same.
 * - The data types of input and output tensors are as follows:
 *   - input1 tensor: float, bfloat16.
 *   - input2 tensor: float, bfloat16.
 *   - output tensor: float, bfloat16.
 *
 * @par Data Layout
 * - The data layouts of the input tensors and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - None.
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlLogAddExp2 function to perform the logaddexp2 operation.
 *
 * @note
 * - The inputs \p input1 and \p input2 are multi-dimensional arrays, supporting up to CNNL_DIM_MAX dimensions.
 * - This operation supports tensor broadcasting. \p input1, \p input2 and \p output must be broadcastable.
 * - You can specify the strides of all dimensions for input1_desc, input2_desc and output_desc with ::cnnlSetTensorDescriptorEx.
 * - On MLU300 series and CE3226:
 *   - The absolute value of (\p input1 / \p input2) is recommended to be greater than 1e-6 for higher precision.
 *   - The inputs \p input1 and \p input2 with NaN or infinity are supported.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.pytorch.org/docs/stable/generated/torch.logaddexp2.html
 */
cnnlStatus_t CNNL_WIN_API cnnlLogAddExp2(cnnlHandle_t handle,
                                         const cnnlTensorDescriptor_t input1_desc,
                                         const void *input1,
                                         const cnnlTensorDescriptor_t input2_desc,
                                         const void *input2,
                                         void *workspace,
                                         const size_t workspace_size,
                                         const cnnlTensorDescriptor_t output_desc,
                                         void *output);

// Group:Cummax
/*!
 * @brief Gets the cumulative maximum of input tensor \p input along the given \p axis.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the cummax
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] axis
 *   Input. An integer that specifies the dimension to compute over.
 * @param[in] output_values_desc
 *   Input. The descriptor of the \p output_values tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output_values
 *   Output. Pointer to the MLU memory that stores \p output_values tensor that
 *   is the cumulative maximum of elements of input tensor along the \p axis dimension.
 * @param[in] output_indices_desc
 *   Input. The descriptor of the \p output_indices tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output_indices
 *   Output. Pointer to the MLU memory that stores \p output_indices tensor that
 *   is the index of maximum in input tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH,
 *   ::CNNL_STATUS_NOT_SUPPORTED, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @par Formula
 * - See "Cummax Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor and output tensors.
 *   Data type of input tensor and values output tensor should be the same.
 *   - input tensor: uint8, int8, int16, int32, half, float, bool, bfloat16.
 *   - output_values tensor: uint8, int8, int16, int32, half, float, bool, bfloat16.
 *   - output_indices tensor: int32, int64.
 *
 *   bfloat16 is only supported on MLU500 series.
 *
 * @par Scale Limitation
 *   - If the data type is int32, each element in \p input should be in the range
 *     of (\f$-2^{24}\f$, \f$2^{24}\f$). If the data type is int8, int16 or int32, the intermediate results
 *     cannot exceed the value range of the corresponding data type.
 *   - Large tensor is only supported on MLU500 series, which means that the input tensor number can be equal to
 *     or larger than 2G.
 *   - The value of \p axis should be in range of [-input_dims, input_dims -1].
 *
 * @note
 *   - During the index calculation, the index of NaN value and next values can only be zero.
 *     For example, given an array [1, nan, 2, 3], index [0, 0, 0, 0] will be returned.
 *   - If the data type of \p input is bool, the input data that is not 0 is treated as 1.
 *     For example, given an array [0, 2, 1, 3], value [0, 1, 1, 1] and index [0, 1, 2, 3] will be returned.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The examples of the cummax operation are as follows:
     @verbatim
     input array by 2 * 3 --> input: [[1,2,3],[4,5,6]]

     1.param:
       axis: 0

     output array by 2 * 3 --> output: values=tensor([[1, 2, 3],     indices=tensor([[0, 0, 0],
                                                      [4, 5, 6]])                    [1, 1, 1]])]

     2.param:
       axis: 1

     output array by 2 * 3 --> output: values=tensor([[1, 2, 3],     indices=tensor([[0, 1, 2],
                                                      [4, 5, 6]])                   [0, 1, 2]])
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.cummax.html
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlCummax(cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const void *input,
                                     int axis,
                                     const cnnlTensorDescriptor_t output_values_desc,
                                     void *output_values,
                                     const cnnlTensorDescriptor_t output_indices_desc,
                                     void *output_indices);



// Group:Bucketize
/*!
 * @brief Returns the indices of the buckets that each element in the \p input tensor belongs to based on the \p boundaries tensor.
 * @param[in] handle
 *   Input. Handle to a CNNL context that is used to manage MLU devices and queues in the
 *   Bucketize operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] desc_boundaries
 *   Input. The descriptor of the \p boundaries tensor, including dimension and data layout.
 * @param[in] boundaries
 *   Input. Pointer to the MLU memory that stores the \p boundaries tensor.
 * @param[in] desc_input
 *   Input. The descriptor of the \p input tensor, including dimension and data layout.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] desc_output
 *   Input. The descriptor of the \p output tensor, including dimension, data type and data layout.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor, which are indices of the buckets that each element of the \p input tensor belongs to.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Bucketize Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported data types of boundaries and input are as follows:
 *   - boundaries tensor: int32, float.
 *   - input tensor: int32, float.
 *   - output tensor: int32.
 *   - Data types of \p boundaries tensor and \p input tensor should be the same.
 *
 * @par Data Layout
 * - The supported data layouts of \p boundaries, \p input and \p output tensors are as follows:
 *   - boundaries tensor: CNNL_LAYOUT_ARRAY.
 *   - input tensor: CNNL_LAYOUT_ARRAY.
 *   - output tensor: CNNL_LATOUT_ARRAY.
 *   - Data Layout of \p input tensor and \p output tensor should be the same.
 *
 * @par Scale Limitation
 * - If data types of \p boundaries tensor and \p input tensor are int32, then each element in \p boundaries tensor and \p input tensor must be between [-16777214, 16777215].
 * - The dimension of \p boundaries tensor must be 1.
 *
 * @par API Dependency
 * - None.
 *
 * @par Performance Optimization
 * - None.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */

cnnlStatus_t CNNL_WIN_API
cnnlBucketize(cnnlHandle_t handle,
              const cnnlTensorDescriptor_t desc_boundaries,
              const void *boundaries,
              const cnnlTensorDescriptor_t desc_input,
              const void *input,
              const cnnlTensorDescriptor_t desc_output,
              void *output);

// Group:FakeQuantizePerTensorAffine
/*!
 * @brief Computes fake quantized values of the input tensor \p input using scale, zero_point,
 *        quant_min and quant_max, and returns the results in the output tensor \p output.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   the FakeQuantizePerTensorAffine operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] scale
 *   Input. A double type scalar to specify the quantization scale.
 * @param[in] zero_point
 *   Input.  An int64 type scalar to specify the quantization offset.
 * @param[in] quant_min
 *   Input.  An int64 type scalar to specify the lower bound of quantized domain.
  * @param[in] quant_max
 *   Input.  An int64 type scalar to specify the upper bound of quantized domain.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH.
 *
 * @par Formula
 * - See "FakeQuantizePerTensorAffine Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensors and output tensor must be the same.
 * - The data types of input and output tensors are as follows:
 *   - input tensor: float, half.
 *   - output tensor: float, half.
 *
 * @par Data Layout
 * - The data layouts of the input tensors and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - In CE3226, 1V and MLU300 series, the tensor number must be less than \f$2^{31}\f$.
 *
 * @note
 * - This function is only supported on CE3226, 1V, MLU300 series and MLU500 series.
 * - The shape of input \p input and the shape of output \p output must be the same.
 * - When input \p input or output \p output is the non-continuous tensor with strides specified, the input \p input supports up to CNNL_DIM_MAX dimensions.
 * - You can specify the strides of all dimensions for input_desc and output_desc with ::cnnlSetTensorDescriptorEx.
 * - The input \p quant_min must be less than or equal to the input \p quant_max.
 * - The input \p zero_point must be between the input \p quant_min and the input \p quant_max.
 * - The input \p scale is double scalar, but it will be cast to float scalar.
 * - The input \p zero_point is int64 scalar, but must be in range of \f$-2^{31}\f$ and \f$2^{31}-1\f$ on 1V.
 * - The input \p quant_min is int64 scalar, but must be in range of \f$-2^{31}\f$ and \f$2^{31}-1\f$ on 1V.
 * - The input \p quant_max is int64 scalar, but must be in range of \f$-2^{31}\f$ and \f$2^{31}-1\f$ on 1V.
 *
 * @par Reference
 * - https://www.pytorch.org/docs/1.9.0/generated/torch.fake_quantize_per_tensor_affine.html
 *
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API
cnnlFakeQuantizePerTensorAffine(cnnlHandle_t handle,
                                const double scale,
                                const int64_t zero_point,
                                const int64_t quant_min,
                                const int64_t quant_max,
                                const cnnlTensorDescriptor_t input_desc,
                                const void *input,
                                const cnnlTensorDescriptor_t output_desc,
                                void *output);

// Group:Exponential
/*!
 * @brief Generates an exponential distribution based on the sampling rate specified in \p lambda.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGenerateRandExponential instead, which supports
 *   \p philox mode only.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the Exponential operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] generator
 *   Input. The descriptor of random generator. For detailed information,
 *   see ::cnnlRandGenerator_t.
 * @param[in] lambda
 *   Input. A host pointer to the rate parameter.
 * @param[in,out] state
 *   Input and output. Pointer to the MLU memory that stores the state tensor,
 *   which is used to generate random sequence.
 *   Set it to NULL if you use \p CNNL_RAND_RNG_FAST as the \p generator type.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the \p output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH,
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Exponential Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 *  - The supported data types for \p lambda and \p output tensors are as follows:
 *    - lambda: half, float, bfloat16.
 *    - output tensor: half, float, bfloat16.
 *
 * @par Scale Limitation
 * - On platforms earlier than MLU500 series, the \p output tensor number should be less than \f$2^{31}\f$.
 * - The random \p generator type should be MTGP32 on 1V.
 * - The input \p lambda value should not be NaN or less than 0.
 * - This operation is not supported on MLU220, MLU270 or MLU290.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/_modules/torch/distributions/exponential.html
 */
CNNL_DEPRECATED_FOR(cnnlGenerateRandExponential)
cnnlStatus_t CNNL_WIN_API cnnlExponential(cnnlHandle_t handle,
                                          const cnnlRandGenerator_t generator,
                                          const void *lambda,
                                          void *state,
                                          const cnnlTensorDescriptor_t output_desc,
                                          void *output);

// Group:Exponential
/*!
 * @brief Generates an exponential distribution based on the sampling rate specified in \p lambda.
 *
 * It only supports \p philox mode. To use other modes, call
 * ::cnnlExponential.
 * The Philox algorithm has three parameters: seed, offset and subsequence. Seed specifies the key
 * in the algorithm, and offset and subsequence specify the counter in the algorithm. On MLU devices
 * , we use index [0, THREAD_NUM) to bind to subsequence, so only seed and offset arguments are needed.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the Exponential operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] rng_type
 *   Input. The rng type of random generator. Only support \p CNNL_RAND_RNG_PHILOX.
 * @param[in] lambda
 *   Input. A host pointer to the rate parameter.
 * @param[in] captured
 *   Input. If \p captured is true, use \p seed_ptr, \p offset_ptr and \p offset_intragraph to specify
 *   seed and offset. If \p captured is false, use \p seed and \p offset.
 * @param[in] seed
 *   Input. Specify the seed of the Philox algorithm when \p captured is false.
 * @param[in] offset
 *   Input. Specify the offset of the Philox algorithm when \p captured is false.
 * @param[in] seed_ptr
 *   Input. Pointer to device seed data. Specify the seed of the Philox algorithm when \p captured is true.
 * @param[in] offset_ptr
 *   Input. Pointer to device offset data. Specify the offset of the Philox algorithm when \p captured is true.
 * @param[in] offset_intragraph
 *   Input. The offset in a graph. Specify the offset of the Philox algorithm when \p captured is true.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the \p output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH,
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Exponential Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported data types for \p lambda and \p output tensors are as follows:
 *   - lambda: half, float, bfloat16.
 *   - output tensor: half, float, Bfloat16.
 *
 *   The bfloat16 data type is supported only on MLU500 series.
 *
 * @par Scale Limitation
 * - On platforms earlier than MLU500 series, the \p output tensor number should be less than \f$2^{31}\f$.
 * - The random \p generator type should be MTGP32 on 1V.
 * - The input \p lambda value should not be NaN or less than 0.
 * - This operation is not supported on MLU220, MLU270 or MLU290.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/_modules/torch/distributions/exponential.html
 */
cnnlStatus_t CNNL_WIN_API cnnlGenerateRandExponential(cnnlHandle_t handle,
                                                      cnnlRandRngType_t rng_type,
                                                      const void *lambda,
                                                      const bool captured,
                                                      const uint64_t seed,
                                                      const uint64_t offset,
                                                      const int64_t *seed_ptr,
                                                      const int64_t *offset_ptr,
                                                      const uint32_t offset_intragraph,
                                                      const cnnlTensorDescriptor_t output_desc,
                                                      void *output);

// Group:cnnlFakeQuantizePerChannelAffine
/*!
 * @brief Computes the fake quantized values of the input tensor \p input across the channel
 *        specified by \p axis.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the FakeQuantizePerChannelAffine operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] axis
 *   Input. An integer that specifies the dimension to compute the fake quantized values.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] scale_desc
 *   Input. The descriptor of the scale tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] scale
 *   Input. Pointer to the MLU memory that stores the scale tensor which is the scale factor of
 *   the quantization.
 * @param[in] zero_point_desc
 *   Input. The descriptor of the zero_point tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] zero_point
 *   Input. Pointer to the MLU memory that stores the zero_point tensor which is the offset of
 *   the quantization.
 * @param[in] quant_min
 *   Input. An int64 type scalar to specify the lower bound of quantized domain.
 * @param[in] quant_max
 *   Input. An int64 type scalar to specify the upper bound of quantized domain.
 * @param[in] output_desc
 *   Input. The descriptor of the \p output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores \p output tensor which is the fake
 *   quantized values of input tensor along the specified dimension.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "FakeQuantizePerChannelAffine Operator" section in "Cambricon CNNL User Guide" for
 *   details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensors and output tensor.
 *   Data type of input tensor and output tensor should be the same.
 *   - input tensor: float.
 *   - scale tensor: float.
 *   - zero_point tensor: int32.
 *   - output tensor: float.
 *
 * @par Data Layout
 * - The data layouts of the input tensors and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - In CE3226, 1V and MLU300 series, the tensor number must be less than \f$2^{31}\f$.
 *
 * @note
 *  - The shape of input \p input and the shape of output \p output must be the same.
 *  - The shape of \p scale tensor is 1D, and the shape of \p scale tensor and that of
 *    \p zero_point tensor must be the same.
 *  - The value of \p axis cannot be negative number and must be less than the shape size of
 *    \p input tensor.
 *  - The input \p quant_max must be greater than or equal to the input \p quant_min.
 *  - The input \p quant_max must be in range of \f$-2^{30}\f$ and \f$2^{30}-1\f$.
 *  - The input \p quant_min must be in range of \f$-2^{30}\f$ and \f$2^{30}-1\f$.
 *  - The value of the \p zero_point tensor must be in range of \p quanti_min and \p quanti_max.
 *    The constraint is implemented at the framework level.
 *  - Every dimension of the \p output tensor should be the same as that of the \p input tensor.
 *  - Every dimension of the \p scale tensor should be the same as that of the \p zero_point tensor.
 *  - The element number of the \p scale tensor should be equal to that of  the \p axis dimension
 *    of the \p input tensor.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.pytorch.org/docs/1.9.0/generated/torch.fake_quantize_per_channel_affine.html
 */
CNNL_DEPRECATED_FOR()
cnnlStatus_t CNNL_WIN_API cnnlFakeQuantizePerChannelAffine(cnnlHandle_t handle,
                                                           const int64_t axis,
                                                           const int64_t quant_min,
                                                           const int64_t quant_max,
                                                           const cnnlTensorDescriptor_t input_desc,
                                                           const void *input,
                                                           const cnnlTensorDescriptor_t scale_desc,
                                                           const void *scale,
                                                           const cnnlTensorDescriptor_t
                                                           zero_point_desc,
                                                           const void *zero_point,
                                                           const cnnlTensorDescriptor_t output_desc,
                                                           void *output);

// Group:SearchSorted
/*!
 * @brief Find indices where elements in \p values should be inserted into \p sorted_sequence so that the order of \p sorted_sequence is preserved.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the SearchSorted operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] desc_sorted_sequence
 *   Input. The descriptor of the \p sorted_sequence tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] sorted_sequence
 *   Input. Pointer to the MLU memory that stores the \p sorted_sequence tensor, which is a sorted sequence in the ascending order.
 * @param[in] desc_values
 *   Input. The descriptor of the \p values tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] values
 *   Input. Pointer to the MLU memory that stores the \p values tensor, which is the values to be inserted.
 * @param[in] desc_input_out
 *   Input. The descriptor of the \p input_out tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input_out
 *   Input. Pointer to the MLU memory that stores the \p input_out tensor, which is used to specify the shape of the output tensor,
 *   This parameter is reserved.
 * @param[out] desc_output
 *   Output. The descriptor of the \p output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the \p output tensor, which are indices where \p values should be inserted
 *   into \p sorted_sequence.
 * @param[in] out_int32
 *   Input. A Boolean value indicating the data type of output. "True" means the data type is int32 while "False" means int64.
 *   This parameter is reserved.
 * @param[in] right
 *   Input. A Boolean value. If False, return the index of the first suitable location found. If True, return the last such index.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Data Type
 * - The data types of input and output tensors are as follows:
 *   - input tensor: float, int32.
 *   - output tensor: int32.
 * - The shape of the output tensor should be the same as that of the \p values tensor.
 *
 * @par Scale Limitation
 * - The data volume of the innermost dimension of \p sorted_sequence should be less than 12000.
 * - If the type of the \p sorted_sequence is int32, the maximum value of input data should be less than \f$2^{24}-1\f$.
 * - The total data amount of each input tensor should be less than \f$2^{31}-1\f$.
 *
 * @note
 * - The shape of the output tensor should be the same as that of the \p values tensor. And there are two situations between the dimension of the
 *   \p sorted_sequence tensor and the dimension of \p values tensor. In the first case, the dimension of \p sorted_sequence tensor is 1. In the
 *   second case, the dimension of the \p sorted_sequence should be the same as that of the \p values tensor while the num of data in each dimension
 *   of the first N-1 dimensions of the two tensors should be the same.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.searchsorted.html
 */

cnnlStatus_t CNNL_WIN_API
cnnlSearchSorted(cnnlHandle_t handle,
                 const cnnlTensorDescriptor_t desc_sorted_sequence,
                 const void *sorted_sequence,
                 const cnnlTensorDescriptor_t desc_values,
                 const void *values,
                 const cnnlTensorDescriptor_t desc_input_out,
                 const void* input_out,
                 const cnnlTensorDescriptor_t desc_output,
                 void *output,
                 bool out_int32,
                 bool right);

// Group:Poisson
/*!
 * @brief Returns a tensor of the same size as \p input with each element sampled from
 * a Poisson distribution with rate parameter given by the corresponding element in \p input.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGenerateRandPoisson instead, which supports
 *   \p philox mode only.
 *
 * @param[in] handle
 *  Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *  in the Poisson operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] generator
 *  Input. The descriptor of random \p generator. For detailed information,
 *  see ::cnnlRandGenerator_t.
 * @param[in] input_desc
 *  Input. The descriptor of \p input tensor which represents the input of the operation.
 *  For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *  Input. Pointer to the MLU memory that stores the \p input tensor.
 * @param[in,out] state
 *  Input and output. Pointer to device state data, which is used to generate random sequence.
 *  Set NULL if you use ::CNNL_RAND_RNG_FAST  and ::CNNL_RAND_RNG_PHILOX \p generator type.
 * @param[in] output_desc
 *  Input. The descriptor of \p output tensor which represents the output of the operation.
 *  For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *  Output. Pointer to the MLU memory that stores the \p output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Poisson Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 *  - This function supports the following data types for input tensor \p input: half, BFloat16, float.
 *
 * @par Scale Limitation
 * - None.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.poisson.html
 */
CNNL_DEPRECATED_FOR(cnnlGenerateRandPoisson)
cnnlStatus_t CNNL_WIN_API
cnnlPoisson(cnnlHandle_t handle,
            const cnnlRandGenerator_t generator,
            const cnnlTensorDescriptor_t input_desc,
            const void *input,
            void *state,
            const cnnlTensorDescriptor_t output_desc,
            void *output);

// Group:Poisson
/*!
 * @brief Returns a tensor of the same size as \p input with each element sampled from
 * a Poisson distribution with rate parameter given by the corresponding element in \p input.
 *
 * It only supports \p philox mode. To use other modes, call
 * ::cnnlPoisson.
 * The Philox algorithm has three parameters: seed, offset and subsequence. Seed specifies the key
 * in the algorithm, and offset and subsequence specify the counter in the algorithm. On MLU devices
 * , we use index [0, THREAD_NUM) to bind to subsequence, so only seed and offset arguments are needed.
 *
 * @param[in] handle
 *  Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *  in the Poisson operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] rng_type
 *   Input. The rng type of random generator. Only support \p CNNL_RAND_RNG_PHILOX.
 * @param[in] input_desc
 *  Input. The descriptor of \p input tensor that represents the input of the operation.
 *  For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *  Input. Pointer to the MLU memory that stores the \p input tensor.
 * @param[in] captured
 *   Input. If \p captured is true, use \p seed_ptr, \p offset_ptr and \p offset_intragraph to specify
 *   seed and offset. If \p captured is false, use \p seed and \p offset.
 * @param[in] seed
 *   Input. Specify the seed of the Philox algorithm when \p captured is false.
 * @param[in] offset
 *   Input. Specify the offset of the Philox algorithm when \p captured is false.
 * @param[in] seed_ptr
 *   Input. Pointer to device seed data. Specify the seed of the Philox algorithm when \p captured is true.
 * @param[in] offset_ptr
 *   Input. Pointer to device offset data. Specify the offset of the Philox algorithm when \p captured is true.
 * @param[in] offset_intragraph
 *   Input. The offset in a graph. Specify the offset of the Philox algorithm when \p captured is true.
 * @param[in] output_desc
 *  Input. The descriptor of \p output tensor which represents the output of the operation.
 *  For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *  Output. Pointer to the MLU memory that stores the \p output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Poisson Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \p input:
 *   - half, BFloat16, float.
 *
 *   The bfloat16 data type is supported only on MLU500 series.
 *
 * @par Scale Limitation
 * - None.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.poisson.html
 */
cnnlStatus_t CNNL_WIN_API cnnlGenerateRandPoisson(cnnlHandle_t handle,
                                                  cnnlRandRngType_t rng_type,
                                                  const cnnlTensorDescriptor_t input_desc,
                                                  const void *input,
                                                  const bool captured,
                                                  const uint64_t seed,
                                                  const uint64_t offset,
                                                  const int64_t *seed_ptr,
                                                  const int64_t *offset_ptr,
                                                  const uint32_t offset_intragraph,
                                                  const cnnlTensorDescriptor_t output_desc,
                                                  void *output);

// Group:Cross
/*!
 * @brief Computes the cross product of input tensors \p a and \p b
 *        along the specified dimension \p dim, and returns the results in the output tensor \p c.
 *
 * This function may need extra MLU memory as the workspace to improve the operation performance.
 * You can get the workspace size with the ::cnnlGetCrossWorkspaceSize
 * function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   the cross operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] a_desc
 *   Input. The descriptor of the a tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. Pointer to the MLU memory that stores the first input tensor.
 * @param[in] b_desc
 *   Input. The descriptor of the b tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. Pointer to the MLU memory that stores the second input tensor.
 * @param[in] c_desc
 *   Input. The descriptor of the c tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] c
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the cross operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the cross operation.
 *   You can get the size of the workspace with the ::cnnlGetCrossWorkspaceSize function.
 * @param[in] dim
 *   Input. The dimension to perform cross product. If \p dim is not given, it defaults to the
 *   first dimension found with the size equal to 3. If \p dim < 0, the actual \p dim value applied
 *   in the computation is (tensor_dim + \p dim), where \p tensor_dim is the dimensions of the input tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Cross Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensors and output tensor must be the same.
 * - The data types of input and output tensors are as follows:
 *   - a tensor: int32, float, half, bfloat16.
 *   - b tensor: int32, float, half, bfloat16.
 *   - c tensor: int32, float, half, bfloat16.
 *
 *   The bfloat16 data type is supported only on MLU500 series.
 *
 * @par Data Layout
 * - The data layouts of the input tensors and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - On 1V, when the data type of \p c is int32, if the value of \p c is out of the range
 *   [-16777216, 16777215], the calculation result may be incorrect.
 * @par API Dependency
 * - ::cnnlGetCrossWorkspaceSize should be called to get the allocated extra workspace before this operation.
 *
 * @note
 * - The inputs \p a and \p b are multi-dimensional arrays, supporting up to CNNL_DIM_MAX dimensions.
 * - The \p dim value can be positive or negative, but must be within [-CNNL_DIM_MAX, CNNL_DIM_MAX - 1].
 * - On the \p dim dimension, the dimension lengths of tensors \p a and \p b are either all 3, or one is
 *   1 and the other is 3.
 * - This operation supports tensor broadcasting. \p a and \p b must be broadcastable.
 * - The dimensions of tensors \p a and \p b must be the same.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the cross operation is as follows:
    @verbatim
    input two arrays by 1 * 2 and 3 * 2 --> a: [[3, 1]]

    --> b: [[7, 3], [2, 2], [2, 7]]

    output array by 3 * 2 --> c: [[0, 5], [15, -4], [-15, -1]]
    @endverbatim
 *
 * @par Reference
 * - https://www.pytorch.org/docs/stable/generated/torch.cross.html
 */
cnnlStatus_t CNNL_WIN_API cnnlCross(const cnnlHandle_t handle,
                                    const cnnlTensorDescriptor_t a_desc,
                                    const void *a,
                                    const cnnlTensorDescriptor_t b_desc,
                                    const void *b,
                                    const cnnlTensorDescriptor_t c_desc,
                                    void *c,
                                    void *workspace,
                                    const size_t workspace_size,
                                    const int dim);

// Group:Cross
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace to
 * optimize the cross operation.
 *
 * The size of the extra workspace is based on the given information of the cross operation,
 * including the input tensors descriptor \p a_desc and \p b_desc. For more information about
 * the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   cross operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] a_desc
 *   Input. Descriptor of input data \p a, including dimension, data type and data layout.
 * @param[in] b_desc
 *   Input. Descriptor of input data \p b, including dimension, data type and data layout.
 * @param[in] c_desc
 *   Input. Descriptor of output data \p c, including dimension, data type and data layout.
 * @param[in] dim
 *   Input. The dimension to perform cross product. If \p dim is not given, it defaults to the
 *   first dimension found with the size equal to 3. If \p dim < 0, the actual \p dim value applied
 *   in the computation is (tensor_dim + \p dim), where \p tensor_dim is the dimensions of the input tensor.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *   the cross operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetCrossWorkspaceSize(cnnlHandle_t handle,
                                                    const cnnlTensorDescriptor_t a_desc,
                                                    const cnnlTensorDescriptor_t b_desc,
                                                    const cnnlTensorDescriptor_t c_desc,
                                                    const int dim,
                                                    size_t *size);
/******************************************************************************
 * Cambricon CNNL OP: Dynamic_Partition
 ******************************************************************************/
// Group:DynamicPartition
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra workspace to
 * optimize the dynamic partition operation.
 *
 * The size of the extra workspace is based on the given information of the dynamic partition operation,
 * including the input tensor descriptor \p data_desc and \p output_desc. For more information about the workspace,
 * see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   dynamic partition operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] data_desc
 *   Input. The descriptor of the \p data tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] partition_desc
 *   Input. The descriptor of the \p partition tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] partition_num
 *   Input. The number of partitions to output.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *   the dynamic partition operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called before the ::cnnlDynamicPartition function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetDynamicPartitionWorkspaceSize(cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t data_desc,
                                     const cnnlTensorDescriptor_t partition_desc,
                                     const int partition_num,
                                     size_t *workspace_size);

// Group:DynamicPartition
/*!
 * @brief Partitions \p data into \p partition_num tensors using indices from \p partition.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   dynamic partition operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] data_desc
 *   Input. The descriptor of the \p data tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] data
 *   Input. Pointer to the MLU memory that stores the \p data tensor, which is the data to be partitioned.
 * @param[in] partition_desc
 *   Input. The descriptor of the \p partition tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] partition
 *   Input. Pointer to the MLU memory that stores the \p partition tensor, which is the indices to partition data.
 * @param[in] partition_num
 *   Input. The number of partitions to output.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the dynamic partition operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the dynamic partition operation.
 *   You can get the size of the workspace with the ::cnnlGetDynamicPartitionWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the \p output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the \p output tensor, which is partitioned data.
 * @param[in] output_count_desc
 *   Input. The descriptor of the \p output_count tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output_count
 *   Output. Pointer to the MLU memory that stores the \p output_count tensor that
 *   is numbers of elements in per output array.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Dynamic Partition Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensors and output tensors.
 *   Data type of \p data tensor and \p output tensor should be the same.
 *   - data tensor: int32, int64, float.
 *   - partition tensor: int32.
 *   - output tensor: int32, int64, float.
 *   - output_count tensor: int32.
 *
 * @par Scale Limitation
 * - Large tensor is not supported and the tensor number of this operation must be less than \f$2^{31}\f$.
 * - This operation is not supported on MLU220, MLU270 or MLU290.
 *
 * @note
 * - The shape of \p data tensor must start with the shape of \p partition tensor.
 * - The value of elements in \p partition tensor must in range [0, \p partition_num).
 * - The total element number and data type of \p output tensor must be the same as
 *   the total element number and data type of \p data tensor.
 * - The total element number of \p output_count tensor must equal to \p partition_num.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://tensorflow.org/versions/r1.15/api_docs/python/tf/dynamic_partition
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlDynamicPartition(cnnlHandle_t handle,
                     const cnnlTensorDescriptor_t data_desc,
                     const void *data,
                     const cnnlTensorDescriptor_t partition_desc,
                     const void *partition,
                     const int partition_num,
                     void *workspace,
                     size_t workspace_size,
                     const cnnlTensorDescriptor_t output_desc,
                     void *output,
                     const cnnlTensorDescriptor_t output_count_desc,
                     void *output_count);

// Group:SortPairs
/*!
 * @brief Returns in \p size the size of the MLU memory that is used as an extra workspace to
 * optimize the SortPairs operation.
 *
 * The size of the extra workspace is based on the given information of the SortPairs operation,
 * including the input tensor descriptor \p keys_in_desc and \p values_in_desc.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a CNNL context that is used to manage MLU devices and queues in the
 *   ax operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] keys_in_desc
 *   Input. Descriptor of input data \p keys_in_desc, including dimension,
 *   data type (half, float, bfloat16), and data layout.
 * @param[in] values_in_desc
 *   Input. Descriptor of input data \p values_in_desc, including dimension,
 *   data type (int32), and data layout.
 * @param[out] keys_out_desc
 *   Output. The descriptor of the keys_out. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] values_out_desc
 *   Output. The descriptor of the values_out. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] begin_bit
 *   Input. start pos of binary bits, number from lower bit, range [0, sizeof(KeyType) * 8), but cur must be 0
 * @param[in] end_bit
 *   Input. end pos of binary bits, range [0, sizeof(KeyType) * 8), but cur must be sizeof(KeyType) * 8 - 1
 * @param[in] sort_type
 *   Input. optional value ASENDING or DESCENDING.
 * @param[in] k
 *   Input. required value that indicates top k values ASENDING or DESCENDING.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *   the SortPairs operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetSortPairsWorkspaceSize(cnnlHandle_t handle,
                              const cnnlTensorDescriptor_t keys_in_desc,
                              const cnnlTensorDescriptor_t values_in_desc,
                              const cnnlTensorDescriptor_t keys_out_desc,
                              const cnnlTensorDescriptor_t values_out_desc,
                              const size_t begin_bit,
                              const size_t end_bit,
                              const cnnlSortType_t sort_type,
                              const uint64_t k,
                              size_t *workspace_size);

// Group:SortPairs
/*!
 * @brief Gathers slices from \p params with shape specified by \p indices.
 *
 * @param[in] handle
 *   Input. Handle to a CNNL context that is used to manage MLU devices and queues in the radix_sort_pairs
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] keys_in_desc
 *   Input. shoule be [barch_num, n], The descriptor of the keys_in. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] keys_in
 *   Input. Pointer to the MLU memory that stores the keys_in.
 * @param[in] values_in_desc
 *   Input. The descriptor of the values_in. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] values_in
 *   Input. Pointer to the MLU memory that stores the values_in.
 * @param[in] begin_bit
 *   Input. start pos of binary bits, number from lower bit, range [0, sizeof(KeyType) * 8), but cur must be 0
 * @param[in] end_bit
 *   Input. end pos of binary bits, range [0, sizeof(KeyType) * 8), but cur must be sizeof(KeyType) * 8 - 1
 * @param[in] sort_type
 *   Input. optional value ASENDING or DESCENDING.
 * @param[in] k
 *   Input. required value that indicates top k values ASENDING or DESCENDING.
 * @param[in] workspace
 *   Input. Pointer to the gdram, it's divided into three parts:
 *   d_keys_temp_storage, d_values_temp_storage, d_compute_temp_storage.
 * @param[in] workspace_size
 *   Input. Bytes number of workspace.
 * @param[out] keys_out_desc
 *   Output. The descriptor of the keys_out. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] keys_out
 *   Output. Pointer to the MLU memory that stores the keys_out.
 * @param[out] values_out_desc
 *   Output. The descriptor of the values_out. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] values_out
 *   Output. Pointer to the MLU memory that stores the values_out.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "GatherNd Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for d_keys_in, d_values_in.
 *   \p params, input keys \p d_keys_in, input values \p d_values_in.
 *   <b>Note that the data type of d_keys_in and d_keys_out, d_values_in and d_values_out must be the same.</b>
 *   - input keys: half, float, bfloat16.
 *   - input values: int32.
 *
 * @note
 * - None.
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the SortPairs operation is as follows:
    @verbatim
    when num_items: 5, inputAyyay[5] = {<5,-1>, <-3,2>, <4,5>, <6,8>, <3,7>}, then
    keys_in = 5 -3 4 6 3, values_in = -1 2  5 8 7

    if sort_type is ASENDING, keys_out = -3 3 4 5 6, values_out = 2 7 5 -1 8.
    if sort_type is DESCENDING, keys_out = 6 5 4 3 -3, values_out = 8 -1 5 7 2.
    @endverbatim
 *
 * @par Reference
 * - https://nvlabs.github.io/cub/structcub_1_1_device_radix_sort.html
 */
cnnlStatus_t CNNL_WIN_API
cnnlSortPairs(cnnlHandle_t handle,
              const cnnlTensorDescriptor_t keys_in_desc,
              const void *keys_in,
              const cnnlTensorDescriptor_t values_in_desc,
              const void *values_in,
              const size_t begin_bit,
              const size_t end_bit,
              const cnnlSortType_t sort_type,
              const uint64_t k,
              void* workspace,
              size_t workspace_size,
              const cnnlTensorDescriptor_t keys_out_desc,
              void *keys_out,
              const cnnlTensorDescriptor_t values_out_desc,
              void *values_out);

// Group:SparseDenseMatMul
/*!
 * @brief Creates the sparse dense matmul descriptor.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlCreateSparseDenseMatMulDescriptor instead.
 *
 * @param[out] matmul_desc
 *   Output. The pointer to the struct that holds information about the sparse dense
 *   matmul descriptor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ALLOC_FAILED
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 *
 */
CNNL_DEPRECATED_FOR(cnnlCreateSparseDenseMatMulDescriptor)
cnnlStatus_t CNNL_WIN_API
cnnlSparseDenseMatmulDescCreate(cnnlSparseDenseMatmulDescriptor_t *matmul_desc);

// Group:SparseDenseMatMul
/*!
 * @brief Creates the sparse dense matmul descriptor.
 *
 * @param[out] matmul_desc
 *   Output. The pointer to the struct that holds information about the sparse dense
 *   matmul descriptor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ALLOC_FAILED
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlCreateSparseDenseMatMulDescriptor(cnnlSparseDenseMatMulDescriptor_t *matmul_desc);

// Group:SparseDenseMatMul
/*!
 * @brief Sets the attribute of a sparse dense matmul descriptor.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlSetSparseDenseMatMulDescAttr instead.
 *
 * @param[in] matmul_desc
 *   Input. The pointer to the struct that holds information about the sparse dense
 *   matmul descriptor.
 * @param[in] attr
 *   Input. Attribute of the sparse dense matmul descriptor to be set. For detailed information,
 *   see ::cnnlSparseDenseMatmulDescAttribute_t.
 * @param[in] buf
 *   Input. The buffer of the attribute.
 * @param[in] size_in_bytes
 *   Input. The size of the buffer of the attribute.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
CNNL_DEPRECATED_FOR(cnnlSetSparseDenseMatMulDescAttr)
cnnlStatus_t CNNL_WIN_API
cnnlSetSparseDenseMatmulDescAttr(cnnlSparseDenseMatmulDescriptor_t matmul_desc,
                                 const cnnlSparseDenseMatmulDescAttribute_t attr,
                                 const void *buf,
                                 const size_t size_in_bytes);

// Group:SparseDenseMatMul
/*!
 * @brief Sets the attribute of a sparse dense matmul descriptor.
 *
 * @param[in] matmul_desc
 *   Input. The pointer to the struct that holds information about the sparse dense
 *   matmul descriptor.
 * @param[in] attr
 *   Input. Attribute of the sparse dense matmul descriptor to be set. For detailed information,
 *   see ::cnnlSparseDenseMatMulDescAttribute_t.
 * @param[in] buf
 *   Input. The buffer of the attribute.
 * @param[in] size_in_bytes
 *   Input. The size of the buffer of the attribute.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetSparseDenseMatMulDescAttr(cnnlSparseDenseMatMulDescriptor_t matmul_desc,
                                 const cnnlSparseDenseMatMulDescAttribute_t attr,
                                 const void *buf,
                                 const size_t size_in_bytes);

// Group:SparseDenseMatMul
/*!
 * @brief Gets the attribute of a sparse dense matmul descriptor.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGetSparseDenseMatMulDescAttr instead.
 *
 * @param[in] matmul_desc
 *   Input. The pointer to the struct that holds information about the sparse dense
 *   matmul descriptor.
 * @param[in] attr
 *   Input. Attribute of the sparse dense matmul descriptor to be get. For detailed information,
 *   see ::cnnlSparseDenseMatmulDescAttribute_t.
 * @param[out] buf
 *   Output. The pointer to the buffer of the attribute.
 * @param[out] size_written
 *   Output. The size of the buffer of the attribute.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
CNNL_DEPRECATED_FOR(cnnlGetSparseDenseMatMulDescAttr)
cnnlStatus_t CNNL_WIN_API
cnnlGetSparseDenseMatmulDescAttr(const cnnlSparseDenseMatmulDescriptor_t matmul_desc,
                                 const cnnlSparseDenseMatmulDescAttribute_t attr,
                                 void *buf,
                                 size_t *size_written);
// Group:SparseDenseMatMul
/*!
 * @brief Gets the attribute of a sparse dense matmul descriptor.
 *
 * @param[in] matmul_desc
 *   Input. The pointer to the struct that holds information about the sparse dense
 *   matmul descriptor.
 * @param[in] attr
 *   Input. Attribute of the sparse dense matmul descriptor to be get. For detailed information,
 *   see ::cnnlSparseDenseMatMulDescAttribute_t.
 * @param[out] buf
 *   Output. The pointer to the buffer of the attribute.
 * @param[out] size_written
 *   Output. The size of the buffer of the attribute.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetSparseDenseMatMulDescAttr(const cnnlSparseDenseMatMulDescriptor_t matmul_desc,
                                 const cnnlSparseDenseMatMulDescAttribute_t attr,
                                 void *buf,
                                 size_t *size_written);

// Group:SparseDenseMatMul
/*!
 * @brief Destroys the sparse dense matmul descriptor.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlDestroySparseDenseMatMulDescriptor instead.
 *
 * @param[in] matmul_desc
 *   Input. The pointer to the struct that holds information about the sparse dense
 *   matmul descriptor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
CNNL_DEPRECATED_FOR(cnnlDestroySparseDenseMatMulDescriptor)
cnnlStatus_t CNNL_WIN_API
cnnlSparseDenseMatmulDescDestroy(cnnlSparseDenseMatmulDescriptor_t matmul_desc);

// Group:SparseDenseMatMul
/*!
 * @brief Destroys the sparse dense matmul descriptor.
 *
 * @param[in] matmul_desc
 *   Input. The pointer to the struct that holds information about the sparse dense
 *   matmul descriptor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlDestroySparseDenseMatMulDescriptor(cnnlSparseDenseMatMulDescriptor_t matmul_desc);

// Group:SparseDenseMatMul
/*!
 * @brief Performs the multiplication of sparse tensor and dense tensor.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlSparseDenseMatMul instead.
 *
 * @param[in] handle
 *   Input. The handle to a CNNL context that is used to manage MLU devices and queues
 *   in the tensor multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] matmul_desc
 *   Input. The descriptor of sparse dense matmul operation. For detailed information,
 *   see ::cnnlSparseDenseMatmulDescriptor_t.
 * @param[in] algo
 *   Input. The algorithm for the multiplication of sparse tensor and dense tensor. For
 *   detailed information, see ::cnnlSparseDenseMatmulAlgo_t.
 * @param[in] alpha
 *   Input. The host pointer to a \p compute_type type scalar scaling factor of the
 *   results of sparse tensor and dense tensor. When \p alpha is zero, the
 *   multiplication is skipped.
 * @param[in] a_desc
 *   Input. The descriptor of the left sparse tensor, which contains the sparse
 *   tensor format, dtype, dims and data. For detailed information, see
 *   ::cnnlSparseTensorDescriptor_t. Only support sparse format CNNL_SPARSE_FORMAT_COO.
 * @param[in] b_desc
 *   Input. The descriptor of the right dense tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. The device data pointer of input and output dense tensor b,
 *   which is described by \p b_desc.
 * @param[in] beta
 *   Input. The host pointer to a \p compute_type type scalar scaling factor of dense
 *   tensor c. When \p beta is zero, the dense c tensor will be initialze to zero first.
 * @param[in] c_desc
 *   Input. The descriptor of output dense tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in,out] c
 *   Input/output. The device data pointer of input and output dense tensor c =
 *   alpha * is_trans_a(a) * is_trans_b(b) + beta * c, which is described by \p c_desc.
 * @param[in] workspace
 *   Input. The device data pointer of workspace that is used as an extra workspace for
 *   the sparse dense matmul operation.
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the
 *   multiplication of sparse tensor and dense tensor. Use
 *   ::cnnlGetSparseDenseMatmulWorkspaceSize to get the workspace size.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - On all hardware platforms, this function supports the combination of the following
 *   data types for input sparse tensor descriptor \p a_desc, input dense tensor descriptor
 *   \p b_desc and output dense tensor descriptor \p c_desc:
 *   - \p a_desc indices, \p a_desc values, \p b_desc offchip data type,
 *     \p c_desc onchip data type: int32, float, float, float
 *   - \p a_desc indices, \p a_desc values, \p b_desc offchip data type,
 *     \p c_desc onchip data type: int64, float, float, float
 *   - When \p a_desc indices use int64 indices, the device implementation is optimized to
 *     use 32 bit indexing.
 *
 * @par API Dependency
 * - Before calling this function to implement sparse dense matmul operation, you need
 *   to prepare all the parameters passed to this function. For more information, see each
 *   parameter's description.
 * - Each set of input paramters corresponds to a unique \p algo and \p workspace. If
 *   any parameter is changed, the associated ::cnnlGetSparseDenseMatmulWorkspaceSize
 *   should be re-called as well.
 *
 * @note
 * - The illegal index is skipped for computation.
 * - Because the attribute is_trans_b = true currently is not supported, transpose the tensor with
 *   cnnlTranspose firstly.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, if the sparcity of sparse tensor
 *   is less than 70%, suggest transfer the sparse tensor to dense tensor first and
 *   then use ::cnnlMatMul_v2. The indices of sparse tensor can be sorted in row-major
 *   order (or equaivalently lexicographic order on the row index and column index) to get
 *   the best performance.
 *
 * @par Reference
 * - https://docs.nvidia.com/cuda/cusparse/index.html#cusparsespmm
 *
 */
CNNL_DEPRECATED_FOR(cnnlSparseDenseMatMul)
cnnlStatus_t CNNL_WIN_API
cnnlSparseDenseMatmul(const cnnlHandle_t handle,
                      const cnnlSparseDenseMatmulDescriptor_t matmul_desc,
                      const cnnlSparseDenseMatmulAlgo_t algo,
                      const void *alpha,
                      const cnnlSparseTensorDescriptor_t a_desc,
                      const cnnlTensorDescriptor_t b_desc,
                      const void *b,
                      const void *beta,
                      const cnnlTensorDescriptor_t c_desc,
                      void *c,
                      void *workspace,
                      const size_t workspace_size);

// Group:SparseDenseMatMul
/*!
 * @brief Performs the multiplication of sparse tensor and dense tensor.
 *
 * @param[in] handle
 *   Input. The handle to a CNNL context that is used to manage MLU devices and queues
 *   in the tensor multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] matmul_desc
 *   Input. The descriptor of sparse dense matmul operation. For detailed information,
 *   see ::cnnlSparseDenseMatMulDescriptor_t.
 * @param[in] algo
 *   Input. The algorithm for the multiplication of sparse tensor and dense tensor. For
 *   detailed information, see ::cnnlSparseDenseMatMulAlgo_t.
 * @param[in] alpha
 *   Input. The host pointer to a \p compute_type type scalar scaling factor of the
 *   results of sparse tensor and dense tensor. When \p alpha is zero, the
 *   multiplication is skipped.
 * @param[in] a_desc
 *   Input. The descriptor of the left sparse tensor, which contains the sparse
 *   tensor format, dtype, dims and data. For detailed information, see
 *   ::cnnlSparseTensorDescriptor_t. Only support sparse format CNNL_SPARSE_FORMAT_COO.
 * @param[in] b_desc
 *   Input. The descriptor of the right dense tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. The device data pointer of input and output dense tensor b,
 *   which is described by \p b_desc.
 * @param[in] beta
 *   Input. The host pointer to a \p compute_type type scalar scaling factor of dense
 *   tensor c. When \p beta is zero, the dense c tensor will be initialze to zero first.
 * @param[in] c_desc
 *   Input. The descriptor of output dense tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in,out] c
 *   Input/output. The device data pointer of input and output dense tensor c =
 *   alpha * is_trans_a(a) * is_trans_b(b) + beta * c, which is described by \p c_desc.
 * @param[in] workspace
 *   Input. The device data pointer of workspace that is used as an extra workspace for
 *   the sparse dense matmul operation.
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the
 *   multiplication of sparse tensor and dense tensor. Use
 *   ::cnnlGetSparseDenseMatMulWorkspaceSize to get the workspace size.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Data Type
 * - On all hardware platforms, this function supports the combination of the following
 *   data types for input sparse tensor descriptor \p a_desc, input dense tensor descriptor
 *   \p b_desc and output dense tensor descriptor \p c_desc:
 *   - \p a_desc indices, \p a_desc values, \p b_desc offchip data type,
 *     \p c_desc onchip data type: int32, float, float, float
 *   - \p a_desc indices, \p a_desc values, \p b_desc offchip data type,
 *     \p c_desc onchip data type: int64, float, float, float
 *   - When \p a_desc indices use int64 indices, the device implementation is optimized to
 *     use 32 bit indexing.
 *
 * @par API Dependency
 * - Before calling this function to implement sparse dense matmul operation, you need
 *   to prepare all the parameters passed to this function. For more information, see each
 *   parameter's description.
 * - Each set of input paramters corresponds to a unique \p algo and \p workspace. If
 *   any parameter is changed, the associated ::cnnlGetSparseDenseMatMulWorkspaceSize
 *   should be re-called as well.
 *
 * @note
 * - The illegal index is skipped for computation.
 * - Because the attribute is_trans_b = true currently is not supported, transpose the tensor with
 *   cnnlTranspose firstly.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, if the sparcity of sparse tensor
 *   is less than 70%, it is recommended to transfer the sparse tensor to dense tensor first and
 *   then use ::cnnlMatMul_v2. The indices of sparse tensor can be sorted in row-major
 *   order (or equaivalently lexicographic order on the row index and column index) to get
 *   the best performance.
 *
 * @par Reference
 * - https://docs.nvidia.com/cuda/cusparse/index.html#cusparsespmm
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlSparseDenseMatMul(const cnnlHandle_t handle,
                      const cnnlSparseDenseMatMulDescriptor_t matmul_desc,
                      const cnnlSparseDenseMatMulAlgo_t algo,
                      const void *alpha,
                      const cnnlSparseTensorDescriptor_t a_desc,
                      const cnnlTensorDescriptor_t b_desc,
                      const void *b,
                      const void *beta,
                      const cnnlTensorDescriptor_t c_desc,
                      void *c,
                      void *workspace,
                      const size_t workspace_size);

// Group:SparseDenseMatMul
/*!
 * @brief Gets the workspace size of the sparse dense tensor multiplication.
 *
 * @deprecated
 * This function is deprecated and will be removed in future release.
 *   Use ::cnnlGetSparseDenseMatMulWorkspaceSize instead.
 *
 * @param[in] handle
 *   Input. The handle to a CNNL context that is used to manage MLU devices and queues
 *   in the tensor multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] matmul_desc
 *   Input. The descriptor of sparse dense matmul operation. For detailed information,
 *   see ::cnnlSparseDenseMatmulDescriptor_t.
 * @param[in] algo
 *   Input. The algorithm for the multiplication of sparse tensor and dense tensor. For
 *   detailed information, see ::cnnlSparseDenseMatmulAlgo_t.
 * @param[in] alpha
 *   Input. The host pointer to a \p compute_type type scalar scaling factor of the
 *   results of sparse tensor and dense tensor.
 * @param[in] a_desc
 *   Input. The descriptor of the left sparse tensor, which contains the sparse
 *   tensor format, dtype, dims and data. For detailed information, see
 *   ::cnnlSparseTensorDescriptor_t. Only support sparse format CNNL_SPARSE_FORMAT_COO.
 * @param[in] b_desc
 *   Input. The descriptor of the right dense tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] beta
 *   Input. The host pointer to a \p compute_type type scalar scaling factor of dense
 *   tensor c.
 * @param[in] c_desc
 *   Input. The descriptor of output dense tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. The pointer to the returned size of the extra workspace in bytes.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be call after ::cnnlCreateSparseTensorDescriptor and
 *   ::cnnlSetSparseTensorDescAttr function for \p a_desc. You also need to call the
 *   ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor for
 *   \p b_desc and \p c_desc before calling this function.
 * - The allocated extra workspace should be passed to the ::cnnlSparseDenseMatmul
 *   function to perform the tensor multiplication.
 *
 * @par note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
CNNL_DEPRECATED_FOR(cnnlGetSparseDenseMatMulWorkspaceSize)
cnnlStatus_t CNNL_WIN_API
cnnlGetSparseDenseMatmulWorkspaceSize(cnnlHandle_t handle,
                                      const cnnlSparseDenseMatmulDescriptor_t matmul_desc,
                                      cnnlSparseDenseMatmulAlgo_t algo,
                                      const void *alpha,
                                      const cnnlSparseTensorDescriptor_t a_desc,
                                      const cnnlTensorDescriptor_t b_desc,
                                      const void *beta,
                                      const cnnlTensorDescriptor_t c_desc,
                                      size_t *workspace_size);
// Group:SparseDenseMatMul
/*!
 * @brief Gets the workspace size of the sparse dense tensor multiplication.
 *
 * @param[in] handle
 *   Input. The handle to a CNNL context that is used to manage MLU devices and queues
 *   in the tensor multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] matmul_desc
 *   Input. The descriptor of sparse dense matmul operation. For detailed information,
 *   see ::cnnlSparseDenseMatMulDescriptor_t.
 * @param[in] algo
 *   Input. The algorithm for the multiplication of sparse tensor and dense tensor. For
 *   detailed information, see ::cnnlSparseDenseMatMulAlgo_t.
 * @param[in] alpha
 *   Input. The host pointer to a \p compute_type type scalar scaling factor of the
 *   results of sparse tensor and dense tensor.
 * @param[in] a_desc
 *   Input. The descriptor of the left sparse tensor, which contains the sparse
 *   tensor format, dtype, dims and data. For detailed information, see
 *   ::cnnlSparseTensorDescriptor_t. Only support sparse format CNNL_SPARSE_FORMAT_COO.
 * @param[in] b_desc
 *   Input. The descriptor of the right dense tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] beta
 *   Input. The host pointer to a \p compute_type type scalar scaling factor of dense
 *   tensor c.
 * @param[in] c_desc
 *   Input. The descriptor of output dense tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. The pointer to the returned size of the extra workspace in bytes.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be call after ::cnnlCreateSparseTensorDescriptor and
 *   ::cnnlSetSparseTensorDescAttr function for \p a_desc. You also need to call the
 *   ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor for
 *   \p b_desc and \p c_desc before calling this function.
 * - The allocated extra workspace should be passed to the ::cnnlSparseDenseMatMul
 *   function to perform the tensor multiplication.
 *
 * @par note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetSparseDenseMatMulWorkspaceSize(cnnlHandle_t handle,
                                      const cnnlSparseDenseMatMulDescriptor_t matmul_desc,
                                      cnnlSparseDenseMatMulAlgo_t algo,
                                      const void *alpha,
                                      const cnnlSparseTensorDescriptor_t a_desc,
                                      const cnnlTensorDescriptor_t b_desc,
                                      const void *beta,
                                      const cnnlTensorDescriptor_t c_desc,
                                      size_t *workspace_size);

// Group:GRUCellForward
/*!
 * @brief Computes the forward process of a GRU cell network in the training scenario.
 *        It uses the input data \p x_gates, \p x_bias, \p h_gates, \p h_bias and \p h
 *        according to the specific network structure and writes the calculation result
 *        into the output memories \p hy and \p storage.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the GRU cell operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] x_gates_desc
 *   Input. The descriptor of \p x_gates tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x_gates
 *   Input. Pointer to the MLU memory that stores input gates data.
 * @param[in] x_bias_desc
 *   Input. The descriptor of \p x_bias tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x_bias
 *   Input. Pointer to the MLU memory that stores bias of input gates data.
 * @param[in] h_gates_desc
 *   Input. The descriptor of \p h_gates tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] h_gates
 *   Input. Pointer to the MLU memory that stores hidden gates data.
 * @param[in] h_bias_desc
 *   Input. The descriptor of \p h_bias tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] h_bias
 *   Input. Pointer to the MLU memory that stores bias of hidden gates data.
 * @param[in] hx_desc
 *   Input. The descriptor of \p h tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] hx
 *   Input. Pointer to the MLU memory that stores input hidden state tensor.
 * @param[in] hy_desc
 *   Input. The descriptor of output hidden state tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] hy
 *   Output. Pointer to the MLU memory that stores output hidden state tensor.
 * @param[in] storage_desc
 *   Input. The descriptor of storage tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] storage
 *   Output. Pointer to the MLU memory that is used as an extra memory space for saving
 *   intermediate results of GRU cell operation. It can be set to NULL.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH,
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Data Type
 * - Data types of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layouts set in descriptors are as follows:
 *   - \p x_gates_desc must be set to ::CNNL_LAYOUT_GNC or ::CNNL_LAYOUT_NGC.
 *   - \p x_bias_desc must be set to ::CNNL_LAYOUT_NC.
 *   - \p h_gates_desc must be set to ::CNNL_LAYOUT_GNC or ::CNNL_LAYOUT_NGC.
 *   - \p h_bias_desc must be set to ::CNNL_LAYOUT_NC.
 *   - \p hx_desc must be set to ::CNNL_LAYOUT_NC.
 *   - \p hy_desc must be be set to ::CNNL_LAYOUT_NC.
 *   - \p storage_desc must be be set to ::CNNL_LAYOUT_GNC or ::CNNL_LAYOUT_NGC.
 *   - The gate layout of gates and bias data must be ::CNNL_GRU_RZN.
 *
 *@par Note
 * - The \p x_bias and \p h_bias can be set to NULL,
 *   which means no bias participates in operations.
 * - The size of the \p x_bias and \p h_bias is [3, C], where 3 means three gates and
 *   C means hidden_size.
 * - The dimemsions of the \p x_gates and \p h_gates are [3, N, C] or [N, 3, C], where 3 means
 *   three gates, N means batches, and C means hidden size.
 * - This operation is not supported on the 1V platforms.
 */

cnnlStatus_t CNNL_WIN_API cnnlGRUCellForward(cnnlHandle_t handle,
                                             const cnnlTensorDescriptor_t x_gates_desc,
                                             const void *x_gates,
                                             const cnnlTensorDescriptor_t x_bias_desc,
                                             const void *x_bias,
                                             const cnnlTensorDescriptor_t h_gates_desc,
                                             const void *h_gates,
                                             const cnnlTensorDescriptor_t h_bias_desc,
                                             const void *h_bias,
                                             const cnnlTensorDescriptor_t hx_desc,
                                             const void *hx,
                                             const cnnlTensorDescriptor_t hy_desc,
                                             void *hy,
                                             const cnnlTensorDescriptor_t storage_desc,
                                             void *storage);

// Group:Inverse
/*!
 * @brief Returns the size of the MLU memory that is used as an extra workspace to
 * optimize the inverse operation.
 *
 * The size of the extra workspace is based on the given information of the inverse operation,
 * including the input tensor descriptor size and its stride information.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   inverse operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. Descriptor of the input tensor, including dimension, data type and data layout.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] output_desc
 *   Input. Descriptor of the output tensor, including dimension, data type and data layout.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] infos_desc
 *   Input. The descriptor of the infos tensor, including dimension, data type and data layout.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *   the inverse operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetInverseWorkspaceSize(cnnlHandle_t handle,
                                                      const cnnlTensorDescriptor_t input_desc,
                                                      const cnnlTensorDescriptor_t output_desc,
                                                      const cnnlTensorDescriptor_t infos_desc,
                                                      size_t *workspace_size);

// Group:Inverse
/*!
 * @brief Computes the inverse of the square matrix \p input. \p input tensor must be a 2D square
 *        matrix, or its batches are arranged in high dimension. This function also returns a
 *        1D tensor \p infos of the same size as the number of input's batch that indicates
 *        the exception of each batch when the \p input is inverted.
 *
 * You can choose to return a row-major or column-major order result by specifying \p is_trans.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   inverse operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the inverse operation. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] is_trans
 *   Input. A Boolean value to decide to return a result by row-major or column-major order.
 *   When \p is_trans is true, the column-major order result will be returned; When \p is_trans is false,
 *   the row-major order result will be returned.
 * @param[in] workspace
 *   Input. Extra space for computation, which is used for big matrix.
 *   It is a pointer to the gdram, which is divided into two parts:
 *   - Workspace for intermediate matrix.
 *   - Workspace for transpose and matmul.
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes. For small matrix, it is 0. For big matrix,
 *   it is equal to the sum of inter_workspace_size and MAX(transpose_workspace_size, matmul_workspace_size).
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] infos_desc
 *   Input. The descriptor of the infos tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] infos
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *   In \p infos, the element values are all integers.
 *   - If the i'th value is 0, the i'th square matrix is invertible.
 *   - If the i'th value is 1, the i'th square matrix is not invertible.
 *   - If the i'th value is -1, the i'th square matrix has an illegal input, i.e. "NaN" or "Inf".
 *
 *   The format of \p infos has to be a 1D tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par Formula
 * - None.
 *
 * @par Data Type
 * - Data type of the input tensor and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: float.
 *   - output tensor: float.
 *   - 1D infos tensor: int32.
 *
 * @par Scale Limitation
 * - Input tensor formats must be greater than one.
 * - The two lowest dimensions of input tensor should be equal, i.e. a square matrix.
 * - The dimension of the square matrix in input tensor should not exceed 8000.
 *
 * @par API Dependency
 * - None.
 *
 * @note
 * - If a matrix is not invertible, there is no guarantee of result correctness. It may simply return a garbage
 *   result in that batch.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the inverse operation is as follows:
     @verbatim
      input tensor x[3, 100, 256, 256], is_trans:true, output tensor y[3, 100, 256, 256], output tensor infos[3*100].
      The input tensor x will be computed the inverse and return to output tensor y through column-major order and get
      a 1D infos tensor indicating the exception of each batch when the input tensor x is inverted.
     @endverbatim
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlInverse_v2(cnnlHandle_t handle,
                                         const cnnlTensorDescriptor_t input_desc,
                                         const void *input,
                                         const bool is_trans,
                                         void *workspace,
                                         const size_t workspace_size,
                                         const cnnlTensorDescriptor_t output_desc,
                                         void *output,
                                         const cnnlTensorDescriptor_t infos_desc,
                                         void *infos);

// Group:GRUCellBackward
/*!
 * @brief Computes the backward process of GRU cell in the training scenario. The specific
 *        network structure is determined to get the gradient of hidden gates in GRU cell.
 *        Based on input data \p grad_output and \p storage, it writes the calculation
 *        result into the output memories \p grad_input, \p grad_hidden and \p grad_hx.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the GRU operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] grad_output_desc
 *   Input. The descriptor of \p grad_output tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] grad_output
 *   Input. Pointer to the MLU memory that stores gradient of output tensor.
 * @param[in] storage_desc
 *   Input. The descriptor of \p storage tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] storage
 *   Input. Pointer to the MLU memory that is used as an extra memory space for saving
 *   intermediate results of GRU Cell Forward operation.
 * @param[in] grad_input_desc
 *   Input. The descriptor of \p grad_input tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] grad_input
 *   Output. Pointer to the MLU memory that stores gradient of input state tensor.
 * @param[in] grad_hidden_desc
 *   Input. The descriptor of \p grad_hidden tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] grad_hidden
 *   Output. Pointer to the MLU memory that stores gradient of hidden state tensor.
 * @param[in] grad_hx_desc
 *   Input. The descriptor of \p grad_hx tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] grad_hx
 *   Output. Pointer to the MLU memory that stores gradient of input hidden state tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - Data types of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.

 * @par Data Layout
 * - The supported data layouts set in descriptors are as follows:
 *   - \p grad_output_desc must be set to ::CNNL_LAYOUT_NC.
 *   - \p grad_storage_desc must be set to ::CNNL_LAYOUT_GNC or ::CNNL_LAYOUT_NGC.
 *   - \p grad_input_desc must be set to ::CNNL_LAYOUT_GNC or ::CNNL_LAYOUT_NGC.
 *   - \p grad_hidden_desc must be set to ::CNNL_LAYOUT_GNC or ::CNNL_LAYOUT_NGC.
 *   - \p grad_hx_desc must be set to ::CNNL_LAYOUT_NC.
 *
 * @par API Dependency
 * - The dimemsions of the \p storage are [5, N, C] or [N, 5, C], where 5 means rg, zg, ng, hx
 *   and hn, N means batches and C means hidden size.
 * - The dimemsions of the \p grad_input are [3, N, C] or [N, 3, C], where 3 means grad_rg,
 *   grad_zg and grad_in, N means batches and C means hidden size.
 * - The dimemsion of the \p grad_hidden are [3, N, C] or [N, 3, C], where 3 means grad_rg,
 *   grad_zg and grad_hn, N means batches and C means hidden size.
 * - This operation is not supported on the 1V platforms.
 */

cnnlStatus_t CNNL_WIN_API cnnlGRUCellBackward(cnnlHandle_t handle,
                                              const cnnlTensorDescriptor_t grad_output_desc,
                                              const void *grad_output,
                                              const cnnlTensorDescriptor_t storage_desc,
                                              const void *storage,
                                              const cnnlTensorDescriptor_t grad_input_desc,
                                              void *grad_input,
                                              const cnnlTensorDescriptor_t grad_hidden_desc,
                                              void *grad_hidden,
                                              const cnnlTensorDescriptor_t grad_hx_desc,
                                              void *grad_hx);

// Group:SoftplusForward_v2
/*!
 * @brief Computes softplus on input tensor \p x, and returns the results in the output
 *        tensor \p y.
 *
 * Softplus Forward is used in activation operation to obtain the nonlinearity for artificial intelligence.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] beta
 *   Input. A host pointer to scaling factor of input tensor \p x.
 *   For detailed information, see
 *   "SoftplusForward Operation" section in "Cambricon CNNL User Guide".
 * @param[in] threshold
 *   Input. A host pointer that is used to compare with the product of \p x and \p beta,
 *   then to decide the return value \p y.
 *   For detailed information, see
 *   "SoftplusForward Operation" section in "Cambricon CNNL User Guide".
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @par Formula
 * - See "SoftplusForward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input and output tensors must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float, bfloat16.
 *   - output tensor: half, float, bfloat16.
 *
 *   bfloat16 is only supported on MLU500 series.
 *
 * - \p beta and \p threshold:
     - If data type of input tensors is float, half or bfloat16, the data
 *   type of \p beta and \p threshold should be a pointer to float.
 *
 *   Currently, the data
 *   type of \p beta and \p threshold can only be a pointer to float.
 *
 * @par Scale Limitation
 * - The value of \p beta cannot be equal to 0.
 *
 * @note
 * - To achieve high-accuracy output data, the product of \p x and \p beta is
 *   recommended to be in [-7.75, 7.75] for all data types.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the softplus forward operation is as follows:
     @verbatim
      input one array by 2 * 3
      --> input: [[0.0, 3.4, 30],[-4.3, -1.2, -0.5]]

      param:
        beta: 1.0
        threshold: 20.0

      output array by 2 * 3
      --> output: [[0.69194, 3.43291, 30],[0.01348, 0.26330, 0.47297]]
     @endverbatim
 */
cnnlStatus_t CNNL_WIN_API cnnlSoftplusForward_v2(cnnlHandle_t handle,
                                                 const void *beta,
                                                 const void *threshold,
                                                 const cnnlTensorDescriptor_t x_desc,
                                                 const void *x,
                                                 const cnnlTensorDescriptor_t y_desc,
                                                 void *y);

// Group:SoftplusBackward_v2
/*!
 * @brief Computes the gradient of an operation of softplus backward.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in this operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] diff_y_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] diff_y
 *   Input. Pointer to the MLU memory that stores the input tensor that is a gradient.
 * @param[in] diff_x_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] beta
 *   Input. A host pointer to scaling factor of input tensor \p x.
 * @param[in] threshold
 *   Input. A host pointer that is used to compare with the product of \p x and \p beta,
 *   then to decide the return value \p diff_x.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @par Formula
 * - See "SoftplusBackward operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Example
 * - The example of the softplus backward operation is as follows:
     @verbatim
      x: [4.0, 1.0, 3.0]
      diff_y: [4.0, 1.0, 3.0]
      diff_x: [4.0, 1.0, 3.0]
     @endverbatim
 *
 * @par Data Type
 * - Data type of input and output tensors should be the same.
 *   The supported data types of input and output tensors are as follows:
 *   - input tensors: float, half, bfloat16.
 *   - output tensors: float, half, bfloat16.
 *
 *   bfloat16 is only supported on MLU500 series.
 *
 * - \p beta and \p threshold:
     - If data type of input tensors is float, half or bfloat16, the data
 *   type of \p beta and \p threshold should be a pointer to float.
 *
 *   Currently, the data
 *   type of \p beta and \p threshold can only be a pointer to float.
 *
 * - Softplus backward operation is an element-wise operation. The dimensions of \p x, \p diff_y
 *   and \p diff_x must be the same.
 * - The number of dimensions is not greater than \p CNNL_DIM_MAX.
 *
 * @note
 * - The product of \p x and \p beta should be in range of [-7.75, 7.75] to guarantee the accuracy of output.
 *
 * @par Requirements
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/master/generated/torch.nn.Softplus.html#torch.nn.Softplus.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlSoftplusBackward_v2(cnnlHandle_t handle,
                                                  const void *beta,
                                                  const void *threshold,
                                                  const cnnlTensorDescriptor_t x_desc,
                                                  const void *x,
                                                  const cnnlTensorDescriptor_t diff_y_desc,
                                                  const void *diff_y,
                                                  const cnnlTensorDescriptor_t diff_x_desc,
                                                  void *diff_x);

/****************************************************************
 * Cambricon CNNL Data Structure: GroupGemm
 ****************************************************************/
/*!
 * @brief Enumeration variables describing the modes of group gemm tensor.
 */
typedef enum {
  CNNL_GROUP_GEMM_ADDRESSING_DOUBLE_POINTER = 0,
  /*!< Tensor group is stored in discrete memory space, where the addresses of every tensor are
       stored in an array of pointers. */
  CNNL_GROUP_GEMM_ADDRESSING_GATHER_IDX     = 1,
  /*!< Tensor group is stored in continuous memory space, where each row of a tensor will be looked up
       in the memory space by index. */
  CNNL_GROUP_GEMM_ADDRESSING_OFFSETS        = 2,
  /*!< Tensor group is stored in discrete memory space, where
       each tensor is located at the address offsets from the location of the first tensor. The offsets
       can be nullptr, which meanings that the group tensor is stored continously. */
  CNNL_GROUP_GEMM_ADDRESSING_3D_TENSOR      = 3,
  /*!< Tensor group is stored in continuous memory space, where each tensor has a stride (in elements) to
       next tensor for group gemm operation. */
} cnnlGroupGemmTensorMode_t;

/*! The descriptor of the group gemm function that holds compute type, max of m, max of n, max of k,
 *  scales and other attributes defined in ::cnnlMatMulDescAttribute_t.
 *
 *  You need to call the ::cnnlCreateGroupGemmDescriptor function to create a descriptor, and call the
 *  ::cnnlSetGroupGemmDescAttr function to set the information of the group gemm to the descriptor.
 *  Also, you need to destroy the Cambricon CNNL context at the end with the
 *  ::cnnlDestroyGroupGemmDescriptor function.
 */
typedef struct cnnlGroupGemmStruct *cnnlGroupGemmDescriptor_t;

/*! The descriptor of the group gemm computation algorithm.
 *
 *  Currently not used.
 *  You need to call the ::cnnlCreateGroupGemmAlgo function to create a descriptor.
 *  Also, you need to destroy the Cambricon CNNL context at the end with the ::cnnlDestroyGroupGemmAlgo function.
 */
typedef struct cnnlGroupGemmAlgoStruct *cnnlGroupGemmAlgo_t;

/*! The descriptor of the group gemm that holds the configured group gemm
 *  algorithm descriptor and its runtime properties.
 *
 *  You need to call the ::cnnlCreateGroupGemmHeuristicResult function to create a descriptor.
 *  Also, you need to destroy the Cambricon CNNL context at the end with the ::cnnlDestroyGroupGemmHeuristicResult function.
 */
typedef struct cnnlGroupGemmHeuristicResult *cnnlGroupGemmHeuristicResult_t;

/*! The descriptor of the group gemm tensor that holds the information including the mode, data type, data pointer
 *  and data pointer mode.
 *
 *  You need to call the ::cnnlCreateGroupGemmTensorDescriptor function to create a
 *  desriptor. Also, you need to destroy the Cambricon CNNL context at the
 *  end with the ::cnnlDestroyGroupGemmTensorDescriptor function.
 */
typedef struct cnnlGroupGemmTensorStruct *cnnlGroupGemmTensorDescriptor_t;

/*! The descriptor of the group gemm that holds the preferences for ::cnnlGroupGemmHeuristicResult_t
 *  configuration.
 *
 *  Currently not used.
 */
typedef struct cnnlGroupGemmPrefer *cnnlGroupGemmPrefer_t;

// Group:GroupGemm
/*!
 * @brief Computes the matrix multiplication operation for group of matrices a[i], b[i] and c[i] in
 *    group gemm tensor descriptors \p a_desc, \p b_desc, and \p c_desc, and then returns the results to the output
 *    group of matrices d[i] in group gemm tensor descriptor \p d_desc.
 *
 *  GroupGemm computes the matrix d[i] = OP(a[i]) @ OP(b[i]) * alpha[i] + c[i] * beta[i], where
 *  @ means matrix multiplication. (OP means whether to transpose based on the value is_trans_a or is_trans_b.)
 *
 *  When beta[i] is 0.0, the add operation is skipped, and the formula is d[i] = OP(a[i]) @ OP(b[i]) * alpha[i].
 *
 *  This function needs extra MLU memory as the workspace to improve the group gemm operation.
 *  You can get the workspace size with ::cnnlGetGroupGemmWorkspaceSize function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon MLU context that is used to manage MLU devices and queues in the
 *   group matrix multiplication. For detailed information, see ::cnnlHandle_t.
 * @param[in] gemm_desc
 *   Input. The descriptor of the group gemm operation. For detailed information, see ::cnnlGroupGemmDescriptor_t.
 * @param[in] algo
 *   Input. A host pointer to the most suitable algorithm to compute the group gemm. For detailed information,
 *   see ::cnnlGroupGemmAlgo_t. Currently it is not supported and should be set to NULL.
 * @param[in] groups
 *   Input. The group number of group gemm.
 * @param[in] is_trans_a
 *   Input. The flag indicating that whether the group of matrices a[i] is transposed. The flag is supported
 *   only when the mode of \p a_desc is not CNNL_GROUP_GEMM_ADDRESSING_GATHER_IDX.
 * @param[in] is_trans_b
 *   Input. The flag indicating that whether the group of matrices b[i] is transposed.
 * @param[in] m_desc
 *   Input. The tensor descriptor of rows of OP(a[i]) and c[i].
 *   The pointer mode enum value in the \p m_desc indicates that the data pointer \p m_array is
 *   passed by reference on the host or device. You can call the
 *   ::cnnlSetTensorDescriptorPointerMode function to set the pointer mode.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] m_array
 *   Input. A device or host pointer to the rows of OP(a[i]) and c[i] for each group.
 * @param[in] n_desc
 *   Input. The tensor descriptor of columns of OP(b[i]) and c[i].
 *   The pointer mode enum value in the \p n_desc indicates that the data pointer \p n_array is
 *   passed by reference on the host or device. You can call the
 *   ::cnnlSetTensorDescriptorPointerMode function to set the pointer mode.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] n_array
 *   Input. The device or host pointer to the columns of OP(b[i]) and c[i] for each group.
 * @param[in] k_desc
 *   Input. A tensor descriptor of columns of OP(a[i]) and rows of OP(b[i]).
 *   The pointer mode enum value in the \p k_desc indicates that the data pointer \p k_array is
 *   passed by reference on the host or device. You can call the
 *   ::cnnlSetTensorDescriptorPointerMode function to set the pointer mode.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] k_array
 *   Input. A device or host pointer to the columns of OP(a[i]) and rows of OP(b[i]) for each group.
 *   \p k_array cannot contain zero when data type of \p a_desc or \p b_desc is CNNL_DTYPE_INT8.
 * @param[in] alpha_desc
 *   Input. The tensor descriptor of the scaling factor of OP(a[i]) @ OP(b[i]).
 *   The pointer mode enum value in the \p alpha_desc indicates that the data pointer \p alpha_array is
 *   passed by reference on the host or device. You can call the
 *   ::cnnlSetTensorDescriptorPointerMode function to set the pointer mode.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 *   If \p alpha_desc is nullptr, you should set \p alpha_array to nullptr too, which means alpha_array[i] is 1.0.
 * @param[in] alpha_array
 *   Input. A device or host pointer to the scaling factor of OP(a[i]) * OP(b[i]) for each group.
 *   The data type of alpha array corresponds to the computeType in ::cnnlGroupGemmDescriptor_t.
 *   If \p alpha_array is nullptr, you should set \p alpha_desc to nullptr too, which means alpha_array[i] is 1.0.
 *   \p alpha_array cannot contain zero when data type of \p a_desc or \p b_desc is CNNL_DTYPE_INT8.
 * @param[in] lda_desc
 *   Input. The tensor descriptor of the leading dimension of matrix a[i].
 *   The pointer mode enum value in the \p lda_desc indicates that the data pointer \p lda_array is
 *   passed by reference on the host or device. You can call the
 *   ::cnnlSetTensorDescriptorPointerMode function to set the pointer mode.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] lda_array
 *   Input. A device or host pointer to the leading dimension of matrix a[i] for each group.
 * @param[in] a_desc
 *   Input. The group gemm tensor descriptor of matrix a[i]. For detailed information, see ::cnnlGroupGemmTensorDescriptor_t.
 *   The element number of tensor a[i] should be \p lda_array [i] * \p m_array [i] with \p lda_array [i] >= max(1, \p m_array [i])
 *   if \p is_trans_a == true, and \p lda_array [i] * \p k_array [i] with \p lda_array [i] >= max(1, \p k_array[i]) otherwise.
 *   The mode of \p a_desc supports CNNL_GROUP_GEMM_ADDRESSING_OFFSETS, CNNL_GROUP_GEMM_ADDRESSING_GATHER_IDX
 *   and CNNL_GROUP_GEMM_ADDRESSING_DOUBLE_POINTER. For CNNL_GROUP_GEMM_ADDRESSING_OFFSETS and CNNL_GROUP_GEMM_ADDRESSING_GATHER_IDX
 *   modes, the \p lda_array should be the same and in the host for each group.
 * @param[in] ldb_desc
 *   Input. The tensor descriptor of the leading dimension of matrix b[i].
 *   The pointer mode enum value in the \p ldb_desc indicates that the data pointer \p ldb_array is
 *   passed by reference on the host or device. You can call the
 *   ::cnnlSetTensorDescriptorPointerMode function to set the pointer mode.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] ldb_array
 *   Input. A device or host pointer to the leading dimension of matrix b[i] for each group.
 * @param[in] b_desc
 *   Input. The group gemm tensor descriptor of matrix b[i]. For detailed information, see ::cnnlGroupGemmTensorDescriptor_t.
 *   The element number of tensor b[i] should be \p ldb_array [i] * \p k_array [i] with \p ldb_array [i] >= max(1, \p k_array [i])
 *   if \p is_trans_b == true, and \p ldb_array [i] * \p n_array [i] with \p ldb_array [i] >= max(1, \p n_array[i]) otherwise.
 *   The mode of \p b_desc supports CNNL_GROUP_GEMM_ADDRESSING_OFFSETS and CNNL_GROUP_GEMM_ADDRESSING_DOUBLE_POINTER.
 *   For CNNL_GROUP_GEMM_ADDRESSING_OFFSETS mode, the \p ldb_array should be the same and in the host for each group.
 * @param[in] beta_desc
 *   Input. The descriptor of the scaling factor of tensor matrix c[i].
 *   The pointer mode enum value in the \p beta_desc indicates that the data pointer \p beta_array is
 *   passed by reference on the host or device. You can call the
 *   ::cnnlSetTensorDescriptorPointerMode function to set the pointer mode.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 *   If \p beta_desc is nullptr, you should set \p beta_array to nullptr too, which means beta_array[i] is 0.0.
 * @param[in] beta_array
 *   Input. A device or host pointer to the scaling factor of matrix c[i] for each group, whose data type
 *   is corresponds to the computeType in ::cnnlGroupGemmDescriptor_t.
 * @param[in] ldc_desc
 *   Input. The descriptor of the leading dimension of matrix c[i].
 *   The pointer mode enum value in the \p ldc_desc indicates that the data pointer \p ldc_array is
 *   passed by reference on the host or device. You can call the
 *   ::cnnlSetTensorDescriptorPointerMode function to set the pointer mode.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 *   When the \p beta_array is all zero, you can set \p ldc_desc to nullptr.
 * @param[in] ldc_array
 *   Input. A device or host pointer to the leading dimension of matrix c[i] for each group.
 *   When the \p beta_array is all zero, you can set \p ldc_array to nullptr.
 * @param[in] c_desc
 *   Input. The group gemm tensor descriptor of matrix c[i]. For detailed information, see ::cnnlGroupGemmTensorDescriptor_t.
 *   The element number of tensor c[i] should be \p ldc_array [i] * \p n_array [i] with \p ldc_array [i] >= max(1, \p n_array [i]).
 *   When the \p beta_array is all zero, you can set \p c_desc to nullptr.
 *   The mode of \p c_desc supports CNNL_GROUP_GEMM_ADDRESSING_OFFSETS and CNNL_GROUP_GEMM_ADDRESSING_DOUBLE_POINTER.
 *   For CNNL_GROUP_GEMM_ADDRESSING_OFFSETS mode, the \p ldc_array should be the same and in the host for each group.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the group gemm operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the group gemm operation.
 *   You can get the size of the workspace with the ::cnnlGetGroupGemmWorkspaceSize function.
 * @param[in] ldd_desc
 *   Input. The descriptor of the leading dimension of matrix d[i].
 *   The pointer mode enum value in the \p ldd_desc indicates that the data pointer \p ldd_array is
 *   passed by reference on the host or device. You can call the
 *   ::cnnlSetTensorDescriptorPointerMode function to set the pointer mode.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] ldd_array
 *   Input. A device or host pointer to the leading dimension of matrix d[i] for each group.
 * @param[in,out] d_desc
 *   Input/output. The group gemm tensor descriptor of matrix d[i]. For detailed information, see ::cnnlGroupGemmTensorDescriptor_t.
 *   The element number of tensor d[i] should be \p ldd_array [i] * \p n_array [i] with \p ldd_array [i] >= max(1, \p n_array [i]).
 *   The mode of \p d_desc supports CNNL_GROUP_GEMM_ADDRESSING_OFFSETS and CNNL_GROUP_GEMM_ADDRESSING_DOUBLE_POINTER.
 *   For CNNL_GROUP_GEMM_ADDRESSING_OFFSETS mode, the \p ldd_array should be the same and in the host for each group.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - The supported combinations of data types are as follows in the order of
 *   \p a_desc data type - \p b_desc data type - \p c_desc data type - \p d_desc data_type:
 *   - bfloat16 - bfloat16 - bfloat16 - bfloat16
 *   - half - half - half - half
 *   - int8 - int8 - bfloat16 - bfloat16
 *   - int8 - int8 - half - half

 *   The bfloat16 data type is supported only on MLU500 series.

 * - The data types of \p m_desc, \p n_desc, \p k_desc, \p lda_desc, \p ldb_desc, \p ldc_desc and \p ldd_desc
 *   must be the same and only support int32.
 * - The data types of \p alpha_desc and \p beta_desc are the same as those of compute type. Currently only support float32.
 *
 * @note
 * - Error proofing is enabled only when the pointer mode of \p m_desc, \p n_desc, \p k_desc, \p lda_desc, \p ldb_desc,
 *   \p ldc_desc, \p ldd_desc, \p alpha_desc and \p beta_desc is CNNL_POINTER_MODE_HOST.
 *
 * @par API Dependency
 * - Before calling this function to implement group gemm operation, you need to prepare all the parameters that
 *   need to be passed to this function. See each parameter description for details. You can set the scale, bias and activation
 *   to fuse by calling ::cnnlSetGroupGemmPerRowColScaleBiasAct. Scale is supported only when data type of \p a_desc and \p b_desc
 *   is CNNL_DTYPE_INT8. Bias is supported only when data type of bias keeps the same as \p d_desc.
 *   Activation is currently not supported and should be set to nullptr.
 * - Each set of input parameters corresponds to a unique \p workspace. If \p gemm_desc, \p groups, \p is_trans_a,
 *   \p is_trans_b, or data types are changed, the associated function ::cnnlGetGroupGemmWorkspaceSize need to be
 *   re-called.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, the data pointers of \p a_desc, \p b_desc, \p c_desc and \p d_desc
 *   should be passed by reference on the device. \p gemm_desc attributes ::CNNL_MATMUL_DESC_GROUP_M_MAX,
 *   ::CNNL_MATMUL_DESC_GROUP_N_MAX and ::CNNL_MATMUL_DESC_GROUP_K_MAX should be set to the real value as the
 *   \p m_array, \p n_array and \p k_array. For example, in MoE (Mixed of Experts) forward, ::CNNL_MATMUL_DESC_GROUP_M_MAX
 *   should be set to the maximum of tokens that one group (expert) can process.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor must meet the following requirements:
 *   - Large tensor is only supported on MLU500 series.
 *
 * @note
 * - The \p m_array only supports \p CNNL_POINTER_MODE_DEVICE for quantized Group Gemm, where the data type of matrix \p A
 *   or \p B is \p CNNL_DTYPE_INT8 or \p CNNL_DTYPE_INT4X2.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlGroupGemm(const cnnlHandle_t handle,
                                        const cnnlGroupGemmDescriptor_t gemm_desc,
                                        const cnnlGroupGemmAlgo_t algo,
                                        const int32_t groups,
                                        const bool is_trans_a,
                                        const bool is_trans_b,
                                        const cnnlTensorDescriptor_t m_desc,
                                        const void *m_array,
                                        const cnnlTensorDescriptor_t n_desc,
                                        const void *n_array,
                                        const cnnlTensorDescriptor_t k_desc,
                                        const void *k_array,
                                        const cnnlTensorDescriptor_t alpha_desc,
                                        const void *alpha_array,
                                        const cnnlTensorDescriptor_t lda_desc,
                                        const void *lda_array,
                                        const cnnlGroupGemmTensorDescriptor_t a_desc,
                                        const cnnlTensorDescriptor_t ldb_desc,
                                        const void *ldb_array,
                                        const cnnlGroupGemmTensorDescriptor_t b_desc,
                                        const cnnlTensorDescriptor_t beta_desc,
                                        const void *beta_array,
                                        const cnnlTensorDescriptor_t ldc_desc,
                                        const void *ldc_array,
                                        const cnnlGroupGemmTensorDescriptor_t c_desc,
                                        void *workspace,
                                        const size_t workspace_size,
                                        const cnnlTensorDescriptor_t ldd_desc,
                                        const void *ldd_array,
                                        const cnnlGroupGemmTensorDescriptor_t d_desc);

// Group:GroupGemm
/*!
 * @brief Gets the workspace size of group gemm operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] gemm_desc
 *   Input. The descriptor of the group gemm operation. For detailed information, see ::cnnlGroupGemmDescriptor_t.
 * @param[in] algo
 *   Input. A host pointer to the most suitable algorithm to compute the group gemm. For detailed information,
 *   see ::cnnlGroupGemmAlgo_t. Currently it is not supported and should be set to NULL.
 * @param[in] groups
 *   Input. The group number of group gemm.
 * @param[in] is_trans_a
 *   Input. The flag indicating that whether the group of matrices a[i] is transposed.
 * @param[in] is_trans_b
 *   Input. The flag indicating that whether the group of matrices b[i] is transposed.
 * @param[in] m_desc
 *   Input. The tensor descriptor of rows of matrix OP(a[i]) and c[i] for each group.
 *   The pointer mode enum value in the \p m_desc indicates that the data pointer is
 *   passed by reference on the host or device. You can call the
 *   ::cnnlSetTensorDescriptorPointerMode function to set the pointer mode.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] n_desc
 *   Input. The tensor descriptor of columns of matrix OP(b[i]) and c[i] for each group.
 *   The pointer mode enum value in the \p n_desc indicates that the data pointer is
 *   passed by reference on the host or device. You can call the
 *   ::cnnlSetTensorDescriptorPointerMode function to set the pointer mode.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] k_desc
 *   Input. A tensor descriptor of columns of OP(a[i]) and rows of OP(b[i]) for each group.
 *   The pointer mode enum value in the \p k_desc indicates that the data pointer is
 *   passed by reference on the host or device. You can call the
 *   ::cnnlSetTensorDescriptorPointerMode function to set the pointer mode.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] alpha_desc
 *   Input. The tensor descriptor of the scaling factor of OP(a[i]) @ OP(b[i]).
 *   The pointer mode enum value in the \p alpha_desc indicates that the data pointer is
 *   passed by reference on the host or device. You can call the
 *   ::cnnlSetTensorDescriptorPointerMode function to set the pointer mode.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 *   If \p alpha_desc is nullptr, you should set \p alpha_array to nullptr too.
 * @param[in] lda_desc
 *   Input. The tensor descriptor of the leading dimension of matrix a[i].
 *   The pointer mode enum value in the \p lda_desc indicates that the data pointer is
 *   passed by reference on the host or device. You can call the
 *   ::cnnlSetTensorDescriptorPointerMode function to set the pointer mode.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] a_desc
 *   Input. The group gemm tensor descriptor of matrix a[i]. For detailed information, see
 *   ::cnnlGroupGemmTensorDescriptor_t.
 * @param[in] ldb_desc
 *   Input. The tensor descriptor of the leading dimension of matrix b[i].
 *   The pointer mode enum value in the \p ldb_desc indicates that the data pointer is
 *   passed by reference on the host or device. You can call the
 *   ::cnnlSetTensorDescriptorPointerMode function to set the pointer mode.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The group gemm tensor descriptor of matrix b[i]. For detailed information, see
 *   ::cnnlGroupGemmTensorDescriptor_t.
 * @param[in] beta_desc
 *   Input. The descriptor of the scaling factor of tensor matrix c[i].
 *   The pointer mode enum value in the \p beta_desc indicates that the data pointer is
 *   passed by reference on the host or device. You can call the
 *   ::cnnlSetTensorDescriptorPointerMode function to set the pointer mode.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] ldc_desc
 *   Input. The descriptor of the leading dimension of matrix c[i].
 *   The pointer mode enum value in the \p ldc_desc indicates that the data pointer is
 *   passed by reference on the host or device. You can call the
 *   ::cnnlSetTensorDescriptorPointerMode function to set the pointer mode.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The group gemm tensor descriptor of matrix c[i]. For detailed information, see
 *   ::cnnlGroupGemmTensorDescriptor_t.
 * @param[in] ldd_desc
 *   Input. The descriptor of the leading dimension of matrix d[i].
 *   The pointer mode enum value in the \p ldd_desc indicates that the data pointer is
 *   passed by reference on the host or device. You can call the
 *   ::cnnlSetTensorDescriptorPointerMode function to set the pointer mode.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] d_desc
 *   Input. The group gemm tensor descriptor of matrix d[i]. For detailed information, see
 *   ::cnnlGroupGemmTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. The host pointer to the returned size of the extra workspace in bytes that is used in
 *   the group gemm operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function before calling the ::cnnlGroupGemm function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetGroupGemmWorkspaceSize(const cnnlHandle_t handle,
                              const cnnlGroupGemmDescriptor_t gemm_desc,
                              const cnnlGroupGemmAlgo_t algo,
                              const int32_t groups,
                              const bool is_trans_a,
                              const bool is_trans_b,
                              const cnnlTensorDescriptor_t m_desc,
                              const cnnlTensorDescriptor_t n_desc,
                              const cnnlTensorDescriptor_t k_desc,
                              const cnnlTensorDescriptor_t alpha_desc,
                              const cnnlTensorDescriptor_t lda_desc,
                              const cnnlGroupGemmTensorDescriptor_t a_desc,
                              const cnnlTensorDescriptor_t ldb_desc,
                              const cnnlGroupGemmTensorDescriptor_t b_desc,
                              const cnnlTensorDescriptor_t beta_desc,
                              const cnnlTensorDescriptor_t ldc_desc,
                              const cnnlGroupGemmTensorDescriptor_t c_desc,
                              const cnnlTensorDescriptor_t ldd_desc,
                              const cnnlGroupGemmTensorDescriptor_t d_desc,
                              size_t *workspace_size);

// Group:GroupGemm
/*!
 * @brief Creates a descriptor pointed by \p gmm_desc for group gemm operation,
 *        and allocates memory for holding the information about the group gemm operation.
 *
 * @param[out] desc
 *   Output. A host pointer to the group gemm descriptor that holds information about the group gemm operation.
 *   For detailed information, see ::cnnlGroupGemmDescriptor_t.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetGroupGemmDescAttr function to initialize
 *   the group gemm descriptor and set its information.
 * - You need to call the ::cnnlDestroyGroupGemmDescriptor function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateGroupGemmDescriptor(cnnlGroupGemmDescriptor_t *desc);

// Group:GroupGemm
/*!
 * @brief Initializes the group gemm descriptor \p gmm_desc
 * that was previously created with the ::cnnlCreateGroupGemmDescriptor function, and sets
 * the information about the group gemm operation to the group gemm
 * descriptor \p gmm_desc. The information includes the attribute \p attr defined in
 * ::cnnlMatMulDescAttribute_t, the host pointer \p buf to the attribute value, and
 * the size of buffer for verification.
 *
 * @param[in,out] desc
 *   Input/output. The descriptor of the group gemm operation. For detailed
 *   information, see ::cnnlGroupGemmDescriptor_t.
 * @param[in] attr
 *   Input. Attribute of group gemm descriptor to be set. For detailed
 *   information, see ::cnnlMatMulDescAttribute_t.
 * @param[in] buf
 *   Input. A host pointer to the attribute value set by this function.
 * @param[in] size_in_bytes
 *   Input. Buffer in bytes for verification.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetGroupGemmDescAttr(cnnlGroupGemmDescriptor_t desc,
                                                   const cnnlMatMulDescAttribute_t attr,
                                                   const void *buf,
                                                   const size_t size_in_bytes);

// Group:GroupGemm
/*!
 * @brief Returns the pointer to the \p buf and size of the buffer \p size_written of the attribute
 * retrieved with the given group gemm descriptor \p gmm_desc and attribute \p attr.
 *
 * You can set the attribute in the group gemm descriptor based on the return value
 * of this function.
 *
 * @param[in] desc
 *   Input. The descriptor of the group gemm operation. For detailed
 *   information, see ::cnnlGroupGemmDescriptor_t.
 * @param[in] attr
 *   Input. Attribute of group gemm descriptor to be retrieved. For detailed information,
 *   see ::cnnlMatMulDescAttribute_t.
 * @param[out] buf
 *   Output. A host pointer to the attribute value to be retrieved by this function.
 * @param[in] size_in_bytes
 *   Input. Buffer in bytes for verification, which is used to check whether the memory size is the same as \p size_written.
 * @param[out] size_written
 *   Output. A host pointer to the number of bytes actually written to the buffer.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetGroupGemmDescAttr(const cnnlGroupGemmDescriptor_t desc,
                                                   cnnlMatMulDescAttribute_t attr,
                                                   void *buf,
                                                   size_t size_in_bytes,
                                                   size_t *size_written);

// Group:GroupGemm
/*!
 * @brief Destroys the group gemm descriptor \p gmm_desc
 *        that was previously created with ::cnnlCreateGroupGemmDescriptor.
 *
 * @param[in] gmm_desc
 *   Input. The group gemm descriptor to be destroyed. For detailed information,
 *   see ::cnnlGroupGemmDescriptor_t.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyGroupGemmDescriptor(cnnlGroupGemmDescriptor_t gmm_desc);

// Group:GroupGemm
/*!
 * @brief Creates a descriptor pointed by \p algo for a group gemm algorithm,
 *        and allocates memory for holding the information about the algorithm.
 *        The information is defined in ::cnnlGroupGemmAlgo_t.
 *
 *
 * @param[out] algo
 *   Output. A host pointer to the group gemm algorithm that holds information about the matrix
 *   group gemm.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - Currently not used.
 * - You need to call the ::cnnlDestroyGroupGemmAlgo function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateGroupGemmAlgo(cnnlGroupGemmAlgo_t *algo);

// Group:GroupGemm
/*!
 * @brief Destroys the group gemm algorithm descriptor \p algo
 *        that was previously created with ::cnnlCreateGroupGemmAlgo.
 *
 * The group gemm descriptor is defined in ::cnnlGroupGemmAlgo_t
 * and holds the information about the group gemm algorithm.
 *
 * @param[in] algo
 *   Input. The group gemm algorithm descriptor to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Currently not used.
 * - You need to call this function after calling the ::cnnlGroupGemm function.
 * - This function should be called to destroy the group gemm algo descriptor.
 *   Otherwise, the memory leak may be occurred.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyGroupGemmAlgo(cnnlGroupGemmAlgo_t algo);

// Group:GroupGemm
/*!
 * @brief Creates a descriptor pointed by \p result for a group gemm heuristic result,
 *        and allocates memory for the result.
 *
 * @param[out] result
 *   Output. A host pointer to the struct of group gemm heuristic result. For detailed information,
 *   see ::cnnlGroupGemmHeuristicResult_t.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - Currently not used.
 * - You need to call the ::cnnlDestroyGroupGemmHeuristicResult function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateGroupGemmHeuristicResult(
    cnnlGroupGemmHeuristicResult_t *result);

// Group:GroupGemm
/*!
 * @brief Destroys the group gemm heuristic result that was previously created with
 *        ::cnnlCreateGroupGemmHeuristicResult.
 *
 * @param[in] result
 *   Input. The group gemm heuristic result to be destroyed. For detailed information,
 *   see ::cnnlGroupGemmHeuristicResult_t.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlGroupGemm function.
 * - This function should be called to destroy the group gemm algo descriptor.
 *   Otherwise, the memory leak may be occurred.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyGroupGemmHeuristicResult(
    cnnlGroupGemmHeuristicResult_t result);

// Group:GroupGemm
/*!
 * @brief Creates a group gemm tensor descriptor pointed by \p desc that holds the mode of the input group gemm tensor.
 *
 * To destroy the group gemm tensor descriptor later, call the ::cnnlDestroyGroupGemmTensorDescriptor function.
 *
 * @param[out] desc
 *   Output. The pointer to the struct that holds information about the group gemm descriptor. For detailed information,
 *   information, see ::cnnlGroupGemmTensorDescriptor_t.
 * @param[in] mode
 *   Input. The mode of group gemm tensor. Currently, it supports only ::CNNL_GROUP_GEMM_ADDRESSING_DOUBLE_POINTER mode.
 *   For detailed information, see ::cnnlGroupGemmTensorMode_t.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - Currently not used.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCreateGroupGemmTensorDescriptor(cnnlGroupGemmTensorDescriptor_t *desc,
                                    cnnlGroupGemmTensorMode_t mode);

// Group:GroupGemm
/*!
*  @brief Initilizes the group tensor descriptor pointed by \p desc that was previously created
 *  with the ::cnnlCreateGroupGemmTensorDescriptor function, and sets the information about
 *  the data type, pointer, pointer mode and other information of the input group gemm tensor.
 * @param[in] desc
 *   Output. The struct that holds information about the group gemm descriptor. For detailed informatin,
 *   see ::cnnlGroupGemmTensorDescriptor_t.
 * @param[in] dtype
 *   Input. The data type of group gemm tensor. For detailed information, see ::cnnlDataType_t.
 * @param[in] pointer
 *   Input. A pointer of group gemm tensor. For ::CNNL_GROUP_GEMM_ADDRESSING_DOUBLE_POINTER mode,
 *   the pointer is a double pointer.
 * @param[in] pointer_mode
 *   Input. An enum value indicating that the data pointer \p pointer is
 *   passed by reference on the host or device. The information is defined in ::cnnlPointerMode_t.
 * @param[in] infos
 *   Input. A device information pointer of group gemm tensor.
 *   - For ::CNNL_GROUP_GEMM_ADDRESSING_DOUBLE_POINTER mode, \p infos is nullptr.
 *   - For ::CNNL_GROUP_GEMM_ADDRESSING_OFFSETS mode, \p infos is the offsets to the \p pointer in bytes.
 *
 *   You can set \p infos to nullptr when group tensor is stored continuously.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - This API cannot set the mode of the group gemm tensor descriptor. To change the mode, destroy the original
 *   mode of the group gemm tensor and create a new one of your choice.
 * - Mode ::CNNL_GROUP_GEMM_ADDRESSING_GATHER_IDX is supported only on MLU500 series.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetGroupGemmTensorDescriptor(cnnlGroupGemmTensorDescriptor_t desc,
                                 cnnlDataType_t dtype,
                                 void *pointer,
                                 cnnlPointerMode_t pointer_mode,
                                 int64_t *infos);

// Group:GroupGemm
/*!
*  @brief Initilizes the group tensor descriptor pointed by \p desc that was previously created
 *  with the ::cnnlCreateGroupGemmTensorDescriptor function, and sets the information about
 *  the data type, index data type, src data pointer, index pointer, index number, src data stride and
 *  gather data size of the input group gemm tensor in gather idx mode.
 * @param[in] desc
 *   Output. The struct that holds information about the group gemm descriptor. For detailed informatin,
 *   see ::cnnlGroupGemmTensorDescriptor_t.
 * @param[in] data_dtype
 *   Input. The data type of group gemm tensor. For detailed information, see ::cnnlDataType_t.
 * @param[in] index_dtype
 *   Input. The data type of indices.  It supports CNNL_DTYPE_INT32 and CNNL_DTYPE_INT64.
 *   For detailed information, see ::cnnlDataType_t.
 * @param[in] src_ptr
 *   Input. Pointer to the MLU memory that stores the data to be gathered.
 * @param[in] indices_ptr
 *   Input. Pointer to the MLU memory that stores the index of each element of \p src_ptr.
 * @param[in] gather_size
 *   Input. The number of data to gather in one index with \p gather_size <= \p src_stride.
 * @param[in] src_stride
 *   Input. The value of type long long int that gives the offset between two adjacent indices of data
 *   \p src_ptr.
 * @param[in] index_num
 *   Input. The elements number of indices. It should be set to the exact elements number.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This API is special for ::CNNL_GROUP_GEMM_ADDRESSING_GATHER_IDX mode to set \p gather_size, \p src_stride
 *   and \p index_num.
 *
 * @note
 * - For better performance, you should set the \p idx_num and use ::CNNL_DTYPE_INT64 \p idx_type.
 * - Mode ::CNNL_GROUP_GEMM_ADDRESSING_GATHER_IDX is supported only on MLU500 series.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetGroupGemmTensorGatherIdxDesc(cnnlGroupGemmTensorDescriptor_t desc,
                                    cnnlDataType_t data_dtype,
                                    cnnlDataType_t index_dtype,
                                    void *src_ptr,
                                    void *indices_ptr,
                                    int64_t gather_size,
                                    int64_t src_stride,
                                    int64_t index_num);

// Group:GroupGemm
/*!
 * @brief Retrieves a group gemm tensor descriptor \p desc that was previously created with the
 *   ::cnnlCreateGroupGemmTensorDescriptor function, and sets the information about the mode,
 *   data type, pointer, pointer mode and other information of input group gemm tensor.
 * @param[in] desc
 *   Input. The struct that holds information about the group gemm descriptor.
 * @param[out] mode
 *   Output. A host pointer to mode of group gemm tensor. Currently, it supports only ::CNNL_GROUP_GEMM_ADDRESSING_DOUBLE_POINTER mode.
 *   For detailed information, see ::cnnlGroupGemmTensorMode_t.
 * @param[out] dtype
 *   Output. A host pointer to data type of group gemm tensor. For detailed information, see ::cnnlDataType_t.
 * @param[out] pointer
 *   Output. A host pointer to data pointer of group gemm tensor. For ::CNNL_GROUP_GEMM_ADDRESSING_DOUBLE_POINTER mode, the pointer is a
 *   double pointer.
 * @param[out] pointer_mode
 *   Output. A host pointer of enum value indicating that the data pointer \p pointer is
 *   passed by reference on the host or device. The information is defined in ::cnnlPointerMode_t.
 * @param[out] infos
 *   Output. A host pointer to other information of group gemm tensor. For ::CNNL_GROUP_GEMM_ADDRESSING_DOUBLE_POINTER mode, the
 *   \p infos is nullptr.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetGroupGemmTensorDescriptor(const cnnlGroupGemmTensorDescriptor_t desc,
                                 cnnlGroupGemmTensorMode_t *mode,
                                 cnnlDataType_t *dtype,
                                 void **pointer,
                                 cnnlPointerMode_t *pointer_mode,
                                 int64_t **infos);

// Group:GroupGemm
/*!
 * @brief Destroys the group gemm tensor descriptor that was previously created with
 *        ::cnnlCreateGroupGemmTensorDescriptor.
 *
 * @param[in] desc
 *   Input. The group gemm tensor descriptor to be destroyed. For detailed information,
 *   see ::cnnlGroupGemmTensorDescriptor_t.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlDestroyGroupGemmTensorDescriptor(cnnlGroupGemmTensorDescriptor_t desc);

// Group:GroupGemm
/*!
 * @brief Sets group gemm descriptor \p gmm_desc with column scale, row scale, biases and activation.
 *
 * @param[in,out] gmm_desc
 *   Input/output. A host pointer to the struct of group gemm descriptor.
 * @param[in] column_scale_desc
 *   Input. A column vector of scales for broadcasting multiplication aross the output matrix columns.
 *   Currently only support the ::CNNL_GROUP_GEMM_ADDRESSING_OFFSETS mode. It can be used as the per-token scale in smooth quant.
 *   It should be set to nullptr when \p row_scale_desc is nullptr. For detailed information, see ::cnnlGroupGemmTensorDescriptor_t.
 *   The data of \p column_scale_desc cannot contain zero.
 * @param[in] row_scale_desc
 *   Input. A row vector of scales for broadcasting multiplication aross the output matrix rows.
 *   Currently only support the ::CNNL_GROUP_GEMM_ADDRESSING_OFFSETS mode. It can be used as the per-channel scale in smooth quant.
 *   For detailed information, see ::cnnlGroupGemmTensorDescriptor_t.
 *   The data of \p row_scale_desc cannot contain zero.
 * @param[in] bias_desc
 *   Input. The descriptor of the bias to be fused. For detailed information, see ::cnnlGroupGemmTensorDescriptor_t.
 * @param[in] active_desc
 *   Input. The descriptor of the activation operation.  For detailed information, see ::cnnlActivationDescriptor_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - Scales \p column_scale_desc and \p row_scale_desc are valid only when data type of \p a_desc and
 *   \p b_desc in ::cnnlGroupGemm is CNNL_DTYPE_INT8. And data type of scales only supports CNNL_DTYPE_FLOAT.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetGroupGemmPerRowColScaleBiasAct(cnnlGroupGemmDescriptor_t gmm_desc,
                                      cnnlGroupGemmTensorDescriptor_t column_scale_desc,
                                      cnnlGroupGemmTensorDescriptor_t row_scale_desc,
                                      cnnlGroupGemmTensorDescriptor_t bias_desc,
                                      cnnlActivationDescriptor_t active_desc);

// Group:Xlogy
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the xlogy operation.
 *
 * The size of the extra workspace is based on the given information of the xlogy operation,
 * including the input tensor descriptors \p input_desc and \p other_desc, and the output
 * tensor descriptor \p output_desc. For more information about the workspace, see "Cambricon
 * CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the xlogy
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] other_desc
 *   Input. The descriptor of the other tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the xlogy
 *   operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetXlogyWorkspaceSize(cnnlHandle_t handle,
                                                    const cnnlTensorDescriptor_t input_desc,
                                                    const cnnlTensorDescriptor_t other_desc,
                                                    const cnnlTensorDescriptor_t output_desc,
                                                    size_t *workspace_size);

// Group:Xlogy
/*!
 * @brief Mutiplies \p input with the natural logarithm of \p other, and returns
 *        the results in the output tensor \p output.
 *
 * This function may need extra MLU memory as the workspace to improve the xlogy performance.
 * You can get the workspace size with the ::cnnlGetXlogyWorkspaceSize
 * function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 * xlogy operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 * ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] other_desc
 *   Input. The descriptor of the other tensor. For detailed information, see
 * ::cnnlTensorDescriptor_t.
 * @param[in] other
 *   Input. Pointer to the MLU memory that stores the other tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 * ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the xlogy operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the xlogy
 * operation. You can get the size of the workspace with the ::cnnlGetXlogyWorkspaceSize
 * function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Xlogy Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensors and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par API Dependency
 * - You need to call the ::cnnlGetXlogyWorkspaceSize function to allocate an extra
 *   \p workspace.
 *
 * @note
 * - The inputs \p input and \p other are multi-dimensional arrays, supporting up to CNNL_DIM_MAX
 * dimensions.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor must meet the following requirements:
 *   - Large tensor is only supported on MLU500 series.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/2.3/special.html#torch.special.xlogy
 */
cnnlStatus_t CNNL_WIN_API cnnlXlogy(cnnlHandle_t handle,
                                    const cnnlTensorDescriptor_t input_desc,
                                    const void *input,
                                    const cnnlTensorDescriptor_t other_desc,
                                    const void *other,
                                    void *workspace,
                                    size_t workspace_size,
                                    const cnnlTensorDescriptor_t output_desc,
                                    void *output);

// Group:RreluWithNoise
/*!
 * @brief Returns in \p workspace_size the size of the MLU memory that is used as an extra workspace for
 * the RReLU with noise operation.
 *
 * The size of the extra workspace is based on the given information of the RReLU with noise operation,
 * including the input tensor descriptor \p input_desc, \p output_desc and \p noise_desc.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   RReLU with noise operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the \p input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of the \p output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] noise_desc
 *   Input. The descriptor of the \p noise tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *   the RReLU with noise operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called before the ::cnnlRreluWithNoise function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetRreluWithNoiseWorkspaceSize(cnnlHandle_t handle,
                                   const cnnlTensorDescriptor_t input_desc,
                                   const cnnlTensorDescriptor_t output_desc,
                                   const cnnlTensorDescriptor_t noise_desc,
                                   size_t *workspace_size);

// Group:RreluWithNoise
/*!
 * @brief Applies the randomized leaky rectified linear unit function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the \p input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to device \p input data.
 * @param[in] captured
 *   Input. Specify whether it is in PyTorch graph scenario.
 *   If \p captured is true, use \p seed_ptr, \p offset_ptr and \p offset_intragraph to specify
 *   seed and offset. If \p captured is false, use \p seed and \p offset.
 * @param[in] seed
 *   Input. Specify the seed of the Philox algorithm when \p captured is false.
 * @param[in] offset
 *   Input. Specify the offset of the Philox algorithm when \p captured is false.
 * @param[in] seed_ptr
 *   Input. Pointer to device \p seed data. Specify the seed of the Philox algorithm when \p captured is true.
 * @param[in] offset_ptr
 *   Input. Pointer to device \p offset data. Specify the offset of the Philox algorithm when \p captured is true.
 * @param[in] offset_intragraph
 *   Input. The offset in a graph. Specify the offset of the Philox algorithm when \p captured is true.
 * @param[in] lower
 *   Input. The lower bound of the uniform distribution.
 * @param[in] upper
 *   Input. The upper bound of the uniform distribution.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the RRELU with
 *   noise operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the RReLU with
 *   noise operation. You can get the size of the workspace with the ::cnnlGetRreluWithNoiseWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the \p output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to device \p output data.
 * @param[in] noise_desc
 *   Input. The descriptor of the \p noise tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] noise
 *   Output. Pointer to device \p noise data.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED.
 *
 * @par Formula
 * - See "RreluWithNoise Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Scale Limitation
 * - \p upper must be greater than or equal to \p lower.
 *
 * @par Data Type
 * - The supported data types of \p input are as follows:
 *   - output: half, bfloat16, float.
 *
 *   The bfloat16 data type is supported only on MLU500 series.
 * - The data type of \p output and \p noise must be same as that of \p input.
 *
 * @par Requirements
 * - None.
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlRreluWithNoise(cnnlHandle_t handle,
                   const cnnlTensorDescriptor_t input_desc,
                   const void *input,
                   const bool captured,
                   const uint64_t seed,
                   const uint64_t offset,
                   const int64_t *seed_ptr,
                   const int64_t *offset_ptr,
                   const uint32_t offset_intragraph,
                   const float lower,
                   const float upper,
                   void *workspace,
                   size_t workspace_size,
                   const cnnlTensorDescriptor_t output_desc,
                   void *output,
                   const cnnlTensorDescriptor_t noise_desc,
                   void *noise);

// Group:GroupGemm
/*!
 * @brief Sets the group-wise scale for group gemm operation.
 *
 * @param[in,out] gmm_desc
 *   Input/output. The descriptor of the group gemm operation. For detailed
 *   information, see ::cnnlGroupGemmDescriptor_t
 * @param[in] scale_attr
 *   Input. The attribute to indicate that which tensor to set group-wise scale. Only supports
 *   ::CNNL_MATMUL_DESC_B_SCALE_POINTER. For detailed information, see
 *   ::cnnlMatMulDescAttribute_t.
 * @param[in] channels
 *   Input. The channels of the original tensor to be quantized. For example, the channels of matrix B: \p K.
 * @param[in] scale_block_size
 *   Input. The size of the quantization block, where each block has its own scale.
 * @param[in] channel_location
 *   Input. The location indicating in which dimension the channel is located. For example,
 *   when \p channel_location is 0, the shape of weight scale is [\p channels / \p scale_block_size, \p groups * \p N],
 *   where \p channels is in the first dimension. Currently only supports 0.
 * @param[in] scale_desc
 *   Input. The descriptor of the \p scale tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] scale
 *   Input. Pointer to device \p scale data.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM.
 *
 * @par Data Type
 * - The supported data types of \p scale are as follows:
 *   - output: float.
 * @note
 *  - The \p scale_block_size only supports 128, 256, 512, and 1024.
 *  - The \p channels must be a multiple of \p scale_block_size.
 *  - Two consecutive 4 bit type elements are stored as 2x4bit in a single byte.
 *    The first element is stored in the 4 LSB and the second element is stored in the 4 MSB.
 *
 * @par Requirements
 * - None.
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetGroupGemmGroupwiseScale(cnnlGroupGemmDescriptor_t gmm_desc,
                               const cnnlMatMulDescAttribute_t scale_attr,
                               const int channels,
                               const int scale_block_size,
                               const int channel_location,
                               const cnnlTensorDescriptor_t scale_desc,
                               void *scale);

// Group:GroupGemm
/*!
 * @brief Sets the group-wise quantization type for group gemm operation.
 *
 * @param[in,out] gmm_desc
 *   Input/output. The descriptor of the group gemm operation. For detailed
 *   information, see ::cnnlGroupGemmDescriptor_t.
 * @param[in] attr
 *   Input. The attribute indicating which tensor to set group-wise quantization type.
 *   It only supports ::CNNL_MATMUL_DESC_B_QUANT_FLAG_POINTER. For detailed information, see
 *   ::cnnlMatMulDescAttribute_t.
 * @param[in] channel
 *   Input. The channel of the matrix to be quantized.
 * @param[in] block_size
 *   Input. The size of the quantization block, where each block has its own quantization type.
 * @param[in] quant_flag_desc
 *   Input. The descriptor of the \p quant_flag tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] quant_flag
 *   Input. Pointer to host \p quant_flag data.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - The supported data types of \p quant_flag are as follows:
 *   - \p quant_flag: int32_t.
 * @note
 *  - The \p block_size only supports 128, 256 and 512.
 *  - Two consecutive 4 bit type elements are stored as 2x4bit in a single byte.
 *    The first element is stored in the 4 LSB and the second element is stored in the 4 MSB.
 *
 * @par Requirements
 * - None.
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetGroupGemmGroupMixedQuantBitFlag(cnnlGroupGemmDescriptor_t gmm_desc,
                                       const cnnlMatMulDescAttribute_t attr,
                                       const int channel,
                                       const int block_size,
                                       const cnnlTensorDescriptor_t quant_flag_desc,
                                       void *quant_flag);

#if defined(__cplusplus)
}
#endif

#endif  // CNNL_H_
